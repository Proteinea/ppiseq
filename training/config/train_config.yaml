defaults:
  - _self_
  - model_configs@esm: esm
  - model_configs@ankh: ankh
  - model_configs@prott5: prott5


backbone: ankh # for later use.
dataset_config:
  repo_id: "ppb_affinity"
  name: "filtered"

pooler: avg

label_transform_config:
  log_base: 10
  eps: 1e-10

lora_config:
  r: 16
  alpha: 32
  bias: none
  use_dora: false
  dropout: 0.0


multichain_config:
  global_pooler: avg
  chains_pooler: attention
  use_ffn: true
  bias: false
  aggregation_method: concat
  shared_global_pooler: false
  shared_chains_pooler: false
  shared_convbert: true


attn_pool_add_config:
  shared_attention: true
  shared_convbert: true
  use_ffn: true
  ffn_multiplier: 1

embed_concat_config:
  concat_first: true

perceiver_config:
  num_latents: 512
  num_heads: 8
  hidden_dim: null
  bias: false
  num_perceiver_layers: 1
  num_self_layers: 1
  gated: false
  activation: "silu"
  shared_perceiver: true

train_config:
  num_train_epochs: 30
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  warmup_steps: 1000
  learning_rate: 5e-4
  weight_decay: 0.0
  logging_steps: 1
  eval_strategy: "epoch"
  gradient_accumulation_steps: 32
  save_total_limit: 1
  metric_for_best_model: "eval_validation_spearmanr"
  greater_is_better: true
  save_strategy: "epoch"
  seed: 7
  remove_unused_columns: false
  save_safetensors: false
  load_best_model_at_end: true
