{
  "best_metric": 0.6586601635373543,
  "best_model_checkpoint": "backbone_ElnaggarLab/ankh-large-setup_convbert_sequence_concat_randomized_weights/checkpoint-1967",
  "epoch": 12.999586947542339,
  "eval_steps": 500,
  "global_step": 1967,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00660883932259397,
      "grad_norm": 2621.8017578125,
      "learning_rate": 1e-06,
      "loss": 152.8596,
      "step": 1
    },
    {
      "epoch": 0.01321767864518794,
      "grad_norm": 2800.776123046875,
      "learning_rate": 2e-06,
      "loss": 167.4473,
      "step": 2
    },
    {
      "epoch": 0.01982651796778191,
      "grad_norm": 2973.69970703125,
      "learning_rate": 3e-06,
      "loss": 186.6323,
      "step": 3
    },
    {
      "epoch": 0.02643535729037588,
      "grad_norm": 2478.34228515625,
      "learning_rate": 4e-06,
      "loss": 127.0957,
      "step": 4
    },
    {
      "epoch": 0.033044196612969846,
      "grad_norm": 2760.014404296875,
      "learning_rate": 5e-06,
      "loss": 159.7657,
      "step": 5
    },
    {
      "epoch": 0.03965303593556382,
      "grad_norm": 2315.873779296875,
      "learning_rate": 6e-06,
      "loss": 119.7924,
      "step": 6
    },
    {
      "epoch": 0.04626187525815779,
      "grad_norm": 2439.25,
      "learning_rate": 7e-06,
      "loss": 129.1405,
      "step": 7
    },
    {
      "epoch": 0.05287071458075176,
      "grad_norm": 2343.318359375,
      "learning_rate": 8e-06,
      "loss": 119.8091,
      "step": 8
    },
    {
      "epoch": 0.05947955390334572,
      "grad_norm": 2136.992919921875,
      "learning_rate": 9e-06,
      "loss": 98.2847,
      "step": 9
    },
    {
      "epoch": 0.06608839322593969,
      "grad_norm": 1806.2950439453125,
      "learning_rate": 1e-05,
      "loss": 76.94,
      "step": 10
    },
    {
      "epoch": 0.07269723254853366,
      "grad_norm": 1715.3199462890625,
      "learning_rate": 1.1e-05,
      "loss": 65.0184,
      "step": 11
    },
    {
      "epoch": 0.07930607187112763,
      "grad_norm": 1410.7950439453125,
      "learning_rate": 1.2e-05,
      "loss": 52.3103,
      "step": 12
    },
    {
      "epoch": 0.0859149111937216,
      "grad_norm": 1582.941162109375,
      "learning_rate": 1.3e-05,
      "loss": 58.7704,
      "step": 13
    },
    {
      "epoch": 0.09252375051631558,
      "grad_norm": 1205.7425537109375,
      "learning_rate": 1.4e-05,
      "loss": 35.0043,
      "step": 14
    },
    {
      "epoch": 0.09913258983890955,
      "grad_norm": 718.5880737304688,
      "learning_rate": 1.5e-05,
      "loss": 21.8779,
      "step": 15
    },
    {
      "epoch": 0.10574142916150352,
      "grad_norm": 752.3369140625,
      "learning_rate": 1.6e-05,
      "loss": 20.6354,
      "step": 16
    },
    {
      "epoch": 0.11235026848409747,
      "grad_norm": 606.3298950195312,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 15.3624,
      "step": 17
    },
    {
      "epoch": 0.11895910780669144,
      "grad_norm": 179.97950744628906,
      "learning_rate": 1.8e-05,
      "loss": 8.4978,
      "step": 18
    },
    {
      "epoch": 0.12556794712928543,
      "grad_norm": 394.4096374511719,
      "learning_rate": 1.9e-05,
      "loss": 9.7742,
      "step": 19
    },
    {
      "epoch": 0.13217678645187939,
      "grad_norm": 468.30206298828125,
      "learning_rate": 2e-05,
      "loss": 10.4842,
      "step": 20
    },
    {
      "epoch": 0.13878562577447337,
      "grad_norm": 585.711181640625,
      "learning_rate": 2.1000000000000002e-05,
      "loss": 23.6205,
      "step": 21
    },
    {
      "epoch": 0.14539446509706733,
      "grad_norm": 431.28759765625,
      "learning_rate": 2.2e-05,
      "loss": 13.471,
      "step": 22
    },
    {
      "epoch": 0.15200330441966128,
      "grad_norm": 509.9788818359375,
      "learning_rate": 2.3e-05,
      "loss": 14.9246,
      "step": 23
    },
    {
      "epoch": 0.15861214374225527,
      "grad_norm": 1027.37841796875,
      "learning_rate": 2.4e-05,
      "loss": 32.3516,
      "step": 24
    },
    {
      "epoch": 0.16522098306484923,
      "grad_norm": 811.6826171875,
      "learning_rate": 2.5e-05,
      "loss": 24.4919,
      "step": 25
    },
    {
      "epoch": 0.1718298223874432,
      "grad_norm": 546.1439819335938,
      "learning_rate": 2.6e-05,
      "loss": 13.7712,
      "step": 26
    },
    {
      "epoch": 0.17843866171003717,
      "grad_norm": 826.693359375,
      "learning_rate": 2.7e-05,
      "loss": 31.6216,
      "step": 27
    },
    {
      "epoch": 0.18504750103263115,
      "grad_norm": 609.0127563476562,
      "learning_rate": 2.8e-05,
      "loss": 11.727,
      "step": 28
    },
    {
      "epoch": 0.1916563403552251,
      "grad_norm": 671.2830810546875,
      "learning_rate": 2.9000000000000004e-05,
      "loss": 14.1633,
      "step": 29
    },
    {
      "epoch": 0.1982651796778191,
      "grad_norm": 147.54013061523438,
      "learning_rate": 3e-05,
      "loss": 9.4892,
      "step": 30
    },
    {
      "epoch": 0.20487401900041305,
      "grad_norm": 138.7666473388672,
      "learning_rate": 3.1e-05,
      "loss": 4.8193,
      "step": 31
    },
    {
      "epoch": 0.21148285832300703,
      "grad_norm": 203.91964721679688,
      "learning_rate": 3.2e-05,
      "loss": 10.2981,
      "step": 32
    },
    {
      "epoch": 0.218091697645601,
      "grad_norm": 285.2981262207031,
      "learning_rate": 3.3e-05,
      "loss": 11.9881,
      "step": 33
    },
    {
      "epoch": 0.22470053696819495,
      "grad_norm": 376.72833251953125,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 18.7265,
      "step": 34
    },
    {
      "epoch": 0.23130937629078893,
      "grad_norm": 489.6687927246094,
      "learning_rate": 3.5000000000000004e-05,
      "loss": 12.479,
      "step": 35
    },
    {
      "epoch": 0.2379182156133829,
      "grad_norm": 406.1006774902344,
      "learning_rate": 3.6e-05,
      "loss": 10.3884,
      "step": 36
    },
    {
      "epoch": 0.24452705493597687,
      "grad_norm": 226.02517700195312,
      "learning_rate": 3.7e-05,
      "loss": 14.9947,
      "step": 37
    },
    {
      "epoch": 0.25113589425857086,
      "grad_norm": 300.77825927734375,
      "learning_rate": 3.8e-05,
      "loss": 12.8052,
      "step": 38
    },
    {
      "epoch": 0.2577447335811648,
      "grad_norm": 93.42426300048828,
      "learning_rate": 3.9e-05,
      "loss": 6.1097,
      "step": 39
    },
    {
      "epoch": 0.26435357290375877,
      "grad_norm": 222.56851196289062,
      "learning_rate": 4e-05,
      "loss": 10.71,
      "step": 40
    },
    {
      "epoch": 0.27096241222635276,
      "grad_norm": 49.766029357910156,
      "learning_rate": 4.1e-05,
      "loss": 4.8948,
      "step": 41
    },
    {
      "epoch": 0.27757125154894674,
      "grad_norm": 133.62965393066406,
      "learning_rate": 4.2000000000000004e-05,
      "loss": 3.2903,
      "step": 42
    },
    {
      "epoch": 0.28418009087154067,
      "grad_norm": 509.9089660644531,
      "learning_rate": 4.2999999999999995e-05,
      "loss": 10.7122,
      "step": 43
    },
    {
      "epoch": 0.29078893019413465,
      "grad_norm": 429.1631164550781,
      "learning_rate": 4.4e-05,
      "loss": 15.4715,
      "step": 44
    },
    {
      "epoch": 0.29739776951672864,
      "grad_norm": 263.92535400390625,
      "learning_rate": 4.4999999999999996e-05,
      "loss": 11.021,
      "step": 45
    },
    {
      "epoch": 0.30400660883932257,
      "grad_norm": 28.35783576965332,
      "learning_rate": 4.6e-05,
      "loss": 13.5442,
      "step": 46
    },
    {
      "epoch": 0.31061544816191655,
      "grad_norm": 11.239806175231934,
      "learning_rate": 4.7000000000000004e-05,
      "loss": 4.1875,
      "step": 47
    },
    {
      "epoch": 0.31722428748451054,
      "grad_norm": 121.1655502319336,
      "learning_rate": 4.8e-05,
      "loss": 11.1833,
      "step": 48
    },
    {
      "epoch": 0.3238331268071045,
      "grad_norm": 154.9054412841797,
      "learning_rate": 4.9000000000000005e-05,
      "loss": 7.1558,
      "step": 49
    },
    {
      "epoch": 0.33044196612969845,
      "grad_norm": 191.84304809570312,
      "learning_rate": 5e-05,
      "loss": 7.8873,
      "step": 50
    },
    {
      "epoch": 0.33705080545229243,
      "grad_norm": 567.0773315429688,
      "learning_rate": 5.1e-05,
      "loss": 15.4352,
      "step": 51
    },
    {
      "epoch": 0.3436596447748864,
      "grad_norm": 208.0484161376953,
      "learning_rate": 5.2e-05,
      "loss": 7.6089,
      "step": 52
    },
    {
      "epoch": 0.3502684840974804,
      "grad_norm": 93.73316192626953,
      "learning_rate": 5.3e-05,
      "loss": 5.6393,
      "step": 53
    },
    {
      "epoch": 0.35687732342007433,
      "grad_norm": 98.72099304199219,
      "learning_rate": 5.4e-05,
      "loss": 11.8685,
      "step": 54
    },
    {
      "epoch": 0.3634861627426683,
      "grad_norm": 77.28652954101562,
      "learning_rate": 5.5e-05,
      "loss": 8.2669,
      "step": 55
    },
    {
      "epoch": 0.3700950020652623,
      "grad_norm": 43.952335357666016,
      "learning_rate": 5.6e-05,
      "loss": 11.8418,
      "step": 56
    },
    {
      "epoch": 0.37670384138785623,
      "grad_norm": 250.16246032714844,
      "learning_rate": 5.7e-05,
      "loss": 8.9812,
      "step": 57
    },
    {
      "epoch": 0.3833126807104502,
      "grad_norm": 627.0062255859375,
      "learning_rate": 5.800000000000001e-05,
      "loss": 13.0012,
      "step": 58
    },
    {
      "epoch": 0.3899215200330442,
      "grad_norm": 526.151123046875,
      "learning_rate": 5.9e-05,
      "loss": 10.8205,
      "step": 59
    },
    {
      "epoch": 0.3965303593556382,
      "grad_norm": 608.7734375,
      "learning_rate": 6e-05,
      "loss": 13.4771,
      "step": 60
    },
    {
      "epoch": 0.4031391986782321,
      "grad_norm": 325.0785217285156,
      "learning_rate": 6.1e-05,
      "loss": 7.6376,
      "step": 61
    },
    {
      "epoch": 0.4097480380008261,
      "grad_norm": 146.01344299316406,
      "learning_rate": 6.2e-05,
      "loss": 9.6968,
      "step": 62
    },
    {
      "epoch": 0.4163568773234201,
      "grad_norm": 445.9652099609375,
      "learning_rate": 6.3e-05,
      "loss": 6.8459,
      "step": 63
    },
    {
      "epoch": 0.42296571664601407,
      "grad_norm": 690.0779418945312,
      "learning_rate": 6.4e-05,
      "loss": 11.6831,
      "step": 64
    },
    {
      "epoch": 0.429574555968608,
      "grad_norm": 1167.9449462890625,
      "learning_rate": 6.500000000000001e-05,
      "loss": 26.9423,
      "step": 65
    },
    {
      "epoch": 0.436183395291202,
      "grad_norm": 768.024169921875,
      "learning_rate": 6.6e-05,
      "loss": 13.5421,
      "step": 66
    },
    {
      "epoch": 0.44279223461379597,
      "grad_norm": 860.5469360351562,
      "learning_rate": 6.7e-05,
      "loss": 17.1481,
      "step": 67
    },
    {
      "epoch": 0.4494010739363899,
      "grad_norm": 322.39910888671875,
      "learning_rate": 6.800000000000001e-05,
      "loss": 3.579,
      "step": 68
    },
    {
      "epoch": 0.4560099132589839,
      "grad_norm": 156.4144287109375,
      "learning_rate": 6.900000000000001e-05,
      "loss": 6.8139,
      "step": 69
    },
    {
      "epoch": 0.46261875258157786,
      "grad_norm": 112.17552947998047,
      "learning_rate": 7.000000000000001e-05,
      "loss": 6.9505,
      "step": 70
    },
    {
      "epoch": 0.46922759190417185,
      "grad_norm": 206.0760955810547,
      "learning_rate": 7.099999999999999e-05,
      "loss": 6.8169,
      "step": 71
    },
    {
      "epoch": 0.4758364312267658,
      "grad_norm": 170.7659454345703,
      "learning_rate": 7.2e-05,
      "loss": 5.1314,
      "step": 72
    },
    {
      "epoch": 0.48244527054935976,
      "grad_norm": 17.15047264099121,
      "learning_rate": 7.3e-05,
      "loss": 6.3438,
      "step": 73
    },
    {
      "epoch": 0.48905410987195375,
      "grad_norm": 414.3172912597656,
      "learning_rate": 7.4e-05,
      "loss": 6.1136,
      "step": 74
    },
    {
      "epoch": 0.49566294919454773,
      "grad_norm": 183.98068237304688,
      "learning_rate": 7.5e-05,
      "loss": 5.2282,
      "step": 75
    },
    {
      "epoch": 0.5022717885171417,
      "grad_norm": 456.2458801269531,
      "learning_rate": 7.6e-05,
      "loss": 11.4417,
      "step": 76
    },
    {
      "epoch": 0.5088806278397356,
      "grad_norm": 215.60430908203125,
      "learning_rate": 7.7e-05,
      "loss": 4.5549,
      "step": 77
    },
    {
      "epoch": 0.5154894671623296,
      "grad_norm": 245.22427368164062,
      "learning_rate": 7.8e-05,
      "loss": 5.6651,
      "step": 78
    },
    {
      "epoch": 0.5220983064849236,
      "grad_norm": 584.2869262695312,
      "learning_rate": 7.9e-05,
      "loss": 12.7016,
      "step": 79
    },
    {
      "epoch": 0.5287071458075175,
      "grad_norm": 412.83514404296875,
      "learning_rate": 8e-05,
      "loss": 13.3543,
      "step": 80
    },
    {
      "epoch": 0.5353159851301115,
      "grad_norm": 93.00777435302734,
      "learning_rate": 8.1e-05,
      "loss": 4.9027,
      "step": 81
    },
    {
      "epoch": 0.5419248244527055,
      "grad_norm": 237.99107360839844,
      "learning_rate": 8.2e-05,
      "loss": 6.1554,
      "step": 82
    },
    {
      "epoch": 0.5485336637752994,
      "grad_norm": 302.04571533203125,
      "learning_rate": 8.300000000000001e-05,
      "loss": 9.5571,
      "step": 83
    },
    {
      "epoch": 0.5551425030978935,
      "grad_norm": 186.71925354003906,
      "learning_rate": 8.400000000000001e-05,
      "loss": 6.709,
      "step": 84
    },
    {
      "epoch": 0.5617513424204874,
      "grad_norm": 180.43307495117188,
      "learning_rate": 8.5e-05,
      "loss": 5.3741,
      "step": 85
    },
    {
      "epoch": 0.5683601817430813,
      "grad_norm": 312.7569580078125,
      "learning_rate": 8.599999999999999e-05,
      "loss": 4.0988,
      "step": 86
    },
    {
      "epoch": 0.5749690210656754,
      "grad_norm": 117.44466400146484,
      "learning_rate": 8.7e-05,
      "loss": 5.8997,
      "step": 87
    },
    {
      "epoch": 0.5815778603882693,
      "grad_norm": 79.43804168701172,
      "learning_rate": 8.8e-05,
      "loss": 5.8996,
      "step": 88
    },
    {
      "epoch": 0.5881866997108632,
      "grad_norm": 213.05682373046875,
      "learning_rate": 8.9e-05,
      "loss": 6.007,
      "step": 89
    },
    {
      "epoch": 0.5947955390334573,
      "grad_norm": 174.1445770263672,
      "learning_rate": 8.999999999999999e-05,
      "loss": 3.9155,
      "step": 90
    },
    {
      "epoch": 0.6014043783560512,
      "grad_norm": 208.77780151367188,
      "learning_rate": 9.1e-05,
      "loss": 3.1868,
      "step": 91
    },
    {
      "epoch": 0.6080132176786451,
      "grad_norm": 139.77183532714844,
      "learning_rate": 9.2e-05,
      "loss": 4.6581,
      "step": 92
    },
    {
      "epoch": 0.6146220570012392,
      "grad_norm": 125.35747528076172,
      "learning_rate": 9.3e-05,
      "loss": 6.0304,
      "step": 93
    },
    {
      "epoch": 0.6212308963238331,
      "grad_norm": 326.17437744140625,
      "learning_rate": 9.400000000000001e-05,
      "loss": 10.4774,
      "step": 94
    },
    {
      "epoch": 0.6278397356464271,
      "grad_norm": 343.7069396972656,
      "learning_rate": 9.5e-05,
      "loss": 5.7148,
      "step": 95
    },
    {
      "epoch": 0.6344485749690211,
      "grad_norm": 521.7389526367188,
      "learning_rate": 9.6e-05,
      "loss": 7.9827,
      "step": 96
    },
    {
      "epoch": 0.641057414291615,
      "grad_norm": 800.0592041015625,
      "learning_rate": 9.7e-05,
      "loss": 18.2586,
      "step": 97
    },
    {
      "epoch": 0.647666253614209,
      "grad_norm": 491.7573547363281,
      "learning_rate": 9.800000000000001e-05,
      "loss": 10.3084,
      "step": 98
    },
    {
      "epoch": 0.654275092936803,
      "grad_norm": 407.54620361328125,
      "learning_rate": 9.900000000000001e-05,
      "loss": 8.1539,
      "step": 99
    },
    {
      "epoch": 0.6608839322593969,
      "grad_norm": 198.75750732421875,
      "learning_rate": 0.0001,
      "loss": 7.4141,
      "step": 100
    },
    {
      "epoch": 0.6674927715819909,
      "grad_norm": 346.62994384765625,
      "learning_rate": 0.000101,
      "loss": 4.5483,
      "step": 101
    },
    {
      "epoch": 0.6741016109045849,
      "grad_norm": 604.565673828125,
      "learning_rate": 0.000102,
      "loss": 12.6699,
      "step": 102
    },
    {
      "epoch": 0.6807104502271788,
      "grad_norm": 790.5040283203125,
      "learning_rate": 0.000103,
      "loss": 13.8127,
      "step": 103
    },
    {
      "epoch": 0.6873192895497728,
      "grad_norm": 652.7489013671875,
      "learning_rate": 0.000104,
      "loss": 9.8433,
      "step": 104
    },
    {
      "epoch": 0.6939281288723668,
      "grad_norm": 251.61595153808594,
      "learning_rate": 0.000105,
      "loss": 6.2941,
      "step": 105
    },
    {
      "epoch": 0.7005369681949608,
      "grad_norm": 450.0060729980469,
      "learning_rate": 0.000106,
      "loss": 9.4661,
      "step": 106
    },
    {
      "epoch": 0.7071458075175547,
      "grad_norm": 496.4705505371094,
      "learning_rate": 0.000107,
      "loss": 5.9244,
      "step": 107
    },
    {
      "epoch": 0.7137546468401487,
      "grad_norm": 333.432373046875,
      "learning_rate": 0.000108,
      "loss": 3.7055,
      "step": 108
    },
    {
      "epoch": 0.7203634861627427,
      "grad_norm": 265.0052795410156,
      "learning_rate": 0.000109,
      "loss": 4.7008,
      "step": 109
    },
    {
      "epoch": 0.7269723254853366,
      "grad_norm": 105.43365478515625,
      "learning_rate": 0.00011,
      "loss": 3.3663,
      "step": 110
    },
    {
      "epoch": 0.7335811648079306,
      "grad_norm": 150.91290283203125,
      "learning_rate": 0.000111,
      "loss": 3.2374,
      "step": 111
    },
    {
      "epoch": 0.7401900041305246,
      "grad_norm": 20.665319442749023,
      "learning_rate": 0.000112,
      "loss": 3.3761,
      "step": 112
    },
    {
      "epoch": 0.7467988434531185,
      "grad_norm": 137.27993774414062,
      "learning_rate": 0.00011300000000000001,
      "loss": 4.8716,
      "step": 113
    },
    {
      "epoch": 0.7534076827757125,
      "grad_norm": 149.79660034179688,
      "learning_rate": 0.000114,
      "loss": 5.3587,
      "step": 114
    },
    {
      "epoch": 0.7600165220983065,
      "grad_norm": 98.01188659667969,
      "learning_rate": 0.000115,
      "loss": 3.1913,
      "step": 115
    },
    {
      "epoch": 0.7666253614209004,
      "grad_norm": 63.813636779785156,
      "learning_rate": 0.00011600000000000001,
      "loss": 3.9536,
      "step": 116
    },
    {
      "epoch": 0.7732342007434945,
      "grad_norm": 326.05596923828125,
      "learning_rate": 0.00011700000000000001,
      "loss": 10.6336,
      "step": 117
    },
    {
      "epoch": 0.7798430400660884,
      "grad_norm": 198.59347534179688,
      "learning_rate": 0.000118,
      "loss": 5.0906,
      "step": 118
    },
    {
      "epoch": 0.7864518793886823,
      "grad_norm": 78.95744323730469,
      "learning_rate": 0.00011899999999999999,
      "loss": 3.3633,
      "step": 119
    },
    {
      "epoch": 0.7930607187112764,
      "grad_norm": 259.57281494140625,
      "learning_rate": 0.00012,
      "loss": 4.8298,
      "step": 120
    },
    {
      "epoch": 0.7996695580338703,
      "grad_norm": 252.213623046875,
      "learning_rate": 0.000121,
      "loss": 3.0216,
      "step": 121
    },
    {
      "epoch": 0.8062783973564642,
      "grad_norm": 425.8802490234375,
      "learning_rate": 0.000122,
      "loss": 5.5111,
      "step": 122
    },
    {
      "epoch": 0.8128872366790583,
      "grad_norm": 84.16822814941406,
      "learning_rate": 0.000123,
      "loss": 5.4819,
      "step": 123
    },
    {
      "epoch": 0.8194960760016522,
      "grad_norm": 396.7996826171875,
      "learning_rate": 0.000124,
      "loss": 4.2788,
      "step": 124
    },
    {
      "epoch": 0.8261049153242461,
      "grad_norm": 417.3693542480469,
      "learning_rate": 0.000125,
      "loss": 4.8956,
      "step": 125
    },
    {
      "epoch": 0.8327137546468402,
      "grad_norm": 308.2403564453125,
      "learning_rate": 0.000126,
      "loss": 4.116,
      "step": 126
    },
    {
      "epoch": 0.8393225939694341,
      "grad_norm": 140.77125549316406,
      "learning_rate": 0.000127,
      "loss": 3.6688,
      "step": 127
    },
    {
      "epoch": 0.8459314332920281,
      "grad_norm": 56.95611572265625,
      "learning_rate": 0.000128,
      "loss": 5.2375,
      "step": 128
    },
    {
      "epoch": 0.8525402726146221,
      "grad_norm": 314.0433654785156,
      "learning_rate": 0.00012900000000000002,
      "loss": 9.8866,
      "step": 129
    },
    {
      "epoch": 0.859149111937216,
      "grad_norm": 287.06640625,
      "learning_rate": 0.00013000000000000002,
      "loss": 4.9756,
      "step": 130
    },
    {
      "epoch": 0.86575795125981,
      "grad_norm": 199.99765014648438,
      "learning_rate": 0.000131,
      "loss": 5.1636,
      "step": 131
    },
    {
      "epoch": 0.872366790582404,
      "grad_norm": 352.05535888671875,
      "learning_rate": 0.000132,
      "loss": 5.1854,
      "step": 132
    },
    {
      "epoch": 0.8789756299049979,
      "grad_norm": 570.1813354492188,
      "learning_rate": 0.000133,
      "loss": 10.3986,
      "step": 133
    },
    {
      "epoch": 0.8855844692275919,
      "grad_norm": 192.8328094482422,
      "learning_rate": 0.000134,
      "loss": 5.4388,
      "step": 134
    },
    {
      "epoch": 0.8921933085501859,
      "grad_norm": 38.29443359375,
      "learning_rate": 0.000135,
      "loss": 1.5279,
      "step": 135
    },
    {
      "epoch": 0.8988021478727798,
      "grad_norm": 407.60919189453125,
      "learning_rate": 0.00013600000000000003,
      "loss": 6.0405,
      "step": 136
    },
    {
      "epoch": 0.9054109871953738,
      "grad_norm": 753.5448608398438,
      "learning_rate": 0.00013700000000000002,
      "loss": 15.0524,
      "step": 137
    },
    {
      "epoch": 0.9120198265179678,
      "grad_norm": 507.1242370605469,
      "learning_rate": 0.00013800000000000002,
      "loss": 10.115,
      "step": 138
    },
    {
      "epoch": 0.9186286658405618,
      "grad_norm": 243.83348083496094,
      "learning_rate": 0.00013900000000000002,
      "loss": 3.4484,
      "step": 139
    },
    {
      "epoch": 0.9252375051631557,
      "grad_norm": 138.37603759765625,
      "learning_rate": 0.00014000000000000001,
      "loss": 1.6109,
      "step": 140
    },
    {
      "epoch": 0.9318463444857497,
      "grad_norm": 325.8599548339844,
      "learning_rate": 0.00014099999999999998,
      "loss": 4.4959,
      "step": 141
    },
    {
      "epoch": 0.9384551838083437,
      "grad_norm": 232.25814819335938,
      "learning_rate": 0.00014199999999999998,
      "loss": 3.4659,
      "step": 142
    },
    {
      "epoch": 0.9450640231309376,
      "grad_norm": 120.64289855957031,
      "learning_rate": 0.00014299999999999998,
      "loss": 4.9864,
      "step": 143
    },
    {
      "epoch": 0.9516728624535316,
      "grad_norm": 84.27030181884766,
      "learning_rate": 0.000144,
      "loss": 5.0024,
      "step": 144
    },
    {
      "epoch": 0.9582817017761256,
      "grad_norm": 236.48782348632812,
      "learning_rate": 0.000145,
      "loss": 6.8444,
      "step": 145
    },
    {
      "epoch": 0.9648905410987195,
      "grad_norm": 98.47806549072266,
      "learning_rate": 0.000146,
      "loss": 4.3378,
      "step": 146
    },
    {
      "epoch": 0.9714993804213135,
      "grad_norm": 65.23780822753906,
      "learning_rate": 0.000147,
      "loss": 2.9209,
      "step": 147
    },
    {
      "epoch": 0.9781082197439075,
      "grad_norm": 163.4247589111328,
      "learning_rate": 0.000148,
      "loss": 2.187,
      "step": 148
    },
    {
      "epoch": 0.9847170590665014,
      "grad_norm": 34.39053726196289,
      "learning_rate": 0.000149,
      "loss": 1.1805,
      "step": 149
    },
    {
      "epoch": 0.9913258983890955,
      "grad_norm": 160.176025390625,
      "learning_rate": 0.00015,
      "loss": 4.6283,
      "step": 150
    },
    {
      "epoch": 0.9979347377116894,
      "grad_norm": 456.4479675292969,
      "learning_rate": 0.000151,
      "loss": 7.381,
      "step": 151
    },
    {
      "epoch": 0.9979347377116894,
      "eval_validation_error_bar": 0.06296433049562061,
      "eval_validation_loss": 8.103615760803223,
      "eval_validation_pearsonr": 0.4472821414009023,
      "eval_validation_rmse": 2.8466849327087402,
      "eval_validation_runtime": 33.3414,
      "eval_validation_samples_per_second": 6.089,
      "eval_validation_spearman": 0.37277363362655214,
      "eval_validation_steps_per_second": 6.089,
      "step": 151
    },
    {
      "epoch": 0.9979347377116894,
      "eval_test_error_bar": 0.05545897623290265,
      "eval_test_loss": 12.132737159729004,
      "eval_test_pearsonr": 0.10658803004028274,
      "eval_test_rmse": 3.483207941055298,
      "eval_test_runtime": 39.5941,
      "eval_test_samples_per_second": 8.234,
      "eval_test_spearman": 0.06607860547479281,
      "eval_test_steps_per_second": 8.234,
      "step": 151
    },
    {
      "epoch": 1.0045435770342834,
      "grad_norm": 473.707763671875,
      "learning_rate": 0.000152,
      "loss": 9.6993,
      "step": 152
    },
    {
      "epoch": 1.0111524163568772,
      "grad_norm": 115.98535919189453,
      "learning_rate": 0.000153,
      "loss": 3.883,
      "step": 153
    },
    {
      "epoch": 1.0177612556794713,
      "grad_norm": 500.0644226074219,
      "learning_rate": 0.000154,
      "loss": 7.0565,
      "step": 154
    },
    {
      "epoch": 1.0243700950020653,
      "grad_norm": 576.3572998046875,
      "learning_rate": 0.000155,
      "loss": 7.6144,
      "step": 155
    },
    {
      "epoch": 1.0309789343246591,
      "grad_norm": 634.8716430664062,
      "learning_rate": 0.000156,
      "loss": 11.7196,
      "step": 156
    },
    {
      "epoch": 1.0375877736472532,
      "grad_norm": 443.78106689453125,
      "learning_rate": 0.000157,
      "loss": 5.4983,
      "step": 157
    },
    {
      "epoch": 1.0441966129698472,
      "grad_norm": 235.90374755859375,
      "learning_rate": 0.000158,
      "loss": 9.4252,
      "step": 158
    },
    {
      "epoch": 1.050805452292441,
      "grad_norm": 58.2034912109375,
      "learning_rate": 0.00015900000000000002,
      "loss": 7.63,
      "step": 159
    },
    {
      "epoch": 1.057414291615035,
      "grad_norm": 97.83956146240234,
      "learning_rate": 0.00016,
      "loss": 4.5438,
      "step": 160
    },
    {
      "epoch": 1.0640231309376291,
      "grad_norm": 238.70944213867188,
      "learning_rate": 0.000161,
      "loss": 6.7228,
      "step": 161
    },
    {
      "epoch": 1.070631970260223,
      "grad_norm": 146.01397705078125,
      "learning_rate": 0.000162,
      "loss": 8.9295,
      "step": 162
    },
    {
      "epoch": 1.077240809582817,
      "grad_norm": 209.32870483398438,
      "learning_rate": 0.000163,
      "loss": 3.211,
      "step": 163
    },
    {
      "epoch": 1.083849648905411,
      "grad_norm": 87.57904052734375,
      "learning_rate": 0.000164,
      "loss": 7.7288,
      "step": 164
    },
    {
      "epoch": 1.090458488228005,
      "grad_norm": 163.89544677734375,
      "learning_rate": 0.000165,
      "loss": 6.7088,
      "step": 165
    },
    {
      "epoch": 1.0970673275505989,
      "grad_norm": 620.9956665039062,
      "learning_rate": 0.00016600000000000002,
      "loss": 12.1586,
      "step": 166
    },
    {
      "epoch": 1.103676166873193,
      "grad_norm": 308.5408630371094,
      "learning_rate": 0.00016700000000000002,
      "loss": 3.7103,
      "step": 167
    },
    {
      "epoch": 1.110285006195787,
      "grad_norm": 586.2052612304688,
      "learning_rate": 0.00016800000000000002,
      "loss": 9.2759,
      "step": 168
    },
    {
      "epoch": 1.1168938455183808,
      "grad_norm": 138.7910614013672,
      "learning_rate": 0.00016900000000000002,
      "loss": 5.5136,
      "step": 169
    },
    {
      "epoch": 1.1235026848409748,
      "grad_norm": 231.12863159179688,
      "learning_rate": 0.00017,
      "loss": 3.2145,
      "step": 170
    },
    {
      "epoch": 1.1301115241635689,
      "grad_norm": 514.1383056640625,
      "learning_rate": 0.000171,
      "loss": 11.0127,
      "step": 171
    },
    {
      "epoch": 1.1367203634861627,
      "grad_norm": 554.6298828125,
      "learning_rate": 0.00017199999999999998,
      "loss": 12.351,
      "step": 172
    },
    {
      "epoch": 1.1433292028087567,
      "grad_norm": 20.65867805480957,
      "learning_rate": 0.000173,
      "loss": 6.5199,
      "step": 173
    },
    {
      "epoch": 1.1499380421313508,
      "grad_norm": 75.14134979248047,
      "learning_rate": 0.000174,
      "loss": 3.847,
      "step": 174
    },
    {
      "epoch": 1.1565468814539446,
      "grad_norm": 227.83145141601562,
      "learning_rate": 0.000175,
      "loss": 3.5266,
      "step": 175
    },
    {
      "epoch": 1.1631557207765386,
      "grad_norm": 65.55352020263672,
      "learning_rate": 0.000176,
      "loss": 4.2512,
      "step": 176
    },
    {
      "epoch": 1.1697645600991327,
      "grad_norm": 157.25491333007812,
      "learning_rate": 0.000177,
      "loss": 6.3856,
      "step": 177
    },
    {
      "epoch": 1.1763733994217265,
      "grad_norm": 19.867341995239258,
      "learning_rate": 0.000178,
      "loss": 2.4859,
      "step": 178
    },
    {
      "epoch": 1.1829822387443205,
      "grad_norm": 131.81423950195312,
      "learning_rate": 0.000179,
      "loss": 4.2967,
      "step": 179
    },
    {
      "epoch": 1.1895910780669146,
      "grad_norm": 184.5274200439453,
      "learning_rate": 0.00017999999999999998,
      "loss": 4.052,
      "step": 180
    },
    {
      "epoch": 1.1961999173895084,
      "grad_norm": 217.2720184326172,
      "learning_rate": 0.000181,
      "loss": 5.7204,
      "step": 181
    },
    {
      "epoch": 1.2028087567121024,
      "grad_norm": 178.1705780029297,
      "learning_rate": 0.000182,
      "loss": 7.4463,
      "step": 182
    },
    {
      "epoch": 1.2094175960346965,
      "grad_norm": 479.3675537109375,
      "learning_rate": 0.000183,
      "loss": 10.5325,
      "step": 183
    },
    {
      "epoch": 1.2160264353572905,
      "grad_norm": 142.27923583984375,
      "learning_rate": 0.000184,
      "loss": 9.2539,
      "step": 184
    },
    {
      "epoch": 1.2226352746798843,
      "grad_norm": 248.04367065429688,
      "learning_rate": 0.000185,
      "loss": 5.2971,
      "step": 185
    },
    {
      "epoch": 1.2292441140024783,
      "grad_norm": 53.621944427490234,
      "learning_rate": 0.000186,
      "loss": 6.2461,
      "step": 186
    },
    {
      "epoch": 1.2358529533250722,
      "grad_norm": 478.0419921875,
      "learning_rate": 0.000187,
      "loss": 10.7639,
      "step": 187
    },
    {
      "epoch": 1.2424617926476662,
      "grad_norm": 545.1976318359375,
      "learning_rate": 0.00018800000000000002,
      "loss": 9.9324,
      "step": 188
    },
    {
      "epoch": 1.2490706319702602,
      "grad_norm": 247.4147491455078,
      "learning_rate": 0.000189,
      "loss": 8.1143,
      "step": 189
    },
    {
      "epoch": 1.2556794712928543,
      "grad_norm": 68.92863464355469,
      "learning_rate": 0.00019,
      "loss": 5.6584,
      "step": 190
    },
    {
      "epoch": 1.262288310615448,
      "grad_norm": 214.2479248046875,
      "learning_rate": 0.000191,
      "loss": 2.5901,
      "step": 191
    },
    {
      "epoch": 1.2688971499380421,
      "grad_norm": 70.69383239746094,
      "learning_rate": 0.000192,
      "loss": 6.607,
      "step": 192
    },
    {
      "epoch": 1.275505989260636,
      "grad_norm": 67.9599838256836,
      "learning_rate": 0.000193,
      "loss": 4.5654,
      "step": 193
    },
    {
      "epoch": 1.28211482858323,
      "grad_norm": 146.4773406982422,
      "learning_rate": 0.000194,
      "loss": 6.3131,
      "step": 194
    },
    {
      "epoch": 1.288723667905824,
      "grad_norm": 143.38702392578125,
      "learning_rate": 0.00019500000000000002,
      "loss": 1.6905,
      "step": 195
    },
    {
      "epoch": 1.295332507228418,
      "grad_norm": 393.8292236328125,
      "learning_rate": 0.00019600000000000002,
      "loss": 6.6385,
      "step": 196
    },
    {
      "epoch": 1.301941346551012,
      "grad_norm": 359.55145263671875,
      "learning_rate": 0.00019700000000000002,
      "loss": 5.6543,
      "step": 197
    },
    {
      "epoch": 1.308550185873606,
      "grad_norm": 538.0074462890625,
      "learning_rate": 0.00019800000000000002,
      "loss": 11.0722,
      "step": 198
    },
    {
      "epoch": 1.3151590251962,
      "grad_norm": 11.368178367614746,
      "learning_rate": 0.000199,
      "loss": 3.0778,
      "step": 199
    },
    {
      "epoch": 1.3217678645187938,
      "grad_norm": 466.6302185058594,
      "learning_rate": 0.0002,
      "loss": 8.687,
      "step": 200
    },
    {
      "epoch": 1.3283767038413878,
      "grad_norm": 604.1619873046875,
      "learning_rate": 0.000201,
      "loss": 13.0364,
      "step": 201
    },
    {
      "epoch": 1.3349855431639819,
      "grad_norm": 591.0838012695312,
      "learning_rate": 0.000202,
      "loss": 10.4694,
      "step": 202
    },
    {
      "epoch": 1.341594382486576,
      "grad_norm": 206.98223876953125,
      "learning_rate": 0.00020300000000000003,
      "loss": 4.1898,
      "step": 203
    },
    {
      "epoch": 1.3482032218091697,
      "grad_norm": 151.76541137695312,
      "learning_rate": 0.000204,
      "loss": 3.0432,
      "step": 204
    },
    {
      "epoch": 1.3548120611317638,
      "grad_norm": 174.12840270996094,
      "learning_rate": 0.000205,
      "loss": 4.8833,
      "step": 205
    },
    {
      "epoch": 1.3614209004543576,
      "grad_norm": 281.4229736328125,
      "learning_rate": 0.000206,
      "loss": 5.7307,
      "step": 206
    },
    {
      "epoch": 1.3680297397769516,
      "grad_norm": 42.37483596801758,
      "learning_rate": 0.000207,
      "loss": 9.2037,
      "step": 207
    },
    {
      "epoch": 1.3746385790995457,
      "grad_norm": 55.96997833251953,
      "learning_rate": 0.000208,
      "loss": 0.6676,
      "step": 208
    },
    {
      "epoch": 1.3812474184221397,
      "grad_norm": 135.89779663085938,
      "learning_rate": 0.00020899999999999998,
      "loss": 2.1982,
      "step": 209
    },
    {
      "epoch": 1.3878562577447335,
      "grad_norm": 121.91911315917969,
      "learning_rate": 0.00021,
      "loss": 5.7502,
      "step": 210
    },
    {
      "epoch": 1.3944650970673276,
      "grad_norm": 325.1066589355469,
      "learning_rate": 0.000211,
      "loss": 13.1556,
      "step": 211
    },
    {
      "epoch": 1.4010739363899214,
      "grad_norm": 19.77605628967285,
      "learning_rate": 0.000212,
      "loss": 4.6013,
      "step": 212
    },
    {
      "epoch": 1.4076827757125154,
      "grad_norm": 17.542940139770508,
      "learning_rate": 0.000213,
      "loss": 3.2406,
      "step": 213
    },
    {
      "epoch": 1.4142916150351095,
      "grad_norm": 129.16184997558594,
      "learning_rate": 0.000214,
      "loss": 3.1315,
      "step": 214
    },
    {
      "epoch": 1.4209004543577035,
      "grad_norm": 75.743896484375,
      "learning_rate": 0.000215,
      "loss": 7.478,
      "step": 215
    },
    {
      "epoch": 1.4275092936802973,
      "grad_norm": 57.2984504699707,
      "learning_rate": 0.000216,
      "loss": 5.6825,
      "step": 216
    },
    {
      "epoch": 1.4341181330028914,
      "grad_norm": 6.516061782836914,
      "learning_rate": 0.00021700000000000002,
      "loss": 1.6803,
      "step": 217
    },
    {
      "epoch": 1.4407269723254854,
      "grad_norm": 87.82440185546875,
      "learning_rate": 0.000218,
      "loss": 5.6664,
      "step": 218
    },
    {
      "epoch": 1.4473358116480792,
      "grad_norm": 250.23936462402344,
      "learning_rate": 0.000219,
      "loss": 5.4653,
      "step": 219
    },
    {
      "epoch": 1.4539446509706733,
      "grad_norm": 330.3681640625,
      "learning_rate": 0.00022,
      "loss": 7.1136,
      "step": 220
    },
    {
      "epoch": 1.4605534902932673,
      "grad_norm": 16.50836753845215,
      "learning_rate": 0.000221,
      "loss": 2.8701,
      "step": 221
    },
    {
      "epoch": 1.4671623296158613,
      "grad_norm": 278.44903564453125,
      "learning_rate": 0.000222,
      "loss": 5.91,
      "step": 222
    },
    {
      "epoch": 1.4737711689384552,
      "grad_norm": 444.6795959472656,
      "learning_rate": 0.000223,
      "loss": 12.8351,
      "step": 223
    },
    {
      "epoch": 1.4803800082610492,
      "grad_norm": 378.316162109375,
      "learning_rate": 0.000224,
      "loss": 10.8499,
      "step": 224
    },
    {
      "epoch": 1.486988847583643,
      "grad_norm": 261.47735595703125,
      "learning_rate": 0.00022500000000000002,
      "loss": 4.7851,
      "step": 225
    },
    {
      "epoch": 1.493597686906237,
      "grad_norm": 139.94093322753906,
      "learning_rate": 0.00022600000000000002,
      "loss": 5.4698,
      "step": 226
    },
    {
      "epoch": 1.500206526228831,
      "grad_norm": 311.738525390625,
      "learning_rate": 0.00022700000000000002,
      "loss": 5.4855,
      "step": 227
    },
    {
      "epoch": 1.5068153655514251,
      "grad_norm": 375.62432861328125,
      "learning_rate": 0.000228,
      "loss": 7.0966,
      "step": 228
    },
    {
      "epoch": 1.513424204874019,
      "grad_norm": 34.60908508300781,
      "learning_rate": 0.000229,
      "loss": 5.9846,
      "step": 229
    },
    {
      "epoch": 1.520033044196613,
      "grad_norm": 53.838069915771484,
      "learning_rate": 0.00023,
      "loss": 4.9368,
      "step": 230
    },
    {
      "epoch": 1.5266418835192068,
      "grad_norm": 699.8902587890625,
      "learning_rate": 0.000231,
      "loss": 16.978,
      "step": 231
    },
    {
      "epoch": 1.5332507228418009,
      "grad_norm": 1025.265380859375,
      "learning_rate": 0.00023200000000000003,
      "loss": 38.2252,
      "step": 232
    },
    {
      "epoch": 1.539859562164395,
      "grad_norm": 1083.5186767578125,
      "learning_rate": 0.00023300000000000003,
      "loss": 38.7326,
      "step": 233
    },
    {
      "epoch": 1.546468401486989,
      "grad_norm": 847.4111938476562,
      "learning_rate": 0.00023400000000000002,
      "loss": 24.8465,
      "step": 234
    },
    {
      "epoch": 1.553077240809583,
      "grad_norm": 315.60479736328125,
      "learning_rate": 0.000235,
      "loss": 5.2829,
      "step": 235
    },
    {
      "epoch": 1.5596860801321768,
      "grad_norm": 93.08726501464844,
      "learning_rate": 0.000236,
      "loss": 1.8445,
      "step": 236
    },
    {
      "epoch": 1.5662949194547706,
      "grad_norm": 687.547119140625,
      "learning_rate": 0.000237,
      "loss": 16.1977,
      "step": 237
    },
    {
      "epoch": 1.5729037587773647,
      "grad_norm": 906.5325317382812,
      "learning_rate": 0.00023799999999999998,
      "loss": 26.2272,
      "step": 238
    },
    {
      "epoch": 1.5795125980999587,
      "grad_norm": 852.3096923828125,
      "learning_rate": 0.00023899999999999998,
      "loss": 26.7318,
      "step": 239
    },
    {
      "epoch": 1.5861214374225527,
      "grad_norm": 963.4967041015625,
      "learning_rate": 0.00024,
      "loss": 33.2433,
      "step": 240
    },
    {
      "epoch": 1.5927302767451468,
      "grad_norm": 627.5100708007812,
      "learning_rate": 0.000241,
      "loss": 15.3338,
      "step": 241
    },
    {
      "epoch": 1.5993391160677406,
      "grad_norm": 164.46498107910156,
      "learning_rate": 0.000242,
      "loss": 5.4887,
      "step": 242
    },
    {
      "epoch": 1.6059479553903344,
      "grad_norm": 282.9655456542969,
      "learning_rate": 0.000243,
      "loss": 9.1569,
      "step": 243
    },
    {
      "epoch": 1.6125567947129285,
      "grad_norm": 724.7139892578125,
      "learning_rate": 0.000244,
      "loss": 20.3656,
      "step": 244
    },
    {
      "epoch": 1.6191656340355225,
      "grad_norm": 611.6530151367188,
      "learning_rate": 0.000245,
      "loss": 15.7961,
      "step": 245
    },
    {
      "epoch": 1.6257744733581165,
      "grad_norm": 584.7357177734375,
      "learning_rate": 0.000246,
      "loss": 14.3766,
      "step": 246
    },
    {
      "epoch": 1.6323833126807106,
      "grad_norm": 130.67835998535156,
      "learning_rate": 0.000247,
      "loss": 5.9849,
      "step": 247
    },
    {
      "epoch": 1.6389921520033044,
      "grad_norm": 81.18351745605469,
      "learning_rate": 0.000248,
      "loss": 4.9503,
      "step": 248
    },
    {
      "epoch": 1.6456009913258984,
      "grad_norm": 224.75184631347656,
      "learning_rate": 0.000249,
      "loss": 7.5837,
      "step": 249
    },
    {
      "epoch": 1.6522098306484923,
      "grad_norm": 326.97998046875,
      "learning_rate": 0.00025,
      "loss": 17.2334,
      "step": 250
    },
    {
      "epoch": 1.6588186699710863,
      "grad_norm": 335.0459899902344,
      "learning_rate": 0.00025100000000000003,
      "loss": 7.7177,
      "step": 251
    },
    {
      "epoch": 1.6654275092936803,
      "grad_norm": 263.8901062011719,
      "learning_rate": 0.000252,
      "loss": 7.1628,
      "step": 252
    },
    {
      "epoch": 1.6720363486162744,
      "grad_norm": 487.6319885253906,
      "learning_rate": 0.000253,
      "loss": 10.5088,
      "step": 253
    },
    {
      "epoch": 1.6786451879388682,
      "grad_norm": 335.79888916015625,
      "learning_rate": 0.000254,
      "loss": 6.6449,
      "step": 254
    },
    {
      "epoch": 1.6852540272614622,
      "grad_norm": 75.32984161376953,
      "learning_rate": 0.000255,
      "loss": 3.0251,
      "step": 255
    },
    {
      "epoch": 1.691862866584056,
      "grad_norm": 115.9355697631836,
      "learning_rate": 0.000256,
      "loss": 5.7868,
      "step": 256
    },
    {
      "epoch": 1.69847170590665,
      "grad_norm": 266.59771728515625,
      "learning_rate": 0.000257,
      "loss": 6.599,
      "step": 257
    },
    {
      "epoch": 1.7050805452292441,
      "grad_norm": 414.4598388671875,
      "learning_rate": 0.00025800000000000004,
      "loss": 7.7694,
      "step": 258
    },
    {
      "epoch": 1.7116893845518382,
      "grad_norm": 131.44252014160156,
      "learning_rate": 0.000259,
      "loss": 4.9441,
      "step": 259
    },
    {
      "epoch": 1.7182982238744322,
      "grad_norm": 88.37635040283203,
      "learning_rate": 0.00026000000000000003,
      "loss": 6.3695,
      "step": 260
    },
    {
      "epoch": 1.724907063197026,
      "grad_norm": 523.7933349609375,
      "learning_rate": 0.000261,
      "loss": 12.0773,
      "step": 261
    },
    {
      "epoch": 1.7315159025196198,
      "grad_norm": 886.4895629882812,
      "learning_rate": 0.000262,
      "loss": 32.076,
      "step": 262
    },
    {
      "epoch": 1.7381247418422139,
      "grad_norm": 752.2332763671875,
      "learning_rate": 0.000263,
      "loss": 20.3543,
      "step": 263
    },
    {
      "epoch": 1.744733581164808,
      "grad_norm": 487.80517578125,
      "learning_rate": 0.000264,
      "loss": 11.0417,
      "step": 264
    },
    {
      "epoch": 1.751342420487402,
      "grad_norm": 393.7746887207031,
      "learning_rate": 0.00026500000000000004,
      "loss": 8.9402,
      "step": 265
    },
    {
      "epoch": 1.757951259809996,
      "grad_norm": 86.07625579833984,
      "learning_rate": 0.000266,
      "loss": 5.042,
      "step": 266
    },
    {
      "epoch": 1.7645600991325898,
      "grad_norm": 243.95912170410156,
      "learning_rate": 0.00026700000000000004,
      "loss": 4.2045,
      "step": 267
    },
    {
      "epoch": 1.7711689384551839,
      "grad_norm": 98.5042724609375,
      "learning_rate": 0.000268,
      "loss": 5.978,
      "step": 268
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 77.02708435058594,
      "learning_rate": 0.00026900000000000003,
      "loss": 3.9005,
      "step": 269
    },
    {
      "epoch": 1.7843866171003717,
      "grad_norm": 82.20419311523438,
      "learning_rate": 0.00027,
      "loss": 4.6361,
      "step": 270
    },
    {
      "epoch": 1.7909954564229658,
      "grad_norm": 133.6117706298828,
      "learning_rate": 0.00027100000000000003,
      "loss": 3.8884,
      "step": 271
    },
    {
      "epoch": 1.7976042957455598,
      "grad_norm": 64.35774230957031,
      "learning_rate": 0.00027200000000000005,
      "loss": 6.4267,
      "step": 272
    },
    {
      "epoch": 1.8042131350681536,
      "grad_norm": 24.82843780517578,
      "learning_rate": 0.000273,
      "loss": 3.4529,
      "step": 273
    },
    {
      "epoch": 1.8108219743907477,
      "grad_norm": 253.91917419433594,
      "learning_rate": 0.00027400000000000005,
      "loss": 5.4442,
      "step": 274
    },
    {
      "epoch": 1.8174308137133415,
      "grad_norm": 576.2398071289062,
      "learning_rate": 0.000275,
      "loss": 11.5137,
      "step": 275
    },
    {
      "epoch": 1.8240396530359355,
      "grad_norm": 435.1265563964844,
      "learning_rate": 0.00027600000000000004,
      "loss": 7.3944,
      "step": 276
    },
    {
      "epoch": 1.8306484923585296,
      "grad_norm": 120.31018829345703,
      "learning_rate": 0.000277,
      "loss": 3.5565,
      "step": 277
    },
    {
      "epoch": 1.8372573316811236,
      "grad_norm": 76.24127197265625,
      "learning_rate": 0.00027800000000000004,
      "loss": 6.6102,
      "step": 278
    },
    {
      "epoch": 1.8438661710037176,
      "grad_norm": 330.53753662109375,
      "learning_rate": 0.000279,
      "loss": 7.5232,
      "step": 279
    },
    {
      "epoch": 1.8504750103263115,
      "grad_norm": 128.8855743408203,
      "learning_rate": 0.00028000000000000003,
      "loss": 2.7551,
      "step": 280
    },
    {
      "epoch": 1.8570838496489053,
      "grad_norm": 42.26445388793945,
      "learning_rate": 0.00028100000000000005,
      "loss": 2.841,
      "step": 281
    },
    {
      "epoch": 1.8636926889714993,
      "grad_norm": 14.56915283203125,
      "learning_rate": 0.00028199999999999997,
      "loss": 5.4865,
      "step": 282
    },
    {
      "epoch": 1.8703015282940934,
      "grad_norm": 124.49256896972656,
      "learning_rate": 0.000283,
      "loss": 7.7401,
      "step": 283
    },
    {
      "epoch": 1.8769103676166874,
      "grad_norm": 24.591550827026367,
      "learning_rate": 0.00028399999999999996,
      "loss": 6.1573,
      "step": 284
    },
    {
      "epoch": 1.8835192069392814,
      "grad_norm": 85.7555923461914,
      "learning_rate": 0.000285,
      "loss": 4.6648,
      "step": 285
    },
    {
      "epoch": 1.8901280462618752,
      "grad_norm": 31.188573837280273,
      "learning_rate": 0.00028599999999999996,
      "loss": 4.3627,
      "step": 286
    },
    {
      "epoch": 1.896736885584469,
      "grad_norm": 366.0574645996094,
      "learning_rate": 0.000287,
      "loss": 11.2241,
      "step": 287
    },
    {
      "epoch": 1.903345724907063,
      "grad_norm": 546.6399536132812,
      "learning_rate": 0.000288,
      "loss": 20.29,
      "step": 288
    },
    {
      "epoch": 1.9099545642296571,
      "grad_norm": 422.43792724609375,
      "learning_rate": 0.000289,
      "loss": 13.0363,
      "step": 289
    },
    {
      "epoch": 1.9165634035522512,
      "grad_norm": 134.35728454589844,
      "learning_rate": 0.00029,
      "loss": 6.7367,
      "step": 290
    },
    {
      "epoch": 1.9231722428748452,
      "grad_norm": 341.85888671875,
      "learning_rate": 0.00029099999999999997,
      "loss": 11.3933,
      "step": 291
    },
    {
      "epoch": 1.929781082197439,
      "grad_norm": 464.85076904296875,
      "learning_rate": 0.000292,
      "loss": 22.372,
      "step": 292
    },
    {
      "epoch": 1.936389921520033,
      "grad_norm": 149.10313415527344,
      "learning_rate": 0.00029299999999999997,
      "loss": 8.4569,
      "step": 293
    },
    {
      "epoch": 1.942998760842627,
      "grad_norm": 288.007080078125,
      "learning_rate": 0.000294,
      "loss": 14.0594,
      "step": 294
    },
    {
      "epoch": 1.949607600165221,
      "grad_norm": 87.5140609741211,
      "learning_rate": 0.000295,
      "loss": 3.4918,
      "step": 295
    },
    {
      "epoch": 1.956216439487815,
      "grad_norm": 306.6309509277344,
      "learning_rate": 0.000296,
      "loss": 8.5204,
      "step": 296
    },
    {
      "epoch": 1.962825278810409,
      "grad_norm": 54.81648635864258,
      "learning_rate": 0.000297,
      "loss": 6.321,
      "step": 297
    },
    {
      "epoch": 1.9694341181330028,
      "grad_norm": 62.91657638549805,
      "learning_rate": 0.000298,
      "loss": 4.5086,
      "step": 298
    },
    {
      "epoch": 1.9760429574555969,
      "grad_norm": 114.96432495117188,
      "learning_rate": 0.000299,
      "loss": 3.4146,
      "step": 299
    },
    {
      "epoch": 1.9826517967781907,
      "grad_norm": 42.265438079833984,
      "learning_rate": 0.0003,
      "loss": 4.9789,
      "step": 300
    },
    {
      "epoch": 1.9892606361007847,
      "grad_norm": 72.93670654296875,
      "learning_rate": 0.000301,
      "loss": 5.1041,
      "step": 301
    },
    {
      "epoch": 1.9958694754233788,
      "grad_norm": 397.7538757324219,
      "learning_rate": 0.000302,
      "loss": 12.0217,
      "step": 302
    },
    {
      "epoch": 1.9958694754233788,
      "eval_validation_error_bar": 0.05802857790288782,
      "eval_validation_loss": 5.770761489868164,
      "eval_validation_pearsonr": 0.49376684963366085,
      "eval_validation_rmse": 2.4022409915924072,
      "eval_validation_runtime": 33.2034,
      "eval_validation_samples_per_second": 6.114,
      "eval_validation_spearman": 0.47042963706978874,
      "eval_validation_steps_per_second": 6.114,
      "step": 302
    },
    {
      "epoch": 1.9958694754233788,
      "eval_test_error_bar": 0.053974159418173975,
      "eval_test_loss": 10.358721733093262,
      "eval_test_pearsonr": 0.09479529494960057,
      "eval_test_rmse": 3.218496799468994,
      "eval_test_runtime": 39.2748,
      "eval_test_samples_per_second": 8.3,
      "eval_test_spearman": 0.1984334185631291,
      "eval_test_steps_per_second": 8.3,
      "step": 302
    },
    {
      "epoch": 2.002478314745973,
      "grad_norm": 211.2170867919922,
      "learning_rate": 0.000303,
      "loss": 4.0009,
      "step": 303
    },
    {
      "epoch": 2.009087154068567,
      "grad_norm": 55.41913604736328,
      "learning_rate": 0.000304,
      "loss": 5.6055,
      "step": 304
    },
    {
      "epoch": 2.0156959933911605,
      "grad_norm": 29.56055450439453,
      "learning_rate": 0.000305,
      "loss": 4.3734,
      "step": 305
    },
    {
      "epoch": 2.0223048327137545,
      "grad_norm": 100.47882080078125,
      "learning_rate": 0.000306,
      "loss": 4.136,
      "step": 306
    },
    {
      "epoch": 2.0289136720363485,
      "grad_norm": 189.02891540527344,
      "learning_rate": 0.000307,
      "loss": 9.6663,
      "step": 307
    },
    {
      "epoch": 2.0355225113589426,
      "grad_norm": 154.35842895507812,
      "learning_rate": 0.000308,
      "loss": 3.6811,
      "step": 308
    },
    {
      "epoch": 2.0421313506815366,
      "grad_norm": 259.6006164550781,
      "learning_rate": 0.00030900000000000003,
      "loss": 6.0717,
      "step": 309
    },
    {
      "epoch": 2.0487401900041307,
      "grad_norm": 362.2579345703125,
      "learning_rate": 0.00031,
      "loss": 9.7234,
      "step": 310
    },
    {
      "epoch": 2.0553490293267247,
      "grad_norm": 298.7022705078125,
      "learning_rate": 0.000311,
      "loss": 6.8042,
      "step": 311
    },
    {
      "epoch": 2.0619578686493183,
      "grad_norm": 9.323670387268066,
      "learning_rate": 0.000312,
      "loss": 3.1366,
      "step": 312
    },
    {
      "epoch": 2.0685667079719123,
      "grad_norm": 473.48876953125,
      "learning_rate": 0.000313,
      "loss": 12.0387,
      "step": 313
    },
    {
      "epoch": 2.0751755472945064,
      "grad_norm": 497.95062255859375,
      "learning_rate": 0.000314,
      "loss": 19.1567,
      "step": 314
    },
    {
      "epoch": 2.0817843866171004,
      "grad_norm": 640.28125,
      "learning_rate": 0.000315,
      "loss": 17.2031,
      "step": 315
    },
    {
      "epoch": 2.0883932259396945,
      "grad_norm": 270.08868408203125,
      "learning_rate": 0.000316,
      "loss": 6.9958,
      "step": 316
    },
    {
      "epoch": 2.0950020652622885,
      "grad_norm": 86.82870483398438,
      "learning_rate": 0.000317,
      "loss": 8.2363,
      "step": 317
    },
    {
      "epoch": 2.101610904584882,
      "grad_norm": 83.60872650146484,
      "learning_rate": 0.00031800000000000003,
      "loss": 3.9335,
      "step": 318
    },
    {
      "epoch": 2.108219743907476,
      "grad_norm": 432.68768310546875,
      "learning_rate": 0.000319,
      "loss": 8.9093,
      "step": 319
    },
    {
      "epoch": 2.11482858323007,
      "grad_norm": 408.0492858886719,
      "learning_rate": 0.00032,
      "loss": 9.1087,
      "step": 320
    },
    {
      "epoch": 2.121437422552664,
      "grad_norm": 239.0144805908203,
      "learning_rate": 0.000321,
      "loss": 7.5942,
      "step": 321
    },
    {
      "epoch": 2.1280462618752582,
      "grad_norm": 195.4450225830078,
      "learning_rate": 0.000322,
      "loss": 9.0825,
      "step": 322
    },
    {
      "epoch": 2.1346551011978523,
      "grad_norm": 46.326690673828125,
      "learning_rate": 0.000323,
      "loss": 4.0232,
      "step": 323
    },
    {
      "epoch": 2.141263940520446,
      "grad_norm": 206.82656860351562,
      "learning_rate": 0.000324,
      "loss": 5.0368,
      "step": 324
    },
    {
      "epoch": 2.14787277984304,
      "grad_norm": 60.439937591552734,
      "learning_rate": 0.00032500000000000004,
      "loss": 3.3566,
      "step": 325
    },
    {
      "epoch": 2.154481619165634,
      "grad_norm": 27.01719093322754,
      "learning_rate": 0.000326,
      "loss": 3.7305,
      "step": 326
    },
    {
      "epoch": 2.161090458488228,
      "grad_norm": 180.2321014404297,
      "learning_rate": 0.00032700000000000003,
      "loss": 13.3388,
      "step": 327
    },
    {
      "epoch": 2.167699297810822,
      "grad_norm": 7.855646133422852,
      "learning_rate": 0.000328,
      "loss": 1.3345,
      "step": 328
    },
    {
      "epoch": 2.174308137133416,
      "grad_norm": 188.67507934570312,
      "learning_rate": 0.00032900000000000003,
      "loss": 6.4664,
      "step": 329
    },
    {
      "epoch": 2.18091697645601,
      "grad_norm": 419.5448913574219,
      "learning_rate": 0.00033,
      "loss": 11.197,
      "step": 330
    },
    {
      "epoch": 2.1875258157786037,
      "grad_norm": 203.66818237304688,
      "learning_rate": 0.000331,
      "loss": 7.0473,
      "step": 331
    },
    {
      "epoch": 2.1941346551011978,
      "grad_norm": 173.3043212890625,
      "learning_rate": 0.00033200000000000005,
      "loss": 10.1774,
      "step": 332
    },
    {
      "epoch": 2.200743494423792,
      "grad_norm": 282.8183898925781,
      "learning_rate": 0.000333,
      "loss": 7.1983,
      "step": 333
    },
    {
      "epoch": 2.207352333746386,
      "grad_norm": 265.5020446777344,
      "learning_rate": 0.00033400000000000004,
      "loss": 6.4708,
      "step": 334
    },
    {
      "epoch": 2.21396117306898,
      "grad_norm": 405.8937072753906,
      "learning_rate": 0.000335,
      "loss": 11.3916,
      "step": 335
    },
    {
      "epoch": 2.220570012391574,
      "grad_norm": 151.43270874023438,
      "learning_rate": 0.00033600000000000004,
      "loss": 5.694,
      "step": 336
    },
    {
      "epoch": 2.2271788517141675,
      "grad_norm": 76.81912231445312,
      "learning_rate": 0.000337,
      "loss": 4.0387,
      "step": 337
    },
    {
      "epoch": 2.2337876910367616,
      "grad_norm": 328.7822570800781,
      "learning_rate": 0.00033800000000000003,
      "loss": 9.165,
      "step": 338
    },
    {
      "epoch": 2.2403965303593556,
      "grad_norm": 492.2032775878906,
      "learning_rate": 0.00033900000000000005,
      "loss": 18.36,
      "step": 339
    },
    {
      "epoch": 2.2470053696819496,
      "grad_norm": 561.2764282226562,
      "learning_rate": 0.00034,
      "loss": 21.3629,
      "step": 340
    },
    {
      "epoch": 2.2536142090045437,
      "grad_norm": 406.8732604980469,
      "learning_rate": 0.00034100000000000005,
      "loss": 15.4223,
      "step": 341
    },
    {
      "epoch": 2.2602230483271377,
      "grad_norm": 160.48434448242188,
      "learning_rate": 0.000342,
      "loss": 7.6056,
      "step": 342
    },
    {
      "epoch": 2.2668318876497313,
      "grad_norm": 181.61802673339844,
      "learning_rate": 0.00034300000000000004,
      "loss": 5.4927,
      "step": 343
    },
    {
      "epoch": 2.2734407269723254,
      "grad_norm": 431.1178283691406,
      "learning_rate": 0.00034399999999999996,
      "loss": 14.8426,
      "step": 344
    },
    {
      "epoch": 2.2800495662949194,
      "grad_norm": 448.0758972167969,
      "learning_rate": 0.000345,
      "loss": 19.3479,
      "step": 345
    },
    {
      "epoch": 2.2866584056175134,
      "grad_norm": 217.47384643554688,
      "learning_rate": 0.000346,
      "loss": 8.5559,
      "step": 346
    },
    {
      "epoch": 2.2932672449401075,
      "grad_norm": 127.2532730102539,
      "learning_rate": 0.000347,
      "loss": 5.3732,
      "step": 347
    },
    {
      "epoch": 2.2998760842627015,
      "grad_norm": 57.68512725830078,
      "learning_rate": 0.000348,
      "loss": 2.6502,
      "step": 348
    },
    {
      "epoch": 2.3064849235852956,
      "grad_norm": 45.21743392944336,
      "learning_rate": 0.00034899999999999997,
      "loss": 2.085,
      "step": 349
    },
    {
      "epoch": 2.313093762907889,
      "grad_norm": 152.6269989013672,
      "learning_rate": 0.00035,
      "loss": 4.6248,
      "step": 350
    },
    {
      "epoch": 2.319702602230483,
      "grad_norm": 20.567684173583984,
      "learning_rate": 0.00035099999999999997,
      "loss": 2.9104,
      "step": 351
    },
    {
      "epoch": 2.3263114415530772,
      "grad_norm": 194.50840759277344,
      "learning_rate": 0.000352,
      "loss": 6.2349,
      "step": 352
    },
    {
      "epoch": 2.3329202808756713,
      "grad_norm": 98.4996566772461,
      "learning_rate": 0.00035299999999999996,
      "loss": 7.362,
      "step": 353
    },
    {
      "epoch": 2.3395291201982653,
      "grad_norm": 65.39766693115234,
      "learning_rate": 0.000354,
      "loss": 5.4661,
      "step": 354
    },
    {
      "epoch": 2.346137959520859,
      "grad_norm": 74.69259643554688,
      "learning_rate": 0.000355,
      "loss": 4.4247,
      "step": 355
    },
    {
      "epoch": 2.352746798843453,
      "grad_norm": 374.80126953125,
      "learning_rate": 0.000356,
      "loss": 13.1789,
      "step": 356
    },
    {
      "epoch": 2.359355638166047,
      "grad_norm": 249.49534606933594,
      "learning_rate": 0.000357,
      "loss": 6.8355,
      "step": 357
    },
    {
      "epoch": 2.365964477488641,
      "grad_norm": 7.1302385330200195,
      "learning_rate": 0.000358,
      "loss": 1.9188,
      "step": 358
    },
    {
      "epoch": 2.372573316811235,
      "grad_norm": 86.32833099365234,
      "learning_rate": 0.000359,
      "loss": 2.5467,
      "step": 359
    },
    {
      "epoch": 2.379182156133829,
      "grad_norm": 49.80738830566406,
      "learning_rate": 0.00035999999999999997,
      "loss": 4.8881,
      "step": 360
    },
    {
      "epoch": 2.385790995456423,
      "grad_norm": 191.02471923828125,
      "learning_rate": 0.000361,
      "loss": 3.9539,
      "step": 361
    },
    {
      "epoch": 2.3923998347790167,
      "grad_norm": 58.481414794921875,
      "learning_rate": 0.000362,
      "loss": 4.0588,
      "step": 362
    },
    {
      "epoch": 2.399008674101611,
      "grad_norm": 158.84300231933594,
      "learning_rate": 0.000363,
      "loss": 4.734,
      "step": 363
    },
    {
      "epoch": 2.405617513424205,
      "grad_norm": 278.2757263183594,
      "learning_rate": 0.000364,
      "loss": 6.9073,
      "step": 364
    },
    {
      "epoch": 2.412226352746799,
      "grad_norm": 178.49842834472656,
      "learning_rate": 0.000365,
      "loss": 6.2246,
      "step": 365
    },
    {
      "epoch": 2.418835192069393,
      "grad_norm": 179.97833251953125,
      "learning_rate": 0.000366,
      "loss": 6.1619,
      "step": 366
    },
    {
      "epoch": 2.425444031391987,
      "grad_norm": 190.4986114501953,
      "learning_rate": 0.000367,
      "loss": 5.8221,
      "step": 367
    },
    {
      "epoch": 2.432052870714581,
      "grad_norm": 94.12517547607422,
      "learning_rate": 0.000368,
      "loss": 5.9301,
      "step": 368
    },
    {
      "epoch": 2.4386617100371746,
      "grad_norm": 69.89539337158203,
      "learning_rate": 0.000369,
      "loss": 6.4048,
      "step": 369
    },
    {
      "epoch": 2.4452705493597686,
      "grad_norm": 35.16455078125,
      "learning_rate": 0.00037,
      "loss": 2.2384,
      "step": 370
    },
    {
      "epoch": 2.4518793886823627,
      "grad_norm": 26.234228134155273,
      "learning_rate": 0.000371,
      "loss": 1.8544,
      "step": 371
    },
    {
      "epoch": 2.4584882280049567,
      "grad_norm": 205.29806518554688,
      "learning_rate": 0.000372,
      "loss": 4.3347,
      "step": 372
    },
    {
      "epoch": 2.4650970673275507,
      "grad_norm": 486.3807067871094,
      "learning_rate": 0.000373,
      "loss": 19.4584,
      "step": 373
    },
    {
      "epoch": 2.4717059066501443,
      "grad_norm": 128.44740295410156,
      "learning_rate": 0.000374,
      "loss": 3.3361,
      "step": 374
    },
    {
      "epoch": 2.4783147459727384,
      "grad_norm": 83.88687133789062,
      "learning_rate": 0.000375,
      "loss": 2.6633,
      "step": 375
    },
    {
      "epoch": 2.4849235852953324,
      "grad_norm": 61.62908172607422,
      "learning_rate": 0.00037600000000000003,
      "loss": 3.1538,
      "step": 376
    },
    {
      "epoch": 2.4915324246179265,
      "grad_norm": 6.3777174949646,
      "learning_rate": 0.000377,
      "loss": 3.9632,
      "step": 377
    },
    {
      "epoch": 2.4981412639405205,
      "grad_norm": 36.794124603271484,
      "learning_rate": 0.000378,
      "loss": 7.1679,
      "step": 378
    },
    {
      "epoch": 2.5047501032631145,
      "grad_norm": 81.1065444946289,
      "learning_rate": 0.000379,
      "loss": 5.4274,
      "step": 379
    },
    {
      "epoch": 2.5113589425857086,
      "grad_norm": 78.94871520996094,
      "learning_rate": 0.00038,
      "loss": 5.4252,
      "step": 380
    },
    {
      "epoch": 2.517967781908302,
      "grad_norm": 117.91647338867188,
      "learning_rate": 0.000381,
      "loss": 3.7915,
      "step": 381
    },
    {
      "epoch": 2.524576621230896,
      "grad_norm": 43.53081130981445,
      "learning_rate": 0.000382,
      "loss": 8.8602,
      "step": 382
    },
    {
      "epoch": 2.5311854605534903,
      "grad_norm": 120.97044372558594,
      "learning_rate": 0.00038300000000000004,
      "loss": 8.4107,
      "step": 383
    },
    {
      "epoch": 2.5377942998760843,
      "grad_norm": 326.7595520019531,
      "learning_rate": 0.000384,
      "loss": 9.0509,
      "step": 384
    },
    {
      "epoch": 2.5444031391986783,
      "grad_norm": 154.18475341796875,
      "learning_rate": 0.00038500000000000003,
      "loss": 5.9265,
      "step": 385
    },
    {
      "epoch": 2.551011978521272,
      "grad_norm": 145.05821228027344,
      "learning_rate": 0.000386,
      "loss": 5.9648,
      "step": 386
    },
    {
      "epoch": 2.5576208178438664,
      "grad_norm": 46.8342399597168,
      "learning_rate": 0.00038700000000000003,
      "loss": 3.2414,
      "step": 387
    },
    {
      "epoch": 2.56422965716646,
      "grad_norm": 38.60120391845703,
      "learning_rate": 0.000388,
      "loss": 9.3293,
      "step": 388
    },
    {
      "epoch": 2.570838496489054,
      "grad_norm": 31.11709213256836,
      "learning_rate": 0.000389,
      "loss": 6.2409,
      "step": 389
    },
    {
      "epoch": 2.577447335811648,
      "grad_norm": 140.1849365234375,
      "learning_rate": 0.00039000000000000005,
      "loss": 5.2202,
      "step": 390
    },
    {
      "epoch": 2.584056175134242,
      "grad_norm": 115.48612976074219,
      "learning_rate": 0.000391,
      "loss": 2.5645,
      "step": 391
    },
    {
      "epoch": 2.590665014456836,
      "grad_norm": 56.0428352355957,
      "learning_rate": 0.00039200000000000004,
      "loss": 5.5619,
      "step": 392
    },
    {
      "epoch": 2.5972738537794298,
      "grad_norm": 198.45193481445312,
      "learning_rate": 0.000393,
      "loss": 6.9855,
      "step": 393
    },
    {
      "epoch": 2.603882693102024,
      "grad_norm": 243.9811248779297,
      "learning_rate": 0.00039400000000000004,
      "loss": 7.6302,
      "step": 394
    },
    {
      "epoch": 2.610491532424618,
      "grad_norm": 159.19786071777344,
      "learning_rate": 0.000395,
      "loss": 5.7174,
      "step": 395
    },
    {
      "epoch": 2.617100371747212,
      "grad_norm": 61.26584243774414,
      "learning_rate": 0.00039600000000000003,
      "loss": 5.0871,
      "step": 396
    },
    {
      "epoch": 2.623709211069806,
      "grad_norm": 317.3348388671875,
      "learning_rate": 0.00039700000000000005,
      "loss": 12.0046,
      "step": 397
    },
    {
      "epoch": 2.6303180503924,
      "grad_norm": 406.0538024902344,
      "learning_rate": 0.000398,
      "loss": 18.9773,
      "step": 398
    },
    {
      "epoch": 2.636926889714994,
      "grad_norm": 287.0665588378906,
      "learning_rate": 0.00039900000000000005,
      "loss": 8.224,
      "step": 399
    },
    {
      "epoch": 2.6435357290375876,
      "grad_norm": 137.17620849609375,
      "learning_rate": 0.0004,
      "loss": 3.8174,
      "step": 400
    },
    {
      "epoch": 2.6501445683601816,
      "grad_norm": 53.42300796508789,
      "learning_rate": 0.00040100000000000004,
      "loss": 6.5065,
      "step": 401
    },
    {
      "epoch": 2.6567534076827757,
      "grad_norm": 167.37139892578125,
      "learning_rate": 0.000402,
      "loss": 4.595,
      "step": 402
    },
    {
      "epoch": 2.6633622470053697,
      "grad_norm": 21.40461540222168,
      "learning_rate": 0.00040300000000000004,
      "loss": 2.913,
      "step": 403
    },
    {
      "epoch": 2.6699710863279638,
      "grad_norm": 13.694770812988281,
      "learning_rate": 0.000404,
      "loss": 2.4381,
      "step": 404
    },
    {
      "epoch": 2.6765799256505574,
      "grad_norm": 101.83575439453125,
      "learning_rate": 0.00040500000000000003,
      "loss": 3.5081,
      "step": 405
    },
    {
      "epoch": 2.683188764973152,
      "grad_norm": 109.94959259033203,
      "learning_rate": 0.00040600000000000006,
      "loss": 2.6248,
      "step": 406
    },
    {
      "epoch": 2.6897976042957454,
      "grad_norm": 81.78389739990234,
      "learning_rate": 0.00040699999999999997,
      "loss": 6.7532,
      "step": 407
    },
    {
      "epoch": 2.6964064436183395,
      "grad_norm": 195.25375366210938,
      "learning_rate": 0.000408,
      "loss": 3.9935,
      "step": 408
    },
    {
      "epoch": 2.7030152829409335,
      "grad_norm": 250.40980529785156,
      "learning_rate": 0.00040899999999999997,
      "loss": 9.637,
      "step": 409
    },
    {
      "epoch": 2.7096241222635276,
      "grad_norm": 348.4378662109375,
      "learning_rate": 0.00041,
      "loss": 10.1874,
      "step": 410
    },
    {
      "epoch": 2.7162329615861216,
      "grad_norm": 191.2338409423828,
      "learning_rate": 0.00041099999999999996,
      "loss": 3.6816,
      "step": 411
    },
    {
      "epoch": 2.722841800908715,
      "grad_norm": 27.060855865478516,
      "learning_rate": 0.000412,
      "loss": 5.2221,
      "step": 412
    },
    {
      "epoch": 2.7294506402313092,
      "grad_norm": 425.9035949707031,
      "learning_rate": 0.000413,
      "loss": 15.3421,
      "step": 413
    },
    {
      "epoch": 2.7360594795539033,
      "grad_norm": 414.6136779785156,
      "learning_rate": 0.000414,
      "loss": 14.7326,
      "step": 414
    },
    {
      "epoch": 2.7426683188764973,
      "grad_norm": 440.19647216796875,
      "learning_rate": 0.000415,
      "loss": 12.9756,
      "step": 415
    },
    {
      "epoch": 2.7492771581990914,
      "grad_norm": 506.73150634765625,
      "learning_rate": 0.000416,
      "loss": 19.6633,
      "step": 416
    },
    {
      "epoch": 2.7558859975216854,
      "grad_norm": 123.87613677978516,
      "learning_rate": 0.000417,
      "loss": 3.2844,
      "step": 417
    },
    {
      "epoch": 2.7624948368442794,
      "grad_norm": 112.8511962890625,
      "learning_rate": 0.00041799999999999997,
      "loss": 4.2398,
      "step": 418
    },
    {
      "epoch": 2.769103676166873,
      "grad_norm": 224.58863830566406,
      "learning_rate": 0.000419,
      "loss": 5.9009,
      "step": 419
    },
    {
      "epoch": 2.775712515489467,
      "grad_norm": 253.2144317626953,
      "learning_rate": 0.00042,
      "loss": 8.2706,
      "step": 420
    },
    {
      "epoch": 2.782321354812061,
      "grad_norm": 152.2349395751953,
      "learning_rate": 0.000421,
      "loss": 4.1978,
      "step": 421
    },
    {
      "epoch": 2.788930194134655,
      "grad_norm": 67.83673095703125,
      "learning_rate": 0.000422,
      "loss": 11.8557,
      "step": 422
    },
    {
      "epoch": 2.795539033457249,
      "grad_norm": 282.36151123046875,
      "learning_rate": 0.000423,
      "loss": 11.9684,
      "step": 423
    },
    {
      "epoch": 2.802147872779843,
      "grad_norm": 258.1356506347656,
      "learning_rate": 0.000424,
      "loss": 6.9386,
      "step": 424
    },
    {
      "epoch": 2.8087567121024373,
      "grad_norm": 352.1377258300781,
      "learning_rate": 0.000425,
      "loss": 12.3397,
      "step": 425
    },
    {
      "epoch": 2.815365551425031,
      "grad_norm": 181.3357696533203,
      "learning_rate": 0.000426,
      "loss": 6.664,
      "step": 426
    },
    {
      "epoch": 2.821974390747625,
      "grad_norm": 152.3177032470703,
      "learning_rate": 0.000427,
      "loss": 8.4418,
      "step": 427
    },
    {
      "epoch": 2.828583230070219,
      "grad_norm": 99.49069213867188,
      "learning_rate": 0.000428,
      "loss": 7.1337,
      "step": 428
    },
    {
      "epoch": 2.835192069392813,
      "grad_norm": 269.3238525390625,
      "learning_rate": 0.000429,
      "loss": 10.873,
      "step": 429
    },
    {
      "epoch": 2.841800908715407,
      "grad_norm": 337.8104248046875,
      "learning_rate": 0.00043,
      "loss": 14.1166,
      "step": 430
    },
    {
      "epoch": 2.8484097480380006,
      "grad_norm": 208.36990356445312,
      "learning_rate": 0.000431,
      "loss": 7.0362,
      "step": 431
    },
    {
      "epoch": 2.8550185873605947,
      "grad_norm": 57.3367805480957,
      "learning_rate": 0.000432,
      "loss": 7.9932,
      "step": 432
    },
    {
      "epoch": 2.8616274266831887,
      "grad_norm": 159.58302307128906,
      "learning_rate": 0.000433,
      "loss": 5.6132,
      "step": 433
    },
    {
      "epoch": 2.8682362660057827,
      "grad_norm": 147.21377563476562,
      "learning_rate": 0.00043400000000000003,
      "loss": 4.454,
      "step": 434
    },
    {
      "epoch": 2.874845105328377,
      "grad_norm": 252.15011596679688,
      "learning_rate": 0.000435,
      "loss": 9.444,
      "step": 435
    },
    {
      "epoch": 2.881453944650971,
      "grad_norm": 275.63067626953125,
      "learning_rate": 0.000436,
      "loss": 9.4947,
      "step": 436
    },
    {
      "epoch": 2.888062783973565,
      "grad_norm": 37.08353042602539,
      "learning_rate": 0.000437,
      "loss": 2.0917,
      "step": 437
    },
    {
      "epoch": 2.8946716232961585,
      "grad_norm": 129.8211212158203,
      "learning_rate": 0.000438,
      "loss": 4.4641,
      "step": 438
    },
    {
      "epoch": 2.9012804626187525,
      "grad_norm": 164.40309143066406,
      "learning_rate": 0.000439,
      "loss": 6.1402,
      "step": 439
    },
    {
      "epoch": 2.9078893019413465,
      "grad_norm": 300.11273193359375,
      "learning_rate": 0.00044,
      "loss": 11.769,
      "step": 440
    },
    {
      "epoch": 2.9144981412639406,
      "grad_norm": 187.6160430908203,
      "learning_rate": 0.000441,
      "loss": 6.2197,
      "step": 441
    },
    {
      "epoch": 2.9211069805865346,
      "grad_norm": 69.85720825195312,
      "learning_rate": 0.000442,
      "loss": 4.6876,
      "step": 442
    },
    {
      "epoch": 2.927715819909128,
      "grad_norm": 41.08512878417969,
      "learning_rate": 0.00044300000000000003,
      "loss": 2.8891,
      "step": 443
    },
    {
      "epoch": 2.9343246592317227,
      "grad_norm": 268.05035400390625,
      "learning_rate": 0.000444,
      "loss": 10.5499,
      "step": 444
    },
    {
      "epoch": 2.9409334985543163,
      "grad_norm": 226.17906188964844,
      "learning_rate": 0.00044500000000000003,
      "loss": 9.832,
      "step": 445
    },
    {
      "epoch": 2.9475423378769103,
      "grad_norm": 98.15502166748047,
      "learning_rate": 0.000446,
      "loss": 2.1554,
      "step": 446
    },
    {
      "epoch": 2.9541511771995044,
      "grad_norm": 215.00672912597656,
      "learning_rate": 0.000447,
      "loss": 13.3597,
      "step": 447
    },
    {
      "epoch": 2.9607600165220984,
      "grad_norm": 116.00495147705078,
      "learning_rate": 0.000448,
      "loss": 2.9288,
      "step": 448
    },
    {
      "epoch": 2.9673688558446925,
      "grad_norm": 220.1229705810547,
      "learning_rate": 0.000449,
      "loss": 8.4531,
      "step": 449
    },
    {
      "epoch": 2.973977695167286,
      "grad_norm": 217.35919189453125,
      "learning_rate": 0.00045000000000000004,
      "loss": 7.1205,
      "step": 450
    },
    {
      "epoch": 2.98058653448988,
      "grad_norm": 240.31968688964844,
      "learning_rate": 0.000451,
      "loss": 13.8337,
      "step": 451
    },
    {
      "epoch": 2.987195373812474,
      "grad_norm": 24.234207153320312,
      "learning_rate": 0.00045200000000000004,
      "loss": 3.6563,
      "step": 452
    },
    {
      "epoch": 2.993804213135068,
      "grad_norm": 19.72699737548828,
      "learning_rate": 0.000453,
      "loss": 3.2495,
      "step": 453
    },
    {
      "epoch": 2.993804213135068,
      "eval_validation_error_bar": 0.062081133460522575,
      "eval_validation_loss": 7.39845609664917,
      "eval_validation_pearsonr": 0.45745298745487417,
      "eval_validation_rmse": 2.720010280609131,
      "eval_validation_runtime": 33.0619,
      "eval_validation_samples_per_second": 6.14,
      "eval_validation_spearman": 0.3924393917675025,
      "eval_validation_steps_per_second": 6.14,
      "step": 453
    },
    {
      "epoch": 2.993804213135068,
      "eval_test_error_bar": 0.05418445877242218,
      "eval_test_loss": 11.046284675598145,
      "eval_test_pearsonr": 0.15713986944462072,
      "eval_test_rmse": 3.3235950469970703,
      "eval_test_runtime": 39.3295,
      "eval_test_samples_per_second": 8.289,
      "eval_test_spearman": 0.18566426791074947,
      "eval_test_steps_per_second": 8.289,
      "step": 453
    },
    {
      "epoch": 3.000413052457662,
      "grad_norm": 57.07109069824219,
      "learning_rate": 0.00045400000000000003,
      "loss": 1.8195,
      "step": 454
    },
    {
      "epoch": 3.0070218917802563,
      "grad_norm": 13.091843605041504,
      "learning_rate": 0.000455,
      "loss": 2.7884,
      "step": 455
    },
    {
      "epoch": 3.01363073110285,
      "grad_norm": 89.65213012695312,
      "learning_rate": 0.000456,
      "loss": 4.5923,
      "step": 456
    },
    {
      "epoch": 3.020239570425444,
      "grad_norm": 60.8084831237793,
      "learning_rate": 0.00045700000000000005,
      "loss": 5.7101,
      "step": 457
    },
    {
      "epoch": 3.026848409748038,
      "grad_norm": 86.46854400634766,
      "learning_rate": 0.000458,
      "loss": 10.3834,
      "step": 458
    },
    {
      "epoch": 3.033457249070632,
      "grad_norm": 154.8168487548828,
      "learning_rate": 0.00045900000000000004,
      "loss": 3.7696,
      "step": 459
    },
    {
      "epoch": 3.040066088393226,
      "grad_norm": 189.22073364257812,
      "learning_rate": 0.00046,
      "loss": 6.7142,
      "step": 460
    },
    {
      "epoch": 3.04667492771582,
      "grad_norm": 205.46087646484375,
      "learning_rate": 0.00046100000000000004,
      "loss": 5.7798,
      "step": 461
    },
    {
      "epoch": 3.053283767038414,
      "grad_norm": 89.47807312011719,
      "learning_rate": 0.000462,
      "loss": 3.3042,
      "step": 462
    },
    {
      "epoch": 3.0598926063610077,
      "grad_norm": 244.17391967773438,
      "learning_rate": 0.00046300000000000003,
      "loss": 8.0859,
      "step": 463
    },
    {
      "epoch": 3.0665014456836017,
      "grad_norm": 184.03468322753906,
      "learning_rate": 0.00046400000000000006,
      "loss": 9.0395,
      "step": 464
    },
    {
      "epoch": 3.0731102850061958,
      "grad_norm": 247.61024475097656,
      "learning_rate": 0.000465,
      "loss": 6.5169,
      "step": 465
    },
    {
      "epoch": 3.07971912432879,
      "grad_norm": 158.09056091308594,
      "learning_rate": 0.00046600000000000005,
      "loss": 4.7251,
      "step": 466
    },
    {
      "epoch": 3.086327963651384,
      "grad_norm": 36.6209602355957,
      "learning_rate": 0.000467,
      "loss": 7.2803,
      "step": 467
    },
    {
      "epoch": 3.092936802973978,
      "grad_norm": 234.8721466064453,
      "learning_rate": 0.00046800000000000005,
      "loss": 9.5225,
      "step": 468
    },
    {
      "epoch": 3.0995456422965715,
      "grad_norm": 262.88165283203125,
      "learning_rate": 0.00046899999999999996,
      "loss": 8.1742,
      "step": 469
    },
    {
      "epoch": 3.1061544816191655,
      "grad_norm": 296.5312805175781,
      "learning_rate": 0.00047,
      "loss": 12.3317,
      "step": 470
    },
    {
      "epoch": 3.1127633209417596,
      "grad_norm": 291.2108154296875,
      "learning_rate": 0.000471,
      "loss": 12.906,
      "step": 471
    },
    {
      "epoch": 3.1193721602643536,
      "grad_norm": 208.8769073486328,
      "learning_rate": 0.000472,
      "loss": 6.6092,
      "step": 472
    },
    {
      "epoch": 3.1259809995869476,
      "grad_norm": 127.21536254882812,
      "learning_rate": 0.000473,
      "loss": 3.1607,
      "step": 473
    },
    {
      "epoch": 3.1325898389095417,
      "grad_norm": 75.1034164428711,
      "learning_rate": 0.000474,
      "loss": 4.3093,
      "step": 474
    },
    {
      "epoch": 3.1391986782321353,
      "grad_norm": 122.06089782714844,
      "learning_rate": 0.000475,
      "loss": 3.8882,
      "step": 475
    },
    {
      "epoch": 3.1458075175547293,
      "grad_norm": 155.32786560058594,
      "learning_rate": 0.00047599999999999997,
      "loss": 7.8016,
      "step": 476
    },
    {
      "epoch": 3.1524163568773234,
      "grad_norm": 83.07182312011719,
      "learning_rate": 0.000477,
      "loss": 2.7667,
      "step": 477
    },
    {
      "epoch": 3.1590251961999174,
      "grad_norm": 43.551639556884766,
      "learning_rate": 0.00047799999999999996,
      "loss": 6.3461,
      "step": 478
    },
    {
      "epoch": 3.1656340355225114,
      "grad_norm": 144.75411987304688,
      "learning_rate": 0.000479,
      "loss": 7.3771,
      "step": 479
    },
    {
      "epoch": 3.1722428748451055,
      "grad_norm": 91.71243286132812,
      "learning_rate": 0.00048,
      "loss": 4.0982,
      "step": 480
    },
    {
      "epoch": 3.178851714167699,
      "grad_norm": 13.126046180725098,
      "learning_rate": 0.000481,
      "loss": 9.3993,
      "step": 481
    },
    {
      "epoch": 3.185460553490293,
      "grad_norm": 187.5621795654297,
      "learning_rate": 0.000482,
      "loss": 10.0858,
      "step": 482
    },
    {
      "epoch": 3.192069392812887,
      "grad_norm": 110.0875015258789,
      "learning_rate": 0.000483,
      "loss": 4.1076,
      "step": 483
    },
    {
      "epoch": 3.198678232135481,
      "grad_norm": 117.16487884521484,
      "learning_rate": 0.000484,
      "loss": 6.5801,
      "step": 484
    },
    {
      "epoch": 3.2052870714580752,
      "grad_norm": 328.41552734375,
      "learning_rate": 0.00048499999999999997,
      "loss": 9.7727,
      "step": 485
    },
    {
      "epoch": 3.2118959107806693,
      "grad_norm": 209.11074829101562,
      "learning_rate": 0.000486,
      "loss": 8.6386,
      "step": 486
    },
    {
      "epoch": 3.2185047501032633,
      "grad_norm": 111.17302703857422,
      "learning_rate": 0.000487,
      "loss": 2.68,
      "step": 487
    },
    {
      "epoch": 3.225113589425857,
      "grad_norm": 86.3035888671875,
      "learning_rate": 0.000488,
      "loss": 1.7986,
      "step": 488
    },
    {
      "epoch": 3.231722428748451,
      "grad_norm": 16.976587295532227,
      "learning_rate": 0.000489,
      "loss": 2.5811,
      "step": 489
    },
    {
      "epoch": 3.238331268071045,
      "grad_norm": 237.70053100585938,
      "learning_rate": 0.00049,
      "loss": 7.9327,
      "step": 490
    },
    {
      "epoch": 3.244940107393639,
      "grad_norm": 398.4268798828125,
      "learning_rate": 0.000491,
      "loss": 17.7167,
      "step": 491
    },
    {
      "epoch": 3.251548946716233,
      "grad_norm": 394.4832763671875,
      "learning_rate": 0.000492,
      "loss": 18.0265,
      "step": 492
    },
    {
      "epoch": 3.258157786038827,
      "grad_norm": 345.4656066894531,
      "learning_rate": 0.0004930000000000001,
      "loss": 13.8748,
      "step": 493
    },
    {
      "epoch": 3.264766625361421,
      "grad_norm": 222.16384887695312,
      "learning_rate": 0.000494,
      "loss": 12.6201,
      "step": 494
    },
    {
      "epoch": 3.2713754646840147,
      "grad_norm": 35.309471130371094,
      "learning_rate": 0.000495,
      "loss": 2.2062,
      "step": 495
    },
    {
      "epoch": 3.277984304006609,
      "grad_norm": 131.18719482421875,
      "learning_rate": 0.000496,
      "loss": 6.1127,
      "step": 496
    },
    {
      "epoch": 3.284593143329203,
      "grad_norm": 195.34109497070312,
      "learning_rate": 0.000497,
      "loss": 6.3193,
      "step": 497
    },
    {
      "epoch": 3.291201982651797,
      "grad_norm": 120.290283203125,
      "learning_rate": 0.000498,
      "loss": 6.6289,
      "step": 498
    },
    {
      "epoch": 3.297810821974391,
      "grad_norm": 189.1802520751953,
      "learning_rate": 0.000499,
      "loss": 10.4768,
      "step": 499
    },
    {
      "epoch": 3.3044196612969845,
      "grad_norm": 57.516944885253906,
      "learning_rate": 0.0005,
      "loss": 5.884,
      "step": 500
    },
    {
      "epoch": 3.3110285006195785,
      "grad_norm": 234.33197021484375,
      "learning_rate": 0.000501,
      "loss": 11.2332,
      "step": 501
    },
    {
      "epoch": 3.3176373399421726,
      "grad_norm": 181.30738830566406,
      "learning_rate": 0.0005020000000000001,
      "loss": 9.2178,
      "step": 502
    },
    {
      "epoch": 3.3242461792647666,
      "grad_norm": 150.7279052734375,
      "learning_rate": 0.000503,
      "loss": 6.7073,
      "step": 503
    },
    {
      "epoch": 3.3308550185873607,
      "grad_norm": 5.654183387756348,
      "learning_rate": 0.000504,
      "loss": 4.0793,
      "step": 504
    },
    {
      "epoch": 3.3374638579099547,
      "grad_norm": 134.30999755859375,
      "learning_rate": 0.000505,
      "loss": 5.2877,
      "step": 505
    },
    {
      "epoch": 3.3440726972325487,
      "grad_norm": 51.324928283691406,
      "learning_rate": 0.000506,
      "loss": 4.384,
      "step": 506
    },
    {
      "epoch": 3.3506815365551423,
      "grad_norm": 82.08345794677734,
      "learning_rate": 0.000507,
      "loss": 7.4566,
      "step": 507
    },
    {
      "epoch": 3.3572903758777364,
      "grad_norm": 59.358463287353516,
      "learning_rate": 0.000508,
      "loss": 4.341,
      "step": 508
    },
    {
      "epoch": 3.3638992152003304,
      "grad_norm": 197.3723602294922,
      "learning_rate": 0.000509,
      "loss": 13.1777,
      "step": 509
    },
    {
      "epoch": 3.3705080545229245,
      "grad_norm": 67.61615753173828,
      "learning_rate": 0.00051,
      "loss": 5.3236,
      "step": 510
    },
    {
      "epoch": 3.3771168938455185,
      "grad_norm": 55.549102783203125,
      "learning_rate": 0.0005110000000000001,
      "loss": 7.7309,
      "step": 511
    },
    {
      "epoch": 3.3837257331681125,
      "grad_norm": 150.52288818359375,
      "learning_rate": 0.000512,
      "loss": 11.2932,
      "step": 512
    },
    {
      "epoch": 3.390334572490706,
      "grad_norm": 274.4075927734375,
      "learning_rate": 0.000513,
      "loss": 13.3645,
      "step": 513
    },
    {
      "epoch": 3.3969434118133,
      "grad_norm": 226.48385620117188,
      "learning_rate": 0.000514,
      "loss": 10.9768,
      "step": 514
    },
    {
      "epoch": 3.403552251135894,
      "grad_norm": 117.30595397949219,
      "learning_rate": 0.000515,
      "loss": 5.4975,
      "step": 515
    },
    {
      "epoch": 3.4101610904584883,
      "grad_norm": 27.83559799194336,
      "learning_rate": 0.0005160000000000001,
      "loss": 4.9727,
      "step": 516
    },
    {
      "epoch": 3.4167699297810823,
      "grad_norm": 194.81211853027344,
      "learning_rate": 0.000517,
      "loss": 6.8295,
      "step": 517
    },
    {
      "epoch": 3.4233787691036763,
      "grad_norm": 14.810172080993652,
      "learning_rate": 0.000518,
      "loss": 2.9559,
      "step": 518
    },
    {
      "epoch": 3.42998760842627,
      "grad_norm": 86.6148452758789,
      "learning_rate": 0.000519,
      "loss": 5.7944,
      "step": 519
    },
    {
      "epoch": 3.436596447748864,
      "grad_norm": 6.993155002593994,
      "learning_rate": 0.0005200000000000001,
      "loss": 2.512,
      "step": 520
    },
    {
      "epoch": 3.443205287071458,
      "grad_norm": 97.50993347167969,
      "learning_rate": 0.000521,
      "loss": 5.8076,
      "step": 521
    },
    {
      "epoch": 3.449814126394052,
      "grad_norm": 66.84406280517578,
      "learning_rate": 0.000522,
      "loss": 4.154,
      "step": 522
    },
    {
      "epoch": 3.456422965716646,
      "grad_norm": 99.22882843017578,
      "learning_rate": 0.000523,
      "loss": 2.0055,
      "step": 523
    },
    {
      "epoch": 3.46303180503924,
      "grad_norm": 120.29180145263672,
      "learning_rate": 0.000524,
      "loss": 3.4098,
      "step": 524
    },
    {
      "epoch": 3.469640644361834,
      "grad_norm": 229.90760803222656,
      "learning_rate": 0.0005250000000000001,
      "loss": 6.8566,
      "step": 525
    },
    {
      "epoch": 3.4762494836844278,
      "grad_norm": 146.18173217773438,
      "learning_rate": 0.000526,
      "loss": 10.8229,
      "step": 526
    },
    {
      "epoch": 3.482858323007022,
      "grad_norm": 83.68851470947266,
      "learning_rate": 0.000527,
      "loss": 2.7409,
      "step": 527
    },
    {
      "epoch": 3.489467162329616,
      "grad_norm": 22.813093185424805,
      "learning_rate": 0.000528,
      "loss": 2.4922,
      "step": 528
    },
    {
      "epoch": 3.49607600165221,
      "grad_norm": 53.303558349609375,
      "learning_rate": 0.0005290000000000001,
      "loss": 3.9958,
      "step": 529
    },
    {
      "epoch": 3.502684840974804,
      "grad_norm": 134.06524658203125,
      "learning_rate": 0.0005300000000000001,
      "loss": 5.3383,
      "step": 530
    },
    {
      "epoch": 3.5092936802973975,
      "grad_norm": 85.78007507324219,
      "learning_rate": 0.000531,
      "loss": 6.5552,
      "step": 531
    },
    {
      "epoch": 3.515902519619992,
      "grad_norm": 11.551551818847656,
      "learning_rate": 0.000532,
      "loss": 2.3312,
      "step": 532
    },
    {
      "epoch": 3.5225113589425856,
      "grad_norm": 79.54254913330078,
      "learning_rate": 0.000533,
      "loss": 4.4869,
      "step": 533
    },
    {
      "epoch": 3.5291201982651796,
      "grad_norm": 57.800872802734375,
      "learning_rate": 0.0005340000000000001,
      "loss": 5.6082,
      "step": 534
    },
    {
      "epoch": 3.5357290375877737,
      "grad_norm": 72.40187072753906,
      "learning_rate": 0.000535,
      "loss": 4.7468,
      "step": 535
    },
    {
      "epoch": 3.5423378769103677,
      "grad_norm": 80.78115844726562,
      "learning_rate": 0.000536,
      "loss": 2.8565,
      "step": 536
    },
    {
      "epoch": 3.5489467162329618,
      "grad_norm": 56.419063568115234,
      "learning_rate": 0.000537,
      "loss": 2.9986,
      "step": 537
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 15.047829627990723,
      "learning_rate": 0.0005380000000000001,
      "loss": 4.4334,
      "step": 538
    },
    {
      "epoch": 3.5621643948781494,
      "grad_norm": 75.76971435546875,
      "learning_rate": 0.0005390000000000001,
      "loss": 4.0636,
      "step": 539
    },
    {
      "epoch": 3.5687732342007434,
      "grad_norm": 133.69546508789062,
      "learning_rate": 0.00054,
      "loss": 6.9099,
      "step": 540
    },
    {
      "epoch": 3.5753820735233375,
      "grad_norm": 44.03228759765625,
      "learning_rate": 0.000541,
      "loss": 2.6942,
      "step": 541
    },
    {
      "epoch": 3.5819909128459315,
      "grad_norm": 47.86353302001953,
      "learning_rate": 0.0005420000000000001,
      "loss": 2.3134,
      "step": 542
    },
    {
      "epoch": 3.5885997521685256,
      "grad_norm": 135.2043914794922,
      "learning_rate": 0.0005430000000000001,
      "loss": 6.0697,
      "step": 543
    },
    {
      "epoch": 3.5952085914911196,
      "grad_norm": 154.2818145751953,
      "learning_rate": 0.0005440000000000001,
      "loss": 9.0413,
      "step": 544
    },
    {
      "epoch": 3.601817430813713,
      "grad_norm": 5.284209728240967,
      "learning_rate": 0.000545,
      "loss": 2.2748,
      "step": 545
    },
    {
      "epoch": 3.6084262701363072,
      "grad_norm": 136.54344177246094,
      "learning_rate": 0.000546,
      "loss": 5.9142,
      "step": 546
    },
    {
      "epoch": 3.6150351094589013,
      "grad_norm": 131.44615173339844,
      "learning_rate": 0.0005470000000000001,
      "loss": 4.0277,
      "step": 547
    },
    {
      "epoch": 3.6216439487814953,
      "grad_norm": 65.26213073730469,
      "learning_rate": 0.0005480000000000001,
      "loss": 1.9401,
      "step": 548
    },
    {
      "epoch": 3.6282527881040894,
      "grad_norm": 16.60719108581543,
      "learning_rate": 0.000549,
      "loss": 2.3899,
      "step": 549
    },
    {
      "epoch": 3.634861627426683,
      "grad_norm": 118.33683776855469,
      "learning_rate": 0.00055,
      "loss": 5.0115,
      "step": 550
    },
    {
      "epoch": 3.6414704667492774,
      "grad_norm": 59.70012664794922,
      "learning_rate": 0.0005510000000000001,
      "loss": 4.1601,
      "step": 551
    },
    {
      "epoch": 3.648079306071871,
      "grad_norm": 44.655311584472656,
      "learning_rate": 0.0005520000000000001,
      "loss": 9.2526,
      "step": 552
    },
    {
      "epoch": 3.654688145394465,
      "grad_norm": 23.989595413208008,
      "learning_rate": 0.0005530000000000001,
      "loss": 1.9975,
      "step": 553
    },
    {
      "epoch": 3.661296984717059,
      "grad_norm": 46.30910873413086,
      "learning_rate": 0.000554,
      "loss": 4.6196,
      "step": 554
    },
    {
      "epoch": 3.667905824039653,
      "grad_norm": 9.529525756835938,
      "learning_rate": 0.000555,
      "loss": 2.6882,
      "step": 555
    },
    {
      "epoch": 3.674514663362247,
      "grad_norm": 37.876651763916016,
      "learning_rate": 0.0005560000000000001,
      "loss": 2.1073,
      "step": 556
    },
    {
      "epoch": 3.681123502684841,
      "grad_norm": 53.65932083129883,
      "learning_rate": 0.0005570000000000001,
      "loss": 5.9855,
      "step": 557
    },
    {
      "epoch": 3.687732342007435,
      "grad_norm": 133.64486694335938,
      "learning_rate": 0.000558,
      "loss": 5.9106,
      "step": 558
    },
    {
      "epoch": 3.694341181330029,
      "grad_norm": 180.0931854248047,
      "learning_rate": 0.000559,
      "loss": 6.3651,
      "step": 559
    },
    {
      "epoch": 3.700950020652623,
      "grad_norm": 224.2718963623047,
      "learning_rate": 0.0005600000000000001,
      "loss": 8.1974,
      "step": 560
    },
    {
      "epoch": 3.707558859975217,
      "grad_norm": 50.793216705322266,
      "learning_rate": 0.0005610000000000001,
      "loss": 2.6561,
      "step": 561
    },
    {
      "epoch": 3.714167699297811,
      "grad_norm": 267.4425964355469,
      "learning_rate": 0.0005620000000000001,
      "loss": 10.0958,
      "step": 562
    },
    {
      "epoch": 3.720776538620405,
      "grad_norm": 325.125244140625,
      "learning_rate": 0.0005629999999999999,
      "loss": 15.7454,
      "step": 563
    },
    {
      "epoch": 3.7273853779429986,
      "grad_norm": 87.85685729980469,
      "learning_rate": 0.0005639999999999999,
      "loss": 6.0994,
      "step": 564
    },
    {
      "epoch": 3.7339942172655927,
      "grad_norm": 8.424059867858887,
      "learning_rate": 0.000565,
      "loss": 6.2755,
      "step": 565
    },
    {
      "epoch": 3.7406030565881867,
      "grad_norm": 139.041015625,
      "learning_rate": 0.000566,
      "loss": 5.2068,
      "step": 566
    },
    {
      "epoch": 3.7472118959107807,
      "grad_norm": 43.24158477783203,
      "learning_rate": 0.000567,
      "loss": 3.3757,
      "step": 567
    },
    {
      "epoch": 3.753820735233375,
      "grad_norm": 4.922252178192139,
      "learning_rate": 0.0005679999999999999,
      "loss": 3.9015,
      "step": 568
    },
    {
      "epoch": 3.7604295745559684,
      "grad_norm": 64.34611511230469,
      "learning_rate": 0.000569,
      "loss": 3.0996,
      "step": 569
    },
    {
      "epoch": 3.7670384138785624,
      "grad_norm": 78.55714416503906,
      "learning_rate": 0.00057,
      "loss": 5.8432,
      "step": 570
    },
    {
      "epoch": 3.7736472532011565,
      "grad_norm": 145.9318389892578,
      "learning_rate": 0.000571,
      "loss": 8.2004,
      "step": 571
    },
    {
      "epoch": 3.7802560925237505,
      "grad_norm": 17.266054153442383,
      "learning_rate": 0.0005719999999999999,
      "loss": 6.052,
      "step": 572
    },
    {
      "epoch": 3.7868649318463445,
      "grad_norm": 18.8421573638916,
      "learning_rate": 0.0005729999999999999,
      "loss": 2.0222,
      "step": 573
    },
    {
      "epoch": 3.7934737711689386,
      "grad_norm": 57.669490814208984,
      "learning_rate": 0.000574,
      "loss": 3.8385,
      "step": 574
    },
    {
      "epoch": 3.8000826104915326,
      "grad_norm": 25.703018188476562,
      "learning_rate": 0.000575,
      "loss": 2.0319,
      "step": 575
    },
    {
      "epoch": 3.806691449814126,
      "grad_norm": 13.895071029663086,
      "learning_rate": 0.000576,
      "loss": 6.1187,
      "step": 576
    },
    {
      "epoch": 3.8133002891367203,
      "grad_norm": 158.7949981689453,
      "learning_rate": 0.0005769999999999999,
      "loss": 5.8575,
      "step": 577
    },
    {
      "epoch": 3.8199091284593143,
      "grad_norm": 105.2989501953125,
      "learning_rate": 0.000578,
      "loss": 3.3259,
      "step": 578
    },
    {
      "epoch": 3.8265179677819083,
      "grad_norm": 211.50418090820312,
      "learning_rate": 0.000579,
      "loss": 12.4207,
      "step": 579
    },
    {
      "epoch": 3.8331268071045024,
      "grad_norm": 50.786808013916016,
      "learning_rate": 0.00058,
      "loss": 5.8676,
      "step": 580
    },
    {
      "epoch": 3.839735646427096,
      "grad_norm": 7.312044620513916,
      "learning_rate": 0.0005809999999999999,
      "loss": 4.5644,
      "step": 581
    },
    {
      "epoch": 3.8463444857496905,
      "grad_norm": 216.4562225341797,
      "learning_rate": 0.0005819999999999999,
      "loss": 13.1615,
      "step": 582
    },
    {
      "epoch": 3.852953325072284,
      "grad_norm": 307.2994384765625,
      "learning_rate": 0.000583,
      "loss": 16.8541,
      "step": 583
    },
    {
      "epoch": 3.859562164394878,
      "grad_norm": 165.61575317382812,
      "learning_rate": 0.000584,
      "loss": 5.9503,
      "step": 584
    },
    {
      "epoch": 3.866171003717472,
      "grad_norm": 56.04490280151367,
      "learning_rate": 0.000585,
      "loss": 5.2772,
      "step": 585
    },
    {
      "epoch": 3.872779843040066,
      "grad_norm": 102.96892547607422,
      "learning_rate": 0.0005859999999999999,
      "loss": 6.5245,
      "step": 586
    },
    {
      "epoch": 3.87938868236266,
      "grad_norm": 193.44180297851562,
      "learning_rate": 0.000587,
      "loss": 6.0809,
      "step": 587
    },
    {
      "epoch": 3.885997521685254,
      "grad_norm": 11.285221099853516,
      "learning_rate": 0.000588,
      "loss": 6.5512,
      "step": 588
    },
    {
      "epoch": 3.892606361007848,
      "grad_norm": 182.35568237304688,
      "learning_rate": 0.000589,
      "loss": 7.7364,
      "step": 589
    },
    {
      "epoch": 3.899215200330442,
      "grad_norm": 60.66985321044922,
      "learning_rate": 0.00059,
      "loss": 5.2905,
      "step": 590
    },
    {
      "epoch": 3.905824039653036,
      "grad_norm": 118.63106536865234,
      "learning_rate": 0.0005909999999999999,
      "loss": 4.9741,
      "step": 591
    },
    {
      "epoch": 3.91243287897563,
      "grad_norm": 28.758285522460938,
      "learning_rate": 0.000592,
      "loss": 3.2052,
      "step": 592
    },
    {
      "epoch": 3.919041718298224,
      "grad_norm": 183.57162475585938,
      "learning_rate": 0.000593,
      "loss": 4.2951,
      "step": 593
    },
    {
      "epoch": 3.925650557620818,
      "grad_norm": 125.52934265136719,
      "learning_rate": 0.000594,
      "loss": 4.5642,
      "step": 594
    },
    {
      "epoch": 3.9322593969434116,
      "grad_norm": 67.40715789794922,
      "learning_rate": 0.0005949999999999999,
      "loss": 4.8243,
      "step": 595
    },
    {
      "epoch": 3.9388682362660057,
      "grad_norm": 36.49899673461914,
      "learning_rate": 0.000596,
      "loss": 4.704,
      "step": 596
    },
    {
      "epoch": 3.9454770755885997,
      "grad_norm": 164.20819091796875,
      "learning_rate": 0.000597,
      "loss": 8.5522,
      "step": 597
    },
    {
      "epoch": 3.9520859149111938,
      "grad_norm": 87.21815490722656,
      "learning_rate": 0.000598,
      "loss": 8.2445,
      "step": 598
    },
    {
      "epoch": 3.958694754233788,
      "grad_norm": 5.095037460327148,
      "learning_rate": 0.000599,
      "loss": 4.5887,
      "step": 599
    },
    {
      "epoch": 3.9653035935563814,
      "grad_norm": 5.0263519287109375,
      "learning_rate": 0.0006,
      "loss": 6.2786,
      "step": 600
    },
    {
      "epoch": 3.971912432878976,
      "grad_norm": 18.727718353271484,
      "learning_rate": 0.000601,
      "loss": 6.1864,
      "step": 601
    },
    {
      "epoch": 3.9785212722015695,
      "grad_norm": 21.520793914794922,
      "learning_rate": 0.000602,
      "loss": 4.2364,
      "step": 602
    },
    {
      "epoch": 3.9851301115241635,
      "grad_norm": 21.47562599182129,
      "learning_rate": 0.000603,
      "loss": 7.0364,
      "step": 603
    },
    {
      "epoch": 3.9917389508467576,
      "grad_norm": 177.2994842529297,
      "learning_rate": 0.000604,
      "loss": 8.7799,
      "step": 604
    },
    {
      "epoch": 3.9983477901693516,
      "grad_norm": 191.97567749023438,
      "learning_rate": 0.000605,
      "loss": 7.5427,
      "step": 605
    },
    {
      "epoch": 3.9983477901693516,
      "eval_validation_error_bar": 0.05771535275470437,
      "eval_validation_loss": 7.732911586761475,
      "eval_validation_pearsonr": 0.49722504078647517,
      "eval_validation_rmse": 2.780811309814453,
      "eval_validation_runtime": 33.2543,
      "eval_validation_samples_per_second": 6.104,
      "eval_validation_spearman": 0.475807631618656,
      "eval_validation_steps_per_second": 6.104,
      "step": 605
    },
    {
      "epoch": 3.9983477901693516,
      "eval_test_error_bar": 0.05542059121285539,
      "eval_test_loss": 12.040358543395996,
      "eval_test_pearsonr": 0.11055763301940756,
      "eval_test_rmse": 3.4699220657348633,
      "eval_test_runtime": 37.6942,
      "eval_test_samples_per_second": 8.649,
      "eval_test_spearman": 0.0726834699512814,
      "eval_test_steps_per_second": 8.649,
      "step": 605
    },
    {
      "epoch": 4.004956629491946,
      "grad_norm": 37.68603515625,
      "learning_rate": 0.000606,
      "loss": 6.6207,
      "step": 606
    },
    {
      "epoch": 4.011565468814539,
      "grad_norm": 39.3463249206543,
      "learning_rate": 0.000607,
      "loss": 7.2842,
      "step": 607
    },
    {
      "epoch": 4.018174308137134,
      "grad_norm": 117.59439086914062,
      "learning_rate": 0.000608,
      "loss": 7.8116,
      "step": 608
    },
    {
      "epoch": 4.024783147459727,
      "grad_norm": 130.8961944580078,
      "learning_rate": 0.000609,
      "loss": 5.3789,
      "step": 609
    },
    {
      "epoch": 4.031391986782321,
      "grad_norm": 10.91856575012207,
      "learning_rate": 0.00061,
      "loss": 5.5826,
      "step": 610
    },
    {
      "epoch": 4.038000826104915,
      "grad_norm": 44.102027893066406,
      "learning_rate": 0.000611,
      "loss": 4.0433,
      "step": 611
    },
    {
      "epoch": 4.044609665427509,
      "grad_norm": 135.11041259765625,
      "learning_rate": 0.000612,
      "loss": 5.1837,
      "step": 612
    },
    {
      "epoch": 4.0512185047501035,
      "grad_norm": 96.05941772460938,
      "learning_rate": 0.000613,
      "loss": 4.3348,
      "step": 613
    },
    {
      "epoch": 4.057827344072697,
      "grad_norm": 56.015777587890625,
      "learning_rate": 0.000614,
      "loss": 3.8241,
      "step": 614
    },
    {
      "epoch": 4.064436183395292,
      "grad_norm": 21.730579376220703,
      "learning_rate": 0.000615,
      "loss": 2.5656,
      "step": 615
    },
    {
      "epoch": 4.071045022717885,
      "grad_norm": 78.75362396240234,
      "learning_rate": 0.000616,
      "loss": 4.59,
      "step": 616
    },
    {
      "epoch": 4.077653862040479,
      "grad_norm": 144.7335662841797,
      "learning_rate": 0.000617,
      "loss": 7.6922,
      "step": 617
    },
    {
      "epoch": 4.084262701363073,
      "grad_norm": 11.802205085754395,
      "learning_rate": 0.0006180000000000001,
      "loss": 4.5858,
      "step": 618
    },
    {
      "epoch": 4.090871540685667,
      "grad_norm": 33.23372268676758,
      "learning_rate": 0.000619,
      "loss": 2.3461,
      "step": 619
    },
    {
      "epoch": 4.097480380008261,
      "grad_norm": 75.33218383789062,
      "learning_rate": 0.00062,
      "loss": 4.2922,
      "step": 620
    },
    {
      "epoch": 4.104089219330855,
      "grad_norm": 54.424442291259766,
      "learning_rate": 0.000621,
      "loss": 5.1215,
      "step": 621
    },
    {
      "epoch": 4.110698058653449,
      "grad_norm": 101.4398422241211,
      "learning_rate": 0.000622,
      "loss": 4.1378,
      "step": 622
    },
    {
      "epoch": 4.117306897976043,
      "grad_norm": 70.322509765625,
      "learning_rate": 0.000623,
      "loss": 6.1008,
      "step": 623
    },
    {
      "epoch": 4.123915737298637,
      "grad_norm": 78.27103424072266,
      "learning_rate": 0.000624,
      "loss": 2.7425,
      "step": 624
    },
    {
      "epoch": 4.130524576621231,
      "grad_norm": 13.891812324523926,
      "learning_rate": 0.000625,
      "loss": 3.6421,
      "step": 625
    },
    {
      "epoch": 4.137133415943825,
      "grad_norm": 6.83100700378418,
      "learning_rate": 0.000626,
      "loss": 4.399,
      "step": 626
    },
    {
      "epoch": 4.143742255266419,
      "grad_norm": 128.96658325195312,
      "learning_rate": 0.0006270000000000001,
      "loss": 7.8277,
      "step": 627
    },
    {
      "epoch": 4.150351094589013,
      "grad_norm": 88.20109558105469,
      "learning_rate": 0.000628,
      "loss": 3.5051,
      "step": 628
    },
    {
      "epoch": 4.156959933911606,
      "grad_norm": 42.332496643066406,
      "learning_rate": 0.000629,
      "loss": 3.3361,
      "step": 629
    },
    {
      "epoch": 4.163568773234201,
      "grad_norm": 26.276145935058594,
      "learning_rate": 0.00063,
      "loss": 2.7317,
      "step": 630
    },
    {
      "epoch": 4.170177612556794,
      "grad_norm": 86.90495300292969,
      "learning_rate": 0.000631,
      "loss": 6.1216,
      "step": 631
    },
    {
      "epoch": 4.176786451879389,
      "grad_norm": 93.02796173095703,
      "learning_rate": 0.000632,
      "loss": 4.8438,
      "step": 632
    },
    {
      "epoch": 4.1833952912019825,
      "grad_norm": 87.1019515991211,
      "learning_rate": 0.000633,
      "loss": 3.2298,
      "step": 633
    },
    {
      "epoch": 4.190004130524577,
      "grad_norm": 33.4607048034668,
      "learning_rate": 0.000634,
      "loss": 7.1804,
      "step": 634
    },
    {
      "epoch": 4.196612969847171,
      "grad_norm": 22.625991821289062,
      "learning_rate": 0.000635,
      "loss": 4.7298,
      "step": 635
    },
    {
      "epoch": 4.203221809169764,
      "grad_norm": 17.11294174194336,
      "learning_rate": 0.0006360000000000001,
      "loss": 5.2016,
      "step": 636
    },
    {
      "epoch": 4.209830648492359,
      "grad_norm": 127.39652252197266,
      "learning_rate": 0.000637,
      "loss": 6.7959,
      "step": 637
    },
    {
      "epoch": 4.216439487814952,
      "grad_norm": 222.14620971679688,
      "learning_rate": 0.000638,
      "loss": 9.6864,
      "step": 638
    },
    {
      "epoch": 4.223048327137547,
      "grad_norm": 252.9322052001953,
      "learning_rate": 0.000639,
      "loss": 12.0405,
      "step": 639
    },
    {
      "epoch": 4.22965716646014,
      "grad_norm": 216.6896209716797,
      "learning_rate": 0.00064,
      "loss": 10.1918,
      "step": 640
    },
    {
      "epoch": 4.236266005782735,
      "grad_norm": 98.3722152709961,
      "learning_rate": 0.0006410000000000001,
      "loss": 4.9789,
      "step": 641
    },
    {
      "epoch": 4.242874845105328,
      "grad_norm": 13.952232360839844,
      "learning_rate": 0.000642,
      "loss": 5.2601,
      "step": 642
    },
    {
      "epoch": 4.249483684427922,
      "grad_norm": 128.04580688476562,
      "learning_rate": 0.000643,
      "loss": 4.7571,
      "step": 643
    },
    {
      "epoch": 4.2560925237505165,
      "grad_norm": 74.18679809570312,
      "learning_rate": 0.000644,
      "loss": 2.1302,
      "step": 644
    },
    {
      "epoch": 4.26270136307311,
      "grad_norm": 42.7882194519043,
      "learning_rate": 0.0006450000000000001,
      "loss": 3.477,
      "step": 645
    },
    {
      "epoch": 4.269310202395705,
      "grad_norm": 17.40290069580078,
      "learning_rate": 0.000646,
      "loss": 1.5144,
      "step": 646
    },
    {
      "epoch": 4.275919041718298,
      "grad_norm": 112.2750244140625,
      "learning_rate": 0.000647,
      "loss": 2.802,
      "step": 647
    },
    {
      "epoch": 4.282527881040892,
      "grad_norm": 248.0504608154297,
      "learning_rate": 0.000648,
      "loss": 11.0047,
      "step": 648
    },
    {
      "epoch": 4.289136720363486,
      "grad_norm": 122.56689453125,
      "learning_rate": 0.0006490000000000001,
      "loss": 6.1182,
      "step": 649
    },
    {
      "epoch": 4.29574555968608,
      "grad_norm": 51.237083435058594,
      "learning_rate": 0.0006500000000000001,
      "loss": 4.999,
      "step": 650
    },
    {
      "epoch": 4.302354399008674,
      "grad_norm": 60.45039367675781,
      "learning_rate": 0.000651,
      "loss": 3.7482,
      "step": 651
    },
    {
      "epoch": 4.308963238331268,
      "grad_norm": 29.045574188232422,
      "learning_rate": 0.000652,
      "loss": 4.8732,
      "step": 652
    },
    {
      "epoch": 4.315572077653862,
      "grad_norm": 107.4975814819336,
      "learning_rate": 0.000653,
      "loss": 3.8421,
      "step": 653
    },
    {
      "epoch": 4.322180916976456,
      "grad_norm": 225.22291564941406,
      "learning_rate": 0.0006540000000000001,
      "loss": 10.63,
      "step": 654
    },
    {
      "epoch": 4.32878975629905,
      "grad_norm": 136.6137237548828,
      "learning_rate": 0.0006550000000000001,
      "loss": 3.5938,
      "step": 655
    },
    {
      "epoch": 4.335398595621644,
      "grad_norm": 82.62619018554688,
      "learning_rate": 0.000656,
      "loss": 3.908,
      "step": 656
    },
    {
      "epoch": 4.342007434944238,
      "grad_norm": 113.94631958007812,
      "learning_rate": 0.000657,
      "loss": 4.1008,
      "step": 657
    },
    {
      "epoch": 4.348616274266832,
      "grad_norm": 84.16854095458984,
      "learning_rate": 0.0006580000000000001,
      "loss": 2.9398,
      "step": 658
    },
    {
      "epoch": 4.355225113589426,
      "grad_norm": 28.13888168334961,
      "learning_rate": 0.0006590000000000001,
      "loss": 4.3605,
      "step": 659
    },
    {
      "epoch": 4.36183395291202,
      "grad_norm": 90.24414825439453,
      "learning_rate": 0.00066,
      "loss": 2.9949,
      "step": 660
    },
    {
      "epoch": 4.368442792234614,
      "grad_norm": 26.75946807861328,
      "learning_rate": 0.000661,
      "loss": 5.0103,
      "step": 661
    },
    {
      "epoch": 4.375051631557207,
      "grad_norm": 76.18446350097656,
      "learning_rate": 0.000662,
      "loss": 3.4511,
      "step": 662
    },
    {
      "epoch": 4.381660470879802,
      "grad_norm": 58.17279052734375,
      "learning_rate": 0.0006630000000000001,
      "loss": 7.5681,
      "step": 663
    },
    {
      "epoch": 4.3882693102023955,
      "grad_norm": 6.276136875152588,
      "learning_rate": 0.0006640000000000001,
      "loss": 3.5368,
      "step": 664
    },
    {
      "epoch": 4.39487814952499,
      "grad_norm": 41.674739837646484,
      "learning_rate": 0.000665,
      "loss": 3.0919,
      "step": 665
    },
    {
      "epoch": 4.401486988847584,
      "grad_norm": 56.1649284362793,
      "learning_rate": 0.000666,
      "loss": 1.6025,
      "step": 666
    },
    {
      "epoch": 4.408095828170177,
      "grad_norm": 51.833580017089844,
      "learning_rate": 0.0006670000000000001,
      "loss": 6.0147,
      "step": 667
    },
    {
      "epoch": 4.414704667492772,
      "grad_norm": 95.6365737915039,
      "learning_rate": 0.0006680000000000001,
      "loss": 4.6125,
      "step": 668
    },
    {
      "epoch": 4.421313506815365,
      "grad_norm": 124.09624481201172,
      "learning_rate": 0.0006690000000000001,
      "loss": 5.074,
      "step": 669
    },
    {
      "epoch": 4.42792234613796,
      "grad_norm": 36.56533432006836,
      "learning_rate": 0.00067,
      "loss": 4.3059,
      "step": 670
    },
    {
      "epoch": 4.434531185460553,
      "grad_norm": 8.16177749633789,
      "learning_rate": 0.000671,
      "loss": 8.6202,
      "step": 671
    },
    {
      "epoch": 4.441140024783148,
      "grad_norm": 86.93869018554688,
      "learning_rate": 0.0006720000000000001,
      "loss": 3.4095,
      "step": 672
    },
    {
      "epoch": 4.447748864105741,
      "grad_norm": 84.60857391357422,
      "learning_rate": 0.0006730000000000001,
      "loss": 5.8659,
      "step": 673
    },
    {
      "epoch": 4.454357703428335,
      "grad_norm": 10.599235534667969,
      "learning_rate": 0.000674,
      "loss": 2.3252,
      "step": 674
    },
    {
      "epoch": 4.4609665427509295,
      "grad_norm": 31.7950439453125,
      "learning_rate": 0.000675,
      "loss": 6.11,
      "step": 675
    },
    {
      "epoch": 4.467575382073523,
      "grad_norm": 68.4195327758789,
      "learning_rate": 0.0006760000000000001,
      "loss": 2.1745,
      "step": 676
    },
    {
      "epoch": 4.474184221396118,
      "grad_norm": 12.80391788482666,
      "learning_rate": 0.0006770000000000001,
      "loss": 3.7019,
      "step": 677
    },
    {
      "epoch": 4.480793060718711,
      "grad_norm": 107.42035675048828,
      "learning_rate": 0.0006780000000000001,
      "loss": 4.3342,
      "step": 678
    },
    {
      "epoch": 4.487401900041306,
      "grad_norm": 84.01542663574219,
      "learning_rate": 0.000679,
      "loss": 2.5917,
      "step": 679
    },
    {
      "epoch": 4.494010739363899,
      "grad_norm": 145.25277709960938,
      "learning_rate": 0.00068,
      "loss": 6.4369,
      "step": 680
    },
    {
      "epoch": 4.500619578686493,
      "grad_norm": 137.34715270996094,
      "learning_rate": 0.0006810000000000001,
      "loss": 8.9755,
      "step": 681
    },
    {
      "epoch": 4.507228418009087,
      "grad_norm": 133.28285217285156,
      "learning_rate": 0.0006820000000000001,
      "loss": 7.5703,
      "step": 682
    },
    {
      "epoch": 4.513837257331681,
      "grad_norm": 327.03216552734375,
      "learning_rate": 0.000683,
      "loss": 14.292,
      "step": 683
    },
    {
      "epoch": 4.520446096654275,
      "grad_norm": 240.5093994140625,
      "learning_rate": 0.000684,
      "loss": 10.7864,
      "step": 684
    },
    {
      "epoch": 4.527054935976869,
      "grad_norm": 205.562744140625,
      "learning_rate": 0.0006850000000000001,
      "loss": 7.0169,
      "step": 685
    },
    {
      "epoch": 4.533663775299463,
      "grad_norm": 5.131333827972412,
      "learning_rate": 0.0006860000000000001,
      "loss": 2.194,
      "step": 686
    },
    {
      "epoch": 4.540272614622057,
      "grad_norm": 19.146106719970703,
      "learning_rate": 0.0006870000000000001,
      "loss": 6.9926,
      "step": 687
    },
    {
      "epoch": 4.546881453944651,
      "grad_norm": 23.047828674316406,
      "learning_rate": 0.0006879999999999999,
      "loss": 5.8203,
      "step": 688
    },
    {
      "epoch": 4.553490293267245,
      "grad_norm": 16.220359802246094,
      "learning_rate": 0.0006889999999999999,
      "loss": 2.6856,
      "step": 689
    },
    {
      "epoch": 4.560099132589839,
      "grad_norm": 7.595759391784668,
      "learning_rate": 0.00069,
      "loss": 6.0434,
      "step": 690
    },
    {
      "epoch": 4.566707971912432,
      "grad_norm": 88.31781005859375,
      "learning_rate": 0.000691,
      "loss": 4.4077,
      "step": 691
    },
    {
      "epoch": 4.573316811235027,
      "grad_norm": 118.93551635742188,
      "learning_rate": 0.000692,
      "loss": 6.8546,
      "step": 692
    },
    {
      "epoch": 4.5799256505576205,
      "grad_norm": 93.43450927734375,
      "learning_rate": 0.0006929999999999999,
      "loss": 9.3665,
      "step": 693
    },
    {
      "epoch": 4.586534489880215,
      "grad_norm": 4.3541259765625,
      "learning_rate": 0.000694,
      "loss": 2.8779,
      "step": 694
    },
    {
      "epoch": 4.5931433292028085,
      "grad_norm": 20.88705062866211,
      "learning_rate": 0.000695,
      "loss": 3.3253,
      "step": 695
    },
    {
      "epoch": 4.599752168525403,
      "grad_norm": 107.4638671875,
      "learning_rate": 0.000696,
      "loss": 5.9936,
      "step": 696
    },
    {
      "epoch": 4.606361007847997,
      "grad_norm": 91.85894012451172,
      "learning_rate": 0.0006969999999999999,
      "loss": 3.4852,
      "step": 697
    },
    {
      "epoch": 4.612969847170591,
      "grad_norm": 74.00260925292969,
      "learning_rate": 0.0006979999999999999,
      "loss": 4.0279,
      "step": 698
    },
    {
      "epoch": 4.619578686493185,
      "grad_norm": 11.431964874267578,
      "learning_rate": 0.000699,
      "loss": 3.714,
      "step": 699
    },
    {
      "epoch": 4.626187525815778,
      "grad_norm": 118.62674713134766,
      "learning_rate": 0.0007,
      "loss": 3.7368,
      "step": 700
    },
    {
      "epoch": 4.632796365138373,
      "grad_norm": 62.008995056152344,
      "learning_rate": 0.000701,
      "loss": 3.611,
      "step": 701
    },
    {
      "epoch": 4.639405204460966,
      "grad_norm": 9.703970909118652,
      "learning_rate": 0.0007019999999999999,
      "loss": 1.7879,
      "step": 702
    },
    {
      "epoch": 4.646014043783561,
      "grad_norm": 74.27837371826172,
      "learning_rate": 0.000703,
      "loss": 3.7972,
      "step": 703
    },
    {
      "epoch": 4.6526228831061545,
      "grad_norm": 17.74338722229004,
      "learning_rate": 0.000704,
      "loss": 1.9683,
      "step": 704
    },
    {
      "epoch": 4.659231722428748,
      "grad_norm": 150.97312927246094,
      "learning_rate": 0.000705,
      "loss": 5.7115,
      "step": 705
    },
    {
      "epoch": 4.6658405617513425,
      "grad_norm": 114.64795684814453,
      "learning_rate": 0.0007059999999999999,
      "loss": 2.6624,
      "step": 706
    },
    {
      "epoch": 4.672449401073936,
      "grad_norm": 37.75380325317383,
      "learning_rate": 0.000707,
      "loss": 5.715,
      "step": 707
    },
    {
      "epoch": 4.679058240396531,
      "grad_norm": 162.1812286376953,
      "learning_rate": 0.000708,
      "loss": 8.8834,
      "step": 708
    },
    {
      "epoch": 4.685667079719124,
      "grad_norm": 149.3885955810547,
      "learning_rate": 0.000709,
      "loss": 5.6197,
      "step": 709
    },
    {
      "epoch": 4.692275919041718,
      "grad_norm": 51.86233901977539,
      "learning_rate": 0.00071,
      "loss": 4.3685,
      "step": 710
    },
    {
      "epoch": 4.698884758364312,
      "grad_norm": 88.95378112792969,
      "learning_rate": 0.0007109999999999999,
      "loss": 6.1844,
      "step": 711
    },
    {
      "epoch": 4.705493597686906,
      "grad_norm": 106.92667388916016,
      "learning_rate": 0.000712,
      "loss": 4.0348,
      "step": 712
    },
    {
      "epoch": 4.7121024370095,
      "grad_norm": 32.11286163330078,
      "learning_rate": 0.000713,
      "loss": 8.4242,
      "step": 713
    },
    {
      "epoch": 4.718711276332094,
      "grad_norm": 60.398841857910156,
      "learning_rate": 0.000714,
      "loss": 2.9218,
      "step": 714
    },
    {
      "epoch": 4.7253201156546885,
      "grad_norm": 49.76536178588867,
      "learning_rate": 0.000715,
      "loss": 6.4843,
      "step": 715
    },
    {
      "epoch": 4.731928954977282,
      "grad_norm": 33.4669303894043,
      "learning_rate": 0.000716,
      "loss": 5.8632,
      "step": 716
    },
    {
      "epoch": 4.7385377942998765,
      "grad_norm": 217.9408416748047,
      "learning_rate": 0.000717,
      "loss": 8.415,
      "step": 717
    },
    {
      "epoch": 4.74514663362247,
      "grad_norm": 191.36842346191406,
      "learning_rate": 0.000718,
      "loss": 9.9348,
      "step": 718
    },
    {
      "epoch": 4.751755472945064,
      "grad_norm": 180.8011016845703,
      "learning_rate": 0.000719,
      "loss": 10.0427,
      "step": 719
    },
    {
      "epoch": 4.758364312267658,
      "grad_norm": 167.24241638183594,
      "learning_rate": 0.0007199999999999999,
      "loss": 7.2057,
      "step": 720
    },
    {
      "epoch": 4.764973151590252,
      "grad_norm": 8.389351844787598,
      "learning_rate": 0.000721,
      "loss": 6.1993,
      "step": 721
    },
    {
      "epoch": 4.771581990912846,
      "grad_norm": 50.371177673339844,
      "learning_rate": 0.000722,
      "loss": 3.5019,
      "step": 722
    },
    {
      "epoch": 4.77819083023544,
      "grad_norm": 68.14373779296875,
      "learning_rate": 0.000723,
      "loss": 4.862,
      "step": 723
    },
    {
      "epoch": 4.7847996695580335,
      "grad_norm": 3.6521525382995605,
      "learning_rate": 0.000724,
      "loss": 2.4384,
      "step": 724
    },
    {
      "epoch": 4.791408508880628,
      "grad_norm": 76.58750915527344,
      "learning_rate": 0.000725,
      "loss": 4.2732,
      "step": 725
    },
    {
      "epoch": 4.798017348203222,
      "grad_norm": 65.09151458740234,
      "learning_rate": 0.000726,
      "loss": 8.2256,
      "step": 726
    },
    {
      "epoch": 4.804626187525816,
      "grad_norm": 210.4722137451172,
      "learning_rate": 0.000727,
      "loss": 6.2266,
      "step": 727
    },
    {
      "epoch": 4.81123502684841,
      "grad_norm": 167.74728393554688,
      "learning_rate": 0.000728,
      "loss": 12.2013,
      "step": 728
    },
    {
      "epoch": 4.817843866171003,
      "grad_norm": 157.31692504882812,
      "learning_rate": 0.000729,
      "loss": 4.239,
      "step": 729
    },
    {
      "epoch": 4.824452705493598,
      "grad_norm": 4.608479976654053,
      "learning_rate": 0.00073,
      "loss": 2.6906,
      "step": 730
    },
    {
      "epoch": 4.831061544816191,
      "grad_norm": 391.6412658691406,
      "learning_rate": 0.000731,
      "loss": 18.2584,
      "step": 731
    },
    {
      "epoch": 4.837670384138786,
      "grad_norm": 494.3197021484375,
      "learning_rate": 0.000732,
      "loss": 25.7741,
      "step": 732
    },
    {
      "epoch": 4.844279223461379,
      "grad_norm": 523.5556030273438,
      "learning_rate": 0.000733,
      "loss": 26.6938,
      "step": 733
    },
    {
      "epoch": 4.850888062783974,
      "grad_norm": 269.9471130371094,
      "learning_rate": 0.000734,
      "loss": 11.3209,
      "step": 734
    },
    {
      "epoch": 4.8574969021065675,
      "grad_norm": 230.1205596923828,
      "learning_rate": 0.000735,
      "loss": 10.0574,
      "step": 735
    },
    {
      "epoch": 4.864105741429162,
      "grad_norm": 60.8138313293457,
      "learning_rate": 0.000736,
      "loss": 4.0379,
      "step": 736
    },
    {
      "epoch": 4.870714580751756,
      "grad_norm": 221.9495849609375,
      "learning_rate": 0.000737,
      "loss": 9.1738,
      "step": 737
    },
    {
      "epoch": 4.877323420074349,
      "grad_norm": 286.7978820800781,
      "learning_rate": 0.000738,
      "loss": 14.4174,
      "step": 738
    },
    {
      "epoch": 4.883932259396944,
      "grad_norm": 380.4751281738281,
      "learning_rate": 0.000739,
      "loss": 19.1913,
      "step": 739
    },
    {
      "epoch": 4.890541098719537,
      "grad_norm": 359.4039001464844,
      "learning_rate": 0.00074,
      "loss": 17.5288,
      "step": 740
    },
    {
      "epoch": 4.897149938042132,
      "grad_norm": 274.6666564941406,
      "learning_rate": 0.000741,
      "loss": 13.1161,
      "step": 741
    },
    {
      "epoch": 4.903758777364725,
      "grad_norm": 182.0517578125,
      "learning_rate": 0.000742,
      "loss": 6.4548,
      "step": 742
    },
    {
      "epoch": 4.910367616687319,
      "grad_norm": 63.48133087158203,
      "learning_rate": 0.0007430000000000001,
      "loss": 3.5787,
      "step": 743
    },
    {
      "epoch": 4.916976456009913,
      "grad_norm": 139.03985595703125,
      "learning_rate": 0.000744,
      "loss": 4.6811,
      "step": 744
    },
    {
      "epoch": 4.923585295332507,
      "grad_norm": 112.92639923095703,
      "learning_rate": 0.000745,
      "loss": 5.2508,
      "step": 745
    },
    {
      "epoch": 4.9301941346551015,
      "grad_norm": 192.24110412597656,
      "learning_rate": 0.000746,
      "loss": 7.7577,
      "step": 746
    },
    {
      "epoch": 4.936802973977695,
      "grad_norm": 235.12265014648438,
      "learning_rate": 0.000747,
      "loss": 9.4033,
      "step": 747
    },
    {
      "epoch": 4.943411813300289,
      "grad_norm": 157.44265747070312,
      "learning_rate": 0.000748,
      "loss": 4.6864,
      "step": 748
    },
    {
      "epoch": 4.950020652622883,
      "grad_norm": 103.08336639404297,
      "learning_rate": 0.000749,
      "loss": 5.1202,
      "step": 749
    },
    {
      "epoch": 4.956629491945477,
      "grad_norm": 18.277055740356445,
      "learning_rate": 0.00075,
      "loss": 4.7427,
      "step": 750
    },
    {
      "epoch": 4.963238331268071,
      "grad_norm": 74.77754211425781,
      "learning_rate": 0.000751,
      "loss": 9.1072,
      "step": 751
    },
    {
      "epoch": 4.969847170590665,
      "grad_norm": 224.72454833984375,
      "learning_rate": 0.0007520000000000001,
      "loss": 10.5888,
      "step": 752
    },
    {
      "epoch": 4.976456009913259,
      "grad_norm": 238.795166015625,
      "learning_rate": 0.000753,
      "loss": 12.2463,
      "step": 753
    },
    {
      "epoch": 4.983064849235853,
      "grad_norm": 193.5296173095703,
      "learning_rate": 0.000754,
      "loss": 8.8512,
      "step": 754
    },
    {
      "epoch": 4.989673688558447,
      "grad_norm": 126.14791107177734,
      "learning_rate": 0.000755,
      "loss": 5.5021,
      "step": 755
    },
    {
      "epoch": 4.996282527881041,
      "grad_norm": 68.93254089355469,
      "learning_rate": 0.000756,
      "loss": 4.9627,
      "step": 756
    },
    {
      "epoch": 4.996282527881041,
      "eval_validation_error_bar": 0.06423294738262172,
      "eval_validation_loss": 9.095311164855957,
      "eval_validation_pearsonr": 0.4391727989903964,
      "eval_validation_rmse": 3.015843391418457,
      "eval_validation_runtime": 33.1531,
      "eval_validation_samples_per_second": 6.123,
      "eval_validation_spearman": 0.34217759288621685,
      "eval_validation_steps_per_second": 6.123,
      "step": 756
    },
    {
      "epoch": 4.996282527881041,
      "eval_test_error_bar": 0.05521722338015952,
      "eval_test_loss": 12.724675178527832,
      "eval_test_pearsonr": 0.127926840876058,
      "eval_test_rmse": 3.567166328430176,
      "eval_test_runtime": 38.5435,
      "eval_test_samples_per_second": 8.458,
      "eval_test_spearman": 0.1006390636773221,
      "eval_test_steps_per_second": 8.458,
      "step": 756
    },
    {
      "epoch": 5.002891367203635,
      "grad_norm": 33.46492004394531,
      "learning_rate": 0.000757,
      "loss": 4.6387,
      "step": 757
    },
    {
      "epoch": 5.009500206526229,
      "grad_norm": 67.30693054199219,
      "learning_rate": 0.000758,
      "loss": 4.8373,
      "step": 758
    },
    {
      "epoch": 5.016109045848823,
      "grad_norm": 50.51084518432617,
      "learning_rate": 0.000759,
      "loss": 2.7714,
      "step": 759
    },
    {
      "epoch": 5.022717885171417,
      "grad_norm": 44.97972106933594,
      "learning_rate": 0.00076,
      "loss": 3.2673,
      "step": 760
    },
    {
      "epoch": 5.029326724494011,
      "grad_norm": 3.535501718521118,
      "learning_rate": 0.0007610000000000001,
      "loss": 3.846,
      "step": 761
    },
    {
      "epoch": 5.035935563816604,
      "grad_norm": 24.67428207397461,
      "learning_rate": 0.000762,
      "loss": 2.9416,
      "step": 762
    },
    {
      "epoch": 5.042544403139199,
      "grad_norm": 89.60038757324219,
      "learning_rate": 0.000763,
      "loss": 3.8645,
      "step": 763
    },
    {
      "epoch": 5.049153242461792,
      "grad_norm": 87.2399673461914,
      "learning_rate": 0.000764,
      "loss": 2.7656,
      "step": 764
    },
    {
      "epoch": 5.055762081784387,
      "grad_norm": 52.06605529785156,
      "learning_rate": 0.0007650000000000001,
      "loss": 4.1864,
      "step": 765
    },
    {
      "epoch": 5.0623709211069805,
      "grad_norm": 75.72177124023438,
      "learning_rate": 0.0007660000000000001,
      "loss": 3.9459,
      "step": 766
    },
    {
      "epoch": 5.068979760429575,
      "grad_norm": 39.0091667175293,
      "learning_rate": 0.000767,
      "loss": 6.2612,
      "step": 767
    },
    {
      "epoch": 5.075588599752169,
      "grad_norm": 28.482120513916016,
      "learning_rate": 0.000768,
      "loss": 3.1939,
      "step": 768
    },
    {
      "epoch": 5.082197439074762,
      "grad_norm": 42.28641128540039,
      "learning_rate": 0.000769,
      "loss": 3.379,
      "step": 769
    },
    {
      "epoch": 5.088806278397357,
      "grad_norm": 11.804248809814453,
      "learning_rate": 0.0007700000000000001,
      "loss": 4.7019,
      "step": 770
    },
    {
      "epoch": 5.09541511771995,
      "grad_norm": 19.005578994750977,
      "learning_rate": 0.000771,
      "loss": 1.5146,
      "step": 771
    },
    {
      "epoch": 5.102023957042545,
      "grad_norm": 6.424261093139648,
      "learning_rate": 0.000772,
      "loss": 0.9777,
      "step": 772
    },
    {
      "epoch": 5.108632796365138,
      "grad_norm": 115.45781707763672,
      "learning_rate": 0.000773,
      "loss": 4.3396,
      "step": 773
    },
    {
      "epoch": 5.115241635687732,
      "grad_norm": 8.487272262573242,
      "learning_rate": 0.0007740000000000001,
      "loss": 4.187,
      "step": 774
    },
    {
      "epoch": 5.121850475010326,
      "grad_norm": 118.63731384277344,
      "learning_rate": 0.0007750000000000001,
      "loss": 4.8294,
      "step": 775
    },
    {
      "epoch": 5.12845931433292,
      "grad_norm": 2.0179920196533203,
      "learning_rate": 0.000776,
      "loss": 4.8759,
      "step": 776
    },
    {
      "epoch": 5.1350681536555145,
      "grad_norm": 75.39197540283203,
      "learning_rate": 0.000777,
      "loss": 2.8287,
      "step": 777
    },
    {
      "epoch": 5.141676992978108,
      "grad_norm": 55.39189910888672,
      "learning_rate": 0.000778,
      "loss": 3.6833,
      "step": 778
    },
    {
      "epoch": 5.148285832300703,
      "grad_norm": 34.158382415771484,
      "learning_rate": 0.0007790000000000001,
      "loss": 3.9803,
      "step": 779
    },
    {
      "epoch": 5.154894671623296,
      "grad_norm": 78.95156860351562,
      "learning_rate": 0.0007800000000000001,
      "loss": 6.3931,
      "step": 780
    },
    {
      "epoch": 5.16150351094589,
      "grad_norm": 163.88504028320312,
      "learning_rate": 0.000781,
      "loss": 9.3462,
      "step": 781
    },
    {
      "epoch": 5.168112350268484,
      "grad_norm": 117.229736328125,
      "learning_rate": 0.000782,
      "loss": 6.341,
      "step": 782
    },
    {
      "epoch": 5.174721189591078,
      "grad_norm": 68.42190551757812,
      "learning_rate": 0.0007830000000000001,
      "loss": 3.7783,
      "step": 783
    },
    {
      "epoch": 5.181330028913672,
      "grad_norm": 4.654984951019287,
      "learning_rate": 0.0007840000000000001,
      "loss": 3.613,
      "step": 784
    },
    {
      "epoch": 5.187938868236266,
      "grad_norm": 60.25565719604492,
      "learning_rate": 0.000785,
      "loss": 4.0817,
      "step": 785
    },
    {
      "epoch": 5.1945477075588595,
      "grad_norm": 30.141868591308594,
      "learning_rate": 0.000786,
      "loss": 4.4086,
      "step": 786
    },
    {
      "epoch": 5.201156546881454,
      "grad_norm": 44.8531379699707,
      "learning_rate": 0.000787,
      "loss": 9.8146,
      "step": 787
    },
    {
      "epoch": 5.207765386204048,
      "grad_norm": 166.04736328125,
      "learning_rate": 0.0007880000000000001,
      "loss": 7.229,
      "step": 788
    },
    {
      "epoch": 5.214374225526642,
      "grad_norm": 95.33074188232422,
      "learning_rate": 0.0007890000000000001,
      "loss": 5.4519,
      "step": 789
    },
    {
      "epoch": 5.220983064849236,
      "grad_norm": 33.22720718383789,
      "learning_rate": 0.00079,
      "loss": 2.3342,
      "step": 790
    },
    {
      "epoch": 5.22759190417183,
      "grad_norm": 102.43479919433594,
      "learning_rate": 0.000791,
      "loss": 3.8928,
      "step": 791
    },
    {
      "epoch": 5.234200743494424,
      "grad_norm": 187.72018432617188,
      "learning_rate": 0.0007920000000000001,
      "loss": 8.7533,
      "step": 792
    },
    {
      "epoch": 5.240809582817017,
      "grad_norm": 175.56581115722656,
      "learning_rate": 0.0007930000000000001,
      "loss": 16.8157,
      "step": 793
    },
    {
      "epoch": 5.247418422139612,
      "grad_norm": 242.35858154296875,
      "learning_rate": 0.0007940000000000001,
      "loss": 13.3281,
      "step": 794
    },
    {
      "epoch": 5.2540272614622054,
      "grad_norm": 134.66783142089844,
      "learning_rate": 0.000795,
      "loss": 11.6703,
      "step": 795
    },
    {
      "epoch": 5.2606361007848,
      "grad_norm": 39.830814361572266,
      "learning_rate": 0.000796,
      "loss": 8.9849,
      "step": 796
    },
    {
      "epoch": 5.2672449401073935,
      "grad_norm": 14.703536033630371,
      "learning_rate": 0.0007970000000000001,
      "loss": 2.8076,
      "step": 797
    },
    {
      "epoch": 5.273853779429988,
      "grad_norm": 52.564231872558594,
      "learning_rate": 0.0007980000000000001,
      "loss": 6.9966,
      "step": 798
    },
    {
      "epoch": 5.280462618752582,
      "grad_norm": 190.53472900390625,
      "learning_rate": 0.000799,
      "loss": 9.8927,
      "step": 799
    },
    {
      "epoch": 5.287071458075175,
      "grad_norm": 136.38291931152344,
      "learning_rate": 0.0008,
      "loss": 6.3441,
      "step": 800
    },
    {
      "epoch": 5.29368029739777,
      "grad_norm": 70.43208312988281,
      "learning_rate": 0.0008010000000000001,
      "loss": 3.1484,
      "step": 801
    },
    {
      "epoch": 5.300289136720363,
      "grad_norm": 16.3372859954834,
      "learning_rate": 0.0008020000000000001,
      "loss": 4.5402,
      "step": 802
    },
    {
      "epoch": 5.306897976042958,
      "grad_norm": 43.05880355834961,
      "learning_rate": 0.0008030000000000001,
      "loss": 4.5109,
      "step": 803
    },
    {
      "epoch": 5.313506815365551,
      "grad_norm": 109.2075424194336,
      "learning_rate": 0.000804,
      "loss": 5.0194,
      "step": 804
    },
    {
      "epoch": 5.320115654688145,
      "grad_norm": 70.31262969970703,
      "learning_rate": 0.000805,
      "loss": 4.5016,
      "step": 805
    },
    {
      "epoch": 5.326724494010739,
      "grad_norm": 42.3292350769043,
      "learning_rate": 0.0008060000000000001,
      "loss": 3.8486,
      "step": 806
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 100.33328247070312,
      "learning_rate": 0.0008070000000000001,
      "loss": 5.6921,
      "step": 807
    },
    {
      "epoch": 5.3399421726559275,
      "grad_norm": 174.09217834472656,
      "learning_rate": 0.000808,
      "loss": 6.4134,
      "step": 808
    },
    {
      "epoch": 5.346551011978521,
      "grad_norm": 173.48739624023438,
      "learning_rate": 0.000809,
      "loss": 7.6733,
      "step": 809
    },
    {
      "epoch": 5.353159851301116,
      "grad_norm": 104.56182861328125,
      "learning_rate": 0.0008100000000000001,
      "loss": 2.6784,
      "step": 810
    },
    {
      "epoch": 5.359768690623709,
      "grad_norm": 38.42133331298828,
      "learning_rate": 0.0008110000000000001,
      "loss": 4.2699,
      "step": 811
    },
    {
      "epoch": 5.366377529946303,
      "grad_norm": 38.77778625488281,
      "learning_rate": 0.0008120000000000001,
      "loss": 2.7546,
      "step": 812
    },
    {
      "epoch": 5.372986369268897,
      "grad_norm": 125.1910629272461,
      "learning_rate": 0.0008129999999999999,
      "loss": 6.523,
      "step": 813
    },
    {
      "epoch": 5.379595208591491,
      "grad_norm": 189.14404296875,
      "learning_rate": 0.0008139999999999999,
      "loss": 6.1304,
      "step": 814
    },
    {
      "epoch": 5.386204047914085,
      "grad_norm": 138.18797302246094,
      "learning_rate": 0.000815,
      "loss": 6.5347,
      "step": 815
    },
    {
      "epoch": 5.392812887236679,
      "grad_norm": 62.82975769042969,
      "learning_rate": 0.000816,
      "loss": 4.7374,
      "step": 816
    },
    {
      "epoch": 5.399421726559273,
      "grad_norm": 26.853107452392578,
      "learning_rate": 0.000817,
      "loss": 2.7804,
      "step": 817
    },
    {
      "epoch": 5.406030565881867,
      "grad_norm": 51.48363494873047,
      "learning_rate": 0.0008179999999999999,
      "loss": 3.6352,
      "step": 818
    },
    {
      "epoch": 5.412639405204461,
      "grad_norm": 64.25698852539062,
      "learning_rate": 0.000819,
      "loss": 5.3432,
      "step": 819
    },
    {
      "epoch": 5.419248244527055,
      "grad_norm": 60.5214958190918,
      "learning_rate": 0.00082,
      "loss": 4.8619,
      "step": 820
    },
    {
      "epoch": 5.425857083849649,
      "grad_norm": 37.664093017578125,
      "learning_rate": 0.000821,
      "loss": 2.0261,
      "step": 821
    },
    {
      "epoch": 5.432465923172243,
      "grad_norm": 100.40505981445312,
      "learning_rate": 0.0008219999999999999,
      "loss": 5.9509,
      "step": 822
    },
    {
      "epoch": 5.439074762494837,
      "grad_norm": 34.89285659790039,
      "learning_rate": 0.000823,
      "loss": 6.4376,
      "step": 823
    },
    {
      "epoch": 5.44568360181743,
      "grad_norm": 139.66152954101562,
      "learning_rate": 0.000824,
      "loss": 6.2465,
      "step": 824
    },
    {
      "epoch": 5.452292441140025,
      "grad_norm": 13.33079719543457,
      "learning_rate": 0.000825,
      "loss": 2.7351,
      "step": 825
    },
    {
      "epoch": 5.4589012804626185,
      "grad_norm": 79.29930877685547,
      "learning_rate": 0.000826,
      "loss": 5.2765,
      "step": 826
    },
    {
      "epoch": 5.465510119785213,
      "grad_norm": 27.237552642822266,
      "learning_rate": 0.0008269999999999999,
      "loss": 1.4362,
      "step": 827
    },
    {
      "epoch": 5.4721189591078065,
      "grad_norm": 17.31821060180664,
      "learning_rate": 0.000828,
      "loss": 2.734,
      "step": 828
    },
    {
      "epoch": 5.478727798430401,
      "grad_norm": 27.28751564025879,
      "learning_rate": 0.000829,
      "loss": 3.2428,
      "step": 829
    },
    {
      "epoch": 5.485336637752995,
      "grad_norm": 2.1247894763946533,
      "learning_rate": 0.00083,
      "loss": 0.5273,
      "step": 830
    },
    {
      "epoch": 5.491945477075588,
      "grad_norm": 19.737104415893555,
      "learning_rate": 0.0008309999999999999,
      "loss": 2.4721,
      "step": 831
    },
    {
      "epoch": 5.498554316398183,
      "grad_norm": 55.946048736572266,
      "learning_rate": 0.000832,
      "loss": 5.1327,
      "step": 832
    },
    {
      "epoch": 5.505163155720776,
      "grad_norm": 8.289485931396484,
      "learning_rate": 0.000833,
      "loss": 2.2709,
      "step": 833
    },
    {
      "epoch": 5.511771995043371,
      "grad_norm": 16.975170135498047,
      "learning_rate": 0.000834,
      "loss": 2.2876,
      "step": 834
    },
    {
      "epoch": 5.518380834365964,
      "grad_norm": 58.9250373840332,
      "learning_rate": 0.000835,
      "loss": 3.3284,
      "step": 835
    },
    {
      "epoch": 5.524989673688559,
      "grad_norm": 18.9669189453125,
      "learning_rate": 0.0008359999999999999,
      "loss": 6.0758,
      "step": 836
    },
    {
      "epoch": 5.5315985130111525,
      "grad_norm": 47.9310188293457,
      "learning_rate": 0.000837,
      "loss": 1.322,
      "step": 837
    },
    {
      "epoch": 5.538207352333746,
      "grad_norm": 63.906639099121094,
      "learning_rate": 0.000838,
      "loss": 4.0007,
      "step": 838
    },
    {
      "epoch": 5.5448161916563405,
      "grad_norm": 51.48151397705078,
      "learning_rate": 0.000839,
      "loss": 3.2574,
      "step": 839
    },
    {
      "epoch": 5.551425030978934,
      "grad_norm": 67.85459899902344,
      "learning_rate": 0.00084,
      "loss": 2.6509,
      "step": 840
    },
    {
      "epoch": 5.558033870301529,
      "grad_norm": 12.226875305175781,
      "learning_rate": 0.000841,
      "loss": 2.5921,
      "step": 841
    },
    {
      "epoch": 5.564642709624122,
      "grad_norm": 107.71675872802734,
      "learning_rate": 0.000842,
      "loss": 11.0121,
      "step": 842
    },
    {
      "epoch": 5.571251548946716,
      "grad_norm": 33.98286819458008,
      "learning_rate": 0.000843,
      "loss": 1.6358,
      "step": 843
    },
    {
      "epoch": 5.57786038826931,
      "grad_norm": 109.7732162475586,
      "learning_rate": 0.000844,
      "loss": 6.2699,
      "step": 844
    },
    {
      "epoch": 5.584469227591904,
      "grad_norm": 93.36189270019531,
      "learning_rate": 0.0008449999999999999,
      "loss": 9.3531,
      "step": 845
    },
    {
      "epoch": 5.591078066914498,
      "grad_norm": 13.030424118041992,
      "learning_rate": 0.000846,
      "loss": 2.3331,
      "step": 846
    },
    {
      "epoch": 5.597686906237092,
      "grad_norm": 7.039696216583252,
      "learning_rate": 0.000847,
      "loss": 2.5029,
      "step": 847
    },
    {
      "epoch": 5.6042957455596865,
      "grad_norm": 16.95750617980957,
      "learning_rate": 0.000848,
      "loss": 4.7718,
      "step": 848
    },
    {
      "epoch": 5.61090458488228,
      "grad_norm": 91.78934478759766,
      "learning_rate": 0.000849,
      "loss": 7.5138,
      "step": 849
    },
    {
      "epoch": 5.617513424204874,
      "grad_norm": 50.56275177001953,
      "learning_rate": 0.00085,
      "loss": 2.5514,
      "step": 850
    },
    {
      "epoch": 5.624122263527468,
      "grad_norm": 60.14962387084961,
      "learning_rate": 0.000851,
      "loss": 3.8749,
      "step": 851
    },
    {
      "epoch": 5.630731102850062,
      "grad_norm": 79.91551208496094,
      "learning_rate": 0.000852,
      "loss": 3.6085,
      "step": 852
    },
    {
      "epoch": 5.637339942172656,
      "grad_norm": 85.37272644042969,
      "learning_rate": 0.000853,
      "loss": 3.0169,
      "step": 853
    },
    {
      "epoch": 5.64394878149525,
      "grad_norm": 52.44318389892578,
      "learning_rate": 0.000854,
      "loss": 6.024,
      "step": 854
    },
    {
      "epoch": 5.650557620817844,
      "grad_norm": 67.76637268066406,
      "learning_rate": 0.000855,
      "loss": 3.8689,
      "step": 855
    },
    {
      "epoch": 5.657166460140438,
      "grad_norm": 13.674332618713379,
      "learning_rate": 0.000856,
      "loss": 2.3717,
      "step": 856
    },
    {
      "epoch": 5.6637752994630315,
      "grad_norm": 96.03055572509766,
      "learning_rate": 0.000857,
      "loss": 5.7518,
      "step": 857
    },
    {
      "epoch": 5.670384138785626,
      "grad_norm": 135.6910858154297,
      "learning_rate": 0.000858,
      "loss": 6.074,
      "step": 858
    },
    {
      "epoch": 5.67699297810822,
      "grad_norm": 31.4376220703125,
      "learning_rate": 0.000859,
      "loss": 7.0857,
      "step": 859
    },
    {
      "epoch": 5.683601817430814,
      "grad_norm": 131.84719848632812,
      "learning_rate": 0.00086,
      "loss": 5.3336,
      "step": 860
    },
    {
      "epoch": 5.690210656753408,
      "grad_norm": 142.95323181152344,
      "learning_rate": 0.000861,
      "loss": 6.1148,
      "step": 861
    },
    {
      "epoch": 5.696819496076001,
      "grad_norm": 182.58609008789062,
      "learning_rate": 0.000862,
      "loss": 10.8011,
      "step": 862
    },
    {
      "epoch": 5.703428335398596,
      "grad_norm": 46.19826889038086,
      "learning_rate": 0.000863,
      "loss": 4.2055,
      "step": 863
    },
    {
      "epoch": 5.710037174721189,
      "grad_norm": 23.407611846923828,
      "learning_rate": 0.000864,
      "loss": 4.5781,
      "step": 864
    },
    {
      "epoch": 5.716646014043784,
      "grad_norm": 105.77986145019531,
      "learning_rate": 0.000865,
      "loss": 9.3933,
      "step": 865
    },
    {
      "epoch": 5.723254853366377,
      "grad_norm": 74.1632080078125,
      "learning_rate": 0.000866,
      "loss": 2.6881,
      "step": 866
    },
    {
      "epoch": 5.729863692688972,
      "grad_norm": 160.0705108642578,
      "learning_rate": 0.000867,
      "loss": 7.9161,
      "step": 867
    },
    {
      "epoch": 5.7364725320115655,
      "grad_norm": 206.95201110839844,
      "learning_rate": 0.0008680000000000001,
      "loss": 7.6649,
      "step": 868
    },
    {
      "epoch": 5.743081371334159,
      "grad_norm": 99.97379302978516,
      "learning_rate": 0.000869,
      "loss": 4.7132,
      "step": 869
    },
    {
      "epoch": 5.749690210656754,
      "grad_norm": 18.780210494995117,
      "learning_rate": 0.00087,
      "loss": 1.5115,
      "step": 870
    },
    {
      "epoch": 5.756299049979347,
      "grad_norm": 30.806049346923828,
      "learning_rate": 0.000871,
      "loss": 3.7474,
      "step": 871
    },
    {
      "epoch": 5.762907889301942,
      "grad_norm": 116.50027465820312,
      "learning_rate": 0.000872,
      "loss": 6.0453,
      "step": 872
    },
    {
      "epoch": 5.769516728624535,
      "grad_norm": 101.6515121459961,
      "learning_rate": 0.000873,
      "loss": 7.6563,
      "step": 873
    },
    {
      "epoch": 5.77612556794713,
      "grad_norm": 265.2171325683594,
      "learning_rate": 0.000874,
      "loss": 14.5438,
      "step": 874
    },
    {
      "epoch": 5.782734407269723,
      "grad_norm": 131.43734741210938,
      "learning_rate": 0.000875,
      "loss": 6.4715,
      "step": 875
    },
    {
      "epoch": 5.789343246592317,
      "grad_norm": 99.3335952758789,
      "learning_rate": 0.000876,
      "loss": 4.3633,
      "step": 876
    },
    {
      "epoch": 5.795952085914911,
      "grad_norm": 35.27339172363281,
      "learning_rate": 0.0008770000000000001,
      "loss": 3.3645,
      "step": 877
    },
    {
      "epoch": 5.802560925237505,
      "grad_norm": 53.56596755981445,
      "learning_rate": 0.000878,
      "loss": 2.967,
      "step": 878
    },
    {
      "epoch": 5.8091697645600995,
      "grad_norm": 126.60990905761719,
      "learning_rate": 0.000879,
      "loss": 5.773,
      "step": 879
    },
    {
      "epoch": 5.815778603882693,
      "grad_norm": 71.32536315917969,
      "learning_rate": 0.00088,
      "loss": 3.6156,
      "step": 880
    },
    {
      "epoch": 5.822387443205287,
      "grad_norm": 170.2926025390625,
      "learning_rate": 0.0008810000000000001,
      "loss": 9.1867,
      "step": 881
    },
    {
      "epoch": 5.828996282527881,
      "grad_norm": 210.60536193847656,
      "learning_rate": 0.000882,
      "loss": 12.192,
      "step": 882
    },
    {
      "epoch": 5.835605121850475,
      "grad_norm": 54.745452880859375,
      "learning_rate": 0.000883,
      "loss": 2.7722,
      "step": 883
    },
    {
      "epoch": 5.842213961173069,
      "grad_norm": 84.70426177978516,
      "learning_rate": 0.000884,
      "loss": 3.4262,
      "step": 884
    },
    {
      "epoch": 5.848822800495663,
      "grad_norm": 5.54773473739624,
      "learning_rate": 0.000885,
      "loss": 2.1384,
      "step": 885
    },
    {
      "epoch": 5.855431639818256,
      "grad_norm": 54.57033920288086,
      "learning_rate": 0.0008860000000000001,
      "loss": 2.7763,
      "step": 886
    },
    {
      "epoch": 5.862040479140851,
      "grad_norm": 146.54368591308594,
      "learning_rate": 0.000887,
      "loss": 5.8523,
      "step": 887
    },
    {
      "epoch": 5.8686493184634445,
      "grad_norm": 121.48627471923828,
      "learning_rate": 0.000888,
      "loss": 4.7346,
      "step": 888
    },
    {
      "epoch": 5.875258157786039,
      "grad_norm": 47.52821350097656,
      "learning_rate": 0.000889,
      "loss": 2.6451,
      "step": 889
    },
    {
      "epoch": 5.881866997108633,
      "grad_norm": 21.484004974365234,
      "learning_rate": 0.0008900000000000001,
      "loss": 1.5701,
      "step": 890
    },
    {
      "epoch": 5.888475836431227,
      "grad_norm": 63.035980224609375,
      "learning_rate": 0.0008910000000000001,
      "loss": 1.9819,
      "step": 891
    },
    {
      "epoch": 5.895084675753821,
      "grad_norm": 126.58589935302734,
      "learning_rate": 0.000892,
      "loss": 5.8598,
      "step": 892
    },
    {
      "epoch": 5.901693515076415,
      "grad_norm": 43.62687301635742,
      "learning_rate": 0.000893,
      "loss": 4.2676,
      "step": 893
    },
    {
      "epoch": 5.908302354399009,
      "grad_norm": 77.69255828857422,
      "learning_rate": 0.000894,
      "loss": 4.1885,
      "step": 894
    },
    {
      "epoch": 5.914911193721602,
      "grad_norm": 4.978867053985596,
      "learning_rate": 0.0008950000000000001,
      "loss": 2.6806,
      "step": 895
    },
    {
      "epoch": 5.921520033044197,
      "grad_norm": 46.016326904296875,
      "learning_rate": 0.000896,
      "loss": 4.8711,
      "step": 896
    },
    {
      "epoch": 5.92812887236679,
      "grad_norm": 85.99480438232422,
      "learning_rate": 0.000897,
      "loss": 3.6803,
      "step": 897
    },
    {
      "epoch": 5.934737711689385,
      "grad_norm": 91.05350494384766,
      "learning_rate": 0.000898,
      "loss": 10.5181,
      "step": 898
    },
    {
      "epoch": 5.9413465510119785,
      "grad_norm": 23.184419631958008,
      "learning_rate": 0.0008990000000000001,
      "loss": 5.0704,
      "step": 899
    },
    {
      "epoch": 5.947955390334572,
      "grad_norm": 104.3073501586914,
      "learning_rate": 0.0009000000000000001,
      "loss": 8.8218,
      "step": 900
    },
    {
      "epoch": 5.954564229657167,
      "grad_norm": 5.4575514793396,
      "learning_rate": 0.000901,
      "loss": 2.9042,
      "step": 901
    },
    {
      "epoch": 5.96117306897976,
      "grad_norm": 3.2263498306274414,
      "learning_rate": 0.000902,
      "loss": 4.702,
      "step": 902
    },
    {
      "epoch": 5.967781908302355,
      "grad_norm": 34.43785858154297,
      "learning_rate": 0.000903,
      "loss": 4.7828,
      "step": 903
    },
    {
      "epoch": 5.974390747624948,
      "grad_norm": 93.05067443847656,
      "learning_rate": 0.0009040000000000001,
      "loss": 3.6561,
      "step": 904
    },
    {
      "epoch": 5.980999586947542,
      "grad_norm": 61.1832160949707,
      "learning_rate": 0.0009050000000000001,
      "loss": 1.8258,
      "step": 905
    },
    {
      "epoch": 5.987608426270136,
      "grad_norm": 18.340356826782227,
      "learning_rate": 0.000906,
      "loss": 3.2742,
      "step": 906
    },
    {
      "epoch": 5.99421726559273,
      "grad_norm": 71.52342224121094,
      "learning_rate": 0.000907,
      "loss": 5.3168,
      "step": 907
    },
    {
      "epoch": 5.99421726559273,
      "eval_validation_error_bar": 0.057566029317518655,
      "eval_validation_loss": 11.508962631225586,
      "eval_validation_pearsonr": 0.48726182467992607,
      "eval_validation_rmse": 3.392486095428467,
      "eval_validation_runtime": 33.1078,
      "eval_validation_samples_per_second": 6.131,
      "eval_validation_spearman": 0.4783445703649009,
      "eval_validation_steps_per_second": 6.131,
      "step": 907
    },
    {
      "epoch": 5.99421726559273,
      "eval_test_error_bar": 0.04992243666344399,
      "eval_test_loss": 13.089054107666016,
      "eval_test_pearsonr": 0.350221350034004,
      "eval_test_rmse": 3.617879867553711,
      "eval_test_runtime": 39.2291,
      "eval_test_samples_per_second": 8.31,
      "eval_test_spearman": 0.3615899196353753,
      "eval_test_steps_per_second": 8.31,
      "step": 907
    },
    {
      "epoch": 6.000826104915324,
      "grad_norm": 132.68124389648438,
      "learning_rate": 0.0009080000000000001,
      "loss": 9.0499,
      "step": 908
    },
    {
      "epoch": 6.007434944237918,
      "grad_norm": 116.06814575195312,
      "learning_rate": 0.0009090000000000001,
      "loss": 3.9617,
      "step": 909
    },
    {
      "epoch": 6.0140437835605125,
      "grad_norm": 166.55699157714844,
      "learning_rate": 0.00091,
      "loss": 6.6778,
      "step": 910
    },
    {
      "epoch": 6.020652622883106,
      "grad_norm": 22.79120445251465,
      "learning_rate": 0.000911,
      "loss": 2.9928,
      "step": 911
    },
    {
      "epoch": 6.0272614622057,
      "grad_norm": 33.790164947509766,
      "learning_rate": 0.000912,
      "loss": 4.8537,
      "step": 912
    },
    {
      "epoch": 6.033870301528294,
      "grad_norm": 70.90111541748047,
      "learning_rate": 0.0009130000000000001,
      "loss": 3.9157,
      "step": 913
    },
    {
      "epoch": 6.040479140850888,
      "grad_norm": 24.41142463684082,
      "learning_rate": 0.0009140000000000001,
      "loss": 3.1896,
      "step": 914
    },
    {
      "epoch": 6.047087980173482,
      "grad_norm": 89.48786926269531,
      "learning_rate": 0.000915,
      "loss": 2.5371,
      "step": 915
    },
    {
      "epoch": 6.053696819496076,
      "grad_norm": 39.6870231628418,
      "learning_rate": 0.000916,
      "loss": 6.3369,
      "step": 916
    },
    {
      "epoch": 6.06030565881867,
      "grad_norm": 25.936935424804688,
      "learning_rate": 0.0009170000000000001,
      "loss": 3.8079,
      "step": 917
    },
    {
      "epoch": 6.066914498141264,
      "grad_norm": 3.7293155193328857,
      "learning_rate": 0.0009180000000000001,
      "loss": 1.3509,
      "step": 918
    },
    {
      "epoch": 6.0735233374638575,
      "grad_norm": 14.32752513885498,
      "learning_rate": 0.0009190000000000001,
      "loss": 5.1648,
      "step": 919
    },
    {
      "epoch": 6.080132176786452,
      "grad_norm": 21.834136962890625,
      "learning_rate": 0.00092,
      "loss": 4.5952,
      "step": 920
    },
    {
      "epoch": 6.086741016109046,
      "grad_norm": 85.02379608154297,
      "learning_rate": 0.000921,
      "loss": 4.1466,
      "step": 921
    },
    {
      "epoch": 6.09334985543164,
      "grad_norm": 205.21044921875,
      "learning_rate": 0.0009220000000000001,
      "loss": 9.2748,
      "step": 922
    },
    {
      "epoch": 6.099958694754234,
      "grad_norm": 195.9363250732422,
      "learning_rate": 0.0009230000000000001,
      "loss": 9.3193,
      "step": 923
    },
    {
      "epoch": 6.106567534076828,
      "grad_norm": 121.93805694580078,
      "learning_rate": 0.000924,
      "loss": 6.3615,
      "step": 924
    },
    {
      "epoch": 6.113176373399422,
      "grad_norm": 95.97332763671875,
      "learning_rate": 0.000925,
      "loss": 4.0037,
      "step": 925
    },
    {
      "epoch": 6.119785212722015,
      "grad_norm": 95.19110107421875,
      "learning_rate": 0.0009260000000000001,
      "loss": 3.4704,
      "step": 926
    },
    {
      "epoch": 6.12639405204461,
      "grad_norm": 35.97616958618164,
      "learning_rate": 0.0009270000000000001,
      "loss": 2.7236,
      "step": 927
    },
    {
      "epoch": 6.1330028913672034,
      "grad_norm": 157.91720581054688,
      "learning_rate": 0.0009280000000000001,
      "loss": 7.5845,
      "step": 928
    },
    {
      "epoch": 6.139611730689798,
      "grad_norm": 164.45980834960938,
      "learning_rate": 0.000929,
      "loss": 8.3616,
      "step": 929
    },
    {
      "epoch": 6.1462205700123915,
      "grad_norm": 96.9352035522461,
      "learning_rate": 0.00093,
      "loss": 5.1629,
      "step": 930
    },
    {
      "epoch": 6.152829409334985,
      "grad_norm": 152.83766174316406,
      "learning_rate": 0.0009310000000000001,
      "loss": 6.6296,
      "step": 931
    },
    {
      "epoch": 6.15943824865758,
      "grad_norm": 186.369873046875,
      "learning_rate": 0.0009320000000000001,
      "loss": 7.9298,
      "step": 932
    },
    {
      "epoch": 6.166047087980173,
      "grad_norm": 86.06794738769531,
      "learning_rate": 0.000933,
      "loss": 3.6054,
      "step": 933
    },
    {
      "epoch": 6.172655927302768,
      "grad_norm": 42.488033294677734,
      "learning_rate": 0.000934,
      "loss": 6.8007,
      "step": 934
    },
    {
      "epoch": 6.179264766625361,
      "grad_norm": 41.783912658691406,
      "learning_rate": 0.0009350000000000001,
      "loss": 1.424,
      "step": 935
    },
    {
      "epoch": 6.185873605947956,
      "grad_norm": 122.30131530761719,
      "learning_rate": 0.0009360000000000001,
      "loss": 5.5484,
      "step": 936
    },
    {
      "epoch": 6.192482445270549,
      "grad_norm": 89.53263092041016,
      "learning_rate": 0.0009370000000000001,
      "loss": 3.9212,
      "step": 937
    },
    {
      "epoch": 6.199091284593143,
      "grad_norm": 53.61071014404297,
      "learning_rate": 0.0009379999999999999,
      "loss": 4.4351,
      "step": 938
    },
    {
      "epoch": 6.205700123915737,
      "grad_norm": 4.34245491027832,
      "learning_rate": 0.000939,
      "loss": 2.8606,
      "step": 939
    },
    {
      "epoch": 6.212308963238331,
      "grad_norm": 24.44974708557129,
      "learning_rate": 0.00094,
      "loss": 2.1873,
      "step": 940
    },
    {
      "epoch": 6.2189178025609255,
      "grad_norm": 16.501758575439453,
      "learning_rate": 0.000941,
      "loss": 1.6265,
      "step": 941
    },
    {
      "epoch": 6.225526641883519,
      "grad_norm": 112.22115325927734,
      "learning_rate": 0.000942,
      "loss": 7.6981,
      "step": 942
    },
    {
      "epoch": 6.232135481206114,
      "grad_norm": 209.3982391357422,
      "learning_rate": 0.0009429999999999999,
      "loss": 9.5797,
      "step": 943
    },
    {
      "epoch": 6.238744320528707,
      "grad_norm": 209.83319091796875,
      "learning_rate": 0.000944,
      "loss": 10.028,
      "step": 944
    },
    {
      "epoch": 6.245353159851301,
      "grad_norm": 153.70108032226562,
      "learning_rate": 0.000945,
      "loss": 7.4694,
      "step": 945
    },
    {
      "epoch": 6.251961999173895,
      "grad_norm": 134.98785400390625,
      "learning_rate": 0.000946,
      "loss": 9.0486,
      "step": 946
    },
    {
      "epoch": 6.258570838496489,
      "grad_norm": 103.79690551757812,
      "learning_rate": 0.0009469999999999999,
      "loss": 6.8121,
      "step": 947
    },
    {
      "epoch": 6.265179677819083,
      "grad_norm": 105.24777221679688,
      "learning_rate": 0.000948,
      "loss": 2.7854,
      "step": 948
    },
    {
      "epoch": 6.271788517141677,
      "grad_norm": 42.483238220214844,
      "learning_rate": 0.000949,
      "loss": 4.9738,
      "step": 949
    },
    {
      "epoch": 6.2783973564642706,
      "grad_norm": 94.61455535888672,
      "learning_rate": 0.00095,
      "loss": 4.0916,
      "step": 950
    },
    {
      "epoch": 6.285006195786865,
      "grad_norm": 168.65786743164062,
      "learning_rate": 0.000951,
      "loss": 14.4751,
      "step": 951
    },
    {
      "epoch": 6.291615035109459,
      "grad_norm": 177.64132690429688,
      "learning_rate": 0.0009519999999999999,
      "loss": 7.3562,
      "step": 952
    },
    {
      "epoch": 6.298223874432053,
      "grad_norm": 75.55819702148438,
      "learning_rate": 0.000953,
      "loss": 2.5324,
      "step": 953
    },
    {
      "epoch": 6.304832713754647,
      "grad_norm": 37.9979362487793,
      "learning_rate": 0.000954,
      "loss": 4.8949,
      "step": 954
    },
    {
      "epoch": 6.311441553077241,
      "grad_norm": 85.23017883300781,
      "learning_rate": 0.000955,
      "loss": 3.3687,
      "step": 955
    },
    {
      "epoch": 6.318050392399835,
      "grad_norm": 19.317668914794922,
      "learning_rate": 0.0009559999999999999,
      "loss": 3.8312,
      "step": 956
    },
    {
      "epoch": 6.324659231722428,
      "grad_norm": 122.24591064453125,
      "learning_rate": 0.000957,
      "loss": 5.1424,
      "step": 957
    },
    {
      "epoch": 6.331268071045023,
      "grad_norm": 93.22037506103516,
      "learning_rate": 0.000958,
      "loss": 4.6113,
      "step": 958
    },
    {
      "epoch": 6.3378769103676165,
      "grad_norm": 21.712142944335938,
      "learning_rate": 0.000959,
      "loss": 2.0783,
      "step": 959
    },
    {
      "epoch": 6.344485749690211,
      "grad_norm": 35.974857330322266,
      "learning_rate": 0.00096,
      "loss": 2.7219,
      "step": 960
    },
    {
      "epoch": 6.3510945890128045,
      "grad_norm": 17.90558433532715,
      "learning_rate": 0.0009609999999999999,
      "loss": 2.5636,
      "step": 961
    },
    {
      "epoch": 6.357703428335398,
      "grad_norm": 3.5584888458251953,
      "learning_rate": 0.000962,
      "loss": 2.1297,
      "step": 962
    },
    {
      "epoch": 6.364312267657993,
      "grad_norm": 98.81605529785156,
      "learning_rate": 0.000963,
      "loss": 6.0823,
      "step": 963
    },
    {
      "epoch": 6.370921106980586,
      "grad_norm": 99.83112335205078,
      "learning_rate": 0.000964,
      "loss": 4.9735,
      "step": 964
    },
    {
      "epoch": 6.377529946303181,
      "grad_norm": 139.94190979003906,
      "learning_rate": 0.000965,
      "loss": 8.7283,
      "step": 965
    },
    {
      "epoch": 6.384138785625774,
      "grad_norm": 2.5610005855560303,
      "learning_rate": 0.000966,
      "loss": 7.4046,
      "step": 966
    },
    {
      "epoch": 6.390747624948369,
      "grad_norm": 50.61283493041992,
      "learning_rate": 0.000967,
      "loss": 3.3714,
      "step": 967
    },
    {
      "epoch": 6.397356464270962,
      "grad_norm": 24.883174896240234,
      "learning_rate": 0.000968,
      "loss": 3.8676,
      "step": 968
    },
    {
      "epoch": 6.403965303593556,
      "grad_norm": 8.296964645385742,
      "learning_rate": 0.000969,
      "loss": 4.2017,
      "step": 969
    },
    {
      "epoch": 6.4105741429161505,
      "grad_norm": 140.759033203125,
      "learning_rate": 0.0009699999999999999,
      "loss": 3.2743,
      "step": 970
    },
    {
      "epoch": 6.417182982238744,
      "grad_norm": 109.00182342529297,
      "learning_rate": 0.000971,
      "loss": 2.8502,
      "step": 971
    },
    {
      "epoch": 6.4237918215613385,
      "grad_norm": 89.63998413085938,
      "learning_rate": 0.000972,
      "loss": 3.431,
      "step": 972
    },
    {
      "epoch": 6.430400660883932,
      "grad_norm": 203.12062072753906,
      "learning_rate": 0.000973,
      "loss": 9.0962,
      "step": 973
    },
    {
      "epoch": 6.437009500206527,
      "grad_norm": 195.8356170654297,
      "learning_rate": 0.000974,
      "loss": 7.409,
      "step": 974
    },
    {
      "epoch": 6.44361833952912,
      "grad_norm": 212.2864227294922,
      "learning_rate": 0.000975,
      "loss": 19.4628,
      "step": 975
    },
    {
      "epoch": 6.450227178851714,
      "grad_norm": 76.62967681884766,
      "learning_rate": 0.000976,
      "loss": 5.8494,
      "step": 976
    },
    {
      "epoch": 6.456836018174308,
      "grad_norm": 33.58921813964844,
      "learning_rate": 0.000977,
      "loss": 4.2269,
      "step": 977
    },
    {
      "epoch": 6.463444857496902,
      "grad_norm": 36.98491287231445,
      "learning_rate": 0.000978,
      "loss": 3.6156,
      "step": 978
    },
    {
      "epoch": 6.470053696819496,
      "grad_norm": 151.64724731445312,
      "learning_rate": 0.000979,
      "loss": 5.0479,
      "step": 979
    },
    {
      "epoch": 6.47666253614209,
      "grad_norm": 164.0742950439453,
      "learning_rate": 0.00098,
      "loss": 7.4854,
      "step": 980
    },
    {
      "epoch": 6.483271375464684,
      "grad_norm": 98.45557403564453,
      "learning_rate": 0.000981,
      "loss": 9.4562,
      "step": 981
    },
    {
      "epoch": 6.489880214787278,
      "grad_norm": 88.70540618896484,
      "learning_rate": 0.000982,
      "loss": 8.7445,
      "step": 982
    },
    {
      "epoch": 6.496489054109872,
      "grad_norm": 50.67165756225586,
      "learning_rate": 0.000983,
      "loss": 3.9482,
      "step": 983
    },
    {
      "epoch": 6.503097893432466,
      "grad_norm": 119.49028778076172,
      "learning_rate": 0.000984,
      "loss": 5.854,
      "step": 984
    },
    {
      "epoch": 6.50970673275506,
      "grad_norm": 115.60911560058594,
      "learning_rate": 0.000985,
      "loss": 8.0497,
      "step": 985
    },
    {
      "epoch": 6.516315572077654,
      "grad_norm": 5.091634750366211,
      "learning_rate": 0.0009860000000000001,
      "loss": 1.3471,
      "step": 986
    },
    {
      "epoch": 6.522924411400248,
      "grad_norm": 91.55659484863281,
      "learning_rate": 0.000987,
      "loss": 6.1565,
      "step": 987
    },
    {
      "epoch": 6.529533250722842,
      "grad_norm": 56.88176727294922,
      "learning_rate": 0.000988,
      "loss": 3.3785,
      "step": 988
    },
    {
      "epoch": 6.536142090045436,
      "grad_norm": 30.144084930419922,
      "learning_rate": 0.000989,
      "loss": 4.0588,
      "step": 989
    },
    {
      "epoch": 6.5427509293680295,
      "grad_norm": 16.17903709411621,
      "learning_rate": 0.00099,
      "loss": 2.4277,
      "step": 990
    },
    {
      "epoch": 6.549359768690624,
      "grad_norm": 136.57298278808594,
      "learning_rate": 0.000991,
      "loss": 6.6988,
      "step": 991
    },
    {
      "epoch": 6.555968608013218,
      "grad_norm": 98.90071105957031,
      "learning_rate": 0.000992,
      "loss": 4.199,
      "step": 992
    },
    {
      "epoch": 6.562577447335812,
      "grad_norm": 35.94020080566406,
      "learning_rate": 0.000993,
      "loss": 4.215,
      "step": 993
    },
    {
      "epoch": 6.569186286658406,
      "grad_norm": 37.30093765258789,
      "learning_rate": 0.000994,
      "loss": 2.3333,
      "step": 994
    },
    {
      "epoch": 6.575795125980999,
      "grad_norm": 12.001484870910645,
      "learning_rate": 0.000995,
      "loss": 3.5039,
      "step": 995
    },
    {
      "epoch": 6.582403965303594,
      "grad_norm": 141.8078155517578,
      "learning_rate": 0.000996,
      "loss": 7.2207,
      "step": 996
    },
    {
      "epoch": 6.589012804626187,
      "grad_norm": 14.181448936462402,
      "learning_rate": 0.000997,
      "loss": 5.6952,
      "step": 997
    },
    {
      "epoch": 6.595621643948782,
      "grad_norm": 61.80042266845703,
      "learning_rate": 0.000998,
      "loss": 1.9175,
      "step": 998
    },
    {
      "epoch": 6.602230483271375,
      "grad_norm": 13.775527000427246,
      "learning_rate": 0.000999,
      "loss": 4.5652,
      "step": 999
    },
    {
      "epoch": 6.608839322593969,
      "grad_norm": 114.26860046386719,
      "learning_rate": 0.001,
      "loss": 5.5857,
      "step": 1000
    },
    {
      "epoch": 6.6154481619165635,
      "grad_norm": 10.743256568908691,
      "learning_rate": 0.0009995049504950494,
      "loss": 5.4008,
      "step": 1001
    },
    {
      "epoch": 6.622057001239157,
      "grad_norm": 56.45634078979492,
      "learning_rate": 0.000999009900990099,
      "loss": 3.1117,
      "step": 1002
    },
    {
      "epoch": 6.628665840561752,
      "grad_norm": 45.5405158996582,
      "learning_rate": 0.0009985148514851485,
      "loss": 4.2838,
      "step": 1003
    },
    {
      "epoch": 6.635274679884345,
      "grad_norm": 52.99272918701172,
      "learning_rate": 0.0009980198019801981,
      "loss": 5.9437,
      "step": 1004
    },
    {
      "epoch": 6.64188351920694,
      "grad_norm": 75.31504821777344,
      "learning_rate": 0.0009975247524752475,
      "loss": 5.3572,
      "step": 1005
    },
    {
      "epoch": 6.648492358529533,
      "grad_norm": 49.44083023071289,
      "learning_rate": 0.000997029702970297,
      "loss": 4.1542,
      "step": 1006
    },
    {
      "epoch": 6.655101197852127,
      "grad_norm": 53.92934799194336,
      "learning_rate": 0.0009965346534653466,
      "loss": 4.5988,
      "step": 1007
    },
    {
      "epoch": 6.661710037174721,
      "grad_norm": 3.8618316650390625,
      "learning_rate": 0.000996039603960396,
      "loss": 2.3876,
      "step": 1008
    },
    {
      "epoch": 6.668318876497315,
      "grad_norm": 72.62823486328125,
      "learning_rate": 0.0009955445544554456,
      "loss": 7.4008,
      "step": 1009
    },
    {
      "epoch": 6.674927715819909,
      "grad_norm": 30.28373908996582,
      "learning_rate": 0.000995049504950495,
      "loss": 11.0993,
      "step": 1010
    },
    {
      "epoch": 6.681536555142503,
      "grad_norm": 98.4186019897461,
      "learning_rate": 0.0009945544554455445,
      "loss": 4.6737,
      "step": 1011
    },
    {
      "epoch": 6.6881453944650975,
      "grad_norm": 47.047664642333984,
      "learning_rate": 0.0009940594059405941,
      "loss": 2.8346,
      "step": 1012
    },
    {
      "epoch": 6.694754233787691,
      "grad_norm": 190.26576232910156,
      "learning_rate": 0.0009935643564356435,
      "loss": 10.8636,
      "step": 1013
    },
    {
      "epoch": 6.701363073110285,
      "grad_norm": 180.9010467529297,
      "learning_rate": 0.0009930693069306932,
      "loss": 5.1763,
      "step": 1014
    },
    {
      "epoch": 6.707971912432879,
      "grad_norm": 68.6242904663086,
      "learning_rate": 0.0009925742574257426,
      "loss": 5.7079,
      "step": 1015
    },
    {
      "epoch": 6.714580751755473,
      "grad_norm": 143.3260955810547,
      "learning_rate": 0.000992079207920792,
      "loss": 4.3805,
      "step": 1016
    },
    {
      "epoch": 6.721189591078067,
      "grad_norm": 19.38767433166504,
      "learning_rate": 0.0009915841584158416,
      "loss": 4.9581,
      "step": 1017
    },
    {
      "epoch": 6.727798430400661,
      "grad_norm": 116.50371551513672,
      "learning_rate": 0.000991089108910891,
      "loss": 4.8514,
      "step": 1018
    },
    {
      "epoch": 6.734407269723254,
      "grad_norm": 106.38980102539062,
      "learning_rate": 0.0009905940594059407,
      "loss": 2.1525,
      "step": 1019
    },
    {
      "epoch": 6.741016109045849,
      "grad_norm": 41.98943328857422,
      "learning_rate": 0.0009900990099009901,
      "loss": 4.0547,
      "step": 1020
    },
    {
      "epoch": 6.7476249483684425,
      "grad_norm": 47.627052307128906,
      "learning_rate": 0.0009896039603960395,
      "loss": 2.5936,
      "step": 1021
    },
    {
      "epoch": 6.754233787691037,
      "grad_norm": 137.93643188476562,
      "learning_rate": 0.0009891089108910892,
      "loss": 7.9161,
      "step": 1022
    },
    {
      "epoch": 6.760842627013631,
      "grad_norm": 90.19297790527344,
      "learning_rate": 0.0009886138613861386,
      "loss": 3.8723,
      "step": 1023
    },
    {
      "epoch": 6.767451466336225,
      "grad_norm": 17.172611236572266,
      "learning_rate": 0.0009881188118811882,
      "loss": 7.3533,
      "step": 1024
    },
    {
      "epoch": 6.774060305658819,
      "grad_norm": 67.1700439453125,
      "learning_rate": 0.0009876237623762376,
      "loss": 2.0154,
      "step": 1025
    },
    {
      "epoch": 6.780669144981412,
      "grad_norm": 42.17992401123047,
      "learning_rate": 0.000987128712871287,
      "loss": 2.667,
      "step": 1026
    },
    {
      "epoch": 6.787277984304007,
      "grad_norm": 12.96058464050293,
      "learning_rate": 0.0009866336633663367,
      "loss": 3.0091,
      "step": 1027
    },
    {
      "epoch": 6.7938868236266,
      "grad_norm": 17.216562271118164,
      "learning_rate": 0.000986138613861386,
      "loss": 4.1042,
      "step": 1028
    },
    {
      "epoch": 6.800495662949195,
      "grad_norm": 46.72793197631836,
      "learning_rate": 0.0009856435643564357,
      "loss": 3.653,
      "step": 1029
    },
    {
      "epoch": 6.807104502271788,
      "grad_norm": 47.95451354980469,
      "learning_rate": 0.0009851485148514852,
      "loss": 2.8502,
      "step": 1030
    },
    {
      "epoch": 6.813713341594383,
      "grad_norm": 90.84894561767578,
      "learning_rate": 0.0009846534653465346,
      "loss": 4.0606,
      "step": 1031
    },
    {
      "epoch": 6.8203221809169765,
      "grad_norm": 23.078031539916992,
      "learning_rate": 0.0009841584158415842,
      "loss": 3.8627,
      "step": 1032
    },
    {
      "epoch": 6.82693102023957,
      "grad_norm": 143.33380126953125,
      "learning_rate": 0.0009836633663366336,
      "loss": 4.4025,
      "step": 1033
    },
    {
      "epoch": 6.833539859562165,
      "grad_norm": 121.40592956542969,
      "learning_rate": 0.0009831683168316833,
      "loss": 4.0243,
      "step": 1034
    },
    {
      "epoch": 6.840148698884758,
      "grad_norm": 112.72836303710938,
      "learning_rate": 0.0009826732673267327,
      "loss": 2.1445,
      "step": 1035
    },
    {
      "epoch": 6.846757538207353,
      "grad_norm": 72.20510864257812,
      "learning_rate": 0.000982178217821782,
      "loss": 1.8355,
      "step": 1036
    },
    {
      "epoch": 6.853366377529946,
      "grad_norm": 75.53189086914062,
      "learning_rate": 0.0009816831683168317,
      "loss": 2.911,
      "step": 1037
    },
    {
      "epoch": 6.85997521685254,
      "grad_norm": 29.558876037597656,
      "learning_rate": 0.0009811881188118811,
      "loss": 2.6209,
      "step": 1038
    },
    {
      "epoch": 6.866584056175134,
      "grad_norm": 114.35041046142578,
      "learning_rate": 0.0009806930693069308,
      "loss": 3.1606,
      "step": 1039
    },
    {
      "epoch": 6.873192895497728,
      "grad_norm": 152.72911071777344,
      "learning_rate": 0.0009801980198019802,
      "loss": 14.0615,
      "step": 1040
    },
    {
      "epoch": 6.879801734820322,
      "grad_norm": 99.7621078491211,
      "learning_rate": 0.0009797029702970296,
      "loss": 5.2565,
      "step": 1041
    },
    {
      "epoch": 6.886410574142916,
      "grad_norm": 87.13556671142578,
      "learning_rate": 0.0009792079207920793,
      "loss": 3.1294,
      "step": 1042
    },
    {
      "epoch": 6.8930194134655105,
      "grad_norm": 93.9419174194336,
      "learning_rate": 0.0009787128712871287,
      "loss": 5.5384,
      "step": 1043
    },
    {
      "epoch": 6.899628252788104,
      "grad_norm": 27.134469985961914,
      "learning_rate": 0.0009782178217821783,
      "loss": 2.5162,
      "step": 1044
    },
    {
      "epoch": 6.906237092110698,
      "grad_norm": 66.0312271118164,
      "learning_rate": 0.0009777227722772277,
      "loss": 14.3784,
      "step": 1045
    },
    {
      "epoch": 6.912845931433292,
      "grad_norm": 86.1677474975586,
      "learning_rate": 0.0009772277227722771,
      "loss": 4.1275,
      "step": 1046
    },
    {
      "epoch": 6.919454770755886,
      "grad_norm": 6.658606052398682,
      "learning_rate": 0.0009767326732673268,
      "loss": 1.2552,
      "step": 1047
    },
    {
      "epoch": 6.92606361007848,
      "grad_norm": 53.453895568847656,
      "learning_rate": 0.0009762376237623762,
      "loss": 3.9127,
      "step": 1048
    },
    {
      "epoch": 6.932672449401074,
      "grad_norm": 26.881427764892578,
      "learning_rate": 0.0009757425742574257,
      "loss": 3.7233,
      "step": 1049
    },
    {
      "epoch": 6.939281288723668,
      "grad_norm": 66.45330047607422,
      "learning_rate": 0.0009752475247524752,
      "loss": 5.1476,
      "step": 1050
    },
    {
      "epoch": 6.945890128046262,
      "grad_norm": 111.97308349609375,
      "learning_rate": 0.0009747524752475248,
      "loss": 4.6304,
      "step": 1051
    },
    {
      "epoch": 6.9524989673688555,
      "grad_norm": 87.90309143066406,
      "learning_rate": 0.0009742574257425743,
      "loss": 5.2856,
      "step": 1052
    },
    {
      "epoch": 6.95910780669145,
      "grad_norm": 75.25090789794922,
      "learning_rate": 0.0009737623762376237,
      "loss": 2.3587,
      "step": 1053
    },
    {
      "epoch": 6.965716646014044,
      "grad_norm": 96.5684585571289,
      "learning_rate": 0.0009732673267326732,
      "loss": 2.3034,
      "step": 1054
    },
    {
      "epoch": 6.972325485336638,
      "grad_norm": 123.8280258178711,
      "learning_rate": 0.0009727722772277228,
      "loss": 4.2686,
      "step": 1055
    },
    {
      "epoch": 6.978934324659232,
      "grad_norm": 47.188419342041016,
      "learning_rate": 0.0009722772277227723,
      "loss": 2.681,
      "step": 1056
    },
    {
      "epoch": 6.985543163981825,
      "grad_norm": 80.03263854980469,
      "learning_rate": 0.0009717821782178218,
      "loss": 1.8094,
      "step": 1057
    },
    {
      "epoch": 6.99215200330442,
      "grad_norm": 62.98078155517578,
      "learning_rate": 0.0009712871287128712,
      "loss": 3.2378,
      "step": 1058
    },
    {
      "epoch": 6.998760842627013,
      "grad_norm": 20.394794464111328,
      "learning_rate": 0.0009707920792079208,
      "loss": 6.5579,
      "step": 1059
    },
    {
      "epoch": 6.998760842627013,
      "eval_validation_error_bar": 0.06032288132686228,
      "eval_validation_loss": 8.017447471618652,
      "eval_validation_pearsonr": 0.51271237377939,
      "eval_validation_rmse": 2.831509828567505,
      "eval_validation_runtime": 32.8473,
      "eval_validation_samples_per_second": 6.18,
      "eval_validation_spearman": 0.4284249031720402,
      "eval_validation_steps_per_second": 6.18,
      "step": 1059
    },
    {
      "epoch": 6.998760842627013,
      "eval_test_error_bar": 0.05232933009453795,
      "eval_test_loss": 12.95361614227295,
      "eval_test_pearsonr": 0.21167670530997001,
      "eval_test_rmse": 3.5991132259368896,
      "eval_test_runtime": 38.7269,
      "eval_test_samples_per_second": 8.418,
      "eval_test_spearman": 0.27777665209517954,
      "eval_test_steps_per_second": 8.418,
      "step": 1059
    },
    {
      "epoch": 7.005369681949608,
      "grad_norm": 30.99190902709961,
      "learning_rate": 0.0009702970297029703,
      "loss": 2.8908,
      "step": 1060
    },
    {
      "epoch": 7.0119785212722014,
      "grad_norm": 42.25035858154297,
      "learning_rate": 0.0009698019801980198,
      "loss": 3.0902,
      "step": 1061
    },
    {
      "epoch": 7.018587360594796,
      "grad_norm": 29.397884368896484,
      "learning_rate": 0.0009693069306930693,
      "loss": 4.232,
      "step": 1062
    },
    {
      "epoch": 7.0251961999173895,
      "grad_norm": 79.1942138671875,
      "learning_rate": 0.0009688118811881188,
      "loss": 4.8889,
      "step": 1063
    },
    {
      "epoch": 7.031805039239983,
      "grad_norm": 87.3807144165039,
      "learning_rate": 0.0009683168316831683,
      "loss": 2.2699,
      "step": 1064
    },
    {
      "epoch": 7.038413878562578,
      "grad_norm": 55.971126556396484,
      "learning_rate": 0.0009678217821782178,
      "loss": 2.2864,
      "step": 1065
    },
    {
      "epoch": 7.045022717885171,
      "grad_norm": 74.47565460205078,
      "learning_rate": 0.0009673267326732673,
      "loss": 2.6384,
      "step": 1066
    },
    {
      "epoch": 7.051631557207766,
      "grad_norm": 8.827086448669434,
      "learning_rate": 0.0009668316831683169,
      "loss": 2.2257,
      "step": 1067
    },
    {
      "epoch": 7.058240396530359,
      "grad_norm": 89.02161407470703,
      "learning_rate": 0.0009663366336633663,
      "loss": 6.0907,
      "step": 1068
    },
    {
      "epoch": 7.064849235852954,
      "grad_norm": 25.817663192749023,
      "learning_rate": 0.0009658415841584158,
      "loss": 2.0575,
      "step": 1069
    },
    {
      "epoch": 7.071458075175547,
      "grad_norm": 87.76016998291016,
      "learning_rate": 0.0009653465346534653,
      "loss": 4.1517,
      "step": 1070
    },
    {
      "epoch": 7.078066914498141,
      "grad_norm": 50.48484420776367,
      "learning_rate": 0.0009648514851485149,
      "loss": 2.4126,
      "step": 1071
    },
    {
      "epoch": 7.0846757538207354,
      "grad_norm": 37.235389709472656,
      "learning_rate": 0.0009643564356435644,
      "loss": 2.8841,
      "step": 1072
    },
    {
      "epoch": 7.091284593143329,
      "grad_norm": 51.363555908203125,
      "learning_rate": 0.0009638613861386138,
      "loss": 3.0186,
      "step": 1073
    },
    {
      "epoch": 7.0978934324659235,
      "grad_norm": 48.341026306152344,
      "learning_rate": 0.0009633663366336633,
      "loss": 4.3967,
      "step": 1074
    },
    {
      "epoch": 7.104502271788517,
      "grad_norm": 6.68897819519043,
      "learning_rate": 0.0009628712871287129,
      "loss": 2.3619,
      "step": 1075
    },
    {
      "epoch": 7.111111111111111,
      "grad_norm": 9.730006217956543,
      "learning_rate": 0.0009623762376237624,
      "loss": 3.927,
      "step": 1076
    },
    {
      "epoch": 7.117719950433705,
      "grad_norm": 10.988102912902832,
      "learning_rate": 0.0009618811881188119,
      "loss": 0.9889,
      "step": 1077
    },
    {
      "epoch": 7.124328789756299,
      "grad_norm": 21.71359634399414,
      "learning_rate": 0.0009613861386138613,
      "loss": 3.3065,
      "step": 1078
    },
    {
      "epoch": 7.130937629078893,
      "grad_norm": 20.775728225708008,
      "learning_rate": 0.0009608910891089109,
      "loss": 6.9198,
      "step": 1079
    },
    {
      "epoch": 7.137546468401487,
      "grad_norm": 54.31003189086914,
      "learning_rate": 0.0009603960396039604,
      "loss": 2.7991,
      "step": 1080
    },
    {
      "epoch": 7.144155307724081,
      "grad_norm": 92.47492980957031,
      "learning_rate": 0.0009599009900990099,
      "loss": 2.4701,
      "step": 1081
    },
    {
      "epoch": 7.150764147046675,
      "grad_norm": 52.42705154418945,
      "learning_rate": 0.0009594059405940594,
      "loss": 4.1157,
      "step": 1082
    },
    {
      "epoch": 7.1573729863692686,
      "grad_norm": 19.651060104370117,
      "learning_rate": 0.0009589108910891089,
      "loss": 1.2212,
      "step": 1083
    },
    {
      "epoch": 7.163981825691863,
      "grad_norm": 19.088972091674805,
      "learning_rate": 0.0009584158415841584,
      "loss": 7.5026,
      "step": 1084
    },
    {
      "epoch": 7.170590665014457,
      "grad_norm": 99.14566802978516,
      "learning_rate": 0.0009579207920792079,
      "loss": 4.5094,
      "step": 1085
    },
    {
      "epoch": 7.177199504337051,
      "grad_norm": 62.615604400634766,
      "learning_rate": 0.0009574257425742574,
      "loss": 3.2603,
      "step": 1086
    },
    {
      "epoch": 7.183808343659645,
      "grad_norm": 64.20233917236328,
      "learning_rate": 0.000956930693069307,
      "loss": 7.2976,
      "step": 1087
    },
    {
      "epoch": 7.190417182982239,
      "grad_norm": 44.75101852416992,
      "learning_rate": 0.0009564356435643564,
      "loss": 2.5892,
      "step": 1088
    },
    {
      "epoch": 7.197026022304833,
      "grad_norm": 110.92241668701172,
      "learning_rate": 0.0009559405940594059,
      "loss": 3.7447,
      "step": 1089
    },
    {
      "epoch": 7.203634861627426,
      "grad_norm": 122.86259460449219,
      "learning_rate": 0.0009554455445544554,
      "loss": 4.9347,
      "step": 1090
    },
    {
      "epoch": 7.210243700950021,
      "grad_norm": 128.9195098876953,
      "learning_rate": 0.000954950495049505,
      "loss": 4.8769,
      "step": 1091
    },
    {
      "epoch": 7.2168525402726145,
      "grad_norm": 54.42203140258789,
      "learning_rate": 0.0009544554455445545,
      "loss": 2.5311,
      "step": 1092
    },
    {
      "epoch": 7.223461379595209,
      "grad_norm": 138.29307556152344,
      "learning_rate": 0.0009539603960396039,
      "loss": 3.9906,
      "step": 1093
    },
    {
      "epoch": 7.2300702189178025,
      "grad_norm": 20.927282333374023,
      "learning_rate": 0.0009534653465346534,
      "loss": 3.5391,
      "step": 1094
    },
    {
      "epoch": 7.236679058240396,
      "grad_norm": 23.08003807067871,
      "learning_rate": 0.000952970297029703,
      "loss": 1.4865,
      "step": 1095
    },
    {
      "epoch": 7.243287897562991,
      "grad_norm": 32.25126266479492,
      "learning_rate": 0.0009524752475247525,
      "loss": 4.4154,
      "step": 1096
    },
    {
      "epoch": 7.249896736885584,
      "grad_norm": 8.4497709274292,
      "learning_rate": 0.000951980198019802,
      "loss": 2.374,
      "step": 1097
    },
    {
      "epoch": 7.256505576208179,
      "grad_norm": 22.41300392150879,
      "learning_rate": 0.0009514851485148514,
      "loss": 2.0145,
      "step": 1098
    },
    {
      "epoch": 7.263114415530772,
      "grad_norm": 36.62359619140625,
      "learning_rate": 0.0009509900990099009,
      "loss": 2.4826,
      "step": 1099
    },
    {
      "epoch": 7.269723254853367,
      "grad_norm": 13.036255836486816,
      "learning_rate": 0.0009504950495049505,
      "loss": 2.0403,
      "step": 1100
    },
    {
      "epoch": 7.27633209417596,
      "grad_norm": 110.16651916503906,
      "learning_rate": 0.00095,
      "loss": 4.4994,
      "step": 1101
    },
    {
      "epoch": 7.282940933498554,
      "grad_norm": 63.70567321777344,
      "learning_rate": 0.0009495049504950495,
      "loss": 1.3282,
      "step": 1102
    },
    {
      "epoch": 7.2895497728211485,
      "grad_norm": 57.82512664794922,
      "learning_rate": 0.0009490099009900989,
      "loss": 6.7501,
      "step": 1103
    },
    {
      "epoch": 7.296158612143742,
      "grad_norm": 41.79136657714844,
      "learning_rate": 0.0009485148514851485,
      "loss": 3.0617,
      "step": 1104
    },
    {
      "epoch": 7.3027674514663365,
      "grad_norm": 53.80123519897461,
      "learning_rate": 0.000948019801980198,
      "loss": 2.5582,
      "step": 1105
    },
    {
      "epoch": 7.30937629078893,
      "grad_norm": 86.63665008544922,
      "learning_rate": 0.0009475247524752475,
      "loss": 5.1463,
      "step": 1106
    },
    {
      "epoch": 7.315985130111525,
      "grad_norm": 236.66250610351562,
      "learning_rate": 0.000947029702970297,
      "loss": 12.0103,
      "step": 1107
    },
    {
      "epoch": 7.322593969434118,
      "grad_norm": 194.61817932128906,
      "learning_rate": 0.0009465346534653465,
      "loss": 5.8872,
      "step": 1108
    },
    {
      "epoch": 7.329202808756712,
      "grad_norm": 115.21003723144531,
      "learning_rate": 0.000946039603960396,
      "loss": 4.9773,
      "step": 1109
    },
    {
      "epoch": 7.335811648079306,
      "grad_norm": 125.5203628540039,
      "learning_rate": 0.0009455445544554455,
      "loss": 8.6361,
      "step": 1110
    },
    {
      "epoch": 7.3424204874019,
      "grad_norm": 60.35877990722656,
      "learning_rate": 0.000945049504950495,
      "loss": 2.237,
      "step": 1111
    },
    {
      "epoch": 7.349029326724494,
      "grad_norm": 74.0223617553711,
      "learning_rate": 0.0009445544554455446,
      "loss": 2.1413,
      "step": 1112
    },
    {
      "epoch": 7.355638166047088,
      "grad_norm": 83.66305541992188,
      "learning_rate": 0.000944059405940594,
      "loss": 2.9761,
      "step": 1113
    },
    {
      "epoch": 7.362247005369682,
      "grad_norm": 71.77059173583984,
      "learning_rate": 0.0009435643564356435,
      "loss": 2.0198,
      "step": 1114
    },
    {
      "epoch": 7.368855844692276,
      "grad_norm": 52.26336669921875,
      "learning_rate": 0.000943069306930693,
      "loss": 2.0651,
      "step": 1115
    },
    {
      "epoch": 7.37546468401487,
      "grad_norm": 45.012184143066406,
      "learning_rate": 0.0009425742574257426,
      "loss": 3.8443,
      "step": 1116
    },
    {
      "epoch": 7.382073523337464,
      "grad_norm": 12.11444091796875,
      "learning_rate": 0.0009420792079207921,
      "loss": 1.7633,
      "step": 1117
    },
    {
      "epoch": 7.388682362660058,
      "grad_norm": 99.37496185302734,
      "learning_rate": 0.0009415841584158415,
      "loss": 5.3073,
      "step": 1118
    },
    {
      "epoch": 7.395291201982651,
      "grad_norm": 158.02243041992188,
      "learning_rate": 0.000941089108910891,
      "loss": 4.8202,
      "step": 1119
    },
    {
      "epoch": 7.401900041305246,
      "grad_norm": 99.04969024658203,
      "learning_rate": 0.0009405940594059406,
      "loss": 2.6633,
      "step": 1120
    },
    {
      "epoch": 7.408508880627839,
      "grad_norm": 136.7889404296875,
      "learning_rate": 0.0009400990099009901,
      "loss": 5.9442,
      "step": 1121
    },
    {
      "epoch": 7.415117719950434,
      "grad_norm": 98.77360534667969,
      "learning_rate": 0.0009396039603960396,
      "loss": 3.444,
      "step": 1122
    },
    {
      "epoch": 7.4217265592730275,
      "grad_norm": 41.54705810546875,
      "learning_rate": 0.000939108910891089,
      "loss": 0.8492,
      "step": 1123
    },
    {
      "epoch": 7.428335398595622,
      "grad_norm": 37.275821685791016,
      "learning_rate": 0.0009386138613861386,
      "loss": 2.0154,
      "step": 1124
    },
    {
      "epoch": 7.434944237918216,
      "grad_norm": 55.85527801513672,
      "learning_rate": 0.0009381188118811881,
      "loss": 2.2994,
      "step": 1125
    },
    {
      "epoch": 7.44155307724081,
      "grad_norm": 2.51705002784729,
      "learning_rate": 0.0009376237623762376,
      "loss": 3.8095,
      "step": 1126
    },
    {
      "epoch": 7.448161916563404,
      "grad_norm": 126.05289459228516,
      "learning_rate": 0.0009371287128712872,
      "loss": 5.5439,
      "step": 1127
    },
    {
      "epoch": 7.454770755885997,
      "grad_norm": 91.85428619384766,
      "learning_rate": 0.0009366336633663367,
      "loss": 4.656,
      "step": 1128
    },
    {
      "epoch": 7.461379595208592,
      "grad_norm": 86.89247131347656,
      "learning_rate": 0.0009361386138613862,
      "loss": 2.4012,
      "step": 1129
    },
    {
      "epoch": 7.467988434531185,
      "grad_norm": 44.88195037841797,
      "learning_rate": 0.0009356435643564357,
      "loss": 3.6904,
      "step": 1130
    },
    {
      "epoch": 7.47459727385378,
      "grad_norm": 18.560619354248047,
      "learning_rate": 0.0009351485148514852,
      "loss": 2.1288,
      "step": 1131
    },
    {
      "epoch": 7.481206113176373,
      "grad_norm": 79.71327209472656,
      "learning_rate": 0.0009346534653465348,
      "loss": 3.7868,
      "step": 1132
    },
    {
      "epoch": 7.487814952498967,
      "grad_norm": 18.25749969482422,
      "learning_rate": 0.0009341584158415842,
      "loss": 1.6062,
      "step": 1133
    },
    {
      "epoch": 7.4944237918215615,
      "grad_norm": 146.796630859375,
      "learning_rate": 0.0009336633663366337,
      "loss": 5.3924,
      "step": 1134
    },
    {
      "epoch": 7.501032631144155,
      "grad_norm": 8.867903709411621,
      "learning_rate": 0.0009331683168316832,
      "loss": 1.3815,
      "step": 1135
    },
    {
      "epoch": 7.50764147046675,
      "grad_norm": 22.178064346313477,
      "learning_rate": 0.0009326732673267328,
      "loss": 4.0439,
      "step": 1136
    },
    {
      "epoch": 7.514250309789343,
      "grad_norm": 55.60981369018555,
      "learning_rate": 0.0009321782178217823,
      "loss": 3.1877,
      "step": 1137
    },
    {
      "epoch": 7.520859149111937,
      "grad_norm": 10.84498405456543,
      "learning_rate": 0.0009316831683168317,
      "loss": 5.881,
      "step": 1138
    },
    {
      "epoch": 7.527467988434531,
      "grad_norm": 18.320924758911133,
      "learning_rate": 0.0009311881188118812,
      "loss": 3.1066,
      "step": 1139
    },
    {
      "epoch": 7.534076827757125,
      "grad_norm": 63.79958724975586,
      "learning_rate": 0.0009306930693069308,
      "loss": 3.4843,
      "step": 1140
    },
    {
      "epoch": 7.540685667079719,
      "grad_norm": 63.29402160644531,
      "learning_rate": 0.0009301980198019803,
      "loss": 6.6687,
      "step": 1141
    },
    {
      "epoch": 7.547294506402313,
      "grad_norm": 80.41896057128906,
      "learning_rate": 0.0009297029702970298,
      "loss": 1.8605,
      "step": 1142
    },
    {
      "epoch": 7.553903345724907,
      "grad_norm": 9.581239700317383,
      "learning_rate": 0.0009292079207920792,
      "loss": 2.0338,
      "step": 1143
    },
    {
      "epoch": 7.560512185047501,
      "grad_norm": 90.3454360961914,
      "learning_rate": 0.0009287128712871288,
      "loss": 4.2179,
      "step": 1144
    },
    {
      "epoch": 7.5671210243700955,
      "grad_norm": 38.1396369934082,
      "learning_rate": 0.0009282178217821783,
      "loss": 1.2241,
      "step": 1145
    },
    {
      "epoch": 7.573729863692689,
      "grad_norm": 37.56281661987305,
      "learning_rate": 0.0009277227722772278,
      "loss": 4.0263,
      "step": 1146
    },
    {
      "epoch": 7.580338703015283,
      "grad_norm": 7.0620808601379395,
      "learning_rate": 0.0009272277227722773,
      "loss": 2.9154,
      "step": 1147
    },
    {
      "epoch": 7.586947542337877,
      "grad_norm": 46.8252067565918,
      "learning_rate": 0.0009267326732673268,
      "loss": 3.0167,
      "step": 1148
    },
    {
      "epoch": 7.593556381660471,
      "grad_norm": 28.44847297668457,
      "learning_rate": 0.0009262376237623763,
      "loss": 1.8337,
      "step": 1149
    },
    {
      "epoch": 7.600165220983065,
      "grad_norm": 60.385704040527344,
      "learning_rate": 0.0009257425742574258,
      "loss": 4.4116,
      "step": 1150
    },
    {
      "epoch": 7.606774060305659,
      "grad_norm": 101.30049133300781,
      "learning_rate": 0.0009252475247524753,
      "loss": 6.6364,
      "step": 1151
    },
    {
      "epoch": 7.613382899628252,
      "grad_norm": 85.3714599609375,
      "learning_rate": 0.0009247524752475249,
      "loss": 3.3843,
      "step": 1152
    },
    {
      "epoch": 7.619991738950847,
      "grad_norm": 146.208251953125,
      "learning_rate": 0.0009242574257425743,
      "loss": 8.8328,
      "step": 1153
    },
    {
      "epoch": 7.6266005782734405,
      "grad_norm": 40.44671630859375,
      "learning_rate": 0.0009237623762376238,
      "loss": 4.1211,
      "step": 1154
    },
    {
      "epoch": 7.633209417596035,
      "grad_norm": 3.6537365913391113,
      "learning_rate": 0.0009232673267326733,
      "loss": 2.2298,
      "step": 1155
    },
    {
      "epoch": 7.639818256918629,
      "grad_norm": 6.5789079666137695,
      "learning_rate": 0.0009227722772277229,
      "loss": 4.7088,
      "step": 1156
    },
    {
      "epoch": 7.646427096241222,
      "grad_norm": 143.55227661132812,
      "learning_rate": 0.0009222772277227724,
      "loss": 6.5169,
      "step": 1157
    },
    {
      "epoch": 7.653035935563817,
      "grad_norm": 128.75149536132812,
      "learning_rate": 0.0009217821782178218,
      "loss": 5.466,
      "step": 1158
    },
    {
      "epoch": 7.65964477488641,
      "grad_norm": 68.474365234375,
      "learning_rate": 0.0009212871287128713,
      "loss": 4.8329,
      "step": 1159
    },
    {
      "epoch": 7.666253614209005,
      "grad_norm": 18.934839248657227,
      "learning_rate": 0.0009207920792079209,
      "loss": 4.1352,
      "step": 1160
    },
    {
      "epoch": 7.672862453531598,
      "grad_norm": 75.59182739257812,
      "learning_rate": 0.0009202970297029704,
      "loss": 16.5661,
      "step": 1161
    },
    {
      "epoch": 7.679471292854193,
      "grad_norm": 39.6375732421875,
      "learning_rate": 0.0009198019801980199,
      "loss": 2.9322,
      "step": 1162
    },
    {
      "epoch": 7.686080132176786,
      "grad_norm": 53.70195770263672,
      "learning_rate": 0.0009193069306930693,
      "loss": 8.8092,
      "step": 1163
    },
    {
      "epoch": 7.692688971499381,
      "grad_norm": 65.35208129882812,
      "learning_rate": 0.0009188118811881188,
      "loss": 13.3201,
      "step": 1164
    },
    {
      "epoch": 7.6992978108219745,
      "grad_norm": 70.64901733398438,
      "learning_rate": 0.0009183168316831684,
      "loss": 2.2626,
      "step": 1165
    },
    {
      "epoch": 7.705906650144568,
      "grad_norm": 47.2166748046875,
      "learning_rate": 0.0009178217821782179,
      "loss": 3.2822,
      "step": 1166
    },
    {
      "epoch": 7.712515489467163,
      "grad_norm": 52.296756744384766,
      "learning_rate": 0.0009173267326732674,
      "loss": 2.3991,
      "step": 1167
    },
    {
      "epoch": 7.719124328789756,
      "grad_norm": 15.04054069519043,
      "learning_rate": 0.0009168316831683168,
      "loss": 3.1175,
      "step": 1168
    },
    {
      "epoch": 7.725733168112351,
      "grad_norm": 9.812904357910156,
      "learning_rate": 0.0009163366336633664,
      "loss": 1.2245,
      "step": 1169
    },
    {
      "epoch": 7.732342007434944,
      "grad_norm": 40.893489837646484,
      "learning_rate": 0.0009158415841584159,
      "loss": 4.6492,
      "step": 1170
    },
    {
      "epoch": 7.738950846757538,
      "grad_norm": 30.901561737060547,
      "learning_rate": 0.0009153465346534654,
      "loss": 3.2775,
      "step": 1171
    },
    {
      "epoch": 7.745559686080132,
      "grad_norm": 37.588722229003906,
      "learning_rate": 0.000914851485148515,
      "loss": 2.1249,
      "step": 1172
    },
    {
      "epoch": 7.752168525402726,
      "grad_norm": 92.44166564941406,
      "learning_rate": 0.0009143564356435644,
      "loss": 2.3794,
      "step": 1173
    },
    {
      "epoch": 7.75877736472532,
      "grad_norm": 15.254884719848633,
      "learning_rate": 0.0009138613861386139,
      "loss": 5.3839,
      "step": 1174
    },
    {
      "epoch": 7.765386204047914,
      "grad_norm": 48.56742477416992,
      "learning_rate": 0.0009133663366336634,
      "loss": 4.4317,
      "step": 1175
    },
    {
      "epoch": 7.771995043370508,
      "grad_norm": 44.23493576049805,
      "learning_rate": 0.0009128712871287129,
      "loss": 1.8864,
      "step": 1176
    },
    {
      "epoch": 7.778603882693102,
      "grad_norm": 24.34855079650879,
      "learning_rate": 0.0009123762376237625,
      "loss": 7.8579,
      "step": 1177
    },
    {
      "epoch": 7.785212722015696,
      "grad_norm": 29.32917594909668,
      "learning_rate": 0.0009118811881188119,
      "loss": 2.0613,
      "step": 1178
    },
    {
      "epoch": 7.79182156133829,
      "grad_norm": 34.770450592041016,
      "learning_rate": 0.0009113861386138614,
      "loss": 3.8015,
      "step": 1179
    },
    {
      "epoch": 7.798430400660884,
      "grad_norm": 28.01447868347168,
      "learning_rate": 0.0009108910891089109,
      "loss": 2.0194,
      "step": 1180
    },
    {
      "epoch": 7.805039239983478,
      "grad_norm": 82.77835083007812,
      "learning_rate": 0.0009103960396039605,
      "loss": 3.9109,
      "step": 1181
    },
    {
      "epoch": 7.811648079306072,
      "grad_norm": 69.40288543701172,
      "learning_rate": 0.00090990099009901,
      "loss": 3.869,
      "step": 1182
    },
    {
      "epoch": 7.8182569186286655,
      "grad_norm": 23.88572120666504,
      "learning_rate": 0.0009094059405940594,
      "loss": 1.2531,
      "step": 1183
    },
    {
      "epoch": 7.82486575795126,
      "grad_norm": 13.124940872192383,
      "learning_rate": 0.0009089108910891089,
      "loss": 2.7355,
      "step": 1184
    },
    {
      "epoch": 7.8314745972738535,
      "grad_norm": 115.48887634277344,
      "learning_rate": 0.0009084158415841585,
      "loss": 5.2186,
      "step": 1185
    },
    {
      "epoch": 7.838083436596448,
      "grad_norm": 95.31230926513672,
      "learning_rate": 0.000907920792079208,
      "loss": 6.6951,
      "step": 1186
    },
    {
      "epoch": 7.844692275919042,
      "grad_norm": 98.2090072631836,
      "learning_rate": 0.0009074257425742575,
      "loss": 2.107,
      "step": 1187
    },
    {
      "epoch": 7.851301115241636,
      "grad_norm": 64.18726348876953,
      "learning_rate": 0.0009069306930693069,
      "loss": 2.5544,
      "step": 1188
    },
    {
      "epoch": 7.85790995456423,
      "grad_norm": 70.38227844238281,
      "learning_rate": 0.0009064356435643565,
      "loss": 3.8586,
      "step": 1189
    },
    {
      "epoch": 7.864518793886823,
      "grad_norm": 29.179248809814453,
      "learning_rate": 0.000905940594059406,
      "loss": 2.7298,
      "step": 1190
    },
    {
      "epoch": 7.871127633209418,
      "grad_norm": 4.868260860443115,
      "learning_rate": 0.0009054455445544555,
      "loss": 3.0288,
      "step": 1191
    },
    {
      "epoch": 7.877736472532011,
      "grad_norm": 68.90760040283203,
      "learning_rate": 0.000904950495049505,
      "loss": 2.7824,
      "step": 1192
    },
    {
      "epoch": 7.884345311854606,
      "grad_norm": 78.1993179321289,
      "learning_rate": 0.0009044554455445545,
      "loss": 3.7479,
      "step": 1193
    },
    {
      "epoch": 7.8909541511771994,
      "grad_norm": 58.29353332519531,
      "learning_rate": 0.000903960396039604,
      "loss": 2.8087,
      "step": 1194
    },
    {
      "epoch": 7.897562990499793,
      "grad_norm": 10.50223159790039,
      "learning_rate": 0.0009034653465346535,
      "loss": 0.674,
      "step": 1195
    },
    {
      "epoch": 7.9041718298223875,
      "grad_norm": 58.33040237426758,
      "learning_rate": 0.000902970297029703,
      "loss": 3.3554,
      "step": 1196
    },
    {
      "epoch": 7.910780669144981,
      "grad_norm": 18.09486198425293,
      "learning_rate": 0.0009024752475247526,
      "loss": 1.6181,
      "step": 1197
    },
    {
      "epoch": 7.917389508467576,
      "grad_norm": 3.255357027053833,
      "learning_rate": 0.000901980198019802,
      "loss": 2.2214,
      "step": 1198
    },
    {
      "epoch": 7.923998347790169,
      "grad_norm": 20.284807205200195,
      "learning_rate": 0.0009014851485148515,
      "loss": 6.3733,
      "step": 1199
    },
    {
      "epoch": 7.930607187112764,
      "grad_norm": 83.45243835449219,
      "learning_rate": 0.000900990099009901,
      "loss": 4.6959,
      "step": 1200
    },
    {
      "epoch": 7.937216026435357,
      "grad_norm": 139.88693237304688,
      "learning_rate": 0.0009004950495049506,
      "loss": 4.5593,
      "step": 1201
    },
    {
      "epoch": 7.943824865757951,
      "grad_norm": 108.21247863769531,
      "learning_rate": 0.0009000000000000001,
      "loss": 5.6147,
      "step": 1202
    },
    {
      "epoch": 7.950433705080545,
      "grad_norm": 33.189762115478516,
      "learning_rate": 0.0008995049504950495,
      "loss": 3.1195,
      "step": 1203
    },
    {
      "epoch": 7.957042544403139,
      "grad_norm": 34.499481201171875,
      "learning_rate": 0.000899009900990099,
      "loss": 2.5576,
      "step": 1204
    },
    {
      "epoch": 7.9636513837257334,
      "grad_norm": 75.9495849609375,
      "learning_rate": 0.0008985148514851486,
      "loss": 3.3491,
      "step": 1205
    },
    {
      "epoch": 7.970260223048327,
      "grad_norm": 79.36792755126953,
      "learning_rate": 0.0008980198019801981,
      "loss": 4.9777,
      "step": 1206
    },
    {
      "epoch": 7.9768690623709215,
      "grad_norm": 80.74080657958984,
      "learning_rate": 0.0008975247524752476,
      "loss": 3.1587,
      "step": 1207
    },
    {
      "epoch": 7.983477901693515,
      "grad_norm": 159.2085723876953,
      "learning_rate": 0.000897029702970297,
      "loss": 8.5207,
      "step": 1208
    },
    {
      "epoch": 7.990086741016109,
      "grad_norm": 59.60451126098633,
      "learning_rate": 0.0008965346534653466,
      "loss": 2.9606,
      "step": 1209
    },
    {
      "epoch": 7.996695580338703,
      "grad_norm": 6.854497909545898,
      "learning_rate": 0.0008960396039603961,
      "loss": 3.8959,
      "step": 1210
    },
    {
      "epoch": 7.996695580338703,
      "eval_validation_error_bar": 0.04763870158768477,
      "eval_validation_loss": 7.0110297203063965,
      "eval_validation_pearsonr": 0.5651197955758441,
      "eval_validation_rmse": 2.6478350162506104,
      "eval_validation_runtime": 33.455,
      "eval_validation_samples_per_second": 6.068,
      "eval_validation_spearman": 0.6186816812534429,
      "eval_validation_steps_per_second": 6.068,
      "step": 1210
    },
    {
      "epoch": 7.996695580338703,
      "eval_test_error_bar": 0.03682025484498023,
      "eval_test_loss": 6.266271114349365,
      "eval_test_pearsonr": 0.6207218102878985,
      "eval_test_rmse": 2.5032520294189453,
      "eval_test_runtime": 36.9222,
      "eval_test_samples_per_second": 8.829,
      "eval_test_spearman": 0.6287502626091215,
      "eval_test_steps_per_second": 8.829,
      "step": 1210
    },
    {
      "epoch": 8.003304419661298,
      "grad_norm": 32.201171875,
      "learning_rate": 0.0008955445544554456,
      "loss": 2.699,
      "step": 1211
    },
    {
      "epoch": 8.009913258983891,
      "grad_norm": 6.436715602874756,
      "learning_rate": 0.0008950495049504951,
      "loss": 1.7681,
      "step": 1212
    },
    {
      "epoch": 8.016522098306485,
      "grad_norm": 41.01704788208008,
      "learning_rate": 0.0008945544554455445,
      "loss": 2.0088,
      "step": 1213
    },
    {
      "epoch": 8.023130937629078,
      "grad_norm": 105.49848175048828,
      "learning_rate": 0.0008940594059405941,
      "loss": 3.6172,
      "step": 1214
    },
    {
      "epoch": 8.029739776951672,
      "grad_norm": 119.25003051757812,
      "learning_rate": 0.0008935643564356436,
      "loss": 6.2751,
      "step": 1215
    },
    {
      "epoch": 8.036348616274267,
      "grad_norm": 25.12312126159668,
      "learning_rate": 0.0008930693069306931,
      "loss": 3.4417,
      "step": 1216
    },
    {
      "epoch": 8.042957455596861,
      "grad_norm": 48.81007766723633,
      "learning_rate": 0.0008925742574257427,
      "loss": 1.2548,
      "step": 1217
    },
    {
      "epoch": 8.049566294919455,
      "grad_norm": 71.39422607421875,
      "learning_rate": 0.0008920792079207921,
      "loss": 4.298,
      "step": 1218
    },
    {
      "epoch": 8.056175134242048,
      "grad_norm": 21.595693588256836,
      "learning_rate": 0.0008915841584158416,
      "loss": 1.9981,
      "step": 1219
    },
    {
      "epoch": 8.062783973564642,
      "grad_norm": 32.70226287841797,
      "learning_rate": 0.0008910891089108911,
      "loss": 2.031,
      "step": 1220
    },
    {
      "epoch": 8.069392812887237,
      "grad_norm": 65.08419799804688,
      "learning_rate": 0.0008905940594059406,
      "loss": 2.8044,
      "step": 1221
    },
    {
      "epoch": 8.07600165220983,
      "grad_norm": 9.094986915588379,
      "learning_rate": 0.0008900990099009902,
      "loss": 5.0304,
      "step": 1222
    },
    {
      "epoch": 8.082610491532424,
      "grad_norm": 69.96272277832031,
      "learning_rate": 0.0008896039603960396,
      "loss": 3.277,
      "step": 1223
    },
    {
      "epoch": 8.089219330855018,
      "grad_norm": 3.777574062347412,
      "learning_rate": 0.0008891089108910891,
      "loss": 3.3861,
      "step": 1224
    },
    {
      "epoch": 8.095828170177613,
      "grad_norm": 53.37098693847656,
      "learning_rate": 0.0008886138613861386,
      "loss": 1.5883,
      "step": 1225
    },
    {
      "epoch": 8.102437009500207,
      "grad_norm": 4.116157531738281,
      "learning_rate": 0.0008881188118811882,
      "loss": 4.9694,
      "step": 1226
    },
    {
      "epoch": 8.1090458488228,
      "grad_norm": 21.84382438659668,
      "learning_rate": 0.0008876237623762377,
      "loss": 3.6243,
      "step": 1227
    },
    {
      "epoch": 8.115654688145394,
      "grad_norm": 31.454345703125,
      "learning_rate": 0.0008871287128712871,
      "loss": 1.988,
      "step": 1228
    },
    {
      "epoch": 8.122263527467988,
      "grad_norm": 57.374366760253906,
      "learning_rate": 0.0008866336633663366,
      "loss": 2.8386,
      "step": 1229
    },
    {
      "epoch": 8.128872366790583,
      "grad_norm": 7.500523567199707,
      "learning_rate": 0.0008861386138613862,
      "loss": 1.9372,
      "step": 1230
    },
    {
      "epoch": 8.135481206113177,
      "grad_norm": 37.885215759277344,
      "learning_rate": 0.0008856435643564357,
      "loss": 2.7902,
      "step": 1231
    },
    {
      "epoch": 8.14209004543577,
      "grad_norm": 42.9238395690918,
      "learning_rate": 0.0008851485148514852,
      "loss": 2.7418,
      "step": 1232
    },
    {
      "epoch": 8.148698884758364,
      "grad_norm": 10.908684730529785,
      "learning_rate": 0.0008846534653465346,
      "loss": 1.7528,
      "step": 1233
    },
    {
      "epoch": 8.155307724080957,
      "grad_norm": 5.659917831420898,
      "learning_rate": 0.0008841584158415842,
      "loss": 1.206,
      "step": 1234
    },
    {
      "epoch": 8.161916563403553,
      "grad_norm": 98.67951202392578,
      "learning_rate": 0.0008836633663366337,
      "loss": 4.4413,
      "step": 1235
    },
    {
      "epoch": 8.168525402726146,
      "grad_norm": 38.653289794921875,
      "learning_rate": 0.0008831683168316832,
      "loss": 2.7166,
      "step": 1236
    },
    {
      "epoch": 8.17513424204874,
      "grad_norm": 28.3868350982666,
      "learning_rate": 0.0008826732673267327,
      "loss": 3.0885,
      "step": 1237
    },
    {
      "epoch": 8.181743081371334,
      "grad_norm": 36.232845306396484,
      "learning_rate": 0.0008821782178217822,
      "loss": 2.2619,
      "step": 1238
    },
    {
      "epoch": 8.188351920693927,
      "grad_norm": 8.26015567779541,
      "learning_rate": 0.0008816831683168317,
      "loss": 1.8732,
      "step": 1239
    },
    {
      "epoch": 8.194960760016523,
      "grad_norm": 45.709625244140625,
      "learning_rate": 0.0008811881188118812,
      "loss": 4.8226,
      "step": 1240
    },
    {
      "epoch": 8.201569599339116,
      "grad_norm": 4.612876892089844,
      "learning_rate": 0.0008806930693069307,
      "loss": 0.8723,
      "step": 1241
    },
    {
      "epoch": 8.20817843866171,
      "grad_norm": 98.22064971923828,
      "learning_rate": 0.0008801980198019803,
      "loss": 3.1102,
      "step": 1242
    },
    {
      "epoch": 8.214787277984303,
      "grad_norm": 148.788330078125,
      "learning_rate": 0.0008797029702970297,
      "loss": 6.0241,
      "step": 1243
    },
    {
      "epoch": 8.221396117306899,
      "grad_norm": 125.92990112304688,
      "learning_rate": 0.0008792079207920792,
      "loss": 7.3499,
      "step": 1244
    },
    {
      "epoch": 8.228004956629492,
      "grad_norm": 55.79713439941406,
      "learning_rate": 0.0008787128712871287,
      "loss": 2.9717,
      "step": 1245
    },
    {
      "epoch": 8.234613795952086,
      "grad_norm": 80.61077117919922,
      "learning_rate": 0.0008782178217821783,
      "loss": 4.9468,
      "step": 1246
    },
    {
      "epoch": 8.24122263527468,
      "grad_norm": 15.308897972106934,
      "learning_rate": 0.0008777227722772278,
      "loss": 1.2932,
      "step": 1247
    },
    {
      "epoch": 8.247831474597273,
      "grad_norm": 77.447998046875,
      "learning_rate": 0.0008772277227722772,
      "loss": 2.4091,
      "step": 1248
    },
    {
      "epoch": 8.254440313919869,
      "grad_norm": 79.86831665039062,
      "learning_rate": 0.0008767326732673267,
      "loss": 2.4752,
      "step": 1249
    },
    {
      "epoch": 8.261049153242462,
      "grad_norm": 143.64028930664062,
      "learning_rate": 0.0008762376237623763,
      "loss": 6.5849,
      "step": 1250
    },
    {
      "epoch": 8.267657992565056,
      "grad_norm": 95.98489379882812,
      "learning_rate": 0.0008757425742574258,
      "loss": 4.5888,
      "step": 1251
    },
    {
      "epoch": 8.27426683188765,
      "grad_norm": 60.25346755981445,
      "learning_rate": 0.0008752475247524753,
      "loss": 4.8736,
      "step": 1252
    },
    {
      "epoch": 8.280875671210243,
      "grad_norm": 50.68522262573242,
      "learning_rate": 0.0008747524752475247,
      "loss": 2.8196,
      "step": 1253
    },
    {
      "epoch": 8.287484510532838,
      "grad_norm": 24.01735496520996,
      "learning_rate": 0.0008742574257425743,
      "loss": 2.1571,
      "step": 1254
    },
    {
      "epoch": 8.294093349855432,
      "grad_norm": 4.526029586791992,
      "learning_rate": 0.0008737623762376238,
      "loss": 3.2245,
      "step": 1255
    },
    {
      "epoch": 8.300702189178025,
      "grad_norm": 60.654090881347656,
      "learning_rate": 0.0008732673267326733,
      "loss": 2.8247,
      "step": 1256
    },
    {
      "epoch": 8.307311028500619,
      "grad_norm": 49.53187942504883,
      "learning_rate": 0.0008727722772277228,
      "loss": 3.9317,
      "step": 1257
    },
    {
      "epoch": 8.313919867823213,
      "grad_norm": 77.2968521118164,
      "learning_rate": 0.0008722772277227722,
      "loss": 3.6938,
      "step": 1258
    },
    {
      "epoch": 8.320528707145808,
      "grad_norm": 86.67333221435547,
      "learning_rate": 0.0008717821782178218,
      "loss": 2.8274,
      "step": 1259
    },
    {
      "epoch": 8.327137546468402,
      "grad_norm": 26.9143123626709,
      "learning_rate": 0.0008712871287128713,
      "loss": 2.2545,
      "step": 1260
    },
    {
      "epoch": 8.333746385790995,
      "grad_norm": 10.070039749145508,
      "learning_rate": 0.0008707920792079208,
      "loss": 4.7661,
      "step": 1261
    },
    {
      "epoch": 8.340355225113589,
      "grad_norm": 65.74390411376953,
      "learning_rate": 0.0008702970297029704,
      "loss": 5.0481,
      "step": 1262
    },
    {
      "epoch": 8.346964064436184,
      "grad_norm": 69.86429595947266,
      "learning_rate": 0.0008698019801980198,
      "loss": 2.6309,
      "step": 1263
    },
    {
      "epoch": 8.353572903758778,
      "grad_norm": 35.829803466796875,
      "learning_rate": 0.0008693069306930693,
      "loss": 2.8195,
      "step": 1264
    },
    {
      "epoch": 8.360181743081371,
      "grad_norm": 106.87882232666016,
      "learning_rate": 0.0008688118811881188,
      "loss": 3.2447,
      "step": 1265
    },
    {
      "epoch": 8.366790582403965,
      "grad_norm": 34.099849700927734,
      "learning_rate": 0.0008683168316831684,
      "loss": 3.5487,
      "step": 1266
    },
    {
      "epoch": 8.373399421726559,
      "grad_norm": 10.094164848327637,
      "learning_rate": 0.0008678217821782179,
      "loss": 1.5435,
      "step": 1267
    },
    {
      "epoch": 8.380008261049154,
      "grad_norm": 5.623963832855225,
      "learning_rate": 0.0008673267326732673,
      "loss": 6.6965,
      "step": 1268
    },
    {
      "epoch": 8.386617100371748,
      "grad_norm": 26.25393295288086,
      "learning_rate": 0.0008668316831683168,
      "loss": 1.5065,
      "step": 1269
    },
    {
      "epoch": 8.393225939694341,
      "grad_norm": 5.477381706237793,
      "learning_rate": 0.0008663366336633663,
      "loss": 1.1341,
      "step": 1270
    },
    {
      "epoch": 8.399834779016935,
      "grad_norm": 11.110857963562012,
      "learning_rate": 0.0008658415841584159,
      "loss": 3.5573,
      "step": 1271
    },
    {
      "epoch": 8.406443618339528,
      "grad_norm": 20.871057510375977,
      "learning_rate": 0.0008653465346534654,
      "loss": 2.075,
      "step": 1272
    },
    {
      "epoch": 8.413052457662124,
      "grad_norm": 20.254518508911133,
      "learning_rate": 0.0008648514851485148,
      "loss": 3.0758,
      "step": 1273
    },
    {
      "epoch": 8.419661296984717,
      "grad_norm": 70.3098373413086,
      "learning_rate": 0.0008643564356435643,
      "loss": 1.9157,
      "step": 1274
    },
    {
      "epoch": 8.426270136307311,
      "grad_norm": 37.20807647705078,
      "learning_rate": 0.0008638613861386139,
      "loss": 2.8019,
      "step": 1275
    },
    {
      "epoch": 8.432878975629905,
      "grad_norm": 61.09765625,
      "learning_rate": 0.0008633663366336634,
      "loss": 2.0607,
      "step": 1276
    },
    {
      "epoch": 8.439487814952498,
      "grad_norm": 28.689680099487305,
      "learning_rate": 0.0008628712871287129,
      "loss": 2.7061,
      "step": 1277
    },
    {
      "epoch": 8.446096654275093,
      "grad_norm": 8.005817413330078,
      "learning_rate": 0.0008623762376237623,
      "loss": 2.021,
      "step": 1278
    },
    {
      "epoch": 8.452705493597687,
      "grad_norm": 80.91885375976562,
      "learning_rate": 0.0008618811881188119,
      "loss": 2.6147,
      "step": 1279
    },
    {
      "epoch": 8.45931433292028,
      "grad_norm": 95.788330078125,
      "learning_rate": 0.0008613861386138614,
      "loss": 2.1427,
      "step": 1280
    },
    {
      "epoch": 8.465923172242874,
      "grad_norm": 185.90322875976562,
      "learning_rate": 0.0008608910891089109,
      "loss": 7.785,
      "step": 1281
    },
    {
      "epoch": 8.47253201156547,
      "grad_norm": 108.67919158935547,
      "learning_rate": 0.0008603960396039604,
      "loss": 3.6719,
      "step": 1282
    },
    {
      "epoch": 8.479140850888063,
      "grad_norm": 106.37017059326172,
      "learning_rate": 0.0008599009900990099,
      "loss": 4.4737,
      "step": 1283
    },
    {
      "epoch": 8.485749690210657,
      "grad_norm": 81.55278778076172,
      "learning_rate": 0.0008594059405940594,
      "loss": 3.3125,
      "step": 1284
    },
    {
      "epoch": 8.49235852953325,
      "grad_norm": 60.16525650024414,
      "learning_rate": 0.0008589108910891089,
      "loss": 2.8012,
      "step": 1285
    },
    {
      "epoch": 8.498967368855844,
      "grad_norm": 47.99127197265625,
      "learning_rate": 0.0008584158415841584,
      "loss": 3.1589,
      "step": 1286
    },
    {
      "epoch": 8.50557620817844,
      "grad_norm": 40.41244888305664,
      "learning_rate": 0.000857920792079208,
      "loss": 3.3496,
      "step": 1287
    },
    {
      "epoch": 8.512185047501033,
      "grad_norm": 145.75830078125,
      "learning_rate": 0.0008574257425742574,
      "loss": 8.2009,
      "step": 1288
    },
    {
      "epoch": 8.518793886823627,
      "grad_norm": 161.06185913085938,
      "learning_rate": 0.0008569306930693069,
      "loss": 7.53,
      "step": 1289
    },
    {
      "epoch": 8.52540272614622,
      "grad_norm": 106.46698760986328,
      "learning_rate": 0.0008564356435643564,
      "loss": 5.8523,
      "step": 1290
    },
    {
      "epoch": 8.532011565468814,
      "grad_norm": 176.2162322998047,
      "learning_rate": 0.000855940594059406,
      "loss": 7.5067,
      "step": 1291
    },
    {
      "epoch": 8.53862040479141,
      "grad_norm": 93.15103912353516,
      "learning_rate": 0.0008554455445544555,
      "loss": 6.8564,
      "step": 1292
    },
    {
      "epoch": 8.545229244114003,
      "grad_norm": 21.60439682006836,
      "learning_rate": 0.0008549504950495049,
      "loss": 3.3068,
      "step": 1293
    },
    {
      "epoch": 8.551838083436596,
      "grad_norm": 3.4634032249450684,
      "learning_rate": 0.0008544554455445544,
      "loss": 3.9484,
      "step": 1294
    },
    {
      "epoch": 8.55844692275919,
      "grad_norm": 18.036821365356445,
      "learning_rate": 0.000853960396039604,
      "loss": 1.826,
      "step": 1295
    },
    {
      "epoch": 8.565055762081784,
      "grad_norm": 77.17325592041016,
      "learning_rate": 0.0008534653465346535,
      "loss": 2.8883,
      "step": 1296
    },
    {
      "epoch": 8.571664601404379,
      "grad_norm": 45.417236328125,
      "learning_rate": 0.000852970297029703,
      "loss": 2.4928,
      "step": 1297
    },
    {
      "epoch": 8.578273440726973,
      "grad_norm": 31.841373443603516,
      "learning_rate": 0.0008524752475247524,
      "loss": 4.242,
      "step": 1298
    },
    {
      "epoch": 8.584882280049566,
      "grad_norm": 116.04981231689453,
      "learning_rate": 0.000851980198019802,
      "loss": 4.3216,
      "step": 1299
    },
    {
      "epoch": 8.59149111937216,
      "grad_norm": 130.20774841308594,
      "learning_rate": 0.0008514851485148515,
      "loss": 10.4534,
      "step": 1300
    },
    {
      "epoch": 8.598099958694753,
      "grad_norm": 4.736927032470703,
      "learning_rate": 0.000850990099009901,
      "loss": 2.208,
      "step": 1301
    },
    {
      "epoch": 8.604708798017349,
      "grad_norm": 41.8231315612793,
      "learning_rate": 0.0008504950495049505,
      "loss": 2.2742,
      "step": 1302
    },
    {
      "epoch": 8.611317637339942,
      "grad_norm": 9.865591049194336,
      "learning_rate": 0.00085,
      "loss": 1.8798,
      "step": 1303
    },
    {
      "epoch": 8.617926476662536,
      "grad_norm": 36.203922271728516,
      "learning_rate": 0.0008495049504950495,
      "loss": 1.5508,
      "step": 1304
    },
    {
      "epoch": 8.62453531598513,
      "grad_norm": 60.10392761230469,
      "learning_rate": 0.000849009900990099,
      "loss": 2.8387,
      "step": 1305
    },
    {
      "epoch": 8.631144155307725,
      "grad_norm": 12.011625289916992,
      "learning_rate": 0.0008485148514851485,
      "loss": 5.0721,
      "step": 1306
    },
    {
      "epoch": 8.637752994630318,
      "grad_norm": 54.491546630859375,
      "learning_rate": 0.0008480198019801981,
      "loss": 3.9265,
      "step": 1307
    },
    {
      "epoch": 8.644361833952912,
      "grad_norm": 29.36775016784668,
      "learning_rate": 0.0008475247524752475,
      "loss": 3.5569,
      "step": 1308
    },
    {
      "epoch": 8.650970673275506,
      "grad_norm": 57.047908782958984,
      "learning_rate": 0.000847029702970297,
      "loss": 3.0156,
      "step": 1309
    },
    {
      "epoch": 8.6575795125981,
      "grad_norm": 3.417064666748047,
      "learning_rate": 0.0008465346534653465,
      "loss": 5.3751,
      "step": 1310
    },
    {
      "epoch": 8.664188351920695,
      "grad_norm": 29.787961959838867,
      "learning_rate": 0.000846039603960396,
      "loss": 3.8098,
      "step": 1311
    },
    {
      "epoch": 8.670797191243288,
      "grad_norm": 8.36147689819336,
      "learning_rate": 0.0008455445544554456,
      "loss": 2.0309,
      "step": 1312
    },
    {
      "epoch": 8.677406030565882,
      "grad_norm": 53.988529205322266,
      "learning_rate": 0.000845049504950495,
      "loss": 3.4727,
      "step": 1313
    },
    {
      "epoch": 8.684014869888475,
      "grad_norm": 72.08236694335938,
      "learning_rate": 0.0008445544554455445,
      "loss": 3.568,
      "step": 1314
    },
    {
      "epoch": 8.69062370921107,
      "grad_norm": 17.211023330688477,
      "learning_rate": 0.000844059405940594,
      "loss": 0.8692,
      "step": 1315
    },
    {
      "epoch": 8.697232548533664,
      "grad_norm": 13.690621376037598,
      "learning_rate": 0.0008435643564356436,
      "loss": 2.2154,
      "step": 1316
    },
    {
      "epoch": 8.703841387856258,
      "grad_norm": 5.7728071212768555,
      "learning_rate": 0.0008430693069306931,
      "loss": 3.9033,
      "step": 1317
    },
    {
      "epoch": 8.710450227178852,
      "grad_norm": 13.639363288879395,
      "learning_rate": 0.0008425742574257425,
      "loss": 1.3639,
      "step": 1318
    },
    {
      "epoch": 8.717059066501445,
      "grad_norm": 11.575243949890137,
      "learning_rate": 0.000842079207920792,
      "loss": 1.8392,
      "step": 1319
    },
    {
      "epoch": 8.72366790582404,
      "grad_norm": 49.36115646362305,
      "learning_rate": 0.0008415841584158416,
      "loss": 1.9304,
      "step": 1320
    },
    {
      "epoch": 8.730276745146634,
      "grad_norm": 76.52705383300781,
      "learning_rate": 0.0008410891089108911,
      "loss": 4.0419,
      "step": 1321
    },
    {
      "epoch": 8.736885584469228,
      "grad_norm": 41.331382751464844,
      "learning_rate": 0.0008405940594059406,
      "loss": 4.7338,
      "step": 1322
    },
    {
      "epoch": 8.743494423791821,
      "grad_norm": 18.52190399169922,
      "learning_rate": 0.00084009900990099,
      "loss": 2.9643,
      "step": 1323
    },
    {
      "epoch": 8.750103263114415,
      "grad_norm": 25.871679306030273,
      "learning_rate": 0.0008396039603960396,
      "loss": 1.961,
      "step": 1324
    },
    {
      "epoch": 8.75671210243701,
      "grad_norm": 8.821419715881348,
      "learning_rate": 0.0008391089108910891,
      "loss": 1.5475,
      "step": 1325
    },
    {
      "epoch": 8.763320941759604,
      "grad_norm": 13.875980377197266,
      "learning_rate": 0.0008386138613861386,
      "loss": 5.3994,
      "step": 1326
    },
    {
      "epoch": 8.769929781082197,
      "grad_norm": 13.089079856872559,
      "learning_rate": 0.0008381188118811881,
      "loss": 3.7644,
      "step": 1327
    },
    {
      "epoch": 8.776538620404791,
      "grad_norm": 10.966923713684082,
      "learning_rate": 0.0008376237623762376,
      "loss": 2.7897,
      "step": 1328
    },
    {
      "epoch": 8.783147459727385,
      "grad_norm": 46.9587287902832,
      "learning_rate": 0.0008371287128712871,
      "loss": 1.8122,
      "step": 1329
    },
    {
      "epoch": 8.78975629904998,
      "grad_norm": 6.744250774383545,
      "learning_rate": 0.0008366336633663366,
      "loss": 3.2444,
      "step": 1330
    },
    {
      "epoch": 8.796365138372574,
      "grad_norm": 33.59452819824219,
      "learning_rate": 0.0008361386138613861,
      "loss": 1.4134,
      "step": 1331
    },
    {
      "epoch": 8.802973977695167,
      "grad_norm": 39.57535171508789,
      "learning_rate": 0.0008356435643564357,
      "loss": 1.7012,
      "step": 1332
    },
    {
      "epoch": 8.80958281701776,
      "grad_norm": 27.10902976989746,
      "learning_rate": 0.0008351485148514851,
      "loss": 1.6547,
      "step": 1333
    },
    {
      "epoch": 8.816191656340354,
      "grad_norm": 82.0682373046875,
      "learning_rate": 0.0008346534653465346,
      "loss": 5.8895,
      "step": 1334
    },
    {
      "epoch": 8.82280049566295,
      "grad_norm": 5.091910362243652,
      "learning_rate": 0.0008341584158415841,
      "loss": 2.7596,
      "step": 1335
    },
    {
      "epoch": 8.829409334985543,
      "grad_norm": 97.2220230102539,
      "learning_rate": 0.0008336633663366337,
      "loss": 5.9882,
      "step": 1336
    },
    {
      "epoch": 8.836018174308137,
      "grad_norm": 43.31476974487305,
      "learning_rate": 0.0008331683168316832,
      "loss": 3.0083,
      "step": 1337
    },
    {
      "epoch": 8.84262701363073,
      "grad_norm": 29.625967025756836,
      "learning_rate": 0.0008326732673267326,
      "loss": 2.0183,
      "step": 1338
    },
    {
      "epoch": 8.849235852953324,
      "grad_norm": 48.8746452331543,
      "learning_rate": 0.0008321782178217821,
      "loss": 2.1222,
      "step": 1339
    },
    {
      "epoch": 8.85584469227592,
      "grad_norm": 65.72369384765625,
      "learning_rate": 0.0008316831683168317,
      "loss": 3.3101,
      "step": 1340
    },
    {
      "epoch": 8.862453531598513,
      "grad_norm": 65.54603576660156,
      "learning_rate": 0.0008311881188118812,
      "loss": 3.0547,
      "step": 1341
    },
    {
      "epoch": 8.869062370921107,
      "grad_norm": 15.842788696289062,
      "learning_rate": 0.0008306930693069307,
      "loss": 1.7917,
      "step": 1342
    },
    {
      "epoch": 8.8756712102437,
      "grad_norm": 22.16660499572754,
      "learning_rate": 0.0008301980198019801,
      "loss": 2.3663,
      "step": 1343
    },
    {
      "epoch": 8.882280049566296,
      "grad_norm": 14.786896705627441,
      "learning_rate": 0.0008297029702970297,
      "loss": 5.6033,
      "step": 1344
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 1.3824461698532104,
      "learning_rate": 0.0008292079207920792,
      "loss": 1.6867,
      "step": 1345
    },
    {
      "epoch": 8.895497728211483,
      "grad_norm": 42.6217041015625,
      "learning_rate": 0.0008287128712871287,
      "loss": 3.1512,
      "step": 1346
    },
    {
      "epoch": 8.902106567534076,
      "grad_norm": 63.970298767089844,
      "learning_rate": 0.0008282178217821782,
      "loss": 3.7559,
      "step": 1347
    },
    {
      "epoch": 8.90871540685667,
      "grad_norm": 14.387022972106934,
      "learning_rate": 0.0008277227722772277,
      "loss": 5.9703,
      "step": 1348
    },
    {
      "epoch": 8.915324246179265,
      "grad_norm": 7.006114959716797,
      "learning_rate": 0.0008272277227722772,
      "loss": 2.3796,
      "step": 1349
    },
    {
      "epoch": 8.921933085501859,
      "grad_norm": 48.069828033447266,
      "learning_rate": 0.0008267326732673267,
      "loss": 4.4662,
      "step": 1350
    },
    {
      "epoch": 8.928541924824453,
      "grad_norm": 51.33463668823242,
      "learning_rate": 0.0008262376237623762,
      "loss": 3.6007,
      "step": 1351
    },
    {
      "epoch": 8.935150764147046,
      "grad_norm": 15.564515113830566,
      "learning_rate": 0.0008257425742574258,
      "loss": 2.3754,
      "step": 1352
    },
    {
      "epoch": 8.94175960346964,
      "grad_norm": 104.14495849609375,
      "learning_rate": 0.0008252475247524752,
      "loss": 7.2536,
      "step": 1353
    },
    {
      "epoch": 8.948368442792235,
      "grad_norm": 16.589685440063477,
      "learning_rate": 0.0008247524752475247,
      "loss": 1.3953,
      "step": 1354
    },
    {
      "epoch": 8.954977282114829,
      "grad_norm": 34.95689010620117,
      "learning_rate": 0.0008242574257425742,
      "loss": 5.5246,
      "step": 1355
    },
    {
      "epoch": 8.961586121437422,
      "grad_norm": 15.731748580932617,
      "learning_rate": 0.0008237623762376238,
      "loss": 4.3354,
      "step": 1356
    },
    {
      "epoch": 8.968194960760016,
      "grad_norm": 17.25478744506836,
      "learning_rate": 0.0008232673267326733,
      "loss": 3.1995,
      "step": 1357
    },
    {
      "epoch": 8.974803800082611,
      "grad_norm": 68.9927978515625,
      "learning_rate": 0.0008227722772277227,
      "loss": 2.0054,
      "step": 1358
    },
    {
      "epoch": 8.981412639405205,
      "grad_norm": 75.37071228027344,
      "learning_rate": 0.0008222772277227722,
      "loss": 4.8482,
      "step": 1359
    },
    {
      "epoch": 8.988021478727799,
      "grad_norm": 80.39096069335938,
      "learning_rate": 0.0008217821782178218,
      "loss": 6.3712,
      "step": 1360
    },
    {
      "epoch": 8.994630318050392,
      "grad_norm": 7.654198169708252,
      "learning_rate": 0.0008212871287128713,
      "loss": 2.6806,
      "step": 1361
    },
    {
      "epoch": 8.994630318050392,
      "eval_validation_error_bar": 0.04691968646107335,
      "eval_validation_loss": 6.564840316772461,
      "eval_validation_pearsonr": 0.5961983902158716,
      "eval_validation_rmse": 2.562194347381592,
      "eval_validation_runtime": 33.2451,
      "eval_validation_samples_per_second": 6.106,
      "eval_validation_spearman": 0.6272478840938167,
      "eval_validation_steps_per_second": 6.106,
      "step": 1361
    },
    {
      "epoch": 8.994630318050392,
      "eval_test_error_bar": 0.03966828175719563,
      "eval_test_loss": 6.7235589027404785,
      "eval_test_pearsonr": 0.5783884210246075,
      "eval_test_rmse": 2.592982530593872,
      "eval_test_runtime": 38.2376,
      "eval_test_samples_per_second": 8.526,
      "eval_test_spearman": 0.5839920793073853,
      "eval_test_steps_per_second": 8.526,
      "step": 1361
    },
    {
      "epoch": 9.001239157372986,
      "grad_norm": 5.563878536224365,
      "learning_rate": 0.0008207920792079208,
      "loss": 1.6576,
      "step": 1362
    },
    {
      "epoch": 9.007847996695581,
      "grad_norm": 25.66364097595215,
      "learning_rate": 0.0008202970297029702,
      "loss": 1.3156,
      "step": 1363
    },
    {
      "epoch": 9.014456836018175,
      "grad_norm": 90.35309600830078,
      "learning_rate": 0.0008198019801980197,
      "loss": 6.4756,
      "step": 1364
    },
    {
      "epoch": 9.021065675340768,
      "grad_norm": 12.759489059448242,
      "learning_rate": 0.0008193069306930693,
      "loss": 2.2967,
      "step": 1365
    },
    {
      "epoch": 9.027674514663362,
      "grad_norm": 27.697484970092773,
      "learning_rate": 0.0008188118811881188,
      "loss": 1.8169,
      "step": 1366
    },
    {
      "epoch": 9.034283353985956,
      "grad_norm": 5.648340702056885,
      "learning_rate": 0.0008183168316831683,
      "loss": 2.1968,
      "step": 1367
    },
    {
      "epoch": 9.04089219330855,
      "grad_norm": 35.60459518432617,
      "learning_rate": 0.0008178217821782177,
      "loss": 4.9029,
      "step": 1368
    },
    {
      "epoch": 9.047501032631144,
      "grad_norm": 46.17405700683594,
      "learning_rate": 0.0008173267326732673,
      "loss": 2.7088,
      "step": 1369
    },
    {
      "epoch": 9.054109871953738,
      "grad_norm": 12.856758117675781,
      "learning_rate": 0.0008168316831683168,
      "loss": 2.316,
      "step": 1370
    },
    {
      "epoch": 9.060718711276332,
      "grad_norm": 41.18098449707031,
      "learning_rate": 0.0008163366336633663,
      "loss": 1.9354,
      "step": 1371
    },
    {
      "epoch": 9.067327550598925,
      "grad_norm": 4.436203479766846,
      "learning_rate": 0.0008158415841584159,
      "loss": 2.1301,
      "step": 1372
    },
    {
      "epoch": 9.07393638992152,
      "grad_norm": 8.904935836791992,
      "learning_rate": 0.0008153465346534653,
      "loss": 2.9468,
      "step": 1373
    },
    {
      "epoch": 9.080545229244114,
      "grad_norm": 9.229334831237793,
      "learning_rate": 0.0008148514851485148,
      "loss": 4.1997,
      "step": 1374
    },
    {
      "epoch": 9.087154068566708,
      "grad_norm": 7.1252617835998535,
      "learning_rate": 0.0008143564356435643,
      "loss": 2.0364,
      "step": 1375
    },
    {
      "epoch": 9.093762907889301,
      "grad_norm": 21.99704360961914,
      "learning_rate": 0.0008138613861386138,
      "loss": 0.8226,
      "step": 1376
    },
    {
      "epoch": 9.100371747211897,
      "grad_norm": 13.284268379211426,
      "learning_rate": 0.0008133663366336634,
      "loss": 4.6008,
      "step": 1377
    },
    {
      "epoch": 9.10698058653449,
      "grad_norm": 24.080442428588867,
      "learning_rate": 0.0008128712871287128,
      "loss": 4.1717,
      "step": 1378
    },
    {
      "epoch": 9.113589425857084,
      "grad_norm": 41.16547775268555,
      "learning_rate": 0.0008123762376237624,
      "loss": 1.3584,
      "step": 1379
    },
    {
      "epoch": 9.120198265179678,
      "grad_norm": 46.70600128173828,
      "learning_rate": 0.000811881188118812,
      "loss": 3.2086,
      "step": 1380
    },
    {
      "epoch": 9.126807104502271,
      "grad_norm": 35.417510986328125,
      "learning_rate": 0.0008113861386138615,
      "loss": 5.0989,
      "step": 1381
    },
    {
      "epoch": 9.133415943824867,
      "grad_norm": 44.229740142822266,
      "learning_rate": 0.000810891089108911,
      "loss": 7.6148,
      "step": 1382
    },
    {
      "epoch": 9.14002478314746,
      "grad_norm": 5.063571929931641,
      "learning_rate": 0.0008103960396039604,
      "loss": 4.4328,
      "step": 1383
    },
    {
      "epoch": 9.146633622470054,
      "grad_norm": 75.33045959472656,
      "learning_rate": 0.00080990099009901,
      "loss": 3.4562,
      "step": 1384
    },
    {
      "epoch": 9.153242461792647,
      "grad_norm": 58.84640884399414,
      "learning_rate": 0.0008094059405940595,
      "loss": 5.7469,
      "step": 1385
    },
    {
      "epoch": 9.159851301115241,
      "grad_norm": 10.20223331451416,
      "learning_rate": 0.000808910891089109,
      "loss": 3.9247,
      "step": 1386
    },
    {
      "epoch": 9.166460140437836,
      "grad_norm": 25.20694351196289,
      "learning_rate": 0.0008084158415841585,
      "loss": 2.7924,
      "step": 1387
    },
    {
      "epoch": 9.17306897976043,
      "grad_norm": 23.657875061035156,
      "learning_rate": 0.0008079207920792079,
      "loss": 3.5975,
      "step": 1388
    },
    {
      "epoch": 9.179677819083023,
      "grad_norm": 21.001306533813477,
      "learning_rate": 0.0008074257425742575,
      "loss": 4.946,
      "step": 1389
    },
    {
      "epoch": 9.186286658405617,
      "grad_norm": 71.60142517089844,
      "learning_rate": 0.000806930693069307,
      "loss": 2.3119,
      "step": 1390
    },
    {
      "epoch": 9.19289549772821,
      "grad_norm": 108.6733169555664,
      "learning_rate": 0.0008064356435643565,
      "loss": 6.6353,
      "step": 1391
    },
    {
      "epoch": 9.199504337050806,
      "grad_norm": 62.56543731689453,
      "learning_rate": 0.000805940594059406,
      "loss": 3.5844,
      "step": 1392
    },
    {
      "epoch": 9.2061131763734,
      "grad_norm": 31.51817512512207,
      "learning_rate": 0.0008054455445544555,
      "loss": 2.0888,
      "step": 1393
    },
    {
      "epoch": 9.212722015695993,
      "grad_norm": 35.664005279541016,
      "learning_rate": 0.000804950495049505,
      "loss": 4.0781,
      "step": 1394
    },
    {
      "epoch": 9.219330855018587,
      "grad_norm": 57.18752670288086,
      "learning_rate": 0.0008044554455445545,
      "loss": 5.0914,
      "step": 1395
    },
    {
      "epoch": 9.225939694341182,
      "grad_norm": 44.36833190917969,
      "learning_rate": 0.000803960396039604,
      "loss": 2.1142,
      "step": 1396
    },
    {
      "epoch": 9.232548533663776,
      "grad_norm": 7.464457035064697,
      "learning_rate": 0.0008034653465346536,
      "loss": 2.2652,
      "step": 1397
    },
    {
      "epoch": 9.23915737298637,
      "grad_norm": 46.5346565246582,
      "learning_rate": 0.000802970297029703,
      "loss": 2.4204,
      "step": 1398
    },
    {
      "epoch": 9.245766212308963,
      "grad_norm": 39.94915771484375,
      "learning_rate": 0.0008024752475247525,
      "loss": 2.182,
      "step": 1399
    },
    {
      "epoch": 9.252375051631557,
      "grad_norm": 14.963604927062988,
      "learning_rate": 0.000801980198019802,
      "loss": 1.3001,
      "step": 1400
    },
    {
      "epoch": 9.258983890954152,
      "grad_norm": 19.64811897277832,
      "learning_rate": 0.0008014851485148516,
      "loss": 2.3936,
      "step": 1401
    },
    {
      "epoch": 9.265592730276746,
      "grad_norm": 10.946341514587402,
      "learning_rate": 0.0008009900990099011,
      "loss": 2.5887,
      "step": 1402
    },
    {
      "epoch": 9.27220156959934,
      "grad_norm": 43.43373107910156,
      "learning_rate": 0.0008004950495049505,
      "loss": 4.2535,
      "step": 1403
    },
    {
      "epoch": 9.278810408921933,
      "grad_norm": 75.79165649414062,
      "learning_rate": 0.0008,
      "loss": 5.451,
      "step": 1404
    },
    {
      "epoch": 9.285419248244526,
      "grad_norm": 17.224592208862305,
      "learning_rate": 0.0007995049504950496,
      "loss": 0.9826,
      "step": 1405
    },
    {
      "epoch": 9.292028087567122,
      "grad_norm": 10.448694229125977,
      "learning_rate": 0.0007990099009900991,
      "loss": 1.8447,
      "step": 1406
    },
    {
      "epoch": 9.298636926889715,
      "grad_norm": 34.35769271850586,
      "learning_rate": 0.0007985148514851486,
      "loss": 0.6112,
      "step": 1407
    },
    {
      "epoch": 9.305245766212309,
      "grad_norm": 33.92228317260742,
      "learning_rate": 0.000798019801980198,
      "loss": 4.7315,
      "step": 1408
    },
    {
      "epoch": 9.311854605534903,
      "grad_norm": 18.937036514282227,
      "learning_rate": 0.0007975247524752476,
      "loss": 2.5249,
      "step": 1409
    },
    {
      "epoch": 9.318463444857496,
      "grad_norm": 8.946035385131836,
      "learning_rate": 0.0007970297029702971,
      "loss": 2.0516,
      "step": 1410
    },
    {
      "epoch": 9.325072284180091,
      "grad_norm": 26.22738265991211,
      "learning_rate": 0.0007965346534653466,
      "loss": 1.4278,
      "step": 1411
    },
    {
      "epoch": 9.331681123502685,
      "grad_norm": 67.37518310546875,
      "learning_rate": 0.0007960396039603961,
      "loss": 2.7587,
      "step": 1412
    },
    {
      "epoch": 9.338289962825279,
      "grad_norm": 79.05218505859375,
      "learning_rate": 0.0007955445544554456,
      "loss": 2.8284,
      "step": 1413
    },
    {
      "epoch": 9.344898802147872,
      "grad_norm": 44.90718460083008,
      "learning_rate": 0.0007950495049504951,
      "loss": 2.6924,
      "step": 1414
    },
    {
      "epoch": 9.351507641470466,
      "grad_norm": 36.45579528808594,
      "learning_rate": 0.0007945544554455446,
      "loss": 3.3092,
      "step": 1415
    },
    {
      "epoch": 9.358116480793061,
      "grad_norm": 20.55837059020996,
      "learning_rate": 0.0007940594059405941,
      "loss": 0.8129,
      "step": 1416
    },
    {
      "epoch": 9.364725320115655,
      "grad_norm": 70.5672836303711,
      "learning_rate": 0.0007935643564356437,
      "loss": 2.4427,
      "step": 1417
    },
    {
      "epoch": 9.371334159438248,
      "grad_norm": 16.56972312927246,
      "learning_rate": 0.0007930693069306931,
      "loss": 3.1692,
      "step": 1418
    },
    {
      "epoch": 9.377942998760842,
      "grad_norm": 25.68592071533203,
      "learning_rate": 0.0007925742574257426,
      "loss": 2.2808,
      "step": 1419
    },
    {
      "epoch": 9.384551838083437,
      "grad_norm": 77.94720458984375,
      "learning_rate": 0.0007920792079207921,
      "loss": 2.5732,
      "step": 1420
    },
    {
      "epoch": 9.391160677406031,
      "grad_norm": 102.55604553222656,
      "learning_rate": 0.0007915841584158417,
      "loss": 3.655,
      "step": 1421
    },
    {
      "epoch": 9.397769516728625,
      "grad_norm": 54.41024398803711,
      "learning_rate": 0.0007910891089108912,
      "loss": 2.2408,
      "step": 1422
    },
    {
      "epoch": 9.404378356051218,
      "grad_norm": 69.5313720703125,
      "learning_rate": 0.0007905940594059406,
      "loss": 1.5799,
      "step": 1423
    },
    {
      "epoch": 9.410987195373812,
      "grad_norm": 24.90764617919922,
      "learning_rate": 0.0007900990099009901,
      "loss": 6.0092,
      "step": 1424
    },
    {
      "epoch": 9.417596034696407,
      "grad_norm": 7.016579627990723,
      "learning_rate": 0.0007896039603960397,
      "loss": 2.601,
      "step": 1425
    },
    {
      "epoch": 9.424204874019,
      "grad_norm": 27.055862426757812,
      "learning_rate": 0.0007891089108910892,
      "loss": 1.4336,
      "step": 1426
    },
    {
      "epoch": 9.430813713341594,
      "grad_norm": 26.398435592651367,
      "learning_rate": 0.0007886138613861387,
      "loss": 3.5292,
      "step": 1427
    },
    {
      "epoch": 9.437422552664188,
      "grad_norm": 44.375,
      "learning_rate": 0.0007881188118811881,
      "loss": 2.0993,
      "step": 1428
    },
    {
      "epoch": 9.444031391986782,
      "grad_norm": 23.594322204589844,
      "learning_rate": 0.0007876237623762377,
      "loss": 3.8081,
      "step": 1429
    },
    {
      "epoch": 9.450640231309377,
      "grad_norm": 49.501426696777344,
      "learning_rate": 0.0007871287128712872,
      "loss": 1.2485,
      "step": 1430
    },
    {
      "epoch": 9.45724907063197,
      "grad_norm": 26.743167877197266,
      "learning_rate": 0.0007866336633663367,
      "loss": 2.7764,
      "step": 1431
    },
    {
      "epoch": 9.463857909954564,
      "grad_norm": 50.49843978881836,
      "learning_rate": 0.0007861386138613862,
      "loss": 2.3531,
      "step": 1432
    },
    {
      "epoch": 9.470466749277158,
      "grad_norm": 4.181617736816406,
      "learning_rate": 0.0007856435643564356,
      "loss": 1.2616,
      "step": 1433
    },
    {
      "epoch": 9.477075588599753,
      "grad_norm": 52.59104919433594,
      "learning_rate": 0.0007851485148514852,
      "loss": 2.3135,
      "step": 1434
    },
    {
      "epoch": 9.483684427922347,
      "grad_norm": 35.888153076171875,
      "learning_rate": 0.0007846534653465347,
      "loss": 2.3073,
      "step": 1435
    },
    {
      "epoch": 9.49029326724494,
      "grad_norm": 68.62918853759766,
      "learning_rate": 0.0007841584158415842,
      "loss": 2.8429,
      "step": 1436
    },
    {
      "epoch": 9.496902106567534,
      "grad_norm": 26.327735900878906,
      "learning_rate": 0.0007836633663366338,
      "loss": 0.7588,
      "step": 1437
    },
    {
      "epoch": 9.503510945890127,
      "grad_norm": 52.02412414550781,
      "learning_rate": 0.0007831683168316832,
      "loss": 4.1468,
      "step": 1438
    },
    {
      "epoch": 9.510119785212723,
      "grad_norm": 39.9492073059082,
      "learning_rate": 0.0007826732673267327,
      "loss": 1.7288,
      "step": 1439
    },
    {
      "epoch": 9.516728624535316,
      "grad_norm": 102.27324676513672,
      "learning_rate": 0.0007821782178217822,
      "loss": 6.734,
      "step": 1440
    },
    {
      "epoch": 9.52333746385791,
      "grad_norm": 45.69184112548828,
      "learning_rate": 0.0007816831683168317,
      "loss": 3.7287,
      "step": 1441
    },
    {
      "epoch": 9.529946303180504,
      "grad_norm": 35.14879608154297,
      "learning_rate": 0.0007811881188118813,
      "loss": 2.0669,
      "step": 1442
    },
    {
      "epoch": 9.536555142503097,
      "grad_norm": 45.546810150146484,
      "learning_rate": 0.0007806930693069307,
      "loss": 3.7514,
      "step": 1443
    },
    {
      "epoch": 9.543163981825693,
      "grad_norm": 11.517829895019531,
      "learning_rate": 0.0007801980198019802,
      "loss": 1.1171,
      "step": 1444
    },
    {
      "epoch": 9.549772821148286,
      "grad_norm": 85.86268615722656,
      "learning_rate": 0.0007797029702970297,
      "loss": 3.8295,
      "step": 1445
    },
    {
      "epoch": 9.55638166047088,
      "grad_norm": 21.81514549255371,
      "learning_rate": 0.0007792079207920793,
      "loss": 2.0952,
      "step": 1446
    },
    {
      "epoch": 9.562990499793473,
      "grad_norm": 97.100830078125,
      "learning_rate": 0.0007787128712871288,
      "loss": 3.5409,
      "step": 1447
    },
    {
      "epoch": 9.569599339116067,
      "grad_norm": 17.036029815673828,
      "learning_rate": 0.0007782178217821782,
      "loss": 1.3542,
      "step": 1448
    },
    {
      "epoch": 9.576208178438662,
      "grad_norm": 47.67151641845703,
      "learning_rate": 0.0007777227722772277,
      "loss": 3.4521,
      "step": 1449
    },
    {
      "epoch": 9.582817017761256,
      "grad_norm": 27.802183151245117,
      "learning_rate": 0.0007772277227722773,
      "loss": 2.7183,
      "step": 1450
    },
    {
      "epoch": 9.58942585708385,
      "grad_norm": 52.5059700012207,
      "learning_rate": 0.0007767326732673268,
      "loss": 3.7812,
      "step": 1451
    },
    {
      "epoch": 9.596034696406443,
      "grad_norm": 13.767899513244629,
      "learning_rate": 0.0007762376237623763,
      "loss": 1.3592,
      "step": 1452
    },
    {
      "epoch": 9.602643535729037,
      "grad_norm": 84.91392517089844,
      "learning_rate": 0.0007757425742574257,
      "loss": 5.7109,
      "step": 1453
    },
    {
      "epoch": 9.609252375051632,
      "grad_norm": 41.81819152832031,
      "learning_rate": 0.0007752475247524753,
      "loss": 3.3272,
      "step": 1454
    },
    {
      "epoch": 9.615861214374226,
      "grad_norm": 23.409976959228516,
      "learning_rate": 0.0007747524752475248,
      "loss": 0.7445,
      "step": 1455
    },
    {
      "epoch": 9.62247005369682,
      "grad_norm": 28.66775894165039,
      "learning_rate": 0.0007742574257425743,
      "loss": 1.9017,
      "step": 1456
    },
    {
      "epoch": 9.629078893019413,
      "grad_norm": 42.14519119262695,
      "learning_rate": 0.0007737623762376238,
      "loss": 1.7994,
      "step": 1457
    },
    {
      "epoch": 9.635687732342008,
      "grad_norm": 28.550769805908203,
      "learning_rate": 0.0007732673267326733,
      "loss": 2.2186,
      "step": 1458
    },
    {
      "epoch": 9.642296571664602,
      "grad_norm": 16.152751922607422,
      "learning_rate": 0.0007727722772277228,
      "loss": 1.4607,
      "step": 1459
    },
    {
      "epoch": 9.648905410987195,
      "grad_norm": 4.459505558013916,
      "learning_rate": 0.0007722772277227723,
      "loss": 1.0741,
      "step": 1460
    },
    {
      "epoch": 9.655514250309789,
      "grad_norm": 63.91606903076172,
      "learning_rate": 0.0007717821782178218,
      "loss": 2.9265,
      "step": 1461
    },
    {
      "epoch": 9.662123089632383,
      "grad_norm": 39.170658111572266,
      "learning_rate": 0.0007712871287128714,
      "loss": 3.3739,
      "step": 1462
    },
    {
      "epoch": 9.668731928954978,
      "grad_norm": 37.95799255371094,
      "learning_rate": 0.0007707920792079208,
      "loss": 1.719,
      "step": 1463
    },
    {
      "epoch": 9.675340768277572,
      "grad_norm": 48.19321060180664,
      "learning_rate": 0.0007702970297029703,
      "loss": 4.265,
      "step": 1464
    },
    {
      "epoch": 9.681949607600165,
      "grad_norm": 73.49513244628906,
      "learning_rate": 0.0007698019801980198,
      "loss": 2.296,
      "step": 1465
    },
    {
      "epoch": 9.688558446922759,
      "grad_norm": 17.359027862548828,
      "learning_rate": 0.0007693069306930694,
      "loss": 1.4135,
      "step": 1466
    },
    {
      "epoch": 9.695167286245352,
      "grad_norm": 14.089313507080078,
      "learning_rate": 0.0007688118811881189,
      "loss": 0.9618,
      "step": 1467
    },
    {
      "epoch": 9.701776125567948,
      "grad_norm": 36.60761260986328,
      "learning_rate": 0.0007683168316831683,
      "loss": 2.5732,
      "step": 1468
    },
    {
      "epoch": 9.708384964890541,
      "grad_norm": 37.37281036376953,
      "learning_rate": 0.0007678217821782178,
      "loss": 2.4395,
      "step": 1469
    },
    {
      "epoch": 9.714993804213135,
      "grad_norm": 50.18526077270508,
      "learning_rate": 0.0007673267326732674,
      "loss": 2.4753,
      "step": 1470
    },
    {
      "epoch": 9.721602643535729,
      "grad_norm": 23.984569549560547,
      "learning_rate": 0.0007668316831683169,
      "loss": 1.4609,
      "step": 1471
    },
    {
      "epoch": 9.728211482858324,
      "grad_norm": 18.82038116455078,
      "learning_rate": 0.0007663366336633664,
      "loss": 1.8677,
      "step": 1472
    },
    {
      "epoch": 9.734820322180918,
      "grad_norm": 69.23988342285156,
      "learning_rate": 0.0007658415841584158,
      "loss": 4.18,
      "step": 1473
    },
    {
      "epoch": 9.741429161503511,
      "grad_norm": 47.40521240234375,
      "learning_rate": 0.0007653465346534654,
      "loss": 1.7862,
      "step": 1474
    },
    {
      "epoch": 9.748038000826105,
      "grad_norm": 8.152920722961426,
      "learning_rate": 0.0007648514851485149,
      "loss": 0.7074,
      "step": 1475
    },
    {
      "epoch": 9.754646840148698,
      "grad_norm": 36.773006439208984,
      "learning_rate": 0.0007643564356435644,
      "loss": 1.6966,
      "step": 1476
    },
    {
      "epoch": 9.761255679471294,
      "grad_norm": 29.665668487548828,
      "learning_rate": 0.0007638613861386139,
      "loss": 2.7755,
      "step": 1477
    },
    {
      "epoch": 9.767864518793887,
      "grad_norm": 14.7373628616333,
      "learning_rate": 0.0007633663366336634,
      "loss": 0.7673,
      "step": 1478
    },
    {
      "epoch": 9.77447335811648,
      "grad_norm": 76.92029571533203,
      "learning_rate": 0.0007628712871287129,
      "loss": 3.8662,
      "step": 1479
    },
    {
      "epoch": 9.781082197439074,
      "grad_norm": 96.37224578857422,
      "learning_rate": 0.0007623762376237624,
      "loss": 4.0657,
      "step": 1480
    },
    {
      "epoch": 9.787691036761668,
      "grad_norm": 23.671236038208008,
      "learning_rate": 0.0007618811881188119,
      "loss": 2.703,
      "step": 1481
    },
    {
      "epoch": 9.794299876084263,
      "grad_norm": 9.196260452270508,
      "learning_rate": 0.0007613861386138615,
      "loss": 2.5239,
      "step": 1482
    },
    {
      "epoch": 9.800908715406857,
      "grad_norm": 52.56816864013672,
      "learning_rate": 0.0007608910891089109,
      "loss": 1.6893,
      "step": 1483
    },
    {
      "epoch": 9.80751755472945,
      "grad_norm": 54.93986511230469,
      "learning_rate": 0.0007603960396039604,
      "loss": 2.426,
      "step": 1484
    },
    {
      "epoch": 9.814126394052044,
      "grad_norm": 53.94663619995117,
      "learning_rate": 0.0007599009900990099,
      "loss": 2.0559,
      "step": 1485
    },
    {
      "epoch": 9.820735233374638,
      "grad_norm": 23.244813919067383,
      "learning_rate": 0.0007594059405940595,
      "loss": 1.2374,
      "step": 1486
    },
    {
      "epoch": 9.827344072697233,
      "grad_norm": 18.450668334960938,
      "learning_rate": 0.000758910891089109,
      "loss": 0.9818,
      "step": 1487
    },
    {
      "epoch": 9.833952912019827,
      "grad_norm": 54.4602165222168,
      "learning_rate": 0.0007584158415841584,
      "loss": 2.1857,
      "step": 1488
    },
    {
      "epoch": 9.84056175134242,
      "grad_norm": 16.868799209594727,
      "learning_rate": 0.0007579207920792079,
      "loss": 1.5082,
      "step": 1489
    },
    {
      "epoch": 9.847170590665014,
      "grad_norm": 37.67301940917969,
      "learning_rate": 0.0007574257425742574,
      "loss": 1.2242,
      "step": 1490
    },
    {
      "epoch": 9.853779429987608,
      "grad_norm": 46.26320266723633,
      "learning_rate": 0.000756930693069307,
      "loss": 1.4638,
      "step": 1491
    },
    {
      "epoch": 9.860388269310203,
      "grad_norm": 13.708301544189453,
      "learning_rate": 0.0007564356435643565,
      "loss": 4.9342,
      "step": 1492
    },
    {
      "epoch": 9.866997108632797,
      "grad_norm": 41.583831787109375,
      "learning_rate": 0.0007559405940594059,
      "loss": 1.3716,
      "step": 1493
    },
    {
      "epoch": 9.87360594795539,
      "grad_norm": 27.860307693481445,
      "learning_rate": 0.0007554455445544554,
      "loss": 2.5435,
      "step": 1494
    },
    {
      "epoch": 9.880214787277984,
      "grad_norm": 25.148487091064453,
      "learning_rate": 0.000754950495049505,
      "loss": 6.3475,
      "step": 1495
    },
    {
      "epoch": 9.886823626600577,
      "grad_norm": 43.281044006347656,
      "learning_rate": 0.0007544554455445545,
      "loss": 4.1727,
      "step": 1496
    },
    {
      "epoch": 9.893432465923173,
      "grad_norm": 5.580582618713379,
      "learning_rate": 0.000753960396039604,
      "loss": 0.4839,
      "step": 1497
    },
    {
      "epoch": 9.900041305245766,
      "grad_norm": 26.35211753845215,
      "learning_rate": 0.0007534653465346534,
      "loss": 1.3832,
      "step": 1498
    },
    {
      "epoch": 9.90665014456836,
      "grad_norm": 10.154475212097168,
      "learning_rate": 0.000752970297029703,
      "loss": 1.5695,
      "step": 1499
    },
    {
      "epoch": 9.913258983890954,
      "grad_norm": 53.243408203125,
      "learning_rate": 0.0007524752475247525,
      "loss": 1.7018,
      "step": 1500
    },
    {
      "epoch": 9.919867823213549,
      "grad_norm": 74.43706512451172,
      "learning_rate": 0.000751980198019802,
      "loss": 3.0306,
      "step": 1501
    },
    {
      "epoch": 9.926476662536142,
      "grad_norm": 11.466606140136719,
      "learning_rate": 0.0007514851485148515,
      "loss": 1.3156,
      "step": 1502
    },
    {
      "epoch": 9.933085501858736,
      "grad_norm": 6.507962703704834,
      "learning_rate": 0.000750990099009901,
      "loss": 2.1902,
      "step": 1503
    },
    {
      "epoch": 9.93969434118133,
      "grad_norm": 59.021400451660156,
      "learning_rate": 0.0007504950495049505,
      "loss": 2.4464,
      "step": 1504
    },
    {
      "epoch": 9.946303180503923,
      "grad_norm": 18.830293655395508,
      "learning_rate": 0.00075,
      "loss": 1.1357,
      "step": 1505
    },
    {
      "epoch": 9.952912019826519,
      "grad_norm": 6.755622386932373,
      "learning_rate": 0.0007495049504950495,
      "loss": 3.9108,
      "step": 1506
    },
    {
      "epoch": 9.959520859149112,
      "grad_norm": 19.53421401977539,
      "learning_rate": 0.0007490099009900991,
      "loss": 1.1996,
      "step": 1507
    },
    {
      "epoch": 9.966129698471706,
      "grad_norm": 24.807621002197266,
      "learning_rate": 0.0007485148514851485,
      "loss": 1.7662,
      "step": 1508
    },
    {
      "epoch": 9.9727385377943,
      "grad_norm": 35.6623420715332,
      "learning_rate": 0.000748019801980198,
      "loss": 2.5479,
      "step": 1509
    },
    {
      "epoch": 9.979347377116893,
      "grad_norm": 15.010448455810547,
      "learning_rate": 0.0007475247524752475,
      "loss": 2.8534,
      "step": 1510
    },
    {
      "epoch": 9.985956216439488,
      "grad_norm": 7.909218788146973,
      "learning_rate": 0.0007470297029702971,
      "loss": 0.9727,
      "step": 1511
    },
    {
      "epoch": 9.992565055762082,
      "grad_norm": 17.683425903320312,
      "learning_rate": 0.0007465346534653466,
      "loss": 0.9467,
      "step": 1512
    },
    {
      "epoch": 9.999173895084676,
      "grad_norm": 65.75923156738281,
      "learning_rate": 0.000746039603960396,
      "loss": 4.4337,
      "step": 1513
    },
    {
      "epoch": 9.999173895084676,
      "eval_validation_error_bar": 0.049163161484888196,
      "eval_validation_loss": 5.36729621887207,
      "eval_validation_pearsonr": 0.575275381756716,
      "eval_validation_rmse": 2.3167426586151123,
      "eval_validation_runtime": 33.0606,
      "eval_validation_samples_per_second": 6.14,
      "eval_validation_spearman": 0.5999519437695041,
      "eval_validation_steps_per_second": 6.14,
      "step": 1513
    },
    {
      "epoch": 9.999173895084676,
      "eval_test_error_bar": 0.039011163248322524,
      "eval_test_loss": 6.199848651885986,
      "eval_test_pearsonr": 0.5811452658318159,
      "eval_test_rmse": 2.4899494647979736,
      "eval_test_runtime": 38.9595,
      "eval_test_samples_per_second": 8.368,
      "eval_test_spearman": 0.5947334267672744,
      "eval_test_steps_per_second": 8.368,
      "step": 1513
    },
    {
      "epoch": 10.00578273440727,
      "grad_norm": 58.95011520385742,
      "learning_rate": 0.0007455445544554455,
      "loss": 2.7025,
      "step": 1514
    },
    {
      "epoch": 10.012391573729865,
      "grad_norm": 46.5892219543457,
      "learning_rate": 0.0007450495049504951,
      "loss": 1.6193,
      "step": 1515
    },
    {
      "epoch": 10.019000413052458,
      "grad_norm": 8.484909057617188,
      "learning_rate": 0.0007445544554455446,
      "loss": 0.8357,
      "step": 1516
    },
    {
      "epoch": 10.025609252375052,
      "grad_norm": 11.018416404724121,
      "learning_rate": 0.0007440594059405941,
      "loss": 2.6521,
      "step": 1517
    },
    {
      "epoch": 10.032218091697645,
      "grad_norm": 39.07554626464844,
      "learning_rate": 0.0007435643564356435,
      "loss": 4.3962,
      "step": 1518
    },
    {
      "epoch": 10.038826931020239,
      "grad_norm": 3.215240955352783,
      "learning_rate": 0.0007430693069306931,
      "loss": 3.4331,
      "step": 1519
    },
    {
      "epoch": 10.045435770342834,
      "grad_norm": 7.73026704788208,
      "learning_rate": 0.0007425742574257426,
      "loss": 2.2391,
      "step": 1520
    },
    {
      "epoch": 10.052044609665428,
      "grad_norm": 14.579256057739258,
      "learning_rate": 0.0007420792079207921,
      "loss": 2.5706,
      "step": 1521
    },
    {
      "epoch": 10.058653448988021,
      "grad_norm": 32.51063537597656,
      "learning_rate": 0.0007415841584158416,
      "loss": 1.4163,
      "step": 1522
    },
    {
      "epoch": 10.065262288310615,
      "grad_norm": 62.577613830566406,
      "learning_rate": 0.000741089108910891,
      "loss": 1.5946,
      "step": 1523
    },
    {
      "epoch": 10.071871127633209,
      "grad_norm": 76.58744049072266,
      "learning_rate": 0.0007405940594059406,
      "loss": 2.2412,
      "step": 1524
    },
    {
      "epoch": 10.078479966955804,
      "grad_norm": 21.93190574645996,
      "learning_rate": 0.0007400990099009901,
      "loss": 3.8315,
      "step": 1525
    },
    {
      "epoch": 10.085088806278398,
      "grad_norm": 38.676856994628906,
      "learning_rate": 0.0007396039603960396,
      "loss": 3.1146,
      "step": 1526
    },
    {
      "epoch": 10.091697645600991,
      "grad_norm": 11.081823348999023,
      "learning_rate": 0.0007391089108910892,
      "loss": 1.6987,
      "step": 1527
    },
    {
      "epoch": 10.098306484923585,
      "grad_norm": 9.749444961547852,
      "learning_rate": 0.0007386138613861386,
      "loss": 4.3702,
      "step": 1528
    },
    {
      "epoch": 10.104915324246178,
      "grad_norm": 5.489101409912109,
      "learning_rate": 0.0007381188118811881,
      "loss": 1.9531,
      "step": 1529
    },
    {
      "epoch": 10.111524163568774,
      "grad_norm": 71.236083984375,
      "learning_rate": 0.0007376237623762376,
      "loss": 2.5917,
      "step": 1530
    },
    {
      "epoch": 10.118133002891367,
      "grad_norm": 33.30613708496094,
      "learning_rate": 0.0007371287128712872,
      "loss": 4.8739,
      "step": 1531
    },
    {
      "epoch": 10.124741842213961,
      "grad_norm": 13.93424129486084,
      "learning_rate": 0.0007366336633663367,
      "loss": 2.5059,
      "step": 1532
    },
    {
      "epoch": 10.131350681536555,
      "grad_norm": 35.59808349609375,
      "learning_rate": 0.0007361386138613861,
      "loss": 3.2494,
      "step": 1533
    },
    {
      "epoch": 10.13795952085915,
      "grad_norm": 43.696319580078125,
      "learning_rate": 0.0007356435643564356,
      "loss": 3.6304,
      "step": 1534
    },
    {
      "epoch": 10.144568360181744,
      "grad_norm": 21.10610580444336,
      "learning_rate": 0.0007351485148514852,
      "loss": 1.9902,
      "step": 1535
    },
    {
      "epoch": 10.151177199504337,
      "grad_norm": 58.57735061645508,
      "learning_rate": 0.0007346534653465347,
      "loss": 3.5361,
      "step": 1536
    },
    {
      "epoch": 10.15778603882693,
      "grad_norm": 61.39545440673828,
      "learning_rate": 0.0007341584158415842,
      "loss": 2.9541,
      "step": 1537
    },
    {
      "epoch": 10.164394878149524,
      "grad_norm": 94.01302337646484,
      "learning_rate": 0.0007336633663366336,
      "loss": 4.069,
      "step": 1538
    },
    {
      "epoch": 10.17100371747212,
      "grad_norm": 71.7181167602539,
      "learning_rate": 0.0007331683168316831,
      "loss": 3.8791,
      "step": 1539
    },
    {
      "epoch": 10.177612556794713,
      "grad_norm": 51.609310150146484,
      "learning_rate": 0.0007326732673267327,
      "loss": 1.4386,
      "step": 1540
    },
    {
      "epoch": 10.184221396117307,
      "grad_norm": 82.93344116210938,
      "learning_rate": 0.0007321782178217822,
      "loss": 2.1168,
      "step": 1541
    },
    {
      "epoch": 10.1908302354399,
      "grad_norm": 22.224018096923828,
      "learning_rate": 0.0007316831683168317,
      "loss": 0.9975,
      "step": 1542
    },
    {
      "epoch": 10.197439074762494,
      "grad_norm": 62.54931640625,
      "learning_rate": 0.0007311881188118811,
      "loss": 5.3865,
      "step": 1543
    },
    {
      "epoch": 10.20404791408509,
      "grad_norm": 25.39143180847168,
      "learning_rate": 0.0007306930693069307,
      "loss": 3.272,
      "step": 1544
    },
    {
      "epoch": 10.210656753407683,
      "grad_norm": 46.629791259765625,
      "learning_rate": 0.0007301980198019802,
      "loss": 1.0897,
      "step": 1545
    },
    {
      "epoch": 10.217265592730277,
      "grad_norm": 21.22125244140625,
      "learning_rate": 0.0007297029702970297,
      "loss": 2.4526,
      "step": 1546
    },
    {
      "epoch": 10.22387443205287,
      "grad_norm": 26.709115982055664,
      "learning_rate": 0.0007292079207920792,
      "loss": 3.9423,
      "step": 1547
    },
    {
      "epoch": 10.230483271375464,
      "grad_norm": 25.946426391601562,
      "learning_rate": 0.0007287128712871287,
      "loss": 1.9648,
      "step": 1548
    },
    {
      "epoch": 10.23709211069806,
      "grad_norm": 53.815982818603516,
      "learning_rate": 0.0007282178217821782,
      "loss": 1.3718,
      "step": 1549
    },
    {
      "epoch": 10.243700950020653,
      "grad_norm": 102.89286041259766,
      "learning_rate": 0.0007277227722772277,
      "loss": 6.8355,
      "step": 1550
    },
    {
      "epoch": 10.250309789343246,
      "grad_norm": 13.820528984069824,
      "learning_rate": 0.0007272277227722772,
      "loss": 2.8001,
      "step": 1551
    },
    {
      "epoch": 10.25691862866584,
      "grad_norm": 27.148460388183594,
      "learning_rate": 0.0007267326732673268,
      "loss": 0.6648,
      "step": 1552
    },
    {
      "epoch": 10.263527467988435,
      "grad_norm": 16.017433166503906,
      "learning_rate": 0.0007262376237623762,
      "loss": 2.1941,
      "step": 1553
    },
    {
      "epoch": 10.270136307311029,
      "grad_norm": 66.79967498779297,
      "learning_rate": 0.0007257425742574257,
      "loss": 4.523,
      "step": 1554
    },
    {
      "epoch": 10.276745146633623,
      "grad_norm": 86.75910186767578,
      "learning_rate": 0.0007252475247524752,
      "loss": 2.7892,
      "step": 1555
    },
    {
      "epoch": 10.283353985956216,
      "grad_norm": 34.17557144165039,
      "learning_rate": 0.0007247524752475248,
      "loss": 2.3862,
      "step": 1556
    },
    {
      "epoch": 10.28996282527881,
      "grad_norm": 68.7713851928711,
      "learning_rate": 0.0007242574257425743,
      "loss": 2.9288,
      "step": 1557
    },
    {
      "epoch": 10.296571664601405,
      "grad_norm": 35.4869270324707,
      "learning_rate": 0.0007237623762376237,
      "loss": 1.1879,
      "step": 1558
    },
    {
      "epoch": 10.303180503923999,
      "grad_norm": 2.1739883422851562,
      "learning_rate": 0.0007232673267326732,
      "loss": 1.8695,
      "step": 1559
    },
    {
      "epoch": 10.309789343246592,
      "grad_norm": 13.958045959472656,
      "learning_rate": 0.0007227722772277228,
      "loss": 1.6665,
      "step": 1560
    },
    {
      "epoch": 10.316398182569186,
      "grad_norm": 7.386800765991211,
      "learning_rate": 0.0007222772277227723,
      "loss": 1.2961,
      "step": 1561
    },
    {
      "epoch": 10.32300702189178,
      "grad_norm": 61.46512222290039,
      "learning_rate": 0.0007217821782178218,
      "loss": 1.2831,
      "step": 1562
    },
    {
      "epoch": 10.329615861214375,
      "grad_norm": 108.17644500732422,
      "learning_rate": 0.0007212871287128712,
      "loss": 2.7376,
      "step": 1563
    },
    {
      "epoch": 10.336224700536969,
      "grad_norm": 72.9325180053711,
      "learning_rate": 0.0007207920792079208,
      "loss": 3.2972,
      "step": 1564
    },
    {
      "epoch": 10.342833539859562,
      "grad_norm": 118.10923767089844,
      "learning_rate": 0.0007202970297029703,
      "loss": 6.2943,
      "step": 1565
    },
    {
      "epoch": 10.349442379182156,
      "grad_norm": 66.39751434326172,
      "learning_rate": 0.0007198019801980198,
      "loss": 3.3038,
      "step": 1566
    },
    {
      "epoch": 10.35605121850475,
      "grad_norm": 6.887818813323975,
      "learning_rate": 0.0007193069306930693,
      "loss": 2.3054,
      "step": 1567
    },
    {
      "epoch": 10.362660057827345,
      "grad_norm": 12.490066528320312,
      "learning_rate": 0.0007188118811881188,
      "loss": 2.9325,
      "step": 1568
    },
    {
      "epoch": 10.369268897149938,
      "grad_norm": 28.9343204498291,
      "learning_rate": 0.0007183168316831683,
      "loss": 1.5359,
      "step": 1569
    },
    {
      "epoch": 10.375877736472532,
      "grad_norm": 10.013076782226562,
      "learning_rate": 0.0007178217821782178,
      "loss": 1.8531,
      "step": 1570
    },
    {
      "epoch": 10.382486575795125,
      "grad_norm": 6.648024559020996,
      "learning_rate": 0.0007173267326732673,
      "loss": 3.7533,
      "step": 1571
    },
    {
      "epoch": 10.389095415117719,
      "grad_norm": 47.92090606689453,
      "learning_rate": 0.0007168316831683169,
      "loss": 1.7535,
      "step": 1572
    },
    {
      "epoch": 10.395704254440314,
      "grad_norm": 78.8933334350586,
      "learning_rate": 0.0007163366336633663,
      "loss": 7.4117,
      "step": 1573
    },
    {
      "epoch": 10.402313093762908,
      "grad_norm": 81.5009994506836,
      "learning_rate": 0.0007158415841584158,
      "loss": 5.4652,
      "step": 1574
    },
    {
      "epoch": 10.408921933085502,
      "grad_norm": 33.2273063659668,
      "learning_rate": 0.0007153465346534653,
      "loss": 1.1844,
      "step": 1575
    },
    {
      "epoch": 10.415530772408095,
      "grad_norm": 13.539799690246582,
      "learning_rate": 0.0007148514851485149,
      "loss": 2.3756,
      "step": 1576
    },
    {
      "epoch": 10.42213961173069,
      "grad_norm": 43.9423828125,
      "learning_rate": 0.0007143564356435644,
      "loss": 3.9124,
      "step": 1577
    },
    {
      "epoch": 10.428748451053284,
      "grad_norm": 52.587459564208984,
      "learning_rate": 0.0007138613861386138,
      "loss": 2.3398,
      "step": 1578
    },
    {
      "epoch": 10.435357290375878,
      "grad_norm": 56.559791564941406,
      "learning_rate": 0.0007133663366336633,
      "loss": 4.6981,
      "step": 1579
    },
    {
      "epoch": 10.441966129698471,
      "grad_norm": 62.151161193847656,
      "learning_rate": 0.0007128712871287129,
      "loss": 2.0508,
      "step": 1580
    },
    {
      "epoch": 10.448574969021065,
      "grad_norm": 19.812538146972656,
      "learning_rate": 0.0007123762376237624,
      "loss": 1.5031,
      "step": 1581
    },
    {
      "epoch": 10.45518380834366,
      "grad_norm": 13.01518726348877,
      "learning_rate": 0.0007118811881188119,
      "loss": 4.5547,
      "step": 1582
    },
    {
      "epoch": 10.461792647666254,
      "grad_norm": 64.55157470703125,
      "learning_rate": 0.0007113861386138613,
      "loss": 4.0437,
      "step": 1583
    },
    {
      "epoch": 10.468401486988848,
      "grad_norm": 77.22045135498047,
      "learning_rate": 0.0007108910891089109,
      "loss": 2.6165,
      "step": 1584
    },
    {
      "epoch": 10.475010326311441,
      "grad_norm": 32.03498458862305,
      "learning_rate": 0.0007103960396039604,
      "loss": 4.1816,
      "step": 1585
    },
    {
      "epoch": 10.481619165634035,
      "grad_norm": 53.255218505859375,
      "learning_rate": 0.0007099009900990099,
      "loss": 1.9336,
      "step": 1586
    },
    {
      "epoch": 10.48822800495663,
      "grad_norm": 50.554866790771484,
      "learning_rate": 0.0007094059405940594,
      "loss": 2.8224,
      "step": 1587
    },
    {
      "epoch": 10.494836844279224,
      "grad_norm": 18.089378356933594,
      "learning_rate": 0.0007089108910891088,
      "loss": 1.476,
      "step": 1588
    },
    {
      "epoch": 10.501445683601817,
      "grad_norm": 14.774365425109863,
      "learning_rate": 0.0007084158415841584,
      "loss": 1.6281,
      "step": 1589
    },
    {
      "epoch": 10.508054522924411,
      "grad_norm": 14.543601036071777,
      "learning_rate": 0.0007079207920792079,
      "loss": 2.7687,
      "step": 1590
    },
    {
      "epoch": 10.514663362247006,
      "grad_norm": 1.4520238637924194,
      "learning_rate": 0.0007074257425742574,
      "loss": 0.8732,
      "step": 1591
    },
    {
      "epoch": 10.5212722015696,
      "grad_norm": 54.760311126708984,
      "learning_rate": 0.000706930693069307,
      "loss": 3.7652,
      "step": 1592
    },
    {
      "epoch": 10.527881040892193,
      "grad_norm": 7.487936973571777,
      "learning_rate": 0.0007064356435643564,
      "loss": 0.5544,
      "step": 1593
    },
    {
      "epoch": 10.534489880214787,
      "grad_norm": 35.242305755615234,
      "learning_rate": 0.0007059405940594059,
      "loss": 2.6532,
      "step": 1594
    },
    {
      "epoch": 10.54109871953738,
      "grad_norm": 15.146188735961914,
      "learning_rate": 0.0007054455445544554,
      "loss": 2.6478,
      "step": 1595
    },
    {
      "epoch": 10.547707558859976,
      "grad_norm": 19.31121826171875,
      "learning_rate": 0.000704950495049505,
      "loss": 2.5107,
      "step": 1596
    },
    {
      "epoch": 10.55431639818257,
      "grad_norm": 42.65496063232422,
      "learning_rate": 0.0007044554455445545,
      "loss": 2.8538,
      "step": 1597
    },
    {
      "epoch": 10.560925237505163,
      "grad_norm": 23.5660343170166,
      "learning_rate": 0.0007039603960396039,
      "loss": 1.5334,
      "step": 1598
    },
    {
      "epoch": 10.567534076827757,
      "grad_norm": 7.340339660644531,
      "learning_rate": 0.0007034653465346534,
      "loss": 2.4525,
      "step": 1599
    },
    {
      "epoch": 10.57414291615035,
      "grad_norm": 58.65119552612305,
      "learning_rate": 0.0007029702970297029,
      "loss": 3.13,
      "step": 1600
    },
    {
      "epoch": 10.580751755472946,
      "grad_norm": 47.27147674560547,
      "learning_rate": 0.0007024752475247525,
      "loss": 2.4946,
      "step": 1601
    },
    {
      "epoch": 10.58736059479554,
      "grad_norm": 25.742366790771484,
      "learning_rate": 0.000701980198019802,
      "loss": 2.5587,
      "step": 1602
    },
    {
      "epoch": 10.593969434118133,
      "grad_norm": 33.84810256958008,
      "learning_rate": 0.0007014851485148514,
      "loss": 6.0561,
      "step": 1603
    },
    {
      "epoch": 10.600578273440727,
      "grad_norm": 13.960168838500977,
      "learning_rate": 0.0007009900990099009,
      "loss": 2.817,
      "step": 1604
    },
    {
      "epoch": 10.60718711276332,
      "grad_norm": 12.306098937988281,
      "learning_rate": 0.0007004950495049505,
      "loss": 1.714,
      "step": 1605
    },
    {
      "epoch": 10.613795952085916,
      "grad_norm": 21.01007843017578,
      "learning_rate": 0.0007,
      "loss": 5.1141,
      "step": 1606
    },
    {
      "epoch": 10.62040479140851,
      "grad_norm": 23.633617401123047,
      "learning_rate": 0.0006995049504950495,
      "loss": 1.6373,
      "step": 1607
    },
    {
      "epoch": 10.627013630731103,
      "grad_norm": 7.062527179718018,
      "learning_rate": 0.0006990099009900989,
      "loss": 0.483,
      "step": 1608
    },
    {
      "epoch": 10.633622470053696,
      "grad_norm": 2.2671468257904053,
      "learning_rate": 0.0006985148514851485,
      "loss": 1.5906,
      "step": 1609
    },
    {
      "epoch": 10.64023130937629,
      "grad_norm": 28.93791961669922,
      "learning_rate": 0.000698019801980198,
      "loss": 1.9459,
      "step": 1610
    },
    {
      "epoch": 10.646840148698885,
      "grad_norm": 2.2316508293151855,
      "learning_rate": 0.0006975247524752475,
      "loss": 1.2357,
      "step": 1611
    },
    {
      "epoch": 10.653448988021479,
      "grad_norm": 67.59001159667969,
      "learning_rate": 0.000697029702970297,
      "loss": 3.5239,
      "step": 1612
    },
    {
      "epoch": 10.660057827344072,
      "grad_norm": 18.589693069458008,
      "learning_rate": 0.0006965346534653465,
      "loss": 3.9124,
      "step": 1613
    },
    {
      "epoch": 10.666666666666666,
      "grad_norm": 32.552452087402344,
      "learning_rate": 0.000696039603960396,
      "loss": 1.2581,
      "step": 1614
    },
    {
      "epoch": 10.673275505989261,
      "grad_norm": 28.264225006103516,
      "learning_rate": 0.0006955445544554455,
      "loss": 2.1055,
      "step": 1615
    },
    {
      "epoch": 10.679884345311855,
      "grad_norm": 29.90753746032715,
      "learning_rate": 0.000695049504950495,
      "loss": 1.1613,
      "step": 1616
    },
    {
      "epoch": 10.686493184634449,
      "grad_norm": 13.199941635131836,
      "learning_rate": 0.0006945544554455446,
      "loss": 3.1715,
      "step": 1617
    },
    {
      "epoch": 10.693102023957042,
      "grad_norm": 4.949573516845703,
      "learning_rate": 0.000694059405940594,
      "loss": 1.0809,
      "step": 1618
    },
    {
      "epoch": 10.699710863279636,
      "grad_norm": 56.70454025268555,
      "learning_rate": 0.0006935643564356435,
      "loss": 2.2365,
      "step": 1619
    },
    {
      "epoch": 10.706319702602231,
      "grad_norm": 63.907257080078125,
      "learning_rate": 0.000693069306930693,
      "loss": 2.7854,
      "step": 1620
    },
    {
      "epoch": 10.712928541924825,
      "grad_norm": 16.488473892211914,
      "learning_rate": 0.0006925742574257426,
      "loss": 2.1098,
      "step": 1621
    },
    {
      "epoch": 10.719537381247418,
      "grad_norm": 19.916860580444336,
      "learning_rate": 0.0006920792079207921,
      "loss": 2.2753,
      "step": 1622
    },
    {
      "epoch": 10.726146220570012,
      "grad_norm": 14.649624824523926,
      "learning_rate": 0.0006915841584158415,
      "loss": 3.1252,
      "step": 1623
    },
    {
      "epoch": 10.732755059892606,
      "grad_norm": 15.494490623474121,
      "learning_rate": 0.000691089108910891,
      "loss": 0.7334,
      "step": 1624
    },
    {
      "epoch": 10.739363899215201,
      "grad_norm": 9.437252044677734,
      "learning_rate": 0.0006905940594059406,
      "loss": 2.8335,
      "step": 1625
    },
    {
      "epoch": 10.745972738537795,
      "grad_norm": 15.58764934539795,
      "learning_rate": 0.0006900990099009901,
      "loss": 2.0704,
      "step": 1626
    },
    {
      "epoch": 10.752581577860388,
      "grad_norm": 40.77093505859375,
      "learning_rate": 0.0006896039603960396,
      "loss": 1.854,
      "step": 1627
    },
    {
      "epoch": 10.759190417182982,
      "grad_norm": 31.710172653198242,
      "learning_rate": 0.000689108910891089,
      "loss": 2.4632,
      "step": 1628
    },
    {
      "epoch": 10.765799256505577,
      "grad_norm": 82.1035385131836,
      "learning_rate": 0.0006886138613861386,
      "loss": 3.1434,
      "step": 1629
    },
    {
      "epoch": 10.77240809582817,
      "grad_norm": 45.35991668701172,
      "learning_rate": 0.0006881188118811881,
      "loss": 1.8313,
      "step": 1630
    },
    {
      "epoch": 10.779016935150764,
      "grad_norm": 70.86849212646484,
      "learning_rate": 0.0006876237623762376,
      "loss": 2.6498,
      "step": 1631
    },
    {
      "epoch": 10.785625774473358,
      "grad_norm": 29.207975387573242,
      "learning_rate": 0.0006871287128712872,
      "loss": 2.9905,
      "step": 1632
    },
    {
      "epoch": 10.792234613795952,
      "grad_norm": 17.43501091003418,
      "learning_rate": 0.0006866336633663367,
      "loss": 2.2532,
      "step": 1633
    },
    {
      "epoch": 10.798843453118547,
      "grad_norm": 29.617210388183594,
      "learning_rate": 0.0006861386138613862,
      "loss": 2.1886,
      "step": 1634
    },
    {
      "epoch": 10.80545229244114,
      "grad_norm": 75.60820770263672,
      "learning_rate": 0.0006856435643564357,
      "loss": 3.2238,
      "step": 1635
    },
    {
      "epoch": 10.812061131763734,
      "grad_norm": 51.2747688293457,
      "learning_rate": 0.0006851485148514852,
      "loss": 2.5638,
      "step": 1636
    },
    {
      "epoch": 10.818669971086328,
      "grad_norm": 85.16791534423828,
      "learning_rate": 0.0006846534653465348,
      "loss": 2.9096,
      "step": 1637
    },
    {
      "epoch": 10.825278810408921,
      "grad_norm": 71.39273071289062,
      "learning_rate": 0.0006841584158415842,
      "loss": 3.1819,
      "step": 1638
    },
    {
      "epoch": 10.831887649731517,
      "grad_norm": 116.76339721679688,
      "learning_rate": 0.0006836633663366337,
      "loss": 4.0607,
      "step": 1639
    },
    {
      "epoch": 10.83849648905411,
      "grad_norm": 36.41032791137695,
      "learning_rate": 0.0006831683168316832,
      "loss": 3.6501,
      "step": 1640
    },
    {
      "epoch": 10.845105328376704,
      "grad_norm": 76.6644515991211,
      "learning_rate": 0.0006826732673267328,
      "loss": 1.9795,
      "step": 1641
    },
    {
      "epoch": 10.851714167699297,
      "grad_norm": 39.387508392333984,
      "learning_rate": 0.0006821782178217823,
      "loss": 2.9612,
      "step": 1642
    },
    {
      "epoch": 10.858323007021891,
      "grad_norm": 33.0686149597168,
      "learning_rate": 0.0006816831683168317,
      "loss": 3.2047,
      "step": 1643
    },
    {
      "epoch": 10.864931846344486,
      "grad_norm": 27.302440643310547,
      "learning_rate": 0.0006811881188118812,
      "loss": 1.5136,
      "step": 1644
    },
    {
      "epoch": 10.87154068566708,
      "grad_norm": 3.624095916748047,
      "learning_rate": 0.0006806930693069308,
      "loss": 3.0656,
      "step": 1645
    },
    {
      "epoch": 10.878149524989674,
      "grad_norm": 37.28758239746094,
      "learning_rate": 0.0006801980198019803,
      "loss": 1.9726,
      "step": 1646
    },
    {
      "epoch": 10.884758364312267,
      "grad_norm": 15.706995010375977,
      "learning_rate": 0.0006797029702970298,
      "loss": 3.47,
      "step": 1647
    },
    {
      "epoch": 10.89136720363486,
      "grad_norm": 73.58233642578125,
      "learning_rate": 0.0006792079207920792,
      "loss": 4.7035,
      "step": 1648
    },
    {
      "epoch": 10.897976042957456,
      "grad_norm": 36.80415725708008,
      "learning_rate": 0.0006787128712871288,
      "loss": 2.7617,
      "step": 1649
    },
    {
      "epoch": 10.90458488228005,
      "grad_norm": 22.97508430480957,
      "learning_rate": 0.0006782178217821783,
      "loss": 1.3903,
      "step": 1650
    },
    {
      "epoch": 10.911193721602643,
      "grad_norm": 34.0252571105957,
      "learning_rate": 0.0006777227722772278,
      "loss": 4.3688,
      "step": 1651
    },
    {
      "epoch": 10.917802560925237,
      "grad_norm": 48.798038482666016,
      "learning_rate": 0.0006772277227722773,
      "loss": 0.731,
      "step": 1652
    },
    {
      "epoch": 10.924411400247832,
      "grad_norm": 43.18721389770508,
      "learning_rate": 0.0006767326732673267,
      "loss": 3.3455,
      "step": 1653
    },
    {
      "epoch": 10.931020239570426,
      "grad_norm": 12.298663139343262,
      "learning_rate": 0.0006762376237623763,
      "loss": 1.4577,
      "step": 1654
    },
    {
      "epoch": 10.93762907889302,
      "grad_norm": 61.239009857177734,
      "learning_rate": 0.0006757425742574258,
      "loss": 2.9891,
      "step": 1655
    },
    {
      "epoch": 10.944237918215613,
      "grad_norm": 93.5625,
      "learning_rate": 0.0006752475247524753,
      "loss": 3.6137,
      "step": 1656
    },
    {
      "epoch": 10.950846757538207,
      "grad_norm": 48.223960876464844,
      "learning_rate": 0.0006747524752475249,
      "loss": 1.8532,
      "step": 1657
    },
    {
      "epoch": 10.957455596860802,
      "grad_norm": 15.770822525024414,
      "learning_rate": 0.0006742574257425743,
      "loss": 1.6023,
      "step": 1658
    },
    {
      "epoch": 10.964064436183396,
      "grad_norm": 5.715639591217041,
      "learning_rate": 0.0006737623762376238,
      "loss": 3.5963,
      "step": 1659
    },
    {
      "epoch": 10.97067327550599,
      "grad_norm": 48.290374755859375,
      "learning_rate": 0.0006732673267326733,
      "loss": 2.5189,
      "step": 1660
    },
    {
      "epoch": 10.977282114828583,
      "grad_norm": 23.587444305419922,
      "learning_rate": 0.0006727722772277228,
      "loss": 3.5936,
      "step": 1661
    },
    {
      "epoch": 10.983890954151176,
      "grad_norm": 12.931022644042969,
      "learning_rate": 0.0006722772277227724,
      "loss": 1.8031,
      "step": 1662
    },
    {
      "epoch": 10.990499793473772,
      "grad_norm": 58.0186882019043,
      "learning_rate": 0.0006717821782178218,
      "loss": 6.9937,
      "step": 1663
    },
    {
      "epoch": 10.997108632796365,
      "grad_norm": 24.259584426879883,
      "learning_rate": 0.0006712871287128713,
      "loss": 1.0827,
      "step": 1664
    },
    {
      "epoch": 10.997108632796365,
      "eval_validation_error_bar": 0.046937066428222256,
      "eval_validation_loss": 5.8291425704956055,
      "eval_validation_pearsonr": 0.5877426302192434,
      "eval_validation_rmse": 2.4143617153167725,
      "eval_validation_runtime": 33.0706,
      "eval_validation_samples_per_second": 6.138,
      "eval_validation_spearman": 0.6270427485322034,
      "eval_validation_steps_per_second": 6.138,
      "step": 1664
    },
    {
      "epoch": 10.997108632796365,
      "eval_test_error_bar": 0.038855363514505935,
      "eval_test_loss": 6.392575263977051,
      "eval_test_pearsonr": 0.590666463909241,
      "eval_test_rmse": 2.5283541679382324,
      "eval_test_runtime": 38.0515,
      "eval_test_samples_per_second": 8.567,
      "eval_test_spearman": 0.5972414672347479,
      "eval_test_steps_per_second": 8.567,
      "step": 1664
    },
    {
      "epoch": 11.003717472118959,
      "grad_norm": 31.131694793701172,
      "learning_rate": 0.0006707920792079208,
      "loss": 1.6792,
      "step": 1665
    },
    {
      "epoch": 11.010326311441553,
      "grad_norm": 31.800718307495117,
      "learning_rate": 0.0006702970297029704,
      "loss": 3.7818,
      "step": 1666
    },
    {
      "epoch": 11.016935150764146,
      "grad_norm": 18.3067684173584,
      "learning_rate": 0.0006698019801980199,
      "loss": 0.9749,
      "step": 1667
    },
    {
      "epoch": 11.023543990086742,
      "grad_norm": 16.991954803466797,
      "learning_rate": 0.0006693069306930693,
      "loss": 2.1705,
      "step": 1668
    },
    {
      "epoch": 11.030152829409335,
      "grad_norm": 22.92991828918457,
      "learning_rate": 0.0006688118811881188,
      "loss": 1.8662,
      "step": 1669
    },
    {
      "epoch": 11.036761668731929,
      "grad_norm": 13.23790168762207,
      "learning_rate": 0.0006683168316831684,
      "loss": 3.7864,
      "step": 1670
    },
    {
      "epoch": 11.043370508054522,
      "grad_norm": 8.640697479248047,
      "learning_rate": 0.0006678217821782179,
      "loss": 4.6962,
      "step": 1671
    },
    {
      "epoch": 11.049979347377118,
      "grad_norm": 49.46377944946289,
      "learning_rate": 0.0006673267326732674,
      "loss": 2.9583,
      "step": 1672
    },
    {
      "epoch": 11.056588186699711,
      "grad_norm": 52.664222717285156,
      "learning_rate": 0.0006668316831683168,
      "loss": 1.3344,
      "step": 1673
    },
    {
      "epoch": 11.063197026022305,
      "grad_norm": 36.4586296081543,
      "learning_rate": 0.0006663366336633664,
      "loss": 1.6454,
      "step": 1674
    },
    {
      "epoch": 11.069805865344899,
      "grad_norm": 106.3894271850586,
      "learning_rate": 0.0006658415841584159,
      "loss": 4.0383,
      "step": 1675
    },
    {
      "epoch": 11.076414704667492,
      "grad_norm": 66.81073760986328,
      "learning_rate": 0.0006653465346534654,
      "loss": 1.5769,
      "step": 1676
    },
    {
      "epoch": 11.083023543990087,
      "grad_norm": 22.591869354248047,
      "learning_rate": 0.0006648514851485149,
      "loss": 1.7967,
      "step": 1677
    },
    {
      "epoch": 11.089632383312681,
      "grad_norm": 93.01849365234375,
      "learning_rate": 0.0006643564356435644,
      "loss": 3.3248,
      "step": 1678
    },
    {
      "epoch": 11.096241222635275,
      "grad_norm": 48.41697311401367,
      "learning_rate": 0.0006638613861386139,
      "loss": 2.2113,
      "step": 1679
    },
    {
      "epoch": 11.102850061957868,
      "grad_norm": 63.8018684387207,
      "learning_rate": 0.0006633663366336634,
      "loss": 7.204,
      "step": 1680
    },
    {
      "epoch": 11.109458901280462,
      "grad_norm": 70.61590576171875,
      "learning_rate": 0.0006628712871287129,
      "loss": 1.5649,
      "step": 1681
    },
    {
      "epoch": 11.116067740603057,
      "grad_norm": 28.382532119750977,
      "learning_rate": 0.0006623762376237625,
      "loss": 1.0504,
      "step": 1682
    },
    {
      "epoch": 11.12267657992565,
      "grad_norm": 54.081581115722656,
      "learning_rate": 0.0006618811881188119,
      "loss": 1.5659,
      "step": 1683
    },
    {
      "epoch": 11.129285419248244,
      "grad_norm": 15.963924407958984,
      "learning_rate": 0.0006613861386138614,
      "loss": 1.4179,
      "step": 1684
    },
    {
      "epoch": 11.135894258570838,
      "grad_norm": 27.041805267333984,
      "learning_rate": 0.0006608910891089109,
      "loss": 3.1755,
      "step": 1685
    },
    {
      "epoch": 11.142503097893432,
      "grad_norm": 73.82416534423828,
      "learning_rate": 0.0006603960396039605,
      "loss": 4.7709,
      "step": 1686
    },
    {
      "epoch": 11.149111937216027,
      "grad_norm": 24.382369995117188,
      "learning_rate": 0.00065990099009901,
      "loss": 1.1677,
      "step": 1687
    },
    {
      "epoch": 11.15572077653862,
      "grad_norm": 26.506542205810547,
      "learning_rate": 0.0006594059405940594,
      "loss": 1.9825,
      "step": 1688
    },
    {
      "epoch": 11.162329615861214,
      "grad_norm": 15.17546558380127,
      "learning_rate": 0.0006589108910891089,
      "loss": 2.4123,
      "step": 1689
    },
    {
      "epoch": 11.168938455183808,
      "grad_norm": 38.444915771484375,
      "learning_rate": 0.0006584158415841585,
      "loss": 4.0065,
      "step": 1690
    },
    {
      "epoch": 11.175547294506403,
      "grad_norm": 34.699241638183594,
      "learning_rate": 0.000657920792079208,
      "loss": 2.3134,
      "step": 1691
    },
    {
      "epoch": 11.182156133828997,
      "grad_norm": 16.38083839416504,
      "learning_rate": 0.0006574257425742575,
      "loss": 4.1857,
      "step": 1692
    },
    {
      "epoch": 11.18876497315159,
      "grad_norm": 18.614479064941406,
      "learning_rate": 0.0006569306930693069,
      "loss": 0.9,
      "step": 1693
    },
    {
      "epoch": 11.195373812474184,
      "grad_norm": 17.30129623413086,
      "learning_rate": 0.0006564356435643565,
      "loss": 2.4953,
      "step": 1694
    },
    {
      "epoch": 11.201982651796778,
      "grad_norm": 12.753083229064941,
      "learning_rate": 0.000655940594059406,
      "loss": 0.8901,
      "step": 1695
    },
    {
      "epoch": 11.208591491119373,
      "grad_norm": 17.57160186767578,
      "learning_rate": 0.0006554455445544555,
      "loss": 2.1437,
      "step": 1696
    },
    {
      "epoch": 11.215200330441967,
      "grad_norm": 25.024600982666016,
      "learning_rate": 0.000654950495049505,
      "loss": 2.1199,
      "step": 1697
    },
    {
      "epoch": 11.22180916976456,
      "grad_norm": 82.70890808105469,
      "learning_rate": 0.0006544554455445545,
      "loss": 2.123,
      "step": 1698
    },
    {
      "epoch": 11.228418009087154,
      "grad_norm": 18.23748779296875,
      "learning_rate": 0.000653960396039604,
      "loss": 2.51,
      "step": 1699
    },
    {
      "epoch": 11.235026848409747,
      "grad_norm": 46.373260498046875,
      "learning_rate": 0.0006534653465346535,
      "loss": 1.1408,
      "step": 1700
    },
    {
      "epoch": 11.241635687732343,
      "grad_norm": 17.549152374267578,
      "learning_rate": 0.000652970297029703,
      "loss": 4.2586,
      "step": 1701
    },
    {
      "epoch": 11.248244527054936,
      "grad_norm": 54.22440719604492,
      "learning_rate": 0.0006524752475247526,
      "loss": 1.5103,
      "step": 1702
    },
    {
      "epoch": 11.25485336637753,
      "grad_norm": 16.977317810058594,
      "learning_rate": 0.000651980198019802,
      "loss": 1.6633,
      "step": 1703
    },
    {
      "epoch": 11.261462205700123,
      "grad_norm": 11.353545188903809,
      "learning_rate": 0.0006514851485148515,
      "loss": 2.2713,
      "step": 1704
    },
    {
      "epoch": 11.268071045022717,
      "grad_norm": 39.19831848144531,
      "learning_rate": 0.000650990099009901,
      "loss": 1.9692,
      "step": 1705
    },
    {
      "epoch": 11.274679884345312,
      "grad_norm": 26.22519302368164,
      "learning_rate": 0.0006504950495049506,
      "loss": 2.2476,
      "step": 1706
    },
    {
      "epoch": 11.281288723667906,
      "grad_norm": 80.70667266845703,
      "learning_rate": 0.0006500000000000001,
      "loss": 1.7731,
      "step": 1707
    },
    {
      "epoch": 11.2878975629905,
      "grad_norm": 102.50575256347656,
      "learning_rate": 0.0006495049504950495,
      "loss": 3.8639,
      "step": 1708
    },
    {
      "epoch": 11.294506402313093,
      "grad_norm": 96.31124114990234,
      "learning_rate": 0.000649009900990099,
      "loss": 4.0206,
      "step": 1709
    },
    {
      "epoch": 11.301115241635689,
      "grad_norm": 57.864776611328125,
      "learning_rate": 0.0006485148514851485,
      "loss": 3.1087,
      "step": 1710
    },
    {
      "epoch": 11.307724080958282,
      "grad_norm": 36.349327087402344,
      "learning_rate": 0.0006480198019801981,
      "loss": 4.6463,
      "step": 1711
    },
    {
      "epoch": 11.314332920280876,
      "grad_norm": 32.78780746459961,
      "learning_rate": 0.0006475247524752476,
      "loss": 3.0241,
      "step": 1712
    },
    {
      "epoch": 11.32094175960347,
      "grad_norm": 29.147451400756836,
      "learning_rate": 0.000647029702970297,
      "loss": 1.4246,
      "step": 1713
    },
    {
      "epoch": 11.327550598926063,
      "grad_norm": 19.750558853149414,
      "learning_rate": 0.0006465346534653465,
      "loss": 1.0254,
      "step": 1714
    },
    {
      "epoch": 11.334159438248658,
      "grad_norm": 30.972856521606445,
      "learning_rate": 0.0006460396039603961,
      "loss": 0.7229,
      "step": 1715
    },
    {
      "epoch": 11.340768277571252,
      "grad_norm": 46.12448501586914,
      "learning_rate": 0.0006455445544554456,
      "loss": 1.695,
      "step": 1716
    },
    {
      "epoch": 11.347377116893846,
      "grad_norm": 46.27797317504883,
      "learning_rate": 0.0006450495049504951,
      "loss": 2.8853,
      "step": 1717
    },
    {
      "epoch": 11.35398595621644,
      "grad_norm": 22.450000762939453,
      "learning_rate": 0.0006445544554455445,
      "loss": 1.0685,
      "step": 1718
    },
    {
      "epoch": 11.360594795539033,
      "grad_norm": 40.237327575683594,
      "learning_rate": 0.0006440594059405941,
      "loss": 2.3114,
      "step": 1719
    },
    {
      "epoch": 11.367203634861628,
      "grad_norm": 35.02046203613281,
      "learning_rate": 0.0006435643564356436,
      "loss": 2.7045,
      "step": 1720
    },
    {
      "epoch": 11.373812474184222,
      "grad_norm": 16.338762283325195,
      "learning_rate": 0.0006430693069306931,
      "loss": 1.7732,
      "step": 1721
    },
    {
      "epoch": 11.380421313506815,
      "grad_norm": 56.678489685058594,
      "learning_rate": 0.0006425742574257426,
      "loss": 2.4765,
      "step": 1722
    },
    {
      "epoch": 11.387030152829409,
      "grad_norm": 57.69676208496094,
      "learning_rate": 0.0006420792079207921,
      "loss": 2.1162,
      "step": 1723
    },
    {
      "epoch": 11.393638992152002,
      "grad_norm": 37.08375549316406,
      "learning_rate": 0.0006415841584158416,
      "loss": 2.8057,
      "step": 1724
    },
    {
      "epoch": 11.400247831474598,
      "grad_norm": 33.640010833740234,
      "learning_rate": 0.0006410891089108911,
      "loss": 2.691,
      "step": 1725
    },
    {
      "epoch": 11.406856670797191,
      "grad_norm": 52.459171295166016,
      "learning_rate": 0.0006405940594059406,
      "loss": 2.3961,
      "step": 1726
    },
    {
      "epoch": 11.413465510119785,
      "grad_norm": 53.3355827331543,
      "learning_rate": 0.0006400990099009902,
      "loss": 1.4438,
      "step": 1727
    },
    {
      "epoch": 11.420074349442379,
      "grad_norm": 7.931975841522217,
      "learning_rate": 0.0006396039603960396,
      "loss": 2.6991,
      "step": 1728
    },
    {
      "epoch": 11.426683188764972,
      "grad_norm": 58.40425491333008,
      "learning_rate": 0.0006391089108910891,
      "loss": 2.9697,
      "step": 1729
    },
    {
      "epoch": 11.433292028087568,
      "grad_norm": 29.761974334716797,
      "learning_rate": 0.0006386138613861386,
      "loss": 2.2127,
      "step": 1730
    },
    {
      "epoch": 11.439900867410161,
      "grad_norm": 26.021371841430664,
      "learning_rate": 0.0006381188118811882,
      "loss": 2.2472,
      "step": 1731
    },
    {
      "epoch": 11.446509706732755,
      "grad_norm": 8.67740249633789,
      "learning_rate": 0.0006376237623762377,
      "loss": 0.7438,
      "step": 1732
    },
    {
      "epoch": 11.453118546055348,
      "grad_norm": 23.59408187866211,
      "learning_rate": 0.0006371287128712871,
      "loss": 1.538,
      "step": 1733
    },
    {
      "epoch": 11.459727385377944,
      "grad_norm": 50.59709930419922,
      "learning_rate": 0.0006366336633663366,
      "loss": 2.1215,
      "step": 1734
    },
    {
      "epoch": 11.466336224700537,
      "grad_norm": 56.433502197265625,
      "learning_rate": 0.0006361386138613862,
      "loss": 3.0077,
      "step": 1735
    },
    {
      "epoch": 11.472945064023131,
      "grad_norm": 7.006457805633545,
      "learning_rate": 0.0006356435643564357,
      "loss": 0.7711,
      "step": 1736
    },
    {
      "epoch": 11.479553903345725,
      "grad_norm": 23.763904571533203,
      "learning_rate": 0.0006351485148514852,
      "loss": 0.7356,
      "step": 1737
    },
    {
      "epoch": 11.486162742668318,
      "grad_norm": 19.187231063842773,
      "learning_rate": 0.0006346534653465346,
      "loss": 1.1206,
      "step": 1738
    },
    {
      "epoch": 11.492771581990914,
      "grad_norm": 29.159832000732422,
      "learning_rate": 0.0006341584158415842,
      "loss": 1.8328,
      "step": 1739
    },
    {
      "epoch": 11.499380421313507,
      "grad_norm": 25.232479095458984,
      "learning_rate": 0.0006336633663366337,
      "loss": 1.5616,
      "step": 1740
    },
    {
      "epoch": 11.5059892606361,
      "grad_norm": 88.89396667480469,
      "learning_rate": 0.0006331683168316832,
      "loss": 3.5622,
      "step": 1741
    },
    {
      "epoch": 11.512598099958694,
      "grad_norm": 53.58933639526367,
      "learning_rate": 0.0006326732673267327,
      "loss": 2.9155,
      "step": 1742
    },
    {
      "epoch": 11.519206939281288,
      "grad_norm": 34.53477096557617,
      "learning_rate": 0.0006321782178217822,
      "loss": 3.4238,
      "step": 1743
    },
    {
      "epoch": 11.525815778603883,
      "grad_norm": 6.094326019287109,
      "learning_rate": 0.0006316831683168317,
      "loss": 1.9272,
      "step": 1744
    },
    {
      "epoch": 11.532424617926477,
      "grad_norm": 9.098432540893555,
      "learning_rate": 0.0006311881188118812,
      "loss": 2.3679,
      "step": 1745
    },
    {
      "epoch": 11.53903345724907,
      "grad_norm": 10.231178283691406,
      "learning_rate": 0.0006306930693069307,
      "loss": 1.3751,
      "step": 1746
    },
    {
      "epoch": 11.545642296571664,
      "grad_norm": 4.752469062805176,
      "learning_rate": 0.0006301980198019803,
      "loss": 1.9036,
      "step": 1747
    },
    {
      "epoch": 11.55225113589426,
      "grad_norm": 26.781776428222656,
      "learning_rate": 0.0006297029702970297,
      "loss": 4.1748,
      "step": 1748
    },
    {
      "epoch": 11.558859975216853,
      "grad_norm": 31.646503448486328,
      "learning_rate": 0.0006292079207920792,
      "loss": 1.1911,
      "step": 1749
    },
    {
      "epoch": 11.565468814539447,
      "grad_norm": 44.01293182373047,
      "learning_rate": 0.0006287128712871287,
      "loss": 2.4593,
      "step": 1750
    },
    {
      "epoch": 11.57207765386204,
      "grad_norm": 17.875656127929688,
      "learning_rate": 0.0006282178217821783,
      "loss": 1.2181,
      "step": 1751
    },
    {
      "epoch": 11.578686493184634,
      "grad_norm": 16.15673828125,
      "learning_rate": 0.0006277227722772278,
      "loss": 2.1911,
      "step": 1752
    },
    {
      "epoch": 11.58529533250723,
      "grad_norm": 6.702024459838867,
      "learning_rate": 0.0006272277227722772,
      "loss": 1.3155,
      "step": 1753
    },
    {
      "epoch": 11.591904171829823,
      "grad_norm": 36.2785758972168,
      "learning_rate": 0.0006267326732673267,
      "loss": 2.7615,
      "step": 1754
    },
    {
      "epoch": 11.598513011152416,
      "grad_norm": 11.097738265991211,
      "learning_rate": 0.0006262376237623763,
      "loss": 2.6386,
      "step": 1755
    },
    {
      "epoch": 11.60512185047501,
      "grad_norm": 31.436817169189453,
      "learning_rate": 0.0006257425742574258,
      "loss": 1.4099,
      "step": 1756
    },
    {
      "epoch": 11.611730689797604,
      "grad_norm": 58.76759338378906,
      "learning_rate": 0.0006252475247524753,
      "loss": 4.2507,
      "step": 1757
    },
    {
      "epoch": 11.618339529120199,
      "grad_norm": 8.342494010925293,
      "learning_rate": 0.0006247524752475247,
      "loss": 1.1328,
      "step": 1758
    },
    {
      "epoch": 11.624948368442793,
      "grad_norm": 31.6315860748291,
      "learning_rate": 0.0006242574257425742,
      "loss": 1.1769,
      "step": 1759
    },
    {
      "epoch": 11.631557207765386,
      "grad_norm": 14.770801544189453,
      "learning_rate": 0.0006237623762376238,
      "loss": 2.3527,
      "step": 1760
    },
    {
      "epoch": 11.63816604708798,
      "grad_norm": 17.147499084472656,
      "learning_rate": 0.0006232673267326733,
      "loss": 2.8028,
      "step": 1761
    },
    {
      "epoch": 11.644774886410573,
      "grad_norm": 12.79828929901123,
      "learning_rate": 0.0006227722772277228,
      "loss": 1.3074,
      "step": 1762
    },
    {
      "epoch": 11.651383725733169,
      "grad_norm": 42.97787094116211,
      "learning_rate": 0.0006222772277227722,
      "loss": 1.1717,
      "step": 1763
    },
    {
      "epoch": 11.657992565055762,
      "grad_norm": 46.51505661010742,
      "learning_rate": 0.0006217821782178218,
      "loss": 4.4177,
      "step": 1764
    },
    {
      "epoch": 11.664601404378356,
      "grad_norm": 15.987053871154785,
      "learning_rate": 0.0006212871287128713,
      "loss": 0.6957,
      "step": 1765
    },
    {
      "epoch": 11.67121024370095,
      "grad_norm": 11.924844741821289,
      "learning_rate": 0.0006207920792079208,
      "loss": 4.4215,
      "step": 1766
    },
    {
      "epoch": 11.677819083023543,
      "grad_norm": 43.226905822753906,
      "learning_rate": 0.0006202970297029703,
      "loss": 3.6821,
      "step": 1767
    },
    {
      "epoch": 11.684427922346138,
      "grad_norm": 7.254946231842041,
      "learning_rate": 0.0006198019801980198,
      "loss": 2.1496,
      "step": 1768
    },
    {
      "epoch": 11.691036761668732,
      "grad_norm": 92.0260009765625,
      "learning_rate": 0.0006193069306930693,
      "loss": 5.008,
      "step": 1769
    },
    {
      "epoch": 11.697645600991326,
      "grad_norm": 44.48291778564453,
      "learning_rate": 0.0006188118811881188,
      "loss": 0.7558,
      "step": 1770
    },
    {
      "epoch": 11.70425444031392,
      "grad_norm": 30.882549285888672,
      "learning_rate": 0.0006183168316831683,
      "loss": 2.4316,
      "step": 1771
    },
    {
      "epoch": 11.710863279636515,
      "grad_norm": 147.38369750976562,
      "learning_rate": 0.0006178217821782179,
      "loss": 5.7317,
      "step": 1772
    },
    {
      "epoch": 11.717472118959108,
      "grad_norm": 24.189245223999023,
      "learning_rate": 0.0006173267326732673,
      "loss": 1.8424,
      "step": 1773
    },
    {
      "epoch": 11.724080958281702,
      "grad_norm": 83.8057861328125,
      "learning_rate": 0.0006168316831683168,
      "loss": 4.5269,
      "step": 1774
    },
    {
      "epoch": 11.730689797604295,
      "grad_norm": 12.957448959350586,
      "learning_rate": 0.0006163366336633663,
      "loss": 2.3501,
      "step": 1775
    },
    {
      "epoch": 11.737298636926889,
      "grad_norm": 5.291742324829102,
      "learning_rate": 0.0006158415841584159,
      "loss": 1.911,
      "step": 1776
    },
    {
      "epoch": 11.743907476249484,
      "grad_norm": 2.504493236541748,
      "learning_rate": 0.0006153465346534654,
      "loss": 0.8758,
      "step": 1777
    },
    {
      "epoch": 11.750516315572078,
      "grad_norm": 71.86448669433594,
      "learning_rate": 0.0006148514851485148,
      "loss": 3.229,
      "step": 1778
    },
    {
      "epoch": 11.757125154894672,
      "grad_norm": 27.0093936920166,
      "learning_rate": 0.0006143564356435643,
      "loss": 0.8461,
      "step": 1779
    },
    {
      "epoch": 11.763733994217265,
      "grad_norm": 65.0073013305664,
      "learning_rate": 0.0006138613861386139,
      "loss": 2.9455,
      "step": 1780
    },
    {
      "epoch": 11.770342833539859,
      "grad_norm": 10.32193660736084,
      "learning_rate": 0.0006133663366336634,
      "loss": 1.0095,
      "step": 1781
    },
    {
      "epoch": 11.776951672862454,
      "grad_norm": 20.757720947265625,
      "learning_rate": 0.0006128712871287129,
      "loss": 1.2212,
      "step": 1782
    },
    {
      "epoch": 11.783560512185048,
      "grad_norm": 1.5711321830749512,
      "learning_rate": 0.0006123762376237623,
      "loss": 1.0686,
      "step": 1783
    },
    {
      "epoch": 11.790169351507641,
      "grad_norm": 80.37543487548828,
      "learning_rate": 0.0006118811881188119,
      "loss": 3.3186,
      "step": 1784
    },
    {
      "epoch": 11.796778190830235,
      "grad_norm": 16.389738082885742,
      "learning_rate": 0.0006113861386138614,
      "loss": 1.8086,
      "step": 1785
    },
    {
      "epoch": 11.80338703015283,
      "grad_norm": 45.8964958190918,
      "learning_rate": 0.0006108910891089109,
      "loss": 2.067,
      "step": 1786
    },
    {
      "epoch": 11.809995869475424,
      "grad_norm": 67.48319244384766,
      "learning_rate": 0.0006103960396039604,
      "loss": 2.0109,
      "step": 1787
    },
    {
      "epoch": 11.816604708798017,
      "grad_norm": 20.572118759155273,
      "learning_rate": 0.0006099009900990099,
      "loss": 1.0343,
      "step": 1788
    },
    {
      "epoch": 11.823213548120611,
      "grad_norm": 26.760391235351562,
      "learning_rate": 0.0006094059405940594,
      "loss": 4.0175,
      "step": 1789
    },
    {
      "epoch": 11.829822387443205,
      "grad_norm": 13.921737670898438,
      "learning_rate": 0.0006089108910891089,
      "loss": 2.8607,
      "step": 1790
    },
    {
      "epoch": 11.8364312267658,
      "grad_norm": 9.238665580749512,
      "learning_rate": 0.0006084158415841584,
      "loss": 0.6681,
      "step": 1791
    },
    {
      "epoch": 11.843040066088394,
      "grad_norm": 4.18587064743042,
      "learning_rate": 0.000607920792079208,
      "loss": 1.852,
      "step": 1792
    },
    {
      "epoch": 11.849648905410987,
      "grad_norm": 4.071692943572998,
      "learning_rate": 0.0006074257425742574,
      "loss": 0.8433,
      "step": 1793
    },
    {
      "epoch": 11.85625774473358,
      "grad_norm": 11.543200492858887,
      "learning_rate": 0.0006069306930693069,
      "loss": 3.8977,
      "step": 1794
    },
    {
      "epoch": 11.862866584056174,
      "grad_norm": 9.939675331115723,
      "learning_rate": 0.0006064356435643564,
      "loss": 1.9124,
      "step": 1795
    },
    {
      "epoch": 11.86947542337877,
      "grad_norm": 31.069507598876953,
      "learning_rate": 0.000605940594059406,
      "loss": 1.1565,
      "step": 1796
    },
    {
      "epoch": 11.876084262701363,
      "grad_norm": 27.377334594726562,
      "learning_rate": 0.0006054455445544555,
      "loss": 2.3474,
      "step": 1797
    },
    {
      "epoch": 11.882693102023957,
      "grad_norm": 26.04225730895996,
      "learning_rate": 0.0006049504950495049,
      "loss": 1.9528,
      "step": 1798
    },
    {
      "epoch": 11.88930194134655,
      "grad_norm": 61.7529411315918,
      "learning_rate": 0.0006044554455445544,
      "loss": 5.7624,
      "step": 1799
    },
    {
      "epoch": 11.895910780669144,
      "grad_norm": 10.7901611328125,
      "learning_rate": 0.000603960396039604,
      "loss": 2.1808,
      "step": 1800
    },
    {
      "epoch": 11.90251961999174,
      "grad_norm": 129.5026092529297,
      "learning_rate": 0.0006034653465346535,
      "loss": 8.4775,
      "step": 1801
    },
    {
      "epoch": 11.909128459314333,
      "grad_norm": 30.306133270263672,
      "learning_rate": 0.000602970297029703,
      "loss": 3.7978,
      "step": 1802
    },
    {
      "epoch": 11.915737298636927,
      "grad_norm": 86.22782897949219,
      "learning_rate": 0.0006024752475247524,
      "loss": 4.4362,
      "step": 1803
    },
    {
      "epoch": 11.92234613795952,
      "grad_norm": 12.362283706665039,
      "learning_rate": 0.000601980198019802,
      "loss": 2.3387,
      "step": 1804
    },
    {
      "epoch": 11.928954977282114,
      "grad_norm": 4.783977031707764,
      "learning_rate": 0.0006014851485148515,
      "loss": 4.1251,
      "step": 1805
    },
    {
      "epoch": 11.93556381660471,
      "grad_norm": 18.88201332092285,
      "learning_rate": 0.000600990099009901,
      "loss": 4.227,
      "step": 1806
    },
    {
      "epoch": 11.942172655927303,
      "grad_norm": 37.02354431152344,
      "learning_rate": 0.0006004950495049505,
      "loss": 0.8539,
      "step": 1807
    },
    {
      "epoch": 11.948781495249897,
      "grad_norm": 68.1973648071289,
      "learning_rate": 0.0006,
      "loss": 1.9092,
      "step": 1808
    },
    {
      "epoch": 11.95539033457249,
      "grad_norm": 6.6719651222229,
      "learning_rate": 0.0005995049504950495,
      "loss": 2.7652,
      "step": 1809
    },
    {
      "epoch": 11.961999173895085,
      "grad_norm": 81.07438659667969,
      "learning_rate": 0.000599009900990099,
      "loss": 2.5807,
      "step": 1810
    },
    {
      "epoch": 11.968608013217679,
      "grad_norm": 10.710210800170898,
      "learning_rate": 0.0005985148514851485,
      "loss": 5.4431,
      "step": 1811
    },
    {
      "epoch": 11.975216852540273,
      "grad_norm": 15.63959789276123,
      "learning_rate": 0.000598019801980198,
      "loss": 2.1118,
      "step": 1812
    },
    {
      "epoch": 11.981825691862866,
      "grad_norm": 45.00873947143555,
      "learning_rate": 0.0005975247524752475,
      "loss": 3.9008,
      "step": 1813
    },
    {
      "epoch": 11.98843453118546,
      "grad_norm": 29.652692794799805,
      "learning_rate": 0.000597029702970297,
      "loss": 2.8276,
      "step": 1814
    },
    {
      "epoch": 11.995043370508055,
      "grad_norm": 48.9515380859375,
      "learning_rate": 0.0005965346534653465,
      "loss": 2.4343,
      "step": 1815
    },
    {
      "epoch": 11.995043370508055,
      "eval_validation_error_bar": 0.04798126631816003,
      "eval_validation_loss": 6.4631757736206055,
      "eval_validation_pearsonr": 0.5953655020058144,
      "eval_validation_rmse": 2.5422775745391846,
      "eval_validation_runtime": 33.3379,
      "eval_validation_samples_per_second": 6.089,
      "eval_validation_spearman": 0.6145416726463394,
      "eval_validation_steps_per_second": 6.089,
      "step": 1815
    },
    {
      "epoch": 11.995043370508055,
      "eval_test_error_bar": 0.03973342605160566,
      "eval_test_loss": 7.366992950439453,
      "eval_test_pearsonr": 0.5586828798682757,
      "eval_test_rmse": 2.7142205238342285,
      "eval_test_runtime": 36.3454,
      "eval_test_samples_per_second": 8.969,
      "eval_test_spearman": 0.5829124546509683,
      "eval_test_steps_per_second": 8.969,
      "step": 1815
    },
    {
      "epoch": 12.001652209830649,
      "grad_norm": 15.174864768981934,
      "learning_rate": 0.000596039603960396,
      "loss": 3.0798,
      "step": 1816
    },
    {
      "epoch": 12.008261049153242,
      "grad_norm": 5.342020511627197,
      "learning_rate": 0.0005955445544554456,
      "loss": 0.897,
      "step": 1817
    },
    {
      "epoch": 12.014869888475836,
      "grad_norm": 10.777058601379395,
      "learning_rate": 0.000595049504950495,
      "loss": 1.5748,
      "step": 1818
    },
    {
      "epoch": 12.02147872779843,
      "grad_norm": 42.4486083984375,
      "learning_rate": 0.0005945544554455445,
      "loss": 1.0757,
      "step": 1819
    },
    {
      "epoch": 12.028087567121025,
      "grad_norm": 77.33097839355469,
      "learning_rate": 0.000594059405940594,
      "loss": 2.0706,
      "step": 1820
    },
    {
      "epoch": 12.034696406443619,
      "grad_norm": 11.79258918762207,
      "learning_rate": 0.0005935643564356436,
      "loss": 2.1881,
      "step": 1821
    },
    {
      "epoch": 12.041305245766212,
      "grad_norm": 28.177820205688477,
      "learning_rate": 0.0005930693069306931,
      "loss": 2.7487,
      "step": 1822
    },
    {
      "epoch": 12.047914085088806,
      "grad_norm": 31.667383193969727,
      "learning_rate": 0.0005925742574257425,
      "loss": 2.8712,
      "step": 1823
    },
    {
      "epoch": 12.0545229244114,
      "grad_norm": 27.744112014770508,
      "learning_rate": 0.000592079207920792,
      "loss": 2.5001,
      "step": 1824
    },
    {
      "epoch": 12.061131763733995,
      "grad_norm": 3.7971303462982178,
      "learning_rate": 0.0005915841584158416,
      "loss": 3.3093,
      "step": 1825
    },
    {
      "epoch": 12.067740603056588,
      "grad_norm": 3.4068362712860107,
      "learning_rate": 0.0005910891089108911,
      "loss": 2.0144,
      "step": 1826
    },
    {
      "epoch": 12.074349442379182,
      "grad_norm": 30.63427734375,
      "learning_rate": 0.0005905940594059406,
      "loss": 2.0149,
      "step": 1827
    },
    {
      "epoch": 12.080958281701776,
      "grad_norm": 25.928390502929688,
      "learning_rate": 0.00059009900990099,
      "loss": 1.0734,
      "step": 1828
    },
    {
      "epoch": 12.087567121024371,
      "grad_norm": 38.379844665527344,
      "learning_rate": 0.0005896039603960396,
      "loss": 2.8285,
      "step": 1829
    },
    {
      "epoch": 12.094175960346965,
      "grad_norm": 66.55538177490234,
      "learning_rate": 0.0005891089108910891,
      "loss": 1.8811,
      "step": 1830
    },
    {
      "epoch": 12.100784799669558,
      "grad_norm": 48.84580993652344,
      "learning_rate": 0.0005886138613861386,
      "loss": 1.0642,
      "step": 1831
    },
    {
      "epoch": 12.107393638992152,
      "grad_norm": 34.604026794433594,
      "learning_rate": 0.0005881188118811881,
      "loss": 2.1774,
      "step": 1832
    },
    {
      "epoch": 12.114002478314745,
      "grad_norm": 4.742715835571289,
      "learning_rate": 0.0005876237623762376,
      "loss": 1.0309,
      "step": 1833
    },
    {
      "epoch": 12.12061131763734,
      "grad_norm": 39.844303131103516,
      "learning_rate": 0.0005871287128712871,
      "loss": 2.2954,
      "step": 1834
    },
    {
      "epoch": 12.127220156959934,
      "grad_norm": 45.94147872924805,
      "learning_rate": 0.0005866336633663366,
      "loss": 1.5243,
      "step": 1835
    },
    {
      "epoch": 12.133828996282528,
      "grad_norm": 48.46144104003906,
      "learning_rate": 0.0005861386138613861,
      "loss": 2.5288,
      "step": 1836
    },
    {
      "epoch": 12.140437835605121,
      "grad_norm": 19.088207244873047,
      "learning_rate": 0.0005856435643564357,
      "loss": 2.1624,
      "step": 1837
    },
    {
      "epoch": 12.147046674927715,
      "grad_norm": 25.16618537902832,
      "learning_rate": 0.0005851485148514851,
      "loss": 3.1769,
      "step": 1838
    },
    {
      "epoch": 12.15365551425031,
      "grad_norm": 11.010762214660645,
      "learning_rate": 0.0005846534653465346,
      "loss": 3.9017,
      "step": 1839
    },
    {
      "epoch": 12.160264353572904,
      "grad_norm": 19.73960304260254,
      "learning_rate": 0.0005841584158415841,
      "loss": 1.4746,
      "step": 1840
    },
    {
      "epoch": 12.166873192895498,
      "grad_norm": 9.480439186096191,
      "learning_rate": 0.0005836633663366337,
      "loss": 1.931,
      "step": 1841
    },
    {
      "epoch": 12.173482032218091,
      "grad_norm": 26.616178512573242,
      "learning_rate": 0.0005831683168316832,
      "loss": 1.9623,
      "step": 1842
    },
    {
      "epoch": 12.180090871540685,
      "grad_norm": 57.7893180847168,
      "learning_rate": 0.0005826732673267326,
      "loss": 2.9691,
      "step": 1843
    },
    {
      "epoch": 12.18669971086328,
      "grad_norm": 45.31064987182617,
      "learning_rate": 0.0005821782178217821,
      "loss": 1.0228,
      "step": 1844
    },
    {
      "epoch": 12.193308550185874,
      "grad_norm": 5.204409122467041,
      "learning_rate": 0.0005816831683168317,
      "loss": 1.1954,
      "step": 1845
    },
    {
      "epoch": 12.199917389508467,
      "grad_norm": 10.520320892333984,
      "learning_rate": 0.0005811881188118812,
      "loss": 2.8485,
      "step": 1846
    },
    {
      "epoch": 12.206526228831061,
      "grad_norm": 12.51092529296875,
      "learning_rate": 0.0005806930693069307,
      "loss": 2.3289,
      "step": 1847
    },
    {
      "epoch": 12.213135068153656,
      "grad_norm": 29.1595516204834,
      "learning_rate": 0.0005801980198019801,
      "loss": 1.2915,
      "step": 1848
    },
    {
      "epoch": 12.21974390747625,
      "grad_norm": 8.72787857055664,
      "learning_rate": 0.0005797029702970297,
      "loss": 1.9899,
      "step": 1849
    },
    {
      "epoch": 12.226352746798844,
      "grad_norm": 10.690706253051758,
      "learning_rate": 0.0005792079207920792,
      "loss": 1.267,
      "step": 1850
    },
    {
      "epoch": 12.232961586121437,
      "grad_norm": 34.34238815307617,
      "learning_rate": 0.0005787128712871287,
      "loss": 0.5155,
      "step": 1851
    },
    {
      "epoch": 12.23957042544403,
      "grad_norm": 30.41523551940918,
      "learning_rate": 0.0005782178217821782,
      "loss": 1.1169,
      "step": 1852
    },
    {
      "epoch": 12.246179264766626,
      "grad_norm": 25.17066192626953,
      "learning_rate": 0.0005777227722772277,
      "loss": 2.9563,
      "step": 1853
    },
    {
      "epoch": 12.25278810408922,
      "grad_norm": 45.518009185791016,
      "learning_rate": 0.0005772277227722772,
      "loss": 6.2297,
      "step": 1854
    },
    {
      "epoch": 12.259396943411813,
      "grad_norm": 23.58082389831543,
      "learning_rate": 0.0005767326732673267,
      "loss": 2.5968,
      "step": 1855
    },
    {
      "epoch": 12.266005782734407,
      "grad_norm": 25.284595489501953,
      "learning_rate": 0.0005762376237623762,
      "loss": 1.0651,
      "step": 1856
    },
    {
      "epoch": 12.272614622057,
      "grad_norm": 2.5624401569366455,
      "learning_rate": 0.0005757425742574258,
      "loss": 0.5969,
      "step": 1857
    },
    {
      "epoch": 12.279223461379596,
      "grad_norm": 9.533699035644531,
      "learning_rate": 0.0005752475247524752,
      "loss": 1.1715,
      "step": 1858
    },
    {
      "epoch": 12.28583230070219,
      "grad_norm": 16.577280044555664,
      "learning_rate": 0.0005747524752475247,
      "loss": 1.9078,
      "step": 1859
    },
    {
      "epoch": 12.292441140024783,
      "grad_norm": 15.114381790161133,
      "learning_rate": 0.0005742574257425742,
      "loss": 1.2225,
      "step": 1860
    },
    {
      "epoch": 12.299049979347377,
      "grad_norm": 5.416032314300537,
      "learning_rate": 0.0005737623762376238,
      "loss": 2.4305,
      "step": 1861
    },
    {
      "epoch": 12.30565881866997,
      "grad_norm": 22.472875595092773,
      "learning_rate": 0.0005732673267326733,
      "loss": 2.4181,
      "step": 1862
    },
    {
      "epoch": 12.312267657992566,
      "grad_norm": 4.961785793304443,
      "learning_rate": 0.0005727722772277227,
      "loss": 3.7612,
      "step": 1863
    },
    {
      "epoch": 12.31887649731516,
      "grad_norm": 36.03618240356445,
      "learning_rate": 0.0005722772277227722,
      "loss": 3.5592,
      "step": 1864
    },
    {
      "epoch": 12.325485336637753,
      "grad_norm": 9.23380184173584,
      "learning_rate": 0.0005717821782178217,
      "loss": 2.0713,
      "step": 1865
    },
    {
      "epoch": 12.332094175960346,
      "grad_norm": 32.55937576293945,
      "learning_rate": 0.0005712871287128713,
      "loss": 2.9921,
      "step": 1866
    },
    {
      "epoch": 12.338703015282942,
      "grad_norm": 4.3990349769592285,
      "learning_rate": 0.0005707920792079208,
      "loss": 2.7736,
      "step": 1867
    },
    {
      "epoch": 12.345311854605535,
      "grad_norm": 51.79499053955078,
      "learning_rate": 0.0005702970297029702,
      "loss": 2.2607,
      "step": 1868
    },
    {
      "epoch": 12.351920693928129,
      "grad_norm": 39.37112808227539,
      "learning_rate": 0.0005698019801980197,
      "loss": 1.787,
      "step": 1869
    },
    {
      "epoch": 12.358529533250723,
      "grad_norm": 57.74641036987305,
      "learning_rate": 0.0005693069306930693,
      "loss": 1.2308,
      "step": 1870
    },
    {
      "epoch": 12.365138372573316,
      "grad_norm": 41.867252349853516,
      "learning_rate": 0.0005688118811881188,
      "loss": 3.4955,
      "step": 1871
    },
    {
      "epoch": 12.371747211895912,
      "grad_norm": 24.42726707458496,
      "learning_rate": 0.0005683168316831683,
      "loss": 2.6973,
      "step": 1872
    },
    {
      "epoch": 12.378356051218505,
      "grad_norm": 24.553678512573242,
      "learning_rate": 0.0005678217821782177,
      "loss": 1.4684,
      "step": 1873
    },
    {
      "epoch": 12.384964890541099,
      "grad_norm": 21.39212417602539,
      "learning_rate": 0.0005673267326732673,
      "loss": 2.6623,
      "step": 1874
    },
    {
      "epoch": 12.391573729863692,
      "grad_norm": 2.9683263301849365,
      "learning_rate": 0.0005668316831683168,
      "loss": 2.3103,
      "step": 1875
    },
    {
      "epoch": 12.398182569186286,
      "grad_norm": 4.838352203369141,
      "learning_rate": 0.0005663366336633663,
      "loss": 1.931,
      "step": 1876
    },
    {
      "epoch": 12.404791408508881,
      "grad_norm": 9.224119186401367,
      "learning_rate": 0.0005658415841584158,
      "loss": 2.6265,
      "step": 1877
    },
    {
      "epoch": 12.411400247831475,
      "grad_norm": 16.0979061126709,
      "learning_rate": 0.0005653465346534653,
      "loss": 3.1429,
      "step": 1878
    },
    {
      "epoch": 12.418009087154068,
      "grad_norm": 42.65022277832031,
      "learning_rate": 0.0005648514851485148,
      "loss": 3.5436,
      "step": 1879
    },
    {
      "epoch": 12.424617926476662,
      "grad_norm": 42.82268524169922,
      "learning_rate": 0.0005643564356435643,
      "loss": 1.9863,
      "step": 1880
    },
    {
      "epoch": 12.431226765799256,
      "grad_norm": 21.9910945892334,
      "learning_rate": 0.0005638613861386138,
      "loss": 1.1391,
      "step": 1881
    },
    {
      "epoch": 12.437835605121851,
      "grad_norm": 33.86802291870117,
      "learning_rate": 0.0005633663366336634,
      "loss": 9.0049,
      "step": 1882
    },
    {
      "epoch": 12.444444444444445,
      "grad_norm": 86.8369140625,
      "learning_rate": 0.0005628712871287128,
      "loss": 5.2078,
      "step": 1883
    },
    {
      "epoch": 12.451053283767038,
      "grad_norm": 3.8933513164520264,
      "learning_rate": 0.0005623762376237624,
      "loss": 0.9768,
      "step": 1884
    },
    {
      "epoch": 12.457662123089632,
      "grad_norm": 24.579622268676758,
      "learning_rate": 0.000561881188118812,
      "loss": 1.6224,
      "step": 1885
    },
    {
      "epoch": 12.464270962412227,
      "grad_norm": 72.07675170898438,
      "learning_rate": 0.0005613861386138615,
      "loss": 2.4571,
      "step": 1886
    },
    {
      "epoch": 12.47087980173482,
      "grad_norm": 77.14700317382812,
      "learning_rate": 0.000560891089108911,
      "loss": 2.1711,
      "step": 1887
    },
    {
      "epoch": 12.477488641057414,
      "grad_norm": 30.21003532409668,
      "learning_rate": 0.0005603960396039604,
      "loss": 2.2771,
      "step": 1888
    },
    {
      "epoch": 12.484097480380008,
      "grad_norm": 41.37518310546875,
      "learning_rate": 0.0005599009900990099,
      "loss": 3.3128,
      "step": 1889
    },
    {
      "epoch": 12.490706319702602,
      "grad_norm": 26.984699249267578,
      "learning_rate": 0.0005594059405940595,
      "loss": 1.3603,
      "step": 1890
    },
    {
      "epoch": 12.497315159025197,
      "grad_norm": 3.7210800647735596,
      "learning_rate": 0.000558910891089109,
      "loss": 1.5963,
      "step": 1891
    },
    {
      "epoch": 12.50392399834779,
      "grad_norm": 17.803730010986328,
      "learning_rate": 0.0005584158415841585,
      "loss": 1.0675,
      "step": 1892
    },
    {
      "epoch": 12.510532837670384,
      "grad_norm": 43.59672546386719,
      "learning_rate": 0.0005579207920792079,
      "loss": 2.7226,
      "step": 1893
    },
    {
      "epoch": 12.517141676992978,
      "grad_norm": 35.8314094543457,
      "learning_rate": 0.0005574257425742575,
      "loss": 2.2857,
      "step": 1894
    },
    {
      "epoch": 12.523750516315571,
      "grad_norm": 52.56092834472656,
      "learning_rate": 0.000556930693069307,
      "loss": 2.0933,
      "step": 1895
    },
    {
      "epoch": 12.530359355638167,
      "grad_norm": 11.296723365783691,
      "learning_rate": 0.0005564356435643565,
      "loss": 1.4745,
      "step": 1896
    },
    {
      "epoch": 12.53696819496076,
      "grad_norm": 53.30204772949219,
      "learning_rate": 0.000555940594059406,
      "loss": 2.6262,
      "step": 1897
    },
    {
      "epoch": 12.543577034283354,
      "grad_norm": 39.48420715332031,
      "learning_rate": 0.0005554455445544555,
      "loss": 1.525,
      "step": 1898
    },
    {
      "epoch": 12.550185873605948,
      "grad_norm": 21.62775993347168,
      "learning_rate": 0.000554950495049505,
      "loss": 4.0937,
      "step": 1899
    },
    {
      "epoch": 12.556794712928541,
      "grad_norm": 6.631651401519775,
      "learning_rate": 0.0005544554455445545,
      "loss": 1.6607,
      "step": 1900
    },
    {
      "epoch": 12.563403552251136,
      "grad_norm": 12.8519926071167,
      "learning_rate": 0.000553960396039604,
      "loss": 2.9219,
      "step": 1901
    },
    {
      "epoch": 12.57001239157373,
      "grad_norm": 16.205081939697266,
      "learning_rate": 0.0005534653465346536,
      "loss": 3.5441,
      "step": 1902
    },
    {
      "epoch": 12.576621230896324,
      "grad_norm": 75.16748046875,
      "learning_rate": 0.000552970297029703,
      "loss": 5.5365,
      "step": 1903
    },
    {
      "epoch": 12.583230070218917,
      "grad_norm": 51.08232498168945,
      "learning_rate": 0.0005524752475247525,
      "loss": 1.7125,
      "step": 1904
    },
    {
      "epoch": 12.589838909541513,
      "grad_norm": 13.07508659362793,
      "learning_rate": 0.000551980198019802,
      "loss": 1.8423,
      "step": 1905
    },
    {
      "epoch": 12.596447748864106,
      "grad_norm": 10.769261360168457,
      "learning_rate": 0.0005514851485148516,
      "loss": 2.2708,
      "step": 1906
    },
    {
      "epoch": 12.6030565881867,
      "grad_norm": 39.3986930847168,
      "learning_rate": 0.0005509900990099011,
      "loss": 3.9494,
      "step": 1907
    },
    {
      "epoch": 12.609665427509293,
      "grad_norm": 17.170778274536133,
      "learning_rate": 0.0005504950495049505,
      "loss": 3.9429,
      "step": 1908
    },
    {
      "epoch": 12.616274266831887,
      "grad_norm": 41.714229583740234,
      "learning_rate": 0.00055,
      "loss": 1.5842,
      "step": 1909
    },
    {
      "epoch": 12.622883106154482,
      "grad_norm": 48.12484359741211,
      "learning_rate": 0.0005495049504950496,
      "loss": 1.6123,
      "step": 1910
    },
    {
      "epoch": 12.629491945477076,
      "grad_norm": 100.54824829101562,
      "learning_rate": 0.0005490099009900991,
      "loss": 3.8909,
      "step": 1911
    },
    {
      "epoch": 12.63610078479967,
      "grad_norm": 33.434078216552734,
      "learning_rate": 0.0005485148514851486,
      "loss": 4.0646,
      "step": 1912
    },
    {
      "epoch": 12.642709624122263,
      "grad_norm": 8.854634284973145,
      "learning_rate": 0.000548019801980198,
      "loss": 2.8989,
      "step": 1913
    },
    {
      "epoch": 12.649318463444857,
      "grad_norm": 63.812355041503906,
      "learning_rate": 0.0005475247524752476,
      "loss": 2.2146,
      "step": 1914
    },
    {
      "epoch": 12.655927302767452,
      "grad_norm": 39.0865478515625,
      "learning_rate": 0.0005470297029702971,
      "loss": 4.0206,
      "step": 1915
    },
    {
      "epoch": 12.662536142090046,
      "grad_norm": 36.902462005615234,
      "learning_rate": 0.0005465346534653466,
      "loss": 5.1062,
      "step": 1916
    },
    {
      "epoch": 12.66914498141264,
      "grad_norm": 46.052764892578125,
      "learning_rate": 0.0005460396039603961,
      "loss": 1.4524,
      "step": 1917
    },
    {
      "epoch": 12.675753820735233,
      "grad_norm": 60.12175369262695,
      "learning_rate": 0.0005455445544554456,
      "loss": 4.0624,
      "step": 1918
    },
    {
      "epoch": 12.682362660057827,
      "grad_norm": 3.408440113067627,
      "learning_rate": 0.0005450495049504951,
      "loss": 0.8254,
      "step": 1919
    },
    {
      "epoch": 12.688971499380422,
      "grad_norm": 52.30525588989258,
      "learning_rate": 0.0005445544554455446,
      "loss": 1.9715,
      "step": 1920
    },
    {
      "epoch": 12.695580338703015,
      "grad_norm": 25.428686141967773,
      "learning_rate": 0.0005440594059405941,
      "loss": 0.9534,
      "step": 1921
    },
    {
      "epoch": 12.702189178025609,
      "grad_norm": 77.06959533691406,
      "learning_rate": 0.0005435643564356437,
      "loss": 2.0049,
      "step": 1922
    },
    {
      "epoch": 12.708798017348203,
      "grad_norm": 8.594114303588867,
      "learning_rate": 0.0005430693069306931,
      "loss": 1.9143,
      "step": 1923
    },
    {
      "epoch": 12.715406856670796,
      "grad_norm": 16.548826217651367,
      "learning_rate": 0.0005425742574257426,
      "loss": 1.1384,
      "step": 1924
    },
    {
      "epoch": 12.722015695993392,
      "grad_norm": 36.273616790771484,
      "learning_rate": 0.0005420792079207921,
      "loss": 1.27,
      "step": 1925
    },
    {
      "epoch": 12.728624535315985,
      "grad_norm": 28.496328353881836,
      "learning_rate": 0.0005415841584158417,
      "loss": 1.1954,
      "step": 1926
    },
    {
      "epoch": 12.735233374638579,
      "grad_norm": 10.66411304473877,
      "learning_rate": 0.0005410891089108912,
      "loss": 2.6087,
      "step": 1927
    },
    {
      "epoch": 12.741842213961172,
      "grad_norm": 11.493834495544434,
      "learning_rate": 0.0005405940594059406,
      "loss": 2.6623,
      "step": 1928
    },
    {
      "epoch": 12.748451053283768,
      "grad_norm": 17.687057495117188,
      "learning_rate": 0.0005400990099009901,
      "loss": 5.6839,
      "step": 1929
    },
    {
      "epoch": 12.755059892606361,
      "grad_norm": 27.20342254638672,
      "learning_rate": 0.0005396039603960396,
      "loss": 2.5049,
      "step": 1930
    },
    {
      "epoch": 12.761668731928955,
      "grad_norm": 31.503948211669922,
      "learning_rate": 0.0005391089108910892,
      "loss": 1.4135,
      "step": 1931
    },
    {
      "epoch": 12.768277571251549,
      "grad_norm": 85.42015838623047,
      "learning_rate": 0.0005386138613861387,
      "loss": 4.1987,
      "step": 1932
    },
    {
      "epoch": 12.774886410574142,
      "grad_norm": 101.63240051269531,
      "learning_rate": 0.0005381188118811881,
      "loss": 3.4033,
      "step": 1933
    },
    {
      "epoch": 12.781495249896738,
      "grad_norm": 46.69009780883789,
      "learning_rate": 0.0005376237623762376,
      "loss": 1.9606,
      "step": 1934
    },
    {
      "epoch": 12.788104089219331,
      "grad_norm": 27.4250431060791,
      "learning_rate": 0.0005371287128712872,
      "loss": 2.138,
      "step": 1935
    },
    {
      "epoch": 12.794712928541925,
      "grad_norm": 24.329984664916992,
      "learning_rate": 0.0005366336633663367,
      "loss": 3.0697,
      "step": 1936
    },
    {
      "epoch": 12.801321767864518,
      "grad_norm": 33.81764221191406,
      "learning_rate": 0.0005361386138613862,
      "loss": 2.3137,
      "step": 1937
    },
    {
      "epoch": 12.807930607187112,
      "grad_norm": 6.3455705642700195,
      "learning_rate": 0.0005356435643564356,
      "loss": 1.5055,
      "step": 1938
    },
    {
      "epoch": 12.814539446509707,
      "grad_norm": 27.426712036132812,
      "learning_rate": 0.0005351485148514852,
      "loss": 1.5192,
      "step": 1939
    },
    {
      "epoch": 12.821148285832301,
      "grad_norm": 66.8585433959961,
      "learning_rate": 0.0005346534653465347,
      "loss": 2.1591,
      "step": 1940
    },
    {
      "epoch": 12.827757125154895,
      "grad_norm": 54.89895248413086,
      "learning_rate": 0.0005341584158415842,
      "loss": 3.1794,
      "step": 1941
    },
    {
      "epoch": 12.834365964477488,
      "grad_norm": 60.29708480834961,
      "learning_rate": 0.0005336633663366337,
      "loss": 1.4993,
      "step": 1942
    },
    {
      "epoch": 12.840974803800083,
      "grad_norm": 17.726825714111328,
      "learning_rate": 0.0005331683168316832,
      "loss": 1.0929,
      "step": 1943
    },
    {
      "epoch": 12.847583643122677,
      "grad_norm": 41.44865798950195,
      "learning_rate": 0.0005326732673267327,
      "loss": 1.613,
      "step": 1944
    },
    {
      "epoch": 12.85419248244527,
      "grad_norm": 26.545684814453125,
      "learning_rate": 0.0005321782178217822,
      "loss": 2.6902,
      "step": 1945
    },
    {
      "epoch": 12.860801321767864,
      "grad_norm": 6.1601691246032715,
      "learning_rate": 0.0005316831683168317,
      "loss": 1.9506,
      "step": 1946
    },
    {
      "epoch": 12.867410161090458,
      "grad_norm": 20.12218475341797,
      "learning_rate": 0.0005311881188118813,
      "loss": 4.4965,
      "step": 1947
    },
    {
      "epoch": 12.874019000413053,
      "grad_norm": 93.48858642578125,
      "learning_rate": 0.0005306930693069307,
      "loss": 3.2474,
      "step": 1948
    },
    {
      "epoch": 12.880627839735647,
      "grad_norm": 106.30573272705078,
      "learning_rate": 0.0005301980198019802,
      "loss": 5.0085,
      "step": 1949
    },
    {
      "epoch": 12.88723667905824,
      "grad_norm": 58.870731353759766,
      "learning_rate": 0.0005297029702970297,
      "loss": 2.1864,
      "step": 1950
    },
    {
      "epoch": 12.893845518380834,
      "grad_norm": 49.003719329833984,
      "learning_rate": 0.0005292079207920793,
      "loss": 2.334,
      "step": 1951
    },
    {
      "epoch": 12.900454357703428,
      "grad_norm": 86.39755249023438,
      "learning_rate": 0.0005287128712871288,
      "loss": 4.4673,
      "step": 1952
    },
    {
      "epoch": 12.907063197026023,
      "grad_norm": 45.27107620239258,
      "learning_rate": 0.0005282178217821782,
      "loss": 2.2513,
      "step": 1953
    },
    {
      "epoch": 12.913672036348617,
      "grad_norm": 7.280818939208984,
      "learning_rate": 0.0005277227722772277,
      "loss": 1.4681,
      "step": 1954
    },
    {
      "epoch": 12.92028087567121,
      "grad_norm": 12.282394409179688,
      "learning_rate": 0.0005272277227722773,
      "loss": 0.6377,
      "step": 1955
    },
    {
      "epoch": 12.926889714993804,
      "grad_norm": 6.339006423950195,
      "learning_rate": 0.0005267326732673268,
      "loss": 2.0849,
      "step": 1956
    },
    {
      "epoch": 12.933498554316397,
      "grad_norm": 4.800782203674316,
      "learning_rate": 0.0005262376237623763,
      "loss": 2.9147,
      "step": 1957
    },
    {
      "epoch": 12.940107393638993,
      "grad_norm": 82.33638000488281,
      "learning_rate": 0.0005257425742574257,
      "loss": 3.7991,
      "step": 1958
    },
    {
      "epoch": 12.946716232961586,
      "grad_norm": 50.86370849609375,
      "learning_rate": 0.0005252475247524753,
      "loss": 1.6796,
      "step": 1959
    },
    {
      "epoch": 12.95332507228418,
      "grad_norm": 19.718826293945312,
      "learning_rate": 0.0005247524752475248,
      "loss": 1.6542,
      "step": 1960
    },
    {
      "epoch": 12.959933911606774,
      "grad_norm": 29.18180274963379,
      "learning_rate": 0.0005242574257425743,
      "loss": 1.6135,
      "step": 1961
    },
    {
      "epoch": 12.966542750929367,
      "grad_norm": 3.878659963607788,
      "learning_rate": 0.0005237623762376238,
      "loss": 0.8478,
      "step": 1962
    },
    {
      "epoch": 12.973151590251963,
      "grad_norm": 30.607934951782227,
      "learning_rate": 0.0005232673267326733,
      "loss": 3.4081,
      "step": 1963
    },
    {
      "epoch": 12.979760429574556,
      "grad_norm": 22.980762481689453,
      "learning_rate": 0.0005227722772277228,
      "loss": 3.9815,
      "step": 1964
    },
    {
      "epoch": 12.98636926889715,
      "grad_norm": 16.00560188293457,
      "learning_rate": 0.0005222772277227723,
      "loss": 2.1546,
      "step": 1965
    },
    {
      "epoch": 12.992978108219743,
      "grad_norm": 11.375141143798828,
      "learning_rate": 0.0005217821782178218,
      "loss": 4.7057,
      "step": 1966
    },
    {
      "epoch": 12.999586947542339,
      "grad_norm": 40.18208694458008,
      "learning_rate": 0.0005212871287128714,
      "loss": 1.4226,
      "step": 1967
    },
    {
      "epoch": 12.999586947542339,
      "eval_validation_error_bar": 0.04416312545588396,
      "eval_validation_loss": 6.953686714172363,
      "eval_validation_pearsonr": 0.6164721036775818,
      "eval_validation_rmse": 2.636984348297119,
      "eval_validation_runtime": 32.9198,
      "eval_validation_samples_per_second": 6.167,
      "eval_validation_spearman": 0.6586601635373543,
      "eval_validation_steps_per_second": 6.167,
      "step": 1967
    },
    {
      "epoch": 12.999586947542339,
      "eval_test_error_bar": 0.03806490033105856,
      "eval_test_loss": 7.669331073760986,
      "eval_test_pearsonr": 0.5879555036258646,
      "eval_test_rmse": 2.7693557739257812,
      "eval_test_runtime": 38.3694,
      "eval_test_samples_per_second": 8.496,
      "eval_test_spearman": 0.6097485915629123,
      "eval_test_steps_per_second": 8.496,
      "step": 1967
    }
  ],
  "logging_steps": 1,
  "max_steps": 3020,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
