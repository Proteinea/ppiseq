{
  "best_metric": 0.7080877922852268,
  "best_model_checkpoint": "backbone_ElnaggarLab/ankh-large-setup_convbert_attn_pooled_addition_weights/checkpoint-1967",
  "epoch": 12.999586947542339,
  "eval_steps": 500,
  "global_step": 1967,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00660883932259397,
      "grad_norm": 7526.060546875,
      "learning_rate": 1e-06,
      "loss": 216.3139,
      "step": 1
    },
    {
      "epoch": 0.01321767864518794,
      "grad_norm": 7957.77978515625,
      "learning_rate": 2e-06,
      "loss": 236.8772,
      "step": 2
    },
    {
      "epoch": 0.01982651796778191,
      "grad_norm": 8216.6875,
      "learning_rate": 3e-06,
      "loss": 251.7234,
      "step": 3
    },
    {
      "epoch": 0.02643535729037588,
      "grad_norm": 6742.09130859375,
      "learning_rate": 4e-06,
      "loss": 165.7013,
      "step": 4
    },
    {
      "epoch": 0.033044196612969846,
      "grad_norm": 7024.791015625,
      "learning_rate": 5e-06,
      "loss": 182.1132,
      "step": 5
    },
    {
      "epoch": 0.03965303593556382,
      "grad_norm": 5455.1435546875,
      "learning_rate": 6e-06,
      "loss": 118.4326,
      "step": 6
    },
    {
      "epoch": 0.04626187525815779,
      "grad_norm": 5087.86767578125,
      "learning_rate": 7e-06,
      "loss": 100.971,
      "step": 7
    },
    {
      "epoch": 0.05287071458075176,
      "grad_norm": 3999.6728515625,
      "learning_rate": 8e-06,
      "loss": 67.952,
      "step": 8
    },
    {
      "epoch": 0.05947955390334572,
      "grad_norm": 2516.890625,
      "learning_rate": 9e-06,
      "loss": 28.0141,
      "step": 9
    },
    {
      "epoch": 0.06608839322593969,
      "grad_norm": 759.7305297851562,
      "learning_rate": 1e-05,
      "loss": 11.8029,
      "step": 10
    },
    {
      "epoch": 0.07269723254853366,
      "grad_norm": 709.5731201171875,
      "learning_rate": 1.1e-05,
      "loss": 7.1908,
      "step": 11
    },
    {
      "epoch": 0.07930607187112763,
      "grad_norm": 2268.944580078125,
      "learning_rate": 1.2e-05,
      "loss": 29.6081,
      "step": 12
    },
    {
      "epoch": 0.0859149111937216,
      "grad_norm": 2323.007568359375,
      "learning_rate": 1.3e-05,
      "loss": 27.1511,
      "step": 13
    },
    {
      "epoch": 0.09252375051631558,
      "grad_norm": 3210.260498046875,
      "learning_rate": 1.4e-05,
      "loss": 42.9013,
      "step": 14
    },
    {
      "epoch": 0.09913258983890955,
      "grad_norm": 3967.78857421875,
      "learning_rate": 1.5e-05,
      "loss": 67.982,
      "step": 15
    },
    {
      "epoch": 0.10574142916150352,
      "grad_norm": 3315.47119140625,
      "learning_rate": 1.6e-05,
      "loss": 47.6989,
      "step": 16
    },
    {
      "epoch": 0.11235026848409747,
      "grad_norm": 2240.278564453125,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 26.3371,
      "step": 17
    },
    {
      "epoch": 0.11895910780669144,
      "grad_norm": 1949.884033203125,
      "learning_rate": 1.8e-05,
      "loss": 20.9554,
      "step": 18
    },
    {
      "epoch": 0.12556794712928543,
      "grad_norm": 1472.47314453125,
      "learning_rate": 1.9e-05,
      "loss": 14.3397,
      "step": 19
    },
    {
      "epoch": 0.13217678645187939,
      "grad_norm": 154.6326904296875,
      "learning_rate": 2e-05,
      "loss": 6.0109,
      "step": 20
    },
    {
      "epoch": 0.13878562577447337,
      "grad_norm": 1199.8604736328125,
      "learning_rate": 2.1000000000000002e-05,
      "loss": 21.603,
      "step": 21
    },
    {
      "epoch": 0.14539446509706733,
      "grad_norm": 2274.11328125,
      "learning_rate": 2.2e-05,
      "loss": 29.4247,
      "step": 22
    },
    {
      "epoch": 0.15200330441966128,
      "grad_norm": 2240.34228515625,
      "learning_rate": 2.3e-05,
      "loss": 27.2575,
      "step": 23
    },
    {
      "epoch": 0.15861214374225527,
      "grad_norm": 770.7196044921875,
      "learning_rate": 2.4e-05,
      "loss": 13.1762,
      "step": 24
    },
    {
      "epoch": 0.16522098306484923,
      "grad_norm": 367.26708984375,
      "learning_rate": 2.5e-05,
      "loss": 10.4999,
      "step": 25
    },
    {
      "epoch": 0.1718298223874432,
      "grad_norm": 506.4676513671875,
      "learning_rate": 2.6e-05,
      "loss": 7.4358,
      "step": 26
    },
    {
      "epoch": 0.17843866171003717,
      "grad_norm": 1918.900634765625,
      "learning_rate": 2.7e-05,
      "loss": 29.4973,
      "step": 27
    },
    {
      "epoch": 0.18504750103263115,
      "grad_norm": 1844.83984375,
      "learning_rate": 2.8e-05,
      "loss": 17.601,
      "step": 28
    },
    {
      "epoch": 0.1916563403552251,
      "grad_norm": 2014.147216796875,
      "learning_rate": 2.9000000000000004e-05,
      "loss": 19.9481,
      "step": 29
    },
    {
      "epoch": 0.1982651796778191,
      "grad_norm": 542.8023071289062,
      "learning_rate": 3e-05,
      "loss": 7.2357,
      "step": 30
    },
    {
      "epoch": 0.20487401900041305,
      "grad_norm": 643.5059204101562,
      "learning_rate": 3.1e-05,
      "loss": 5.6139,
      "step": 31
    },
    {
      "epoch": 0.21148285832300703,
      "grad_norm": 981.0850219726562,
      "learning_rate": 3.2e-05,
      "loss": 10.6191,
      "step": 32
    },
    {
      "epoch": 0.218091697645601,
      "grad_norm": 928.7728271484375,
      "learning_rate": 3.3e-05,
      "loss": 13.3568,
      "step": 33
    },
    {
      "epoch": 0.22470053696819495,
      "grad_norm": 845.8483276367188,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 15.166,
      "step": 34
    },
    {
      "epoch": 0.23130937629078893,
      "grad_norm": 121.00323486328125,
      "learning_rate": 3.5000000000000004e-05,
      "loss": 5.3725,
      "step": 35
    },
    {
      "epoch": 0.2379182156133829,
      "grad_norm": 1580.3126220703125,
      "learning_rate": 3.6e-05,
      "loss": 15.6908,
      "step": 36
    },
    {
      "epoch": 0.24452705493597687,
      "grad_norm": 2133.176025390625,
      "learning_rate": 3.7e-05,
      "loss": 28.1278,
      "step": 37
    },
    {
      "epoch": 0.25113589425857086,
      "grad_norm": 3080.37109375,
      "learning_rate": 3.8e-05,
      "loss": 46.2814,
      "step": 38
    },
    {
      "epoch": 0.2577447335811648,
      "grad_norm": 2019.9512939453125,
      "learning_rate": 3.9e-05,
      "loss": 20.2791,
      "step": 39
    },
    {
      "epoch": 0.26435357290375877,
      "grad_norm": 186.7786407470703,
      "learning_rate": 4e-05,
      "loss": 9.4631,
      "step": 40
    },
    {
      "epoch": 0.27096241222635276,
      "grad_norm": 1233.1483154296875,
      "learning_rate": 4.1e-05,
      "loss": 9.8741,
      "step": 41
    },
    {
      "epoch": 0.27757125154894674,
      "grad_norm": 2302.337890625,
      "learning_rate": 4.2000000000000004e-05,
      "loss": 22.492,
      "step": 42
    },
    {
      "epoch": 0.28418009087154067,
      "grad_norm": 1377.9522705078125,
      "learning_rate": 4.2999999999999995e-05,
      "loss": 11.2879,
      "step": 43
    },
    {
      "epoch": 0.29078893019413465,
      "grad_norm": 916.5003662109375,
      "learning_rate": 4.4e-05,
      "loss": 13.6301,
      "step": 44
    },
    {
      "epoch": 0.29739776951672864,
      "grad_norm": 246.2389678955078,
      "learning_rate": 4.4999999999999996e-05,
      "loss": 5.7511,
      "step": 45
    },
    {
      "epoch": 0.30400660883932257,
      "grad_norm": 244.87490844726562,
      "learning_rate": 4.6e-05,
      "loss": 8.8647,
      "step": 46
    },
    {
      "epoch": 0.31061544816191655,
      "grad_norm": 123.2276611328125,
      "learning_rate": 4.7000000000000004e-05,
      "loss": 3.9586,
      "step": 47
    },
    {
      "epoch": 0.31722428748451054,
      "grad_norm": 560.5586547851562,
      "learning_rate": 4.8e-05,
      "loss": 8.5284,
      "step": 48
    },
    {
      "epoch": 0.3238331268071045,
      "grad_norm": 629.3458251953125,
      "learning_rate": 4.9000000000000005e-05,
      "loss": 5.0763,
      "step": 49
    },
    {
      "epoch": 0.33044196612969845,
      "grad_norm": 183.53915405273438,
      "learning_rate": 5e-05,
      "loss": 4.0431,
      "step": 50
    },
    {
      "epoch": 0.33705080545229243,
      "grad_norm": 184.70933532714844,
      "learning_rate": 5.1e-05,
      "loss": 5.9775,
      "step": 51
    },
    {
      "epoch": 0.3436596447748864,
      "grad_norm": 1742.5093994140625,
      "learning_rate": 5.2e-05,
      "loss": 17.5916,
      "step": 52
    },
    {
      "epoch": 0.3502684840974804,
      "grad_norm": 880.7767333984375,
      "learning_rate": 5.3e-05,
      "loss": 7.9214,
      "step": 53
    },
    {
      "epoch": 0.35687732342007433,
      "grad_norm": 107.55974578857422,
      "learning_rate": 5.4e-05,
      "loss": 7.548,
      "step": 54
    },
    {
      "epoch": 0.3634861627426683,
      "grad_norm": 635.4795532226562,
      "learning_rate": 5.5e-05,
      "loss": 4.8674,
      "step": 55
    },
    {
      "epoch": 0.3700950020652623,
      "grad_norm": 115.07649993896484,
      "learning_rate": 5.6e-05,
      "loss": 9.2199,
      "step": 56
    },
    {
      "epoch": 0.37670384138785623,
      "grad_norm": 1285.287109375,
      "learning_rate": 5.7e-05,
      "loss": 11.0078,
      "step": 57
    },
    {
      "epoch": 0.3833126807104502,
      "grad_norm": 1687.455078125,
      "learning_rate": 5.800000000000001e-05,
      "loss": 15.7055,
      "step": 58
    },
    {
      "epoch": 0.3899215200330442,
      "grad_norm": 992.7335815429688,
      "learning_rate": 5.9e-05,
      "loss": 9.2727,
      "step": 59
    },
    {
      "epoch": 0.3965303593556382,
      "grad_norm": 289.70684814453125,
      "learning_rate": 6e-05,
      "loss": 5.4951,
      "step": 60
    },
    {
      "epoch": 0.4031391986782321,
      "grad_norm": 1140.513916015625,
      "learning_rate": 6.1e-05,
      "loss": 10.3809,
      "step": 61
    },
    {
      "epoch": 0.4097480380008261,
      "grad_norm": 216.028564453125,
      "learning_rate": 6.2e-05,
      "loss": 9.6033,
      "step": 62
    },
    {
      "epoch": 0.4163568773234201,
      "grad_norm": 655.240966796875,
      "learning_rate": 6.3e-05,
      "loss": 5.5544,
      "step": 63
    },
    {
      "epoch": 0.42296571664601407,
      "grad_norm": 1538.2215576171875,
      "learning_rate": 6.4e-05,
      "loss": 13.0883,
      "step": 64
    },
    {
      "epoch": 0.429574555968608,
      "grad_norm": 129.56272888183594,
      "learning_rate": 6.500000000000001e-05,
      "loss": 3.0754,
      "step": 65
    },
    {
      "epoch": 0.436183395291202,
      "grad_norm": 1092.4385986328125,
      "learning_rate": 6.6e-05,
      "loss": 6.9205,
      "step": 66
    },
    {
      "epoch": 0.44279223461379597,
      "grad_norm": 1792.322509765625,
      "learning_rate": 6.7e-05,
      "loss": 16.4628,
      "step": 67
    },
    {
      "epoch": 0.4494010739363899,
      "grad_norm": 782.0278930664062,
      "learning_rate": 6.800000000000001e-05,
      "loss": 4.7062,
      "step": 68
    },
    {
      "epoch": 0.4560099132589839,
      "grad_norm": 1097.5186767578125,
      "learning_rate": 6.900000000000001e-05,
      "loss": 9.9891,
      "step": 69
    },
    {
      "epoch": 0.46261875258157786,
      "grad_norm": 1091.41845703125,
      "learning_rate": 7.000000000000001e-05,
      "loss": 9.6209,
      "step": 70
    },
    {
      "epoch": 0.46922759190417185,
      "grad_norm": 545.972900390625,
      "learning_rate": 7.099999999999999e-05,
      "loss": 5.0106,
      "step": 71
    },
    {
      "epoch": 0.4758364312267658,
      "grad_norm": 1214.581298828125,
      "learning_rate": 7.2e-05,
      "loss": 10.7527,
      "step": 72
    },
    {
      "epoch": 0.48244527054935976,
      "grad_norm": 1749.1820068359375,
      "learning_rate": 7.3e-05,
      "loss": 15.9639,
      "step": 73
    },
    {
      "epoch": 0.48905410987195375,
      "grad_norm": 1099.00048828125,
      "learning_rate": 7.4e-05,
      "loss": 5.3366,
      "step": 74
    },
    {
      "epoch": 0.49566294919454773,
      "grad_norm": 811.7166748046875,
      "learning_rate": 7.5e-05,
      "loss": 7.5074,
      "step": 75
    },
    {
      "epoch": 0.5022717885171417,
      "grad_norm": 737.6307373046875,
      "learning_rate": 7.6e-05,
      "loss": 11.4741,
      "step": 76
    },
    {
      "epoch": 0.5088806278397356,
      "grad_norm": 163.32875061035156,
      "learning_rate": 7.7e-05,
      "loss": 3.1398,
      "step": 77
    },
    {
      "epoch": 0.5154894671623296,
      "grad_norm": 1216.2965087890625,
      "learning_rate": 7.8e-05,
      "loss": 9.5344,
      "step": 78
    },
    {
      "epoch": 0.5220983064849236,
      "grad_norm": 1291.3095703125,
      "learning_rate": 7.9e-05,
      "loss": 10.7259,
      "step": 79
    },
    {
      "epoch": 0.5287071458075175,
      "grad_norm": 1060.18310546875,
      "learning_rate": 8e-05,
      "loss": 12.0662,
      "step": 80
    },
    {
      "epoch": 0.5353159851301115,
      "grad_norm": 249.33334350585938,
      "learning_rate": 8.1e-05,
      "loss": 5.3029,
      "step": 81
    },
    {
      "epoch": 0.5419248244527055,
      "grad_norm": 745.6005249023438,
      "learning_rate": 8.2e-05,
      "loss": 5.6812,
      "step": 82
    },
    {
      "epoch": 0.5485336637752994,
      "grad_norm": 128.2383270263672,
      "learning_rate": 8.300000000000001e-05,
      "loss": 4.0011,
      "step": 83
    },
    {
      "epoch": 0.5551425030978935,
      "grad_norm": 1733.4599609375,
      "learning_rate": 8.400000000000001e-05,
      "loss": 13.9675,
      "step": 84
    },
    {
      "epoch": 0.5617513424204874,
      "grad_norm": 1581.0042724609375,
      "learning_rate": 8.5e-05,
      "loss": 15.7636,
      "step": 85
    },
    {
      "epoch": 0.5683601817430813,
      "grad_norm": 342.1176452636719,
      "learning_rate": 8.599999999999999e-05,
      "loss": 4.3698,
      "step": 86
    },
    {
      "epoch": 0.5749690210656754,
      "grad_norm": 637.3935546875,
      "learning_rate": 8.7e-05,
      "loss": 9.2013,
      "step": 87
    },
    {
      "epoch": 0.5815778603882693,
      "grad_norm": 1365.854736328125,
      "learning_rate": 8.8e-05,
      "loss": 14.6269,
      "step": 88
    },
    {
      "epoch": 0.5881866997108632,
      "grad_norm": 492.4983215332031,
      "learning_rate": 8.9e-05,
      "loss": 6.0097,
      "step": 89
    },
    {
      "epoch": 0.5947955390334573,
      "grad_norm": 848.91455078125,
      "learning_rate": 8.999999999999999e-05,
      "loss": 6.1654,
      "step": 90
    },
    {
      "epoch": 0.6014043783560512,
      "grad_norm": 1122.0526123046875,
      "learning_rate": 9.1e-05,
      "loss": 7.1464,
      "step": 91
    },
    {
      "epoch": 0.6080132176786451,
      "grad_norm": 815.6082763671875,
      "learning_rate": 9.2e-05,
      "loss": 8.2037,
      "step": 92
    },
    {
      "epoch": 0.6146220570012392,
      "grad_norm": 1026.93798828125,
      "learning_rate": 9.3e-05,
      "loss": 11.1159,
      "step": 93
    },
    {
      "epoch": 0.6212308963238331,
      "grad_norm": 1347.508544921875,
      "learning_rate": 9.400000000000001e-05,
      "loss": 12.938,
      "step": 94
    },
    {
      "epoch": 0.6278397356464271,
      "grad_norm": 209.005859375,
      "learning_rate": 9.5e-05,
      "loss": 1.6599,
      "step": 95
    },
    {
      "epoch": 0.6344485749690211,
      "grad_norm": 765.8615112304688,
      "learning_rate": 9.6e-05,
      "loss": 6.4635,
      "step": 96
    },
    {
      "epoch": 0.641057414291615,
      "grad_norm": 982.8146362304688,
      "learning_rate": 9.7e-05,
      "loss": 12.8774,
      "step": 97
    },
    {
      "epoch": 0.647666253614209,
      "grad_norm": 867.8087768554688,
      "learning_rate": 9.800000000000001e-05,
      "loss": 11.4216,
      "step": 98
    },
    {
      "epoch": 0.654275092936803,
      "grad_norm": 1020.1129760742188,
      "learning_rate": 9.900000000000001e-05,
      "loss": 9.8344,
      "step": 99
    },
    {
      "epoch": 0.6608839322593969,
      "grad_norm": 1585.99853515625,
      "learning_rate": 0.0001,
      "loss": 17.6622,
      "step": 100
    },
    {
      "epoch": 0.6674927715819909,
      "grad_norm": 831.0250854492188,
      "learning_rate": 0.000101,
      "loss": 4.4518,
      "step": 101
    },
    {
      "epoch": 0.6741016109045849,
      "grad_norm": 901.8787841796875,
      "learning_rate": 0.000102,
      "loss": 9.5413,
      "step": 102
    },
    {
      "epoch": 0.6807104502271788,
      "grad_norm": 1568.3388671875,
      "learning_rate": 0.000103,
      "loss": 15.0621,
      "step": 103
    },
    {
      "epoch": 0.6873192895497728,
      "grad_norm": 944.246826171875,
      "learning_rate": 0.000104,
      "loss": 6.118,
      "step": 104
    },
    {
      "epoch": 0.6939281288723668,
      "grad_norm": 1031.270263671875,
      "learning_rate": 0.000105,
      "loss": 10.1896,
      "step": 105
    },
    {
      "epoch": 0.7005369681949608,
      "grad_norm": 2008.3336181640625,
      "learning_rate": 0.000106,
      "loss": 25.5251,
      "step": 106
    },
    {
      "epoch": 0.7071458075175547,
      "grad_norm": 837.1318359375,
      "learning_rate": 0.000107,
      "loss": 5.9926,
      "step": 107
    },
    {
      "epoch": 0.7137546468401487,
      "grad_norm": 1059.7972412109375,
      "learning_rate": 0.000108,
      "loss": 8.5817,
      "step": 108
    },
    {
      "epoch": 0.7203634861627427,
      "grad_norm": 1411.3192138671875,
      "learning_rate": 0.000109,
      "loss": 13.5719,
      "step": 109
    },
    {
      "epoch": 0.7269723254853366,
      "grad_norm": 1043.6083984375,
      "learning_rate": 0.00011,
      "loss": 7.8753,
      "step": 110
    },
    {
      "epoch": 0.7335811648079306,
      "grad_norm": 980.4797973632812,
      "learning_rate": 0.000111,
      "loss": 8.7575,
      "step": 111
    },
    {
      "epoch": 0.7401900041305246,
      "grad_norm": 1383.7122802734375,
      "learning_rate": 0.000112,
      "loss": 11.7871,
      "step": 112
    },
    {
      "epoch": 0.7467988434531185,
      "grad_norm": 847.1012573242188,
      "learning_rate": 0.00011300000000000001,
      "loss": 8.2841,
      "step": 113
    },
    {
      "epoch": 0.7534076827757125,
      "grad_norm": 962.0890502929688,
      "learning_rate": 0.000114,
      "loss": 8.7286,
      "step": 114
    },
    {
      "epoch": 0.7600165220983065,
      "grad_norm": 1504.8966064453125,
      "learning_rate": 0.000115,
      "loss": 15.0023,
      "step": 115
    },
    {
      "epoch": 0.7666253614209004,
      "grad_norm": 644.72216796875,
      "learning_rate": 0.00011600000000000001,
      "loss": 5.4504,
      "step": 116
    },
    {
      "epoch": 0.7732342007434945,
      "grad_norm": 605.6417846679688,
      "learning_rate": 0.00011700000000000001,
      "loss": 9.3957,
      "step": 117
    },
    {
      "epoch": 0.7798430400660884,
      "grad_norm": 1141.673828125,
      "learning_rate": 0.000118,
      "loss": 12.2062,
      "step": 118
    },
    {
      "epoch": 0.7864518793886823,
      "grad_norm": 844.9420776367188,
      "learning_rate": 0.00011899999999999999,
      "loss": 7.3114,
      "step": 119
    },
    {
      "epoch": 0.7930607187112764,
      "grad_norm": 1511.1973876953125,
      "learning_rate": 0.00012,
      "loss": 16.7683,
      "step": 120
    },
    {
      "epoch": 0.7996695580338703,
      "grad_norm": 1748.7518310546875,
      "learning_rate": 0.000121,
      "loss": 19.0475,
      "step": 121
    },
    {
      "epoch": 0.8062783973564642,
      "grad_norm": 547.899169921875,
      "learning_rate": 0.000122,
      "loss": 3.4146,
      "step": 122
    },
    {
      "epoch": 0.8128872366790583,
      "grad_norm": 756.9176025390625,
      "learning_rate": 0.000123,
      "loss": 8.5659,
      "step": 123
    },
    {
      "epoch": 0.8194960760016522,
      "grad_norm": 1053.205810546875,
      "learning_rate": 0.000124,
      "loss": 7.3989,
      "step": 124
    },
    {
      "epoch": 0.8261049153242461,
      "grad_norm": 603.489013671875,
      "learning_rate": 0.000125,
      "loss": 4.03,
      "step": 125
    },
    {
      "epoch": 0.8327137546468402,
      "grad_norm": 1054.1689453125,
      "learning_rate": 0.000126,
      "loss": 8.129,
      "step": 126
    },
    {
      "epoch": 0.8393225939694341,
      "grad_norm": 1248.37158203125,
      "learning_rate": 0.000127,
      "loss": 11.6776,
      "step": 127
    },
    {
      "epoch": 0.8459314332920281,
      "grad_norm": 928.495849609375,
      "learning_rate": 0.000128,
      "loss": 10.456,
      "step": 128
    },
    {
      "epoch": 0.8525402726146221,
      "grad_norm": 894.2531127929688,
      "learning_rate": 0.00012900000000000002,
      "loss": 12.0192,
      "step": 129
    },
    {
      "epoch": 0.859149111937216,
      "grad_norm": 1298.9862060546875,
      "learning_rate": 0.00013000000000000002,
      "loss": 13.123,
      "step": 130
    },
    {
      "epoch": 0.86575795125981,
      "grad_norm": 603.3993530273438,
      "learning_rate": 0.000131,
      "loss": 5.5987,
      "step": 131
    },
    {
      "epoch": 0.872366790582404,
      "grad_norm": 1112.511962890625,
      "learning_rate": 0.000132,
      "loss": 9.5658,
      "step": 132
    },
    {
      "epoch": 0.8789756299049979,
      "grad_norm": 1615.1439208984375,
      "learning_rate": 0.000133,
      "loss": 17.7686,
      "step": 133
    },
    {
      "epoch": 0.8855844692275919,
      "grad_norm": 559.1466064453125,
      "learning_rate": 0.000134,
      "loss": 6.0133,
      "step": 134
    },
    {
      "epoch": 0.8921933085501859,
      "grad_norm": 763.56103515625,
      "learning_rate": 0.000135,
      "loss": 4.4036,
      "step": 135
    },
    {
      "epoch": 0.8988021478727798,
      "grad_norm": 1227.803466796875,
      "learning_rate": 0.00013600000000000003,
      "loss": 11.3417,
      "step": 136
    },
    {
      "epoch": 0.9054109871953738,
      "grad_norm": 820.8599243164062,
      "learning_rate": 0.00013700000000000002,
      "loss": 8.3589,
      "step": 137
    },
    {
      "epoch": 0.9120198265179678,
      "grad_norm": 985.2047729492188,
      "learning_rate": 0.00013800000000000002,
      "loss": 10.7398,
      "step": 138
    },
    {
      "epoch": 0.9186286658405618,
      "grad_norm": 1609.58740234375,
      "learning_rate": 0.00013900000000000002,
      "loss": 18.9694,
      "step": 139
    },
    {
      "epoch": 0.9252375051631557,
      "grad_norm": 975.90380859375,
      "learning_rate": 0.00014000000000000001,
      "loss": 7.3045,
      "step": 140
    },
    {
      "epoch": 0.9318463444857497,
      "grad_norm": 668.0943603515625,
      "learning_rate": 0.00014099999999999998,
      "loss": 5.6805,
      "step": 141
    },
    {
      "epoch": 0.9384551838083437,
      "grad_norm": 1006.8385009765625,
      "learning_rate": 0.00014199999999999998,
      "loss": 8.5861,
      "step": 142
    },
    {
      "epoch": 0.9450640231309376,
      "grad_norm": 624.86474609375,
      "learning_rate": 0.00014299999999999998,
      "loss": 5.6281,
      "step": 143
    },
    {
      "epoch": 0.9516728624535316,
      "grad_norm": 1063.9219970703125,
      "learning_rate": 0.000144,
      "loss": 11.7405,
      "step": 144
    },
    {
      "epoch": 0.9582817017761256,
      "grad_norm": 1568.284912109375,
      "learning_rate": 0.000145,
      "loss": 20.1315,
      "step": 145
    },
    {
      "epoch": 0.9648905410987195,
      "grad_norm": 812.8900756835938,
      "learning_rate": 0.000146,
      "loss": 8.6321,
      "step": 146
    },
    {
      "epoch": 0.9714993804213135,
      "grad_norm": 592.1196899414062,
      "learning_rate": 0.000147,
      "loss": 4.6457,
      "step": 147
    },
    {
      "epoch": 0.9781082197439075,
      "grad_norm": 1049.9862060546875,
      "learning_rate": 0.000148,
      "loss": 9.3392,
      "step": 148
    },
    {
      "epoch": 0.9847170590665014,
      "grad_norm": 553.2450561523438,
      "learning_rate": 0.000149,
      "loss": 3.4659,
      "step": 149
    },
    {
      "epoch": 0.9913258983890955,
      "grad_norm": 612.87060546875,
      "learning_rate": 0.00015,
      "loss": 5.9499,
      "step": 150
    },
    {
      "epoch": 0.9979347377116894,
      "grad_norm": 1345.3084716796875,
      "learning_rate": 0.000151,
      "loss": 14.894,
      "step": 151
    },
    {
      "epoch": 0.9979347377116894,
      "eval_validation_error_bar": 0.0560035456433306,
      "eval_validation_loss": 7.924386024475098,
      "eval_validation_pearsonr": 0.5405982549866422,
      "eval_validation_rmse": 2.815028667449951,
      "eval_validation_runtime": 41.325,
      "eval_validation_samples_per_second": 4.912,
      "eval_validation_spearman": 0.5039233969322132,
      "eval_validation_steps_per_second": 4.912,
      "step": 151
    },
    {
      "epoch": 0.9979347377116894,
      "eval_test_error_bar": 0.0542286286225117,
      "eval_test_loss": 11.670145034790039,
      "eval_test_pearsonr": 0.14110959951956656,
      "eval_test_rmse": 3.4161593914031982,
      "eval_test_runtime": 55.5687,
      "eval_test_samples_per_second": 5.867,
      "eval_test_spearman": 0.18286302812609365,
      "eval_test_steps_per_second": 5.867,
      "step": 151
    },
    {
      "epoch": 1.0045435770342834,
      "grad_norm": 963.797119140625,
      "learning_rate": 0.000152,
      "loss": 10.6323,
      "step": 152
    },
    {
      "epoch": 1.0111524163568772,
      "grad_norm": 648.750732421875,
      "learning_rate": 0.000153,
      "loss": 6.0853,
      "step": 153
    },
    {
      "epoch": 1.0177612556794713,
      "grad_norm": 1236.86865234375,
      "learning_rate": 0.000154,
      "loss": 12.455,
      "step": 154
    },
    {
      "epoch": 1.0243700950020653,
      "grad_norm": 381.9679870605469,
      "learning_rate": 0.000155,
      "loss": 2.6077,
      "step": 155
    },
    {
      "epoch": 1.0309789343246591,
      "grad_norm": 971.7028198242188,
      "learning_rate": 0.000156,
      "loss": 10.046,
      "step": 156
    },
    {
      "epoch": 1.0375877736472532,
      "grad_norm": 1385.1136474609375,
      "learning_rate": 0.000157,
      "loss": 15.3725,
      "step": 157
    },
    {
      "epoch": 1.0441966129698472,
      "grad_norm": 1135.0474853515625,
      "learning_rate": 0.000158,
      "loss": 15.0926,
      "step": 158
    },
    {
      "epoch": 1.050805452292441,
      "grad_norm": 797.5615844726562,
      "learning_rate": 0.00015900000000000002,
      "loss": 10.3711,
      "step": 159
    },
    {
      "epoch": 1.057414291615035,
      "grad_norm": 1230.6275634765625,
      "learning_rate": 0.00016,
      "loss": 13.5457,
      "step": 160
    },
    {
      "epoch": 1.0640231309376291,
      "grad_norm": 425.0432434082031,
      "learning_rate": 0.000161,
      "loss": 5.3091,
      "step": 161
    },
    {
      "epoch": 1.070631970260223,
      "grad_norm": 928.0302734375,
      "learning_rate": 0.000162,
      "loss": 10.8364,
      "step": 162
    },
    {
      "epoch": 1.077240809582817,
      "grad_norm": 1249.861328125,
      "learning_rate": 0.000163,
      "loss": 12.5073,
      "step": 163
    },
    {
      "epoch": 1.083849648905411,
      "grad_norm": 986.6483764648438,
      "learning_rate": 0.000164,
      "loss": 11.9584,
      "step": 164
    },
    {
      "epoch": 1.090458488228005,
      "grad_norm": 713.52685546875,
      "learning_rate": 0.000165,
      "loss": 10.5553,
      "step": 165
    },
    {
      "epoch": 1.0970673275505989,
      "grad_norm": 677.9669189453125,
      "learning_rate": 0.00016600000000000002,
      "loss": 8.4161,
      "step": 166
    },
    {
      "epoch": 1.103676166873193,
      "grad_norm": 749.3506469726562,
      "learning_rate": 0.00016700000000000002,
      "loss": 6.1214,
      "step": 167
    },
    {
      "epoch": 1.110285006195787,
      "grad_norm": 1330.80615234375,
      "learning_rate": 0.00016800000000000002,
      "loss": 16.675,
      "step": 168
    },
    {
      "epoch": 1.1168938455183808,
      "grad_norm": 1339.738525390625,
      "learning_rate": 0.00016900000000000002,
      "loss": 20.3177,
      "step": 169
    },
    {
      "epoch": 1.1235026848409748,
      "grad_norm": 808.0868530273438,
      "learning_rate": 0.00017,
      "loss": 7.4259,
      "step": 170
    },
    {
      "epoch": 1.1301115241635689,
      "grad_norm": 667.2116088867188,
      "learning_rate": 0.000171,
      "loss": 8.1059,
      "step": 171
    },
    {
      "epoch": 1.1367203634861627,
      "grad_norm": 1098.492919921875,
      "learning_rate": 0.00017199999999999998,
      "loss": 13.0784,
      "step": 172
    },
    {
      "epoch": 1.1433292028087567,
      "grad_norm": 147.31597900390625,
      "learning_rate": 0.000173,
      "loss": 4.5293,
      "step": 173
    },
    {
      "epoch": 1.1499380421313508,
      "grad_norm": 1130.0101318359375,
      "learning_rate": 0.000174,
      "loss": 14.1011,
      "step": 174
    },
    {
      "epoch": 1.1565468814539446,
      "grad_norm": 1274.26611328125,
      "learning_rate": 0.000175,
      "loss": 14.5434,
      "step": 175
    },
    {
      "epoch": 1.1631557207765386,
      "grad_norm": 677.8931884765625,
      "learning_rate": 0.000176,
      "loss": 6.7344,
      "step": 176
    },
    {
      "epoch": 1.1697645600991327,
      "grad_norm": 721.5599975585938,
      "learning_rate": 0.000177,
      "loss": 9.1997,
      "step": 177
    },
    {
      "epoch": 1.1763733994217265,
      "grad_norm": 1160.2122802734375,
      "learning_rate": 0.000178,
      "loss": 13.136,
      "step": 178
    },
    {
      "epoch": 1.1829822387443205,
      "grad_norm": 242.5540008544922,
      "learning_rate": 0.000179,
      "loss": 4.2103,
      "step": 179
    },
    {
      "epoch": 1.1895910780669146,
      "grad_norm": 1171.4427490234375,
      "learning_rate": 0.00017999999999999998,
      "loss": 13.9082,
      "step": 180
    },
    {
      "epoch": 1.1961999173895084,
      "grad_norm": 1218.2880859375,
      "learning_rate": 0.000181,
      "loss": 17.0675,
      "step": 181
    },
    {
      "epoch": 1.2028087567121024,
      "grad_norm": 653.9171142578125,
      "learning_rate": 0.000182,
      "loss": 9.2288,
      "step": 182
    },
    {
      "epoch": 1.2094175960346965,
      "grad_norm": 449.1050109863281,
      "learning_rate": 0.000183,
      "loss": 4.5594,
      "step": 183
    },
    {
      "epoch": 1.2160264353572905,
      "grad_norm": 947.0314331054688,
      "learning_rate": 0.000184,
      "loss": 12.2281,
      "step": 184
    },
    {
      "epoch": 1.2226352746798843,
      "grad_norm": 578.54052734375,
      "learning_rate": 0.000185,
      "loss": 5.0297,
      "step": 185
    },
    {
      "epoch": 1.2292441140024783,
      "grad_norm": 1284.887939453125,
      "learning_rate": 0.000186,
      "loss": 16.7227,
      "step": 186
    },
    {
      "epoch": 1.2358529533250722,
      "grad_norm": 1436.6474609375,
      "learning_rate": 0.000187,
      "loss": 23.5713,
      "step": 187
    },
    {
      "epoch": 1.2424617926476662,
      "grad_norm": 860.6934204101562,
      "learning_rate": 0.00018800000000000002,
      "loss": 8.8071,
      "step": 188
    },
    {
      "epoch": 1.2490706319702602,
      "grad_norm": 564.6292114257812,
      "learning_rate": 0.000189,
      "loss": 7.3531,
      "step": 189
    },
    {
      "epoch": 1.2556794712928543,
      "grad_norm": 955.360595703125,
      "learning_rate": 0.00019,
      "loss": 12.2578,
      "step": 190
    },
    {
      "epoch": 1.262288310615448,
      "grad_norm": 417.0008850097656,
      "learning_rate": 0.000191,
      "loss": 2.7552,
      "step": 191
    },
    {
      "epoch": 1.2688971499380421,
      "grad_norm": 867.3544921875,
      "learning_rate": 0.000192,
      "loss": 11.2003,
      "step": 192
    },
    {
      "epoch": 1.275505989260636,
      "grad_norm": 1443.2398681640625,
      "learning_rate": 0.000193,
      "loss": 21.5752,
      "step": 193
    },
    {
      "epoch": 1.28211482858323,
      "grad_norm": 893.0665283203125,
      "learning_rate": 0.000194,
      "loss": 10.6732,
      "step": 194
    },
    {
      "epoch": 1.288723667905824,
      "grad_norm": 729.5048828125,
      "learning_rate": 0.00019500000000000002,
      "loss": 6.6426,
      "step": 195
    },
    {
      "epoch": 1.295332507228418,
      "grad_norm": 727.0667114257812,
      "learning_rate": 0.00019600000000000002,
      "loss": 7.3907,
      "step": 196
    },
    {
      "epoch": 1.301941346551012,
      "grad_norm": 253.43800354003906,
      "learning_rate": 0.00019700000000000002,
      "loss": 3.075,
      "step": 197
    },
    {
      "epoch": 1.308550185873606,
      "grad_norm": 1210.2381591796875,
      "learning_rate": 0.00019800000000000002,
      "loss": 16.456,
      "step": 198
    },
    {
      "epoch": 1.3151590251962,
      "grad_norm": 1133.47119140625,
      "learning_rate": 0.000199,
      "loss": 14.621,
      "step": 199
    },
    {
      "epoch": 1.3217678645187938,
      "grad_norm": 719.745849609375,
      "learning_rate": 0.0002,
      "loss": 7.9796,
      "step": 200
    },
    {
      "epoch": 1.3283767038413878,
      "grad_norm": 481.60565185546875,
      "learning_rate": 0.000201,
      "loss": 5.5113,
      "step": 201
    },
    {
      "epoch": 1.3349855431639819,
      "grad_norm": 854.8103637695312,
      "learning_rate": 0.000202,
      "loss": 9.2079,
      "step": 202
    },
    {
      "epoch": 1.341594382486576,
      "grad_norm": 157.1779327392578,
      "learning_rate": 0.00020300000000000003,
      "loss": 4.4975,
      "step": 203
    },
    {
      "epoch": 1.3482032218091697,
      "grad_norm": 931.8746948242188,
      "learning_rate": 0.000204,
      "loss": 10.8201,
      "step": 204
    },
    {
      "epoch": 1.3548120611317638,
      "grad_norm": 841.6820678710938,
      "learning_rate": 0.000205,
      "loss": 11.8081,
      "step": 205
    },
    {
      "epoch": 1.3614209004543576,
      "grad_norm": 643.7811889648438,
      "learning_rate": 0.000206,
      "loss": 8.4301,
      "step": 206
    },
    {
      "epoch": 1.3680297397769516,
      "grad_norm": 854.039306640625,
      "learning_rate": 0.000207,
      "loss": 14.6609,
      "step": 207
    },
    {
      "epoch": 1.3746385790995457,
      "grad_norm": 1258.1220703125,
      "learning_rate": 0.000208,
      "loss": 14.7224,
      "step": 208
    },
    {
      "epoch": 1.3812474184221397,
      "grad_norm": 675.0956420898438,
      "learning_rate": 0.00020899999999999998,
      "loss": 8.6121,
      "step": 209
    },
    {
      "epoch": 1.3878562577447335,
      "grad_norm": 746.4140625,
      "learning_rate": 0.00021,
      "loss": 6.2169,
      "step": 210
    },
    {
      "epoch": 1.3944650970673276,
      "grad_norm": 1392.762939453125,
      "learning_rate": 0.000211,
      "loss": 23.4482,
      "step": 211
    },
    {
      "epoch": 1.4010739363899214,
      "grad_norm": 485.5047912597656,
      "learning_rate": 0.000212,
      "loss": 7.6276,
      "step": 212
    },
    {
      "epoch": 1.4076827757125154,
      "grad_norm": 640.2217407226562,
      "learning_rate": 0.000213,
      "loss": 5.7788,
      "step": 213
    },
    {
      "epoch": 1.4142916150351095,
      "grad_norm": 1074.5186767578125,
      "learning_rate": 0.000214,
      "loss": 13.6503,
      "step": 214
    },
    {
      "epoch": 1.4209004543577035,
      "grad_norm": 614.2854614257812,
      "learning_rate": 0.000215,
      "loss": 8.9846,
      "step": 215
    },
    {
      "epoch": 1.4275092936802973,
      "grad_norm": 710.492919921875,
      "learning_rate": 0.000216,
      "loss": 9.3389,
      "step": 216
    },
    {
      "epoch": 1.4341181330028914,
      "grad_norm": 990.2216796875,
      "learning_rate": 0.00021700000000000002,
      "loss": 11.2275,
      "step": 217
    },
    {
      "epoch": 1.4407269723254854,
      "grad_norm": 439.24566650390625,
      "learning_rate": 0.000218,
      "loss": 5.4812,
      "step": 218
    },
    {
      "epoch": 1.4473358116480792,
      "grad_norm": 790.13623046875,
      "learning_rate": 0.000219,
      "loss": 9.4365,
      "step": 219
    },
    {
      "epoch": 1.4539446509706733,
      "grad_norm": 1046.4764404296875,
      "learning_rate": 0.00022,
      "loss": 13.0471,
      "step": 220
    },
    {
      "epoch": 1.4605534902932673,
      "grad_norm": 495.3584289550781,
      "learning_rate": 0.000221,
      "loss": 3.3075,
      "step": 221
    },
    {
      "epoch": 1.4671623296158613,
      "grad_norm": 806.6990356445312,
      "learning_rate": 0.000222,
      "loss": 10.0076,
      "step": 222
    },
    {
      "epoch": 1.4737711689384552,
      "grad_norm": 1171.147216796875,
      "learning_rate": 0.000223,
      "loss": 19.2858,
      "step": 223
    },
    {
      "epoch": 1.4803800082610492,
      "grad_norm": 597.7379760742188,
      "learning_rate": 0.000224,
      "loss": 10.0704,
      "step": 224
    },
    {
      "epoch": 1.486988847583643,
      "grad_norm": 755.4679565429688,
      "learning_rate": 0.00022500000000000002,
      "loss": 8.747,
      "step": 225
    },
    {
      "epoch": 1.493597686906237,
      "grad_norm": 1378.2938232421875,
      "learning_rate": 0.00022600000000000002,
      "loss": 26.1015,
      "step": 226
    },
    {
      "epoch": 1.500206526228831,
      "grad_norm": 721.2839965820312,
      "learning_rate": 0.00022700000000000002,
      "loss": 8.876,
      "step": 227
    },
    {
      "epoch": 1.5068153655514251,
      "grad_norm": 673.2756958007812,
      "learning_rate": 0.000228,
      "loss": 6.9314,
      "step": 228
    },
    {
      "epoch": 1.513424204874019,
      "grad_norm": 1052.6904296875,
      "learning_rate": 0.000229,
      "loss": 14.6076,
      "step": 229
    },
    {
      "epoch": 1.520033044196613,
      "grad_norm": 737.4981079101562,
      "learning_rate": 0.00023,
      "loss": 8.3917,
      "step": 230
    },
    {
      "epoch": 1.5266418835192068,
      "grad_norm": 698.888427734375,
      "learning_rate": 0.000231,
      "loss": 7.0378,
      "step": 231
    },
    {
      "epoch": 1.5332507228418009,
      "grad_norm": 1131.831298828125,
      "learning_rate": 0.00023200000000000003,
      "loss": 17.3849,
      "step": 232
    },
    {
      "epoch": 1.539859562164395,
      "grad_norm": 684.8172607421875,
      "learning_rate": 0.00023300000000000003,
      "loss": 6.9503,
      "step": 233
    },
    {
      "epoch": 1.546468401486989,
      "grad_norm": 826.049560546875,
      "learning_rate": 0.00023400000000000002,
      "loss": 10.0034,
      "step": 234
    },
    {
      "epoch": 1.553077240809583,
      "grad_norm": 1205.492431640625,
      "learning_rate": 0.000235,
      "loss": 16.7807,
      "step": 235
    },
    {
      "epoch": 1.5596860801321768,
      "grad_norm": 692.5433959960938,
      "learning_rate": 0.000236,
      "loss": 6.512,
      "step": 236
    },
    {
      "epoch": 1.5662949194547706,
      "grad_norm": 671.427001953125,
      "learning_rate": 0.000237,
      "loss": 5.9573,
      "step": 237
    },
    {
      "epoch": 1.5729037587773647,
      "grad_norm": 992.8313598632812,
      "learning_rate": 0.00023799999999999998,
      "loss": 13.7445,
      "step": 238
    },
    {
      "epoch": 1.5795125980999587,
      "grad_norm": 612.2266235351562,
      "learning_rate": 0.00023899999999999998,
      "loss": 7.9903,
      "step": 239
    },
    {
      "epoch": 1.5861214374225527,
      "grad_norm": 934.8873291015625,
      "learning_rate": 0.00024,
      "loss": 15.3682,
      "step": 240
    },
    {
      "epoch": 1.5927302767451468,
      "grad_norm": 1065.7037353515625,
      "learning_rate": 0.000241,
      "loss": 16.4342,
      "step": 241
    },
    {
      "epoch": 1.5993391160677406,
      "grad_norm": 390.7046203613281,
      "learning_rate": 0.000242,
      "loss": 4.673,
      "step": 242
    },
    {
      "epoch": 1.6059479553903344,
      "grad_norm": 796.9322509765625,
      "learning_rate": 0.000243,
      "loss": 11.7285,
      "step": 243
    },
    {
      "epoch": 1.6125567947129285,
      "grad_norm": 1212.511962890625,
      "learning_rate": 0.000244,
      "loss": 18.6088,
      "step": 244
    },
    {
      "epoch": 1.6191656340355225,
      "grad_norm": 547.6798095703125,
      "learning_rate": 0.000245,
      "loss": 4.9405,
      "step": 245
    },
    {
      "epoch": 1.6257744733581165,
      "grad_norm": 717.2339477539062,
      "learning_rate": 0.000246,
      "loss": 7.2901,
      "step": 246
    },
    {
      "epoch": 1.6323833126807106,
      "grad_norm": 1256.2823486328125,
      "learning_rate": 0.000247,
      "loss": 21.7367,
      "step": 247
    },
    {
      "epoch": 1.6389921520033044,
      "grad_norm": 393.66607666015625,
      "learning_rate": 0.000248,
      "loss": 4.6401,
      "step": 248
    },
    {
      "epoch": 1.6456009913258984,
      "grad_norm": 967.0535888671875,
      "learning_rate": 0.000249,
      "loss": 14.8127,
      "step": 249
    },
    {
      "epoch": 1.6522098306484923,
      "grad_norm": 1103.3426513671875,
      "learning_rate": 0.00025,
      "loss": 24.0888,
      "step": 250
    },
    {
      "epoch": 1.6588186699710863,
      "grad_norm": 138.26939392089844,
      "learning_rate": 0.00025100000000000003,
      "loss": 3.8434,
      "step": 251
    },
    {
      "epoch": 1.6654275092936803,
      "grad_norm": 731.6987915039062,
      "learning_rate": 0.000252,
      "loss": 9.1378,
      "step": 252
    },
    {
      "epoch": 1.6720363486162744,
      "grad_norm": 945.1315307617188,
      "learning_rate": 0.000253,
      "loss": 12.4844,
      "step": 253
    },
    {
      "epoch": 1.6786451879388682,
      "grad_norm": 510.32830810546875,
      "learning_rate": 0.000254,
      "loss": 4.7039,
      "step": 254
    },
    {
      "epoch": 1.6852540272614622,
      "grad_norm": 1116.8702392578125,
      "learning_rate": 0.000255,
      "loss": 16.1161,
      "step": 255
    },
    {
      "epoch": 1.691862866584056,
      "grad_norm": 1679.1876220703125,
      "learning_rate": 0.000256,
      "loss": 34.2316,
      "step": 256
    },
    {
      "epoch": 1.69847170590665,
      "grad_norm": 942.8587646484375,
      "learning_rate": 0.000257,
      "loss": 11.69,
      "step": 257
    },
    {
      "epoch": 1.7050805452292441,
      "grad_norm": 845.8092041015625,
      "learning_rate": 0.00025800000000000004,
      "loss": 10.9948,
      "step": 258
    },
    {
      "epoch": 1.7116893845518382,
      "grad_norm": 981.594970703125,
      "learning_rate": 0.000259,
      "loss": 13.4447,
      "step": 259
    },
    {
      "epoch": 1.7182982238744322,
      "grad_norm": 929.1450805664062,
      "learning_rate": 0.00026000000000000003,
      "loss": 16.3893,
      "step": 260
    },
    {
      "epoch": 1.724907063197026,
      "grad_norm": 740.9034423828125,
      "learning_rate": 0.000261,
      "loss": 10.078,
      "step": 261
    },
    {
      "epoch": 1.7315159025196198,
      "grad_norm": 1327.715576171875,
      "learning_rate": 0.000262,
      "loss": 24.828,
      "step": 262
    },
    {
      "epoch": 1.7381247418422139,
      "grad_norm": 444.13922119140625,
      "learning_rate": 0.000263,
      "loss": 4.0287,
      "step": 263
    },
    {
      "epoch": 1.744733581164808,
      "grad_norm": 1083.29931640625,
      "learning_rate": 0.000264,
      "loss": 16.5723,
      "step": 264
    },
    {
      "epoch": 1.751342420487402,
      "grad_norm": 1094.89794921875,
      "learning_rate": 0.00026500000000000004,
      "loss": 17.8517,
      "step": 265
    },
    {
      "epoch": 1.757951259809996,
      "grad_norm": 703.618408203125,
      "learning_rate": 0.000266,
      "loss": 11.8367,
      "step": 266
    },
    {
      "epoch": 1.7645600991325898,
      "grad_norm": 663.1153564453125,
      "learning_rate": 0.00026700000000000004,
      "loss": 7.9896,
      "step": 267
    },
    {
      "epoch": 1.7711689384551839,
      "grad_norm": 1185.2928466796875,
      "learning_rate": 0.000268,
      "loss": 22.0544,
      "step": 268
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 665.4340209960938,
      "learning_rate": 0.00026900000000000003,
      "loss": 8.0104,
      "step": 269
    },
    {
      "epoch": 1.7843866171003717,
      "grad_norm": 895.520751953125,
      "learning_rate": 0.00027,
      "loss": 12.1374,
      "step": 270
    },
    {
      "epoch": 1.7909954564229658,
      "grad_norm": 1043.3299560546875,
      "learning_rate": 0.00027100000000000003,
      "loss": 14.6878,
      "step": 271
    },
    {
      "epoch": 1.7976042957455598,
      "grad_norm": 585.1140747070312,
      "learning_rate": 0.00027200000000000005,
      "loss": 8.6087,
      "step": 272
    },
    {
      "epoch": 1.8042131350681536,
      "grad_norm": 720.6844482421875,
      "learning_rate": 0.000273,
      "loss": 7.9204,
      "step": 273
    },
    {
      "epoch": 1.8108219743907477,
      "grad_norm": 1070.82861328125,
      "learning_rate": 0.00027400000000000005,
      "loss": 14.6322,
      "step": 274
    },
    {
      "epoch": 1.8174308137133415,
      "grad_norm": 351.02569580078125,
      "learning_rate": 0.000275,
      "loss": 2.6286,
      "step": 275
    },
    {
      "epoch": 1.8240396530359355,
      "grad_norm": 698.0571899414062,
      "learning_rate": 0.00027600000000000004,
      "loss": 6.7814,
      "step": 276
    },
    {
      "epoch": 1.8306484923585296,
      "grad_norm": 852.9895629882812,
      "learning_rate": 0.000277,
      "loss": 13.4266,
      "step": 277
    },
    {
      "epoch": 1.8372573316811236,
      "grad_norm": 398.98760986328125,
      "learning_rate": 0.00027800000000000004,
      "loss": 6.5738,
      "step": 278
    },
    {
      "epoch": 1.8438661710037176,
      "grad_norm": 632.3529663085938,
      "learning_rate": 0.000279,
      "loss": 6.7845,
      "step": 279
    },
    {
      "epoch": 1.8504750103263115,
      "grad_norm": 826.2899780273438,
      "learning_rate": 0.00028000000000000003,
      "loss": 11.739,
      "step": 280
    },
    {
      "epoch": 1.8570838496489053,
      "grad_norm": 171.80880737304688,
      "learning_rate": 0.00028100000000000005,
      "loss": 2.7454,
      "step": 281
    },
    {
      "epoch": 1.8636926889714993,
      "grad_norm": 756.1471557617188,
      "learning_rate": 0.00028199999999999997,
      "loss": 12.6868,
      "step": 282
    },
    {
      "epoch": 1.8703015282940934,
      "grad_norm": 1102.7099609375,
      "learning_rate": 0.000283,
      "loss": 22.8724,
      "step": 283
    },
    {
      "epoch": 1.8769103676166874,
      "grad_norm": 478.8797912597656,
      "learning_rate": 0.00028399999999999996,
      "loss": 6.9247,
      "step": 284
    },
    {
      "epoch": 1.8835192069392814,
      "grad_norm": 1204.35888671875,
      "learning_rate": 0.000285,
      "loss": 21.1987,
      "step": 285
    },
    {
      "epoch": 1.8901280462618752,
      "grad_norm": 1662.10546875,
      "learning_rate": 0.00028599999999999996,
      "loss": 34.9805,
      "step": 286
    },
    {
      "epoch": 1.896736885584469,
      "grad_norm": 862.1602783203125,
      "learning_rate": 0.000287,
      "loss": 13.7429,
      "step": 287
    },
    {
      "epoch": 1.903345724907063,
      "grad_norm": 780.3167114257812,
      "learning_rate": 0.000288,
      "loss": 12.3429,
      "step": 288
    },
    {
      "epoch": 1.9099545642296571,
      "grad_norm": 1034.171630859375,
      "learning_rate": 0.000289,
      "loss": 15.1072,
      "step": 289
    },
    {
      "epoch": 1.9165634035522512,
      "grad_norm": 375.9296569824219,
      "learning_rate": 0.00029,
      "loss": 5.1358,
      "step": 290
    },
    {
      "epoch": 1.9231722428748452,
      "grad_norm": 1069.8760986328125,
      "learning_rate": 0.00029099999999999997,
      "loss": 18.8943,
      "step": 291
    },
    {
      "epoch": 1.929781082197439,
      "grad_norm": 1417.5880126953125,
      "learning_rate": 0.000292,
      "loss": 34.8776,
      "step": 292
    },
    {
      "epoch": 1.936389921520033,
      "grad_norm": 367.42022705078125,
      "learning_rate": 0.00029299999999999997,
      "loss": 4.0593,
      "step": 293
    },
    {
      "epoch": 1.942998760842627,
      "grad_norm": 796.326171875,
      "learning_rate": 0.000294,
      "loss": 12.058,
      "step": 294
    },
    {
      "epoch": 1.949607600165221,
      "grad_norm": 1599.3031005859375,
      "learning_rate": 0.000295,
      "loss": 30.6213,
      "step": 295
    },
    {
      "epoch": 1.956216439487815,
      "grad_norm": 810.6488647460938,
      "learning_rate": 0.000296,
      "loss": 9.5187,
      "step": 296
    },
    {
      "epoch": 1.962825278810409,
      "grad_norm": 1647.130859375,
      "learning_rate": 0.000297,
      "loss": 29.2263,
      "step": 297
    },
    {
      "epoch": 1.9694341181330028,
      "grad_norm": 2226.4794921875,
      "learning_rate": 0.000298,
      "loss": 46.9386,
      "step": 298
    },
    {
      "epoch": 1.9760429574555969,
      "grad_norm": 1393.115478515625,
      "learning_rate": 0.000299,
      "loss": 21.0919,
      "step": 299
    },
    {
      "epoch": 1.9826517967781907,
      "grad_norm": 203.34194946289062,
      "learning_rate": 0.0003,
      "loss": 3.7116,
      "step": 300
    },
    {
      "epoch": 1.9892606361007847,
      "grad_norm": 372.0379943847656,
      "learning_rate": 0.000301,
      "loss": 7.4398,
      "step": 301
    },
    {
      "epoch": 1.9958694754233788,
      "grad_norm": 154.6348876953125,
      "learning_rate": 0.000302,
      "loss": 2.7934,
      "step": 302
    },
    {
      "epoch": 1.9958694754233788,
      "eval_validation_error_bar": 0.05157299124265317,
      "eval_validation_loss": 31.50142478942871,
      "eval_validation_pearsonr": 0.5614231065107205,
      "eval_validation_rmse": 5.612613201141357,
      "eval_validation_runtime": 41.1277,
      "eval_validation_samples_per_second": 4.936,
      "eval_validation_spearman": 0.5685891550733486,
      "eval_validation_steps_per_second": 4.936,
      "step": 302
    },
    {
      "epoch": 1.9958694754233788,
      "eval_test_error_bar": 0.051357838381944985,
      "eval_test_loss": 36.95982360839844,
      "eval_test_pearsonr": 0.36276067708712867,
      "eval_test_rmse": 6.079459190368652,
      "eval_test_runtime": 55.9523,
      "eval_test_samples_per_second": 5.826,
      "eval_test_spearman": 0.3146762970532789,
      "eval_test_steps_per_second": 5.826,
      "step": 302
    },
    {
      "epoch": 2.002478314745973,
      "grad_norm": 1096.0413818359375,
      "learning_rate": 0.000303,
      "loss": 19.7476,
      "step": 303
    },
    {
      "epoch": 2.009087154068567,
      "grad_norm": 1420.9212646484375,
      "learning_rate": 0.000304,
      "loss": 33.6281,
      "step": 304
    },
    {
      "epoch": 2.0156959933911605,
      "grad_norm": 564.4071044921875,
      "learning_rate": 0.000305,
      "loss": 8.8324,
      "step": 305
    },
    {
      "epoch": 2.0223048327137545,
      "grad_norm": 1052.653076171875,
      "learning_rate": 0.000306,
      "loss": 19.7865,
      "step": 306
    },
    {
      "epoch": 2.0289136720363485,
      "grad_norm": 1207.98046875,
      "learning_rate": 0.000307,
      "loss": 26.0708,
      "step": 307
    },
    {
      "epoch": 2.0355225113589426,
      "grad_norm": 178.83131408691406,
      "learning_rate": 0.000308,
      "loss": 3.8677,
      "step": 308
    },
    {
      "epoch": 2.0421313506815366,
      "grad_norm": 969.814697265625,
      "learning_rate": 0.00030900000000000003,
      "loss": 14.9515,
      "step": 309
    },
    {
      "epoch": 2.0487401900041307,
      "grad_norm": 1404.7259521484375,
      "learning_rate": 0.00031,
      "loss": 29.1787,
      "step": 310
    },
    {
      "epoch": 2.0553490293267247,
      "grad_norm": 434.63214111328125,
      "learning_rate": 0.000311,
      "loss": 5.9005,
      "step": 311
    },
    {
      "epoch": 2.0619578686493183,
      "grad_norm": 1244.2259521484375,
      "learning_rate": 0.000312,
      "loss": 25.0944,
      "step": 312
    },
    {
      "epoch": 2.0685667079719123,
      "grad_norm": 1542.3660888671875,
      "learning_rate": 0.000313,
      "loss": 38.2165,
      "step": 313
    },
    {
      "epoch": 2.0751755472945064,
      "grad_norm": 585.7236938476562,
      "learning_rate": 0.000314,
      "loss": 13.2041,
      "step": 314
    },
    {
      "epoch": 2.0817843866171004,
      "grad_norm": 1064.282470703125,
      "learning_rate": 0.000315,
      "loss": 19.0969,
      "step": 315
    },
    {
      "epoch": 2.0883932259396945,
      "grad_norm": 990.7438354492188,
      "learning_rate": 0.000316,
      "loss": 21.8855,
      "step": 316
    },
    {
      "epoch": 2.0950020652622885,
      "grad_norm": 273.964599609375,
      "learning_rate": 0.000317,
      "loss": 8.6136,
      "step": 317
    },
    {
      "epoch": 2.101610904584882,
      "grad_norm": 1151.9031982421875,
      "learning_rate": 0.00031800000000000003,
      "loss": 18.5589,
      "step": 318
    },
    {
      "epoch": 2.108219743907476,
      "grad_norm": 1927.3209228515625,
      "learning_rate": 0.000319,
      "loss": 38.2183,
      "step": 319
    },
    {
      "epoch": 2.11482858323007,
      "grad_norm": 1063.4464111328125,
      "learning_rate": 0.00032,
      "loss": 13.4174,
      "step": 320
    },
    {
      "epoch": 2.121437422552664,
      "grad_norm": 1254.3363037109375,
      "learning_rate": 0.000321,
      "loss": 18.3058,
      "step": 321
    },
    {
      "epoch": 2.1280462618752582,
      "grad_norm": 2096.85498046875,
      "learning_rate": 0.000322,
      "loss": 38.9429,
      "step": 322
    },
    {
      "epoch": 2.1346551011978523,
      "grad_norm": 1054.9422607421875,
      "learning_rate": 0.000323,
      "loss": 13.3185,
      "step": 323
    },
    {
      "epoch": 2.141263940520446,
      "grad_norm": 717.1460571289062,
      "learning_rate": 0.000324,
      "loss": 8.5436,
      "step": 324
    },
    {
      "epoch": 2.14787277984304,
      "grad_norm": 854.2952270507812,
      "learning_rate": 0.00032500000000000004,
      "loss": 12.7988,
      "step": 325
    },
    {
      "epoch": 2.154481619165634,
      "grad_norm": 452.0403137207031,
      "learning_rate": 0.000326,
      "loss": 4.5997,
      "step": 326
    },
    {
      "epoch": 2.161090458488228,
      "grad_norm": 720.9676513671875,
      "learning_rate": 0.00032700000000000003,
      "loss": 18.4966,
      "step": 327
    },
    {
      "epoch": 2.167699297810822,
      "grad_norm": 1136.4683837890625,
      "learning_rate": 0.000328,
      "loss": 18.247,
      "step": 328
    },
    {
      "epoch": 2.174308137133416,
      "grad_norm": 200.05091857910156,
      "learning_rate": 0.00032900000000000003,
      "loss": 4.3161,
      "step": 329
    },
    {
      "epoch": 2.18091697645601,
      "grad_norm": 633.5211181640625,
      "learning_rate": 0.00033,
      "loss": 9.1733,
      "step": 330
    },
    {
      "epoch": 2.1875258157786037,
      "grad_norm": 1300.3919677734375,
      "learning_rate": 0.000331,
      "loss": 30.2851,
      "step": 331
    },
    {
      "epoch": 2.1941346551011978,
      "grad_norm": 432.5079345703125,
      "learning_rate": 0.00033200000000000005,
      "loss": 11.6235,
      "step": 332
    },
    {
      "epoch": 2.200743494423792,
      "grad_norm": 693.8925170898438,
      "learning_rate": 0.000333,
      "loss": 16.0757,
      "step": 333
    },
    {
      "epoch": 2.207352333746386,
      "grad_norm": 984.6693115234375,
      "learning_rate": 0.00033400000000000004,
      "loss": 17.8422,
      "step": 334
    },
    {
      "epoch": 2.21396117306898,
      "grad_norm": 256.6982421875,
      "learning_rate": 0.000335,
      "loss": 4.6446,
      "step": 335
    },
    {
      "epoch": 2.220570012391574,
      "grad_norm": 759.3616333007812,
      "learning_rate": 0.00033600000000000004,
      "loss": 12.2344,
      "step": 336
    },
    {
      "epoch": 2.2271788517141675,
      "grad_norm": 1177.98828125,
      "learning_rate": 0.000337,
      "loss": 23.8687,
      "step": 337
    },
    {
      "epoch": 2.2337876910367616,
      "grad_norm": 355.06402587890625,
      "learning_rate": 0.00033800000000000003,
      "loss": 4.6608,
      "step": 338
    },
    {
      "epoch": 2.2403965303593556,
      "grad_norm": 1246.0672607421875,
      "learning_rate": 0.00033900000000000005,
      "loss": 22.8276,
      "step": 339
    },
    {
      "epoch": 2.2470053696819496,
      "grad_norm": 1949.6741943359375,
      "learning_rate": 0.00034,
      "loss": 50.5814,
      "step": 340
    },
    {
      "epoch": 2.2536142090045437,
      "grad_norm": 1016.7911376953125,
      "learning_rate": 0.00034100000000000005,
      "loss": 17.4107,
      "step": 341
    },
    {
      "epoch": 2.2602230483271377,
      "grad_norm": 590.2028198242188,
      "learning_rate": 0.000342,
      "loss": 8.2661,
      "step": 342
    },
    {
      "epoch": 2.2668318876497313,
      "grad_norm": 809.4909057617188,
      "learning_rate": 0.00034300000000000004,
      "loss": 11.3068,
      "step": 343
    },
    {
      "epoch": 2.2734407269723254,
      "grad_norm": 504.2781066894531,
      "learning_rate": 0.00034399999999999996,
      "loss": 6.2004,
      "step": 344
    },
    {
      "epoch": 2.2800495662949194,
      "grad_norm": 706.9434814453125,
      "learning_rate": 0.000345,
      "loss": 8.8419,
      "step": 345
    },
    {
      "epoch": 2.2866584056175134,
      "grad_norm": 1340.208740234375,
      "learning_rate": 0.000346,
      "loss": 27.5648,
      "step": 346
    },
    {
      "epoch": 2.2932672449401075,
      "grad_norm": 649.4628295898438,
      "learning_rate": 0.000347,
      "loss": 9.9264,
      "step": 347
    },
    {
      "epoch": 2.2998760842627015,
      "grad_norm": 585.8361206054688,
      "learning_rate": 0.000348,
      "loss": 8.0734,
      "step": 348
    },
    {
      "epoch": 2.3064849235852956,
      "grad_norm": 732.431396484375,
      "learning_rate": 0.00034899999999999997,
      "loss": 10.1399,
      "step": 349
    },
    {
      "epoch": 2.313093762907889,
      "grad_norm": 66.71202087402344,
      "learning_rate": 0.00035,
      "loss": 2.6209,
      "step": 350
    },
    {
      "epoch": 2.319702602230483,
      "grad_norm": 825.1351928710938,
      "learning_rate": 0.00035099999999999997,
      "loss": 16.5253,
      "step": 351
    },
    {
      "epoch": 2.3263114415530772,
      "grad_norm": 901.314697265625,
      "learning_rate": 0.000352,
      "loss": 15.3979,
      "step": 352
    },
    {
      "epoch": 2.3329202808756713,
      "grad_norm": 126.61077117919922,
      "learning_rate": 0.00035299999999999996,
      "loss": 16.0594,
      "step": 353
    },
    {
      "epoch": 2.3395291201982653,
      "grad_norm": 394.7855224609375,
      "learning_rate": 0.000354,
      "loss": 7.1636,
      "step": 354
    },
    {
      "epoch": 2.346137959520859,
      "grad_norm": 119.19059753417969,
      "learning_rate": 0.000355,
      "loss": 9.4354,
      "step": 355
    },
    {
      "epoch": 2.352746798843453,
      "grad_norm": 2420.35107421875,
      "learning_rate": 0.000356,
      "loss": 59.1191,
      "step": 356
    },
    {
      "epoch": 2.359355638166047,
      "grad_norm": 2228.505126953125,
      "learning_rate": 0.000357,
      "loss": 44.9069,
      "step": 357
    },
    {
      "epoch": 2.365964477488641,
      "grad_norm": 1025.6507568359375,
      "learning_rate": 0.000358,
      "loss": 16.0298,
      "step": 358
    },
    {
      "epoch": 2.372573316811235,
      "grad_norm": 961.9234008789062,
      "learning_rate": 0.000359,
      "loss": 6.5143,
      "step": 359
    },
    {
      "epoch": 2.379182156133829,
      "grad_norm": 2684.799560546875,
      "learning_rate": 0.00035999999999999997,
      "loss": 42.7165,
      "step": 360
    },
    {
      "epoch": 2.385790995456423,
      "grad_norm": 988.310302734375,
      "learning_rate": 0.000361,
      "loss": 9.2787,
      "step": 361
    },
    {
      "epoch": 2.3923998347790167,
      "grad_norm": 3851.445068359375,
      "learning_rate": 0.000362,
      "loss": 55.555,
      "step": 362
    },
    {
      "epoch": 2.399008674101611,
      "grad_norm": 3871.92724609375,
      "learning_rate": 0.000363,
      "loss": 56.0159,
      "step": 363
    },
    {
      "epoch": 2.405617513424205,
      "grad_norm": 2006.845947265625,
      "learning_rate": 0.000364,
      "loss": 18.025,
      "step": 364
    },
    {
      "epoch": 2.412226352746799,
      "grad_norm": 921.0702514648438,
      "learning_rate": 0.000365,
      "loss": 7.1574,
      "step": 365
    },
    {
      "epoch": 2.418835192069393,
      "grad_norm": 4363.37451171875,
      "learning_rate": 0.000366,
      "loss": 70.9659,
      "step": 366
    },
    {
      "epoch": 2.425444031391987,
      "grad_norm": 3692.743896484375,
      "learning_rate": 0.000367,
      "loss": 50.5261,
      "step": 367
    },
    {
      "epoch": 2.432052870714581,
      "grad_norm": 1525.420654296875,
      "learning_rate": 0.000368,
      "loss": 13.4467,
      "step": 368
    },
    {
      "epoch": 2.4386617100371746,
      "grad_norm": 1766.5885009765625,
      "learning_rate": 0.000369,
      "loss": 18.2819,
      "step": 369
    },
    {
      "epoch": 2.4452705493597686,
      "grad_norm": 4349.1416015625,
      "learning_rate": 0.00037,
      "loss": 81.7387,
      "step": 370
    },
    {
      "epoch": 2.4518793886823627,
      "grad_norm": 2329.91943359375,
      "learning_rate": 0.000371,
      "loss": 33.7539,
      "step": 371
    },
    {
      "epoch": 2.4584882280049567,
      "grad_norm": 1425.27099609375,
      "learning_rate": 0.000372,
      "loss": 16.2882,
      "step": 372
    },
    {
      "epoch": 2.4650970673275507,
      "grad_norm": 409.7201843261719,
      "learning_rate": 0.000373,
      "loss": 12.1169,
      "step": 373
    },
    {
      "epoch": 2.4717059066501443,
      "grad_norm": 2197.517333984375,
      "learning_rate": 0.000374,
      "loss": 29.1383,
      "step": 374
    },
    {
      "epoch": 2.4783147459727384,
      "grad_norm": 1568.6328125,
      "learning_rate": 0.000375,
      "loss": 39.6273,
      "step": 375
    },
    {
      "epoch": 2.4849235852953324,
      "grad_norm": 2412.487060546875,
      "learning_rate": 0.00037600000000000003,
      "loss": 65.7912,
      "step": 376
    },
    {
      "epoch": 2.4915324246179265,
      "grad_norm": 953.590576171875,
      "learning_rate": 0.000377,
      "loss": 15.2783,
      "step": 377
    },
    {
      "epoch": 2.4981412639405205,
      "grad_norm": 1763.8077392578125,
      "learning_rate": 0.000378,
      "loss": 46.5638,
      "step": 378
    },
    {
      "epoch": 2.5047501032631145,
      "grad_norm": 578.1707153320312,
      "learning_rate": 0.000379,
      "loss": 9.7426,
      "step": 379
    },
    {
      "epoch": 2.5113589425857086,
      "grad_norm": 1307.48388671875,
      "learning_rate": 0.00038,
      "loss": 18.8601,
      "step": 380
    },
    {
      "epoch": 2.517967781908302,
      "grad_norm": 1525.0018310546875,
      "learning_rate": 0.000381,
      "loss": 32.659,
      "step": 381
    },
    {
      "epoch": 2.524576621230896,
      "grad_norm": 1210.298095703125,
      "learning_rate": 0.000382,
      "loss": 29.2292,
      "step": 382
    },
    {
      "epoch": 2.5311854605534903,
      "grad_norm": 145.82733154296875,
      "learning_rate": 0.00038300000000000004,
      "loss": 9.7041,
      "step": 383
    },
    {
      "epoch": 2.5377942998760843,
      "grad_norm": 1002.2028198242188,
      "learning_rate": 0.000384,
      "loss": 28.6065,
      "step": 384
    },
    {
      "epoch": 2.5444031391986783,
      "grad_norm": 134.2875518798828,
      "learning_rate": 0.00038500000000000003,
      "loss": 7.4164,
      "step": 385
    },
    {
      "epoch": 2.551011978521272,
      "grad_norm": 4501.33447265625,
      "learning_rate": 0.000386,
      "loss": 204.0159,
      "step": 386
    },
    {
      "epoch": 2.5576208178438664,
      "grad_norm": 3703.721435546875,
      "learning_rate": 0.00038700000000000003,
      "loss": 191.9219,
      "step": 387
    },
    {
      "epoch": 2.56422965716646,
      "grad_norm": 499.64862060546875,
      "learning_rate": 0.000388,
      "loss": 53.2676,
      "step": 388
    },
    {
      "epoch": 2.570838496489054,
      "grad_norm": 5552.15380859375,
      "learning_rate": 0.000389,
      "loss": 228.29,
      "step": 389
    },
    {
      "epoch": 2.577447335811648,
      "grad_norm": 4557.70703125,
      "learning_rate": 0.00039000000000000005,
      "loss": 177.9068,
      "step": 390
    },
    {
      "epoch": 2.584056175134242,
      "grad_norm": 2380.872314453125,
      "learning_rate": 0.000391,
      "loss": 94.0394,
      "step": 391
    },
    {
      "epoch": 2.590665014456836,
      "grad_norm": 6579.0546875,
      "learning_rate": 0.00039200000000000004,
      "loss": 245.8634,
      "step": 392
    },
    {
      "epoch": 2.5972738537794298,
      "grad_norm": 2533.498291015625,
      "learning_rate": 0.000393,
      "loss": 41.1475,
      "step": 393
    },
    {
      "epoch": 2.603882693102024,
      "grad_norm": 6437.44970703125,
      "learning_rate": 0.00039400000000000004,
      "loss": 228.7904,
      "step": 394
    },
    {
      "epoch": 2.610491532424618,
      "grad_norm": 8230.439453125,
      "learning_rate": 0.000395,
      "loss": 383.3146,
      "step": 395
    },
    {
      "epoch": 2.617100371747212,
      "grad_norm": 2850.015869140625,
      "learning_rate": 0.00039600000000000003,
      "loss": 59.437,
      "step": 396
    },
    {
      "epoch": 2.623709211069806,
      "grad_norm": 5521.533203125,
      "learning_rate": 0.00039700000000000005,
      "loss": 217.6503,
      "step": 397
    },
    {
      "epoch": 2.6303180503924,
      "grad_norm": 8529.8818359375,
      "learning_rate": 0.000398,
      "loss": 460.4331,
      "step": 398
    },
    {
      "epoch": 2.636926889714994,
      "grad_norm": 4772.115234375,
      "learning_rate": 0.00039900000000000005,
      "loss": 185.8667,
      "step": 399
    },
    {
      "epoch": 2.6435357290375876,
      "grad_norm": 1546.1497802734375,
      "learning_rate": 0.0004,
      "loss": 33.3917,
      "step": 400
    },
    {
      "epoch": 2.6501445683601816,
      "grad_norm": 2821.9404296875,
      "learning_rate": 0.00040100000000000004,
      "loss": 107.7342,
      "step": 401
    },
    {
      "epoch": 2.6567534076827757,
      "grad_norm": 228.4521942138672,
      "learning_rate": 0.000402,
      "loss": 15.2812,
      "step": 402
    },
    {
      "epoch": 2.6633622470053697,
      "grad_norm": 763.2550659179688,
      "learning_rate": 0.00040300000000000004,
      "loss": 29.3556,
      "step": 403
    },
    {
      "epoch": 2.6699710863279638,
      "grad_norm": 693.7418212890625,
      "learning_rate": 0.000404,
      "loss": 67.2484,
      "step": 404
    },
    {
      "epoch": 2.6765799256505574,
      "grad_norm": 1591.2335205078125,
      "learning_rate": 0.00040500000000000003,
      "loss": 34.9077,
      "step": 405
    },
    {
      "epoch": 2.683188764973152,
      "grad_norm": 581.4572143554688,
      "learning_rate": 0.00040600000000000006,
      "loss": 20.1777,
      "step": 406
    },
    {
      "epoch": 2.6897976042957454,
      "grad_norm": 2294.788330078125,
      "learning_rate": 0.00040699999999999997,
      "loss": 60.1921,
      "step": 407
    },
    {
      "epoch": 2.6964064436183395,
      "grad_norm": 3104.0888671875,
      "learning_rate": 0.000408,
      "loss": 92.1122,
      "step": 408
    },
    {
      "epoch": 2.7030152829409335,
      "grad_norm": 514.1002197265625,
      "learning_rate": 0.00040899999999999997,
      "loss": 10.8563,
      "step": 409
    },
    {
      "epoch": 2.7096241222635276,
      "grad_norm": 4143.51171875,
      "learning_rate": 0.00041,
      "loss": 158.0518,
      "step": 410
    },
    {
      "epoch": 2.7162329615861216,
      "grad_norm": 4994.81982421875,
      "learning_rate": 0.00041099999999999996,
      "loss": 207.6657,
      "step": 411
    },
    {
      "epoch": 2.722841800908715,
      "grad_norm": 2825.368408203125,
      "learning_rate": 0.000412,
      "loss": 105.6921,
      "step": 412
    },
    {
      "epoch": 2.7294506402313092,
      "grad_norm": 2894.064453125,
      "learning_rate": 0.000413,
      "loss": 118.6493,
      "step": 413
    },
    {
      "epoch": 2.7360594795539033,
      "grad_norm": 3452.4951171875,
      "learning_rate": 0.000414,
      "loss": 163.1761,
      "step": 414
    },
    {
      "epoch": 2.7426683188764973,
      "grad_norm": 915.33935546875,
      "learning_rate": 0.000415,
      "loss": 19.8828,
      "step": 415
    },
    {
      "epoch": 2.7492771581990914,
      "grad_norm": 4003.18310546875,
      "learning_rate": 0.000416,
      "loss": 218.234,
      "step": 416
    },
    {
      "epoch": 2.7558859975216854,
      "grad_norm": 3049.622314453125,
      "learning_rate": 0.000417,
      "loss": 137.0353,
      "step": 417
    },
    {
      "epoch": 2.7624948368442794,
      "grad_norm": 2225.28955078125,
      "learning_rate": 0.00041799999999999997,
      "loss": 82.2532,
      "step": 418
    },
    {
      "epoch": 2.769103676166873,
      "grad_norm": 1140.91162109375,
      "learning_rate": 0.000419,
      "loss": 25.3289,
      "step": 419
    },
    {
      "epoch": 2.775712515489467,
      "grad_norm": 2609.907470703125,
      "learning_rate": 0.00042,
      "loss": 97.2819,
      "step": 420
    },
    {
      "epoch": 2.782321354812061,
      "grad_norm": 1126.32666015625,
      "learning_rate": 0.000421,
      "loss": 22.4762,
      "step": 421
    },
    {
      "epoch": 2.788930194134655,
      "grad_norm": 814.1690673828125,
      "learning_rate": 0.000422,
      "loss": 21.0251,
      "step": 422
    },
    {
      "epoch": 2.795539033457249,
      "grad_norm": 1693.8983154296875,
      "learning_rate": 0.000423,
      "loss": 50.3174,
      "step": 423
    },
    {
      "epoch": 2.802147872779843,
      "grad_norm": 713.48779296875,
      "learning_rate": 0.000424,
      "loss": 13.8754,
      "step": 424
    },
    {
      "epoch": 2.8087567121024373,
      "grad_norm": 1134.6624755859375,
      "learning_rate": 0.000425,
      "loss": 30.4171,
      "step": 425
    },
    {
      "epoch": 2.815365551425031,
      "grad_norm": 1131.7283935546875,
      "learning_rate": 0.000426,
      "loss": 30.6067,
      "step": 426
    },
    {
      "epoch": 2.821974390747625,
      "grad_norm": 301.8132019042969,
      "learning_rate": 0.000427,
      "loss": 8.4123,
      "step": 427
    },
    {
      "epoch": 2.828583230070219,
      "grad_norm": 1753.634033203125,
      "learning_rate": 0.000428,
      "loss": 61.0284,
      "step": 428
    },
    {
      "epoch": 2.835192069392813,
      "grad_norm": 2110.212158203125,
      "learning_rate": 0.000429,
      "loss": 72.294,
      "step": 429
    },
    {
      "epoch": 2.841800908715407,
      "grad_norm": 1025.3687744140625,
      "learning_rate": 0.00043,
      "loss": 18.5199,
      "step": 430
    },
    {
      "epoch": 2.8484097480380006,
      "grad_norm": 504.71246337890625,
      "learning_rate": 0.000431,
      "loss": 6.9274,
      "step": 431
    },
    {
      "epoch": 2.8550185873605947,
      "grad_norm": 582.671142578125,
      "learning_rate": 0.000432,
      "loss": 15.0068,
      "step": 432
    },
    {
      "epoch": 2.8616274266831887,
      "grad_norm": 86.26802062988281,
      "learning_rate": 0.000433,
      "loss": 5.644,
      "step": 433
    },
    {
      "epoch": 2.8682362660057827,
      "grad_norm": 874.2062377929688,
      "learning_rate": 0.00043400000000000003,
      "loss": 16.1617,
      "step": 434
    },
    {
      "epoch": 2.874845105328377,
      "grad_norm": 1159.44970703125,
      "learning_rate": 0.000435,
      "loss": 28.8574,
      "step": 435
    },
    {
      "epoch": 2.881453944650971,
      "grad_norm": 277.2184753417969,
      "learning_rate": 0.000436,
      "loss": 6.4302,
      "step": 436
    },
    {
      "epoch": 2.888062783973565,
      "grad_norm": 2257.04931640625,
      "learning_rate": 0.000437,
      "loss": 67.7628,
      "step": 437
    },
    {
      "epoch": 2.8946716232961585,
      "grad_norm": 3009.265625,
      "learning_rate": 0.000438,
      "loss": 115.6283,
      "step": 438
    },
    {
      "epoch": 2.9012804626187525,
      "grad_norm": 968.5748291015625,
      "learning_rate": 0.000439,
      "loss": 16.4263,
      "step": 439
    },
    {
      "epoch": 2.9078893019413465,
      "grad_norm": 369.59197998046875,
      "learning_rate": 0.00044,
      "loss": 14.4712,
      "step": 440
    },
    {
      "epoch": 2.9144981412639406,
      "grad_norm": 1061.376953125,
      "learning_rate": 0.000441,
      "loss": 21.3697,
      "step": 441
    },
    {
      "epoch": 2.9211069805865346,
      "grad_norm": 472.49310302734375,
      "learning_rate": 0.000442,
      "loss": 16.9708,
      "step": 442
    },
    {
      "epoch": 2.927715819909128,
      "grad_norm": 1639.3349609375,
      "learning_rate": 0.00044300000000000003,
      "loss": 54.3949,
      "step": 443
    },
    {
      "epoch": 2.9343246592317227,
      "grad_norm": 1145.6595458984375,
      "learning_rate": 0.000444,
      "loss": 28.2388,
      "step": 444
    },
    {
      "epoch": 2.9409334985543163,
      "grad_norm": 141.73187255859375,
      "learning_rate": 0.00044500000000000003,
      "loss": 12.759,
      "step": 445
    },
    {
      "epoch": 2.9475423378769103,
      "grad_norm": 304.94354248046875,
      "learning_rate": 0.000446,
      "loss": 6.8479,
      "step": 446
    },
    {
      "epoch": 2.9541511771995044,
      "grad_norm": 337.9093933105469,
      "learning_rate": 0.000447,
      "loss": 10.8228,
      "step": 447
    },
    {
      "epoch": 2.9607600165220984,
      "grad_norm": 3105.89111328125,
      "learning_rate": 0.000448,
      "loss": 87.4967,
      "step": 448
    },
    {
      "epoch": 2.9673688558446925,
      "grad_norm": 4447.4462890625,
      "learning_rate": 0.000449,
      "loss": 165.4222,
      "step": 449
    },
    {
      "epoch": 2.973977695167286,
      "grad_norm": 1900.02294921875,
      "learning_rate": 0.00045000000000000004,
      "loss": 66.4684,
      "step": 450
    },
    {
      "epoch": 2.98058653448988,
      "grad_norm": 2304.669189453125,
      "learning_rate": 0.000451,
      "loss": 73.9389,
      "step": 451
    },
    {
      "epoch": 2.987195373812474,
      "grad_norm": 4991.2958984375,
      "learning_rate": 0.00045200000000000004,
      "loss": 191.3173,
      "step": 452
    },
    {
      "epoch": 2.993804213135068,
      "grad_norm": 2202.0908203125,
      "learning_rate": 0.000453,
      "loss": 56.6432,
      "step": 453
    },
    {
      "epoch": 2.993804213135068,
      "eval_validation_error_bar": 0.05995844618685915,
      "eval_validation_loss": 19.500024795532227,
      "eval_validation_pearsonr": 0.38773020376570966,
      "eval_validation_rmse": 4.4158830642700195,
      "eval_validation_runtime": 42.0651,
      "eval_validation_samples_per_second": 4.826,
      "eval_validation_spearman": 0.4354346578701037,
      "eval_validation_steps_per_second": 4.826,
      "step": 453
    },
    {
      "epoch": 2.993804213135068,
      "eval_test_error_bar": 0.05120685565257845,
      "eval_test_loss": 22.711469650268555,
      "eval_test_pearsonr": 0.3648072532475526,
      "eval_test_rmse": 4.765655040740967,
      "eval_test_runtime": 56.2646,
      "eval_test_samples_per_second": 5.794,
      "eval_test_spearman": 0.3199841918390542,
      "eval_test_steps_per_second": 5.794,
      "step": 453
    },
    {
      "epoch": 3.000413052457662,
      "grad_norm": 1494.51904296875,
      "learning_rate": 0.00045400000000000003,
      "loss": 30.7693,
      "step": 454
    },
    {
      "epoch": 3.0070218917802563,
      "grad_norm": 1835.361572265625,
      "learning_rate": 0.000455,
      "loss": 40.5434,
      "step": 455
    },
    {
      "epoch": 3.01363073110285,
      "grad_norm": 874.1834716796875,
      "learning_rate": 0.000456,
      "loss": 16.7769,
      "step": 456
    },
    {
      "epoch": 3.020239570425444,
      "grad_norm": 1797.323486328125,
      "learning_rate": 0.00045700000000000005,
      "loss": 47.1985,
      "step": 457
    },
    {
      "epoch": 3.026848409748038,
      "grad_norm": 2528.217529296875,
      "learning_rate": 0.000458,
      "loss": 100.8411,
      "step": 458
    },
    {
      "epoch": 3.033457249070632,
      "grad_norm": 915.2232055664062,
      "learning_rate": 0.00045900000000000004,
      "loss": 18.4946,
      "step": 459
    },
    {
      "epoch": 3.040066088393226,
      "grad_norm": 792.1661987304688,
      "learning_rate": 0.00046,
      "loss": 16.7836,
      "step": 460
    },
    {
      "epoch": 3.04667492771582,
      "grad_norm": 1670.6551513671875,
      "learning_rate": 0.00046100000000000004,
      "loss": 51.761,
      "step": 461
    },
    {
      "epoch": 3.053283767038414,
      "grad_norm": 985.7852172851562,
      "learning_rate": 0.000462,
      "loss": 21.8514,
      "step": 462
    },
    {
      "epoch": 3.0598926063610077,
      "grad_norm": 546.223388671875,
      "learning_rate": 0.00046300000000000003,
      "loss": 7.8504,
      "step": 463
    },
    {
      "epoch": 3.0665014456836017,
      "grad_norm": 702.5478515625,
      "learning_rate": 0.00046400000000000006,
      "loss": 14.9749,
      "step": 464
    },
    {
      "epoch": 3.0731102850061958,
      "grad_norm": 174.64198303222656,
      "learning_rate": 0.000465,
      "loss": 7.7012,
      "step": 465
    },
    {
      "epoch": 3.07971912432879,
      "grad_norm": 621.1558227539062,
      "learning_rate": 0.00046600000000000005,
      "loss": 12.923,
      "step": 466
    },
    {
      "epoch": 3.086327963651384,
      "grad_norm": 1207.9453125,
      "learning_rate": 0.000467,
      "loss": 42.388,
      "step": 467
    },
    {
      "epoch": 3.092936802973978,
      "grad_norm": 657.7523803710938,
      "learning_rate": 0.00046800000000000005,
      "loss": 17.0934,
      "step": 468
    },
    {
      "epoch": 3.0995456422965715,
      "grad_norm": 609.7234497070312,
      "learning_rate": 0.00046899999999999996,
      "loss": 13.4032,
      "step": 469
    },
    {
      "epoch": 3.1061544816191655,
      "grad_norm": 1014.197021484375,
      "learning_rate": 0.00047,
      "loss": 21.4925,
      "step": 470
    },
    {
      "epoch": 3.1127633209417596,
      "grad_norm": 467.39801025390625,
      "learning_rate": 0.000471,
      "loss": 10.2455,
      "step": 471
    },
    {
      "epoch": 3.1193721602643536,
      "grad_norm": 1026.2801513671875,
      "learning_rate": 0.000472,
      "loss": 21.1389,
      "step": 472
    },
    {
      "epoch": 3.1259809995869476,
      "grad_norm": 1038.6456298828125,
      "learning_rate": 0.000473,
      "loss": 20.5513,
      "step": 473
    },
    {
      "epoch": 3.1325898389095417,
      "grad_norm": 686.2940673828125,
      "learning_rate": 0.000474,
      "loss": 12.9961,
      "step": 474
    },
    {
      "epoch": 3.1391986782321353,
      "grad_norm": 632.4479370117188,
      "learning_rate": 0.000475,
      "loss": 11.6068,
      "step": 475
    },
    {
      "epoch": 3.1458075175547293,
      "grad_norm": 776.7674560546875,
      "learning_rate": 0.00047599999999999997,
      "loss": 25.8584,
      "step": 476
    },
    {
      "epoch": 3.1524163568773234,
      "grad_norm": 92.8768081665039,
      "learning_rate": 0.000477,
      "loss": 4.5797,
      "step": 477
    },
    {
      "epoch": 3.1590251961999174,
      "grad_norm": 694.32763671875,
      "learning_rate": 0.00047799999999999996,
      "loss": 19.3644,
      "step": 478
    },
    {
      "epoch": 3.1656340355225114,
      "grad_norm": 950.6785888671875,
      "learning_rate": 0.000479,
      "loss": 31.5364,
      "step": 479
    },
    {
      "epoch": 3.1722428748451055,
      "grad_norm": 118.93280792236328,
      "learning_rate": 0.00048,
      "loss": 12.0219,
      "step": 480
    },
    {
      "epoch": 3.178851714167699,
      "grad_norm": 1660.4774169921875,
      "learning_rate": 0.000481,
      "loss": 40.9068,
      "step": 481
    },
    {
      "epoch": 3.185460553490293,
      "grad_norm": 1640.505859375,
      "learning_rate": 0.000482,
      "loss": 34.3568,
      "step": 482
    },
    {
      "epoch": 3.192069392812887,
      "grad_norm": 509.537109375,
      "learning_rate": 0.000483,
      "loss": 53.6512,
      "step": 483
    },
    {
      "epoch": 3.198678232135481,
      "grad_norm": 3882.0205078125,
      "learning_rate": 0.000484,
      "loss": 94.7001,
      "step": 484
    },
    {
      "epoch": 3.2052870714580752,
      "grad_norm": 5086.68359375,
      "learning_rate": 0.00048499999999999997,
      "loss": 156.4226,
      "step": 485
    },
    {
      "epoch": 3.2118959107806693,
      "grad_norm": 1015.0166625976562,
      "learning_rate": 0.000486,
      "loss": 15.4242,
      "step": 486
    },
    {
      "epoch": 3.2185047501032633,
      "grad_norm": 5754.62255859375,
      "learning_rate": 0.000487,
      "loss": 181.4519,
      "step": 487
    },
    {
      "epoch": 3.225113589425857,
      "grad_norm": 8317.2431640625,
      "learning_rate": 0.000488,
      "loss": 354.1503,
      "step": 488
    },
    {
      "epoch": 3.231722428748451,
      "grad_norm": 3943.521484375,
      "learning_rate": 0.000489,
      "loss": 122.4811,
      "step": 489
    },
    {
      "epoch": 3.238331268071045,
      "grad_norm": 508.904541015625,
      "learning_rate": 0.00049,
      "loss": 17.9232,
      "step": 490
    },
    {
      "epoch": 3.244940107393639,
      "grad_norm": 361.8887634277344,
      "learning_rate": 0.000491,
      "loss": 13.2861,
      "step": 491
    },
    {
      "epoch": 3.251548946716233,
      "grad_norm": 1031.681396484375,
      "learning_rate": 0.000492,
      "loss": 14.7782,
      "step": 492
    },
    {
      "epoch": 3.258157786038827,
      "grad_norm": 270.6067199707031,
      "learning_rate": 0.0004930000000000001,
      "loss": 4.1619,
      "step": 493
    },
    {
      "epoch": 3.264766625361421,
      "grad_norm": 2063.996826171875,
      "learning_rate": 0.000494,
      "loss": 58.206,
      "step": 494
    },
    {
      "epoch": 3.2713754646840147,
      "grad_norm": 2193.0615234375,
      "learning_rate": 0.000495,
      "loss": 62.1203,
      "step": 495
    },
    {
      "epoch": 3.277984304006609,
      "grad_norm": 571.0468139648438,
      "learning_rate": 0.000496,
      "loss": 18.7037,
      "step": 496
    },
    {
      "epoch": 3.284593143329203,
      "grad_norm": 2539.97314453125,
      "learning_rate": 0.000497,
      "loss": 78.1976,
      "step": 497
    },
    {
      "epoch": 3.291201982651797,
      "grad_norm": 3550.155517578125,
      "learning_rate": 0.000498,
      "loss": 151.8299,
      "step": 498
    },
    {
      "epoch": 3.297810821974391,
      "grad_norm": 1391.8590087890625,
      "learning_rate": 0.000499,
      "loss": 53.5863,
      "step": 499
    },
    {
      "epoch": 3.3044196612969845,
      "grad_norm": 738.51708984375,
      "learning_rate": 0.0005,
      "loss": 23.0253,
      "step": 500
    },
    {
      "epoch": 3.3110285006195785,
      "grad_norm": 573.2587280273438,
      "learning_rate": 0.000501,
      "loss": 13.6702,
      "step": 501
    },
    {
      "epoch": 3.3176373399421726,
      "grad_norm": 60.837989807128906,
      "learning_rate": 0.0005020000000000001,
      "loss": 5.7672,
      "step": 502
    },
    {
      "epoch": 3.3242461792647666,
      "grad_norm": 1777.265380859375,
      "learning_rate": 0.000503,
      "loss": 52.2544,
      "step": 503
    },
    {
      "epoch": 3.3308550185873607,
      "grad_norm": 1083.5198974609375,
      "learning_rate": 0.000504,
      "loss": 33.865,
      "step": 504
    },
    {
      "epoch": 3.3374638579099547,
      "grad_norm": 987.2692260742188,
      "learning_rate": 0.000505,
      "loss": 32.543,
      "step": 505
    },
    {
      "epoch": 3.3440726972325487,
      "grad_norm": 2342.0849609375,
      "learning_rate": 0.000506,
      "loss": 57.3767,
      "step": 506
    },
    {
      "epoch": 3.3506815365551423,
      "grad_norm": 783.3121337890625,
      "learning_rate": 0.000507,
      "loss": 12.8121,
      "step": 507
    },
    {
      "epoch": 3.3572903758777364,
      "grad_norm": 3298.99267578125,
      "learning_rate": 0.000508,
      "loss": 101.0935,
      "step": 508
    },
    {
      "epoch": 3.3638992152003304,
      "grad_norm": 4528.16015625,
      "learning_rate": 0.000509,
      "loss": 174.7445,
      "step": 509
    },
    {
      "epoch": 3.3705080545229245,
      "grad_norm": 2058.078125,
      "learning_rate": 0.00051,
      "loss": 44.9574,
      "step": 510
    },
    {
      "epoch": 3.3771168938455185,
      "grad_norm": 1175.0367431640625,
      "learning_rate": 0.0005110000000000001,
      "loss": 22.0145,
      "step": 511
    },
    {
      "epoch": 3.3837257331681125,
      "grad_norm": 2293.728271484375,
      "learning_rate": 0.000512,
      "loss": 77.3105,
      "step": 512
    },
    {
      "epoch": 3.390334572490706,
      "grad_norm": 552.6987915039062,
      "learning_rate": 0.000513,
      "loss": 13.3239,
      "step": 513
    },
    {
      "epoch": 3.3969434118133,
      "grad_norm": 2030.2147216796875,
      "learning_rate": 0.000514,
      "loss": 55.9215,
      "step": 514
    },
    {
      "epoch": 3.403552251135894,
      "grad_norm": 2227.62646484375,
      "learning_rate": 0.000515,
      "loss": 71.6007,
      "step": 515
    },
    {
      "epoch": 3.4101610904584883,
      "grad_norm": 479.71160888671875,
      "learning_rate": 0.0005160000000000001,
      "loss": 8.8341,
      "step": 516
    },
    {
      "epoch": 3.4167699297810823,
      "grad_norm": 2399.49462890625,
      "learning_rate": 0.000517,
      "loss": 80.5798,
      "step": 517
    },
    {
      "epoch": 3.4233787691036763,
      "grad_norm": 2385.910888671875,
      "learning_rate": 0.000518,
      "loss": 94.6583,
      "step": 518
    },
    {
      "epoch": 3.42998760842627,
      "grad_norm": 527.8014526367188,
      "learning_rate": 0.000519,
      "loss": 10.2764,
      "step": 519
    },
    {
      "epoch": 3.436596447748864,
      "grad_norm": 1620.135986328125,
      "learning_rate": 0.0005200000000000001,
      "loss": 52.5023,
      "step": 520
    },
    {
      "epoch": 3.443205287071458,
      "grad_norm": 2102.715576171875,
      "learning_rate": 0.000521,
      "loss": 70.0403,
      "step": 521
    },
    {
      "epoch": 3.449814126394052,
      "grad_norm": 691.5897216796875,
      "learning_rate": 0.000522,
      "loss": 16.4247,
      "step": 522
    },
    {
      "epoch": 3.456422965716646,
      "grad_norm": 1680.772216796875,
      "learning_rate": 0.000523,
      "loss": 34.961,
      "step": 523
    },
    {
      "epoch": 3.46303180503924,
      "grad_norm": 2436.5537109375,
      "learning_rate": 0.000524,
      "loss": 70.5734,
      "step": 524
    },
    {
      "epoch": 3.469640644361834,
      "grad_norm": 918.4667358398438,
      "learning_rate": 0.0005250000000000001,
      "loss": 13.1629,
      "step": 525
    },
    {
      "epoch": 3.4762494836844278,
      "grad_norm": 824.5267333984375,
      "learning_rate": 0.000526,
      "loss": 22.3258,
      "step": 526
    },
    {
      "epoch": 3.482858323007022,
      "grad_norm": 1372.5986328125,
      "learning_rate": 0.000527,
      "loss": 25.4938,
      "step": 527
    },
    {
      "epoch": 3.489467162329616,
      "grad_norm": 423.2409973144531,
      "learning_rate": 0.000528,
      "loss": 5.0566,
      "step": 528
    },
    {
      "epoch": 3.49607600165221,
      "grad_norm": 1141.1951904296875,
      "learning_rate": 0.0005290000000000001,
      "loss": 30.4435,
      "step": 529
    },
    {
      "epoch": 3.502684840974804,
      "grad_norm": 1427.8809814453125,
      "learning_rate": 0.0005300000000000001,
      "loss": 43.9224,
      "step": 530
    },
    {
      "epoch": 3.5092936802973975,
      "grad_norm": 675.5340576171875,
      "learning_rate": 0.000531,
      "loss": 18.7474,
      "step": 531
    },
    {
      "epoch": 3.515902519619992,
      "grad_norm": 852.2174682617188,
      "learning_rate": 0.000532,
      "loss": 20.9022,
      "step": 532
    },
    {
      "epoch": 3.5225113589425856,
      "grad_norm": 1102.5450439453125,
      "learning_rate": 0.000533,
      "loss": 36.0045,
      "step": 533
    },
    {
      "epoch": 3.5291201982651796,
      "grad_norm": 103.08573150634766,
      "learning_rate": 0.0005340000000000001,
      "loss": 4.6576,
      "step": 534
    },
    {
      "epoch": 3.5357290375877737,
      "grad_norm": 1086.060546875,
      "learning_rate": 0.000535,
      "loss": 49.5448,
      "step": 535
    },
    {
      "epoch": 3.5423378769103677,
      "grad_norm": 846.386474609375,
      "learning_rate": 0.000536,
      "loss": 32.5354,
      "step": 536
    },
    {
      "epoch": 3.5489467162329618,
      "grad_norm": 442.9703369140625,
      "learning_rate": 0.000537,
      "loss": 39.5844,
      "step": 537
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 242.58334350585938,
      "learning_rate": 0.0005380000000000001,
      "loss": 10.364,
      "step": 538
    },
    {
      "epoch": 3.5621643948781494,
      "grad_norm": 425.2262268066406,
      "learning_rate": 0.0005390000000000001,
      "loss": 17.1335,
      "step": 539
    },
    {
      "epoch": 3.5687732342007434,
      "grad_norm": 1290.3812255859375,
      "learning_rate": 0.00054,
      "loss": 30.874,
      "step": 540
    },
    {
      "epoch": 3.5753820735233375,
      "grad_norm": 693.1226806640625,
      "learning_rate": 0.000541,
      "loss": 13.5511,
      "step": 541
    },
    {
      "epoch": 3.5819909128459315,
      "grad_norm": 1477.05712890625,
      "learning_rate": 0.0005420000000000001,
      "loss": 44.5313,
      "step": 542
    },
    {
      "epoch": 3.5885997521685256,
      "grad_norm": 575.2695922851562,
      "learning_rate": 0.0005430000000000001,
      "loss": 60.1009,
      "step": 543
    },
    {
      "epoch": 3.5952085914911196,
      "grad_norm": 1384.7464599609375,
      "learning_rate": 0.0005440000000000001,
      "loss": 20.4599,
      "step": 544
    },
    {
      "epoch": 3.601817430813713,
      "grad_norm": 1280.3642578125,
      "learning_rate": 0.000545,
      "loss": 33.5131,
      "step": 545
    },
    {
      "epoch": 3.6084262701363072,
      "grad_norm": 297.5306396484375,
      "learning_rate": 0.000546,
      "loss": 33.1208,
      "step": 546
    },
    {
      "epoch": 3.6150351094589013,
      "grad_norm": 3050.6611328125,
      "learning_rate": 0.0005470000000000001,
      "loss": 83.2791,
      "step": 547
    },
    {
      "epoch": 3.6216439487814953,
      "grad_norm": 649.2041625976562,
      "learning_rate": 0.0005480000000000001,
      "loss": 49.9314,
      "step": 548
    },
    {
      "epoch": 3.6282527881040894,
      "grad_norm": 1261.4949951171875,
      "learning_rate": 0.000549,
      "loss": 26.8819,
      "step": 549
    },
    {
      "epoch": 3.634861627426683,
      "grad_norm": 2228.101318359375,
      "learning_rate": 0.00055,
      "loss": 70.2614,
      "step": 550
    },
    {
      "epoch": 3.6414704667492774,
      "grad_norm": 1200.19873046875,
      "learning_rate": 0.0005510000000000001,
      "loss": 25.5924,
      "step": 551
    },
    {
      "epoch": 3.648079306071871,
      "grad_norm": 2753.2578125,
      "learning_rate": 0.0005520000000000001,
      "loss": 132.2959,
      "step": 552
    },
    {
      "epoch": 3.654688145394465,
      "grad_norm": 2223.988525390625,
      "learning_rate": 0.0005530000000000001,
      "loss": 88.0759,
      "step": 553
    },
    {
      "epoch": 3.661296984717059,
      "grad_norm": 1191.513671875,
      "learning_rate": 0.000554,
      "loss": 37.6101,
      "step": 554
    },
    {
      "epoch": 3.667905824039653,
      "grad_norm": 798.2471313476562,
      "learning_rate": 0.000555,
      "loss": 19.2953,
      "step": 555
    },
    {
      "epoch": 3.674514663362247,
      "grad_norm": 2218.15673828125,
      "learning_rate": 0.0005560000000000001,
      "loss": 84.7528,
      "step": 556
    },
    {
      "epoch": 3.681123502684841,
      "grad_norm": 2135.427978515625,
      "learning_rate": 0.0005570000000000001,
      "loss": 83.4577,
      "step": 557
    },
    {
      "epoch": 3.687732342007435,
      "grad_norm": 471.8246765136719,
      "learning_rate": 0.000558,
      "loss": 11.4529,
      "step": 558
    },
    {
      "epoch": 3.694341181330029,
      "grad_norm": 214.56643676757812,
      "learning_rate": 0.000559,
      "loss": 6.4286,
      "step": 559
    },
    {
      "epoch": 3.700950020652623,
      "grad_norm": 852.1569213867188,
      "learning_rate": 0.0005600000000000001,
      "loss": 28.8661,
      "step": 560
    },
    {
      "epoch": 3.707558859975217,
      "grad_norm": 593.9190063476562,
      "learning_rate": 0.0005610000000000001,
      "loss": 22.0342,
      "step": 561
    },
    {
      "epoch": 3.714167699297811,
      "grad_norm": 489.8082275390625,
      "learning_rate": 0.0005620000000000001,
      "loss": 22.8596,
      "step": 562
    },
    {
      "epoch": 3.720776538620405,
      "grad_norm": 351.48504638671875,
      "learning_rate": 0.0005629999999999999,
      "loss": 9.9085,
      "step": 563
    },
    {
      "epoch": 3.7273853779429986,
      "grad_norm": 377.7432861328125,
      "learning_rate": 0.0005639999999999999,
      "loss": 10.6123,
      "step": 564
    },
    {
      "epoch": 3.7339942172655927,
      "grad_norm": 688.4863891601562,
      "learning_rate": 0.000565,
      "loss": 22.9067,
      "step": 565
    },
    {
      "epoch": 3.7406030565881867,
      "grad_norm": 520.135986328125,
      "learning_rate": 0.000566,
      "loss": 16.6889,
      "step": 566
    },
    {
      "epoch": 3.7472118959107807,
      "grad_norm": 212.71160888671875,
      "learning_rate": 0.000567,
      "loss": 2.9877,
      "step": 567
    },
    {
      "epoch": 3.753820735233375,
      "grad_norm": 592.1948852539062,
      "learning_rate": 0.0005679999999999999,
      "loss": 13.1135,
      "step": 568
    },
    {
      "epoch": 3.7604295745559684,
      "grad_norm": 497.9066162109375,
      "learning_rate": 0.000569,
      "loss": 14.0128,
      "step": 569
    },
    {
      "epoch": 3.7670384138785624,
      "grad_norm": 313.9790954589844,
      "learning_rate": 0.00057,
      "loss": 10.2855,
      "step": 570
    },
    {
      "epoch": 3.7736472532011565,
      "grad_norm": 671.7161865234375,
      "learning_rate": 0.000571,
      "loss": 19.142,
      "step": 571
    },
    {
      "epoch": 3.7802560925237505,
      "grad_norm": 374.6335754394531,
      "learning_rate": 0.0005719999999999999,
      "loss": 6.7695,
      "step": 572
    },
    {
      "epoch": 3.7868649318463445,
      "grad_norm": 480.66497802734375,
      "learning_rate": 0.0005729999999999999,
      "loss": 9.1605,
      "step": 573
    },
    {
      "epoch": 3.7934737711689386,
      "grad_norm": 402.6162109375,
      "learning_rate": 0.000574,
      "loss": 10.8356,
      "step": 574
    },
    {
      "epoch": 3.8000826104915326,
      "grad_norm": 74.1277084350586,
      "learning_rate": 0.000575,
      "loss": 2.9352,
      "step": 575
    },
    {
      "epoch": 3.806691449814126,
      "grad_norm": 497.7612609863281,
      "learning_rate": 0.000576,
      "loss": 12.6007,
      "step": 576
    },
    {
      "epoch": 3.8133002891367203,
      "grad_norm": 473.09149169921875,
      "learning_rate": 0.0005769999999999999,
      "loss": 18.564,
      "step": 577
    },
    {
      "epoch": 3.8199091284593143,
      "grad_norm": 627.3652954101562,
      "learning_rate": 0.000578,
      "loss": 28.7163,
      "step": 578
    },
    {
      "epoch": 3.8265179677819083,
      "grad_norm": 119.73089599609375,
      "learning_rate": 0.000579,
      "loss": 14.8834,
      "step": 579
    },
    {
      "epoch": 3.8331268071045024,
      "grad_norm": 129.65179443359375,
      "learning_rate": 0.00058,
      "loss": 5.6335,
      "step": 580
    },
    {
      "epoch": 3.839735646427096,
      "grad_norm": 971.1339111328125,
      "learning_rate": 0.0005809999999999999,
      "loss": 29.2256,
      "step": 581
    },
    {
      "epoch": 3.8463444857496905,
      "grad_norm": 2189.04833984375,
      "learning_rate": 0.0005819999999999999,
      "loss": 78.3026,
      "step": 582
    },
    {
      "epoch": 3.852953325072284,
      "grad_norm": 466.9087829589844,
      "learning_rate": 0.000583,
      "loss": 11.4902,
      "step": 583
    },
    {
      "epoch": 3.859562164394878,
      "grad_norm": 483.40032958984375,
      "learning_rate": 0.000584,
      "loss": 11.7251,
      "step": 584
    },
    {
      "epoch": 3.866171003717472,
      "grad_norm": 653.0125122070312,
      "learning_rate": 0.000585,
      "loss": 12.1352,
      "step": 585
    },
    {
      "epoch": 3.872779843040066,
      "grad_norm": 500.0279846191406,
      "learning_rate": 0.0005859999999999999,
      "loss": 11.19,
      "step": 586
    },
    {
      "epoch": 3.87938868236266,
      "grad_norm": 268.3924560546875,
      "learning_rate": 0.000587,
      "loss": 10.9042,
      "step": 587
    },
    {
      "epoch": 3.885997521685254,
      "grad_norm": 3221.900146484375,
      "learning_rate": 0.000588,
      "loss": 123.0651,
      "step": 588
    },
    {
      "epoch": 3.892606361007848,
      "grad_norm": 2728.05859375,
      "learning_rate": 0.000589,
      "loss": 101.0642,
      "step": 589
    },
    {
      "epoch": 3.899215200330442,
      "grad_norm": 365.2497863769531,
      "learning_rate": 0.00059,
      "loss": 20.6883,
      "step": 590
    },
    {
      "epoch": 3.905824039653036,
      "grad_norm": 2509.4345703125,
      "learning_rate": 0.0005909999999999999,
      "loss": 92.6519,
      "step": 591
    },
    {
      "epoch": 3.91243287897563,
      "grad_norm": 2279.955810546875,
      "learning_rate": 0.000592,
      "loss": 95.4452,
      "step": 592
    },
    {
      "epoch": 3.919041718298224,
      "grad_norm": 825.6975708007812,
      "learning_rate": 0.000593,
      "loss": 30.1712,
      "step": 593
    },
    {
      "epoch": 3.925650557620818,
      "grad_norm": 1670.8778076171875,
      "learning_rate": 0.000594,
      "loss": 57.6016,
      "step": 594
    },
    {
      "epoch": 3.9322593969434116,
      "grad_norm": 2038.6434326171875,
      "learning_rate": 0.0005949999999999999,
      "loss": 85.4154,
      "step": 595
    },
    {
      "epoch": 3.9388682362660057,
      "grad_norm": 478.1816711425781,
      "learning_rate": 0.000596,
      "loss": 12.8749,
      "step": 596
    },
    {
      "epoch": 3.9454770755885997,
      "grad_norm": 1078.867431640625,
      "learning_rate": 0.000597,
      "loss": 49.1606,
      "step": 597
    },
    {
      "epoch": 3.9520859149111938,
      "grad_norm": 1045.60888671875,
      "learning_rate": 0.000598,
      "loss": 48.2462,
      "step": 598
    },
    {
      "epoch": 3.958694754233788,
      "grad_norm": 683.0560302734375,
      "learning_rate": 0.000599,
      "loss": 26.6259,
      "step": 599
    },
    {
      "epoch": 3.9653035935563814,
      "grad_norm": 441.401611328125,
      "learning_rate": 0.0006,
      "loss": 14.7198,
      "step": 600
    },
    {
      "epoch": 3.971912432878976,
      "grad_norm": 373.4822692871094,
      "learning_rate": 0.000601,
      "loss": 12.146,
      "step": 601
    },
    {
      "epoch": 3.9785212722015695,
      "grad_norm": 203.69757080078125,
      "learning_rate": 0.000602,
      "loss": 8.3376,
      "step": 602
    },
    {
      "epoch": 3.9851301115241635,
      "grad_norm": 171.93052673339844,
      "learning_rate": 0.000603,
      "loss": 8.1447,
      "step": 603
    },
    {
      "epoch": 3.9917389508467576,
      "grad_norm": 158.4612274169922,
      "learning_rate": 0.000604,
      "loss": 8.0202,
      "step": 604
    },
    {
      "epoch": 3.9983477901693516,
      "grad_norm": 75.90162658691406,
      "learning_rate": 0.000605,
      "loss": 5.4747,
      "step": 605
    },
    {
      "epoch": 3.9983477901693516,
      "eval_validation_error_bar": 0.057671635361643686,
      "eval_validation_loss": 14.768876075744629,
      "eval_validation_pearsonr": 0.4761683623330912,
      "eval_validation_rmse": 3.843029499053955,
      "eval_validation_runtime": 41.3027,
      "eval_validation_samples_per_second": 4.915,
      "eval_validation_spearman": 0.4765521446010146,
      "eval_validation_steps_per_second": 4.915,
      "step": 605
    },
    {
      "epoch": 3.9983477901693516,
      "eval_test_error_bar": 0.05390887631595415,
      "eval_test_loss": 19.096355438232422,
      "eval_test_pearsonr": 0.10879805660007606,
      "eval_test_rmse": 4.369937896728516,
      "eval_test_runtime": 55.8659,
      "eval_test_samples_per_second": 5.835,
      "eval_test_spearman": 0.20222474724106945,
      "eval_test_steps_per_second": 5.835,
      "step": 605
    },
    {
      "epoch": 4.004956629491946,
      "grad_norm": 389.2536926269531,
      "learning_rate": 0.000606,
      "loss": 9.0359,
      "step": 606
    },
    {
      "epoch": 4.011565468814539,
      "grad_norm": 373.084228515625,
      "learning_rate": 0.000607,
      "loss": 20.1728,
      "step": 607
    },
    {
      "epoch": 4.018174308137134,
      "grad_norm": 188.41098022460938,
      "learning_rate": 0.000608,
      "loss": 15.0515,
      "step": 608
    },
    {
      "epoch": 4.024783147459727,
      "grad_norm": 51.69502258300781,
      "learning_rate": 0.000609,
      "loss": 4.8502,
      "step": 609
    },
    {
      "epoch": 4.031391986782321,
      "grad_norm": 167.42652893066406,
      "learning_rate": 0.00061,
      "loss": 14.007,
      "step": 610
    },
    {
      "epoch": 4.038000826104915,
      "grad_norm": 188.65386962890625,
      "learning_rate": 0.000611,
      "loss": 11.5235,
      "step": 611
    },
    {
      "epoch": 4.044609665427509,
      "grad_norm": 71.9207534790039,
      "learning_rate": 0.000612,
      "loss": 7.4353,
      "step": 612
    },
    {
      "epoch": 4.0512185047501035,
      "grad_norm": 389.11773681640625,
      "learning_rate": 0.000613,
      "loss": 14.6878,
      "step": 613
    },
    {
      "epoch": 4.057827344072697,
      "grad_norm": 224.49354553222656,
      "learning_rate": 0.000614,
      "loss": 6.1472,
      "step": 614
    },
    {
      "epoch": 4.064436183395292,
      "grad_norm": 1713.2772216796875,
      "learning_rate": 0.000615,
      "loss": 67.9221,
      "step": 615
    },
    {
      "epoch": 4.071045022717885,
      "grad_norm": 1553.4217529296875,
      "learning_rate": 0.000616,
      "loss": 42.685,
      "step": 616
    },
    {
      "epoch": 4.077653862040479,
      "grad_norm": 574.8636474609375,
      "learning_rate": 0.000617,
      "loss": 17.1311,
      "step": 617
    },
    {
      "epoch": 4.084262701363073,
      "grad_norm": 1989.5977783203125,
      "learning_rate": 0.0006180000000000001,
      "loss": 106.6181,
      "step": 618
    },
    {
      "epoch": 4.090871540685667,
      "grad_norm": 2224.301025390625,
      "learning_rate": 0.000619,
      "loss": 118.8617,
      "step": 619
    },
    {
      "epoch": 4.097480380008261,
      "grad_norm": 962.8865356445312,
      "learning_rate": 0.00062,
      "loss": 27.1992,
      "step": 620
    },
    {
      "epoch": 4.104089219330855,
      "grad_norm": 1949.8670654296875,
      "learning_rate": 0.000621,
      "loss": 78.016,
      "step": 621
    },
    {
      "epoch": 4.110698058653449,
      "grad_norm": 2310.701416015625,
      "learning_rate": 0.000622,
      "loss": 100.0366,
      "step": 622
    },
    {
      "epoch": 4.117306897976043,
      "grad_norm": 1026.3968505859375,
      "learning_rate": 0.000623,
      "loss": 24.9257,
      "step": 623
    },
    {
      "epoch": 4.123915737298637,
      "grad_norm": 1806.7821044921875,
      "learning_rate": 0.000624,
      "loss": 78.5885,
      "step": 624
    },
    {
      "epoch": 4.130524576621231,
      "grad_norm": 2225.336181640625,
      "learning_rate": 0.000625,
      "loss": 131.6482,
      "step": 625
    },
    {
      "epoch": 4.137133415943825,
      "grad_norm": 595.433837890625,
      "learning_rate": 0.000626,
      "loss": 19.2711,
      "step": 626
    },
    {
      "epoch": 4.143742255266419,
      "grad_norm": 1268.8587646484375,
      "learning_rate": 0.0006270000000000001,
      "loss": 57.6392,
      "step": 627
    },
    {
      "epoch": 4.150351094589013,
      "grad_norm": 1670.1640625,
      "learning_rate": 0.000628,
      "loss": 77.4836,
      "step": 628
    },
    {
      "epoch": 4.156959933911606,
      "grad_norm": 797.1688842773438,
      "learning_rate": 0.000629,
      "loss": 22.4849,
      "step": 629
    },
    {
      "epoch": 4.163568773234201,
      "grad_norm": 656.8021240234375,
      "learning_rate": 0.00063,
      "loss": 19.958,
      "step": 630
    },
    {
      "epoch": 4.170177612556794,
      "grad_norm": 956.0167236328125,
      "learning_rate": 0.000631,
      "loss": 50.0029,
      "step": 631
    },
    {
      "epoch": 4.176786451879389,
      "grad_norm": 548.7402954101562,
      "learning_rate": 0.000632,
      "loss": 29.1238,
      "step": 632
    },
    {
      "epoch": 4.1833952912019825,
      "grad_norm": 456.5341491699219,
      "learning_rate": 0.000633,
      "loss": 12.6309,
      "step": 633
    },
    {
      "epoch": 4.190004130524577,
      "grad_norm": 634.5210571289062,
      "learning_rate": 0.000634,
      "loss": 22.5224,
      "step": 634
    },
    {
      "epoch": 4.196612969847171,
      "grad_norm": 139.73419189453125,
      "learning_rate": 0.000635,
      "loss": 5.4289,
      "step": 635
    },
    {
      "epoch": 4.203221809169764,
      "grad_norm": 452.5666198730469,
      "learning_rate": 0.0006360000000000001,
      "loss": 17.1143,
      "step": 636
    },
    {
      "epoch": 4.209830648492359,
      "grad_norm": 575.0989990234375,
      "learning_rate": 0.000637,
      "loss": 24.6581,
      "step": 637
    },
    {
      "epoch": 4.216439487814952,
      "grad_norm": 356.4485778808594,
      "learning_rate": 0.000638,
      "loss": 14.4653,
      "step": 638
    },
    {
      "epoch": 4.223048327137547,
      "grad_norm": 265.5533447265625,
      "learning_rate": 0.000639,
      "loss": 7.0677,
      "step": 639
    },
    {
      "epoch": 4.22965716646014,
      "grad_norm": 326.5167236328125,
      "learning_rate": 0.00064,
      "loss": 11.4013,
      "step": 640
    },
    {
      "epoch": 4.236266005782735,
      "grad_norm": 127.63558959960938,
      "learning_rate": 0.0006410000000000001,
      "loss": 7.1515,
      "step": 641
    },
    {
      "epoch": 4.242874845105328,
      "grad_norm": 441.9315490722656,
      "learning_rate": 0.000642,
      "loss": 20.7589,
      "step": 642
    },
    {
      "epoch": 4.249483684427922,
      "grad_norm": 479.0644836425781,
      "learning_rate": 0.000643,
      "loss": 16.4955,
      "step": 643
    },
    {
      "epoch": 4.2560925237505165,
      "grad_norm": 207.1662139892578,
      "learning_rate": 0.000644,
      "loss": 4.7595,
      "step": 644
    },
    {
      "epoch": 4.26270136307311,
      "grad_norm": 278.5605163574219,
      "learning_rate": 0.0006450000000000001,
      "loss": 11.9239,
      "step": 645
    },
    {
      "epoch": 4.269310202395705,
      "grad_norm": 399.78839111328125,
      "learning_rate": 0.000646,
      "loss": 11.2114,
      "step": 646
    },
    {
      "epoch": 4.275919041718298,
      "grad_norm": 16.57042121887207,
      "learning_rate": 0.000647,
      "loss": 1.4275,
      "step": 647
    },
    {
      "epoch": 4.282527881040892,
      "grad_norm": 505.99835205078125,
      "learning_rate": 0.000648,
      "loss": 20.1809,
      "step": 648
    },
    {
      "epoch": 4.289136720363486,
      "grad_norm": 68.4302749633789,
      "learning_rate": 0.0006490000000000001,
      "loss": 9.5349,
      "step": 649
    },
    {
      "epoch": 4.29574555968608,
      "grad_norm": 640.2213745117188,
      "learning_rate": 0.0006500000000000001,
      "loss": 28.2567,
      "step": 650
    },
    {
      "epoch": 4.302354399008674,
      "grad_norm": 630.7791748046875,
      "learning_rate": 0.000651,
      "loss": 22.4307,
      "step": 651
    },
    {
      "epoch": 4.308963238331268,
      "grad_norm": 416.6817932128906,
      "learning_rate": 0.000652,
      "loss": 12.6863,
      "step": 652
    },
    {
      "epoch": 4.315572077653862,
      "grad_norm": 460.6052551269531,
      "learning_rate": 0.000653,
      "loss": 14.5723,
      "step": 653
    },
    {
      "epoch": 4.322180916976456,
      "grad_norm": 285.2546691894531,
      "learning_rate": 0.0006540000000000001,
      "loss": 9.2137,
      "step": 654
    },
    {
      "epoch": 4.32878975629905,
      "grad_norm": 97.73297119140625,
      "learning_rate": 0.0006550000000000001,
      "loss": 5.5663,
      "step": 655
    },
    {
      "epoch": 4.335398595621644,
      "grad_norm": 497.2327880859375,
      "learning_rate": 0.000656,
      "loss": 9.5771,
      "step": 656
    },
    {
      "epoch": 4.342007434944238,
      "grad_norm": 420.01220703125,
      "learning_rate": 0.000657,
      "loss": 13.5588,
      "step": 657
    },
    {
      "epoch": 4.348616274266832,
      "grad_norm": 398.295654296875,
      "learning_rate": 0.0006580000000000001,
      "loss": 25.7812,
      "step": 658
    },
    {
      "epoch": 4.355225113589426,
      "grad_norm": 1267.0177001953125,
      "learning_rate": 0.0006590000000000001,
      "loss": 59.0349,
      "step": 659
    },
    {
      "epoch": 4.36183395291202,
      "grad_norm": 449.9338073730469,
      "learning_rate": 0.00066,
      "loss": 12.0488,
      "step": 660
    },
    {
      "epoch": 4.368442792234614,
      "grad_norm": 885.6467895507812,
      "learning_rate": 0.000661,
      "loss": 37.048,
      "step": 661
    },
    {
      "epoch": 4.375051631557207,
      "grad_norm": 556.9329223632812,
      "learning_rate": 0.000662,
      "loss": 37.8242,
      "step": 662
    },
    {
      "epoch": 4.381660470879802,
      "grad_norm": 98.4545669555664,
      "learning_rate": 0.0006630000000000001,
      "loss": 9.9072,
      "step": 663
    },
    {
      "epoch": 4.3882693102023955,
      "grad_norm": 728.5447998046875,
      "learning_rate": 0.0006640000000000001,
      "loss": 32.5843,
      "step": 664
    },
    {
      "epoch": 4.39487814952499,
      "grad_norm": 580.5476684570312,
      "learning_rate": 0.000665,
      "loss": 20.2926,
      "step": 665
    },
    {
      "epoch": 4.401486988847584,
      "grad_norm": 481.2409973144531,
      "learning_rate": 0.000666,
      "loss": 13.4004,
      "step": 666
    },
    {
      "epoch": 4.408095828170177,
      "grad_norm": 316.28228759765625,
      "learning_rate": 0.0006670000000000001,
      "loss": 10.2177,
      "step": 667
    },
    {
      "epoch": 4.414704667492772,
      "grad_norm": 91.72061920166016,
      "learning_rate": 0.0006680000000000001,
      "loss": 9.6155,
      "step": 668
    },
    {
      "epoch": 4.421313506815365,
      "grad_norm": 149.76748657226562,
      "learning_rate": 0.0006690000000000001,
      "loss": 6.2682,
      "step": 669
    },
    {
      "epoch": 4.42792234613796,
      "grad_norm": 134.9673309326172,
      "learning_rate": 0.00067,
      "loss": 6.4218,
      "step": 670
    },
    {
      "epoch": 4.434531185460553,
      "grad_norm": 165.59799194335938,
      "learning_rate": 0.000671,
      "loss": 9.5803,
      "step": 671
    },
    {
      "epoch": 4.441140024783148,
      "grad_norm": 249.43092346191406,
      "learning_rate": 0.0006720000000000001,
      "loss": 10.3328,
      "step": 672
    },
    {
      "epoch": 4.447748864105741,
      "grad_norm": 693.51220703125,
      "learning_rate": 0.0006730000000000001,
      "loss": 32.7476,
      "step": 673
    },
    {
      "epoch": 4.454357703428335,
      "grad_norm": 933.71630859375,
      "learning_rate": 0.000674,
      "loss": 35.1684,
      "step": 674
    },
    {
      "epoch": 4.4609665427509295,
      "grad_norm": 267.3848571777344,
      "learning_rate": 0.000675,
      "loss": 10.6152,
      "step": 675
    },
    {
      "epoch": 4.467575382073523,
      "grad_norm": 689.6349487304688,
      "learning_rate": 0.0006760000000000001,
      "loss": 20.4485,
      "step": 676
    },
    {
      "epoch": 4.474184221396118,
      "grad_norm": 791.5816650390625,
      "learning_rate": 0.0006770000000000001,
      "loss": 29.7648,
      "step": 677
    },
    {
      "epoch": 4.480793060718711,
      "grad_norm": 127.84979248046875,
      "learning_rate": 0.0006780000000000001,
      "loss": 5.0146,
      "step": 678
    },
    {
      "epoch": 4.487401900041306,
      "grad_norm": 1067.129638671875,
      "learning_rate": 0.000679,
      "loss": 55.7468,
      "step": 679
    },
    {
      "epoch": 4.494010739363899,
      "grad_norm": 1443.836669921875,
      "learning_rate": 0.00068,
      "loss": 92.9831,
      "step": 680
    },
    {
      "epoch": 4.500619578686493,
      "grad_norm": 1185.0966796875,
      "learning_rate": 0.0006810000000000001,
      "loss": 69.0169,
      "step": 681
    },
    {
      "epoch": 4.507228418009087,
      "grad_norm": 207.477294921875,
      "learning_rate": 0.0006820000000000001,
      "loss": 15.2372,
      "step": 682
    },
    {
      "epoch": 4.513837257331681,
      "grad_norm": 585.6651611328125,
      "learning_rate": 0.000683,
      "loss": 36.8556,
      "step": 683
    },
    {
      "epoch": 4.520446096654275,
      "grad_norm": 534.7479248046875,
      "learning_rate": 0.000684,
      "loss": 32.5774,
      "step": 684
    },
    {
      "epoch": 4.527054935976869,
      "grad_norm": 318.1955871582031,
      "learning_rate": 0.0006850000000000001,
      "loss": 12.846,
      "step": 685
    },
    {
      "epoch": 4.533663775299463,
      "grad_norm": 444.1658630371094,
      "learning_rate": 0.0006860000000000001,
      "loss": 16.8404,
      "step": 686
    },
    {
      "epoch": 4.540272614622057,
      "grad_norm": 592.4620971679688,
      "learning_rate": 0.0006870000000000001,
      "loss": 26.4335,
      "step": 687
    },
    {
      "epoch": 4.546881453944651,
      "grad_norm": 151.54388427734375,
      "learning_rate": 0.0006879999999999999,
      "loss": 6.7601,
      "step": 688
    },
    {
      "epoch": 4.553490293267245,
      "grad_norm": 547.7653198242188,
      "learning_rate": 0.0006889999999999999,
      "loss": 24.0697,
      "step": 689
    },
    {
      "epoch": 4.560099132589839,
      "grad_norm": 699.2554321289062,
      "learning_rate": 0.00069,
      "loss": 41.6955,
      "step": 690
    },
    {
      "epoch": 4.566707971912432,
      "grad_norm": 328.5431823730469,
      "learning_rate": 0.000691,
      "loss": 13.2289,
      "step": 691
    },
    {
      "epoch": 4.573316811235027,
      "grad_norm": 233.62400817871094,
      "learning_rate": 0.000692,
      "loss": 12.565,
      "step": 692
    },
    {
      "epoch": 4.5799256505576205,
      "grad_norm": 244.78814697265625,
      "learning_rate": 0.0006929999999999999,
      "loss": 15.0601,
      "step": 693
    },
    {
      "epoch": 4.586534489880215,
      "grad_norm": 68.17073059082031,
      "learning_rate": 0.000694,
      "loss": 4.8311,
      "step": 694
    },
    {
      "epoch": 4.5931433292028085,
      "grad_norm": 26.566598892211914,
      "learning_rate": 0.000695,
      "loss": 4.7967,
      "step": 695
    },
    {
      "epoch": 4.599752168525403,
      "grad_norm": 131.8784637451172,
      "learning_rate": 0.000696,
      "loss": 6.5036,
      "step": 696
    },
    {
      "epoch": 4.606361007847997,
      "grad_norm": 73.43547058105469,
      "learning_rate": 0.0006969999999999999,
      "loss": 3.7815,
      "step": 697
    },
    {
      "epoch": 4.612969847170591,
      "grad_norm": 41.76596450805664,
      "learning_rate": 0.0006979999999999999,
      "loss": 4.672,
      "step": 698
    },
    {
      "epoch": 4.619578686493185,
      "grad_norm": 119.26935577392578,
      "learning_rate": 0.000699,
      "loss": 5.4195,
      "step": 699
    },
    {
      "epoch": 4.626187525815778,
      "grad_norm": 43.60723114013672,
      "learning_rate": 0.0007,
      "loss": 5.3813,
      "step": 700
    },
    {
      "epoch": 4.632796365138373,
      "grad_norm": 364.17242431640625,
      "learning_rate": 0.000701,
      "loss": 17.5116,
      "step": 701
    },
    {
      "epoch": 4.639405204460966,
      "grad_norm": 255.84857177734375,
      "learning_rate": 0.0007019999999999999,
      "loss": 11.0521,
      "step": 702
    },
    {
      "epoch": 4.646014043783561,
      "grad_norm": 51.82554626464844,
      "learning_rate": 0.000703,
      "loss": 5.4885,
      "step": 703
    },
    {
      "epoch": 4.6526228831061545,
      "grad_norm": 118.89459991455078,
      "learning_rate": 0.000704,
      "loss": 4.4352,
      "step": 704
    },
    {
      "epoch": 4.659231722428748,
      "grad_norm": 129.37391662597656,
      "learning_rate": 0.000705,
      "loss": 7.7753,
      "step": 705
    },
    {
      "epoch": 4.6658405617513425,
      "grad_norm": 58.112125396728516,
      "learning_rate": 0.0007059999999999999,
      "loss": 1.707,
      "step": 706
    },
    {
      "epoch": 4.672449401073936,
      "grad_norm": 328.2498779296875,
      "learning_rate": 0.000707,
      "loss": 10.6499,
      "step": 707
    },
    {
      "epoch": 4.679058240396531,
      "grad_norm": 427.22174072265625,
      "learning_rate": 0.000708,
      "loss": 19.7489,
      "step": 708
    },
    {
      "epoch": 4.685667079719124,
      "grad_norm": 26.05126190185547,
      "learning_rate": 0.000709,
      "loss": 3.0076,
      "step": 709
    },
    {
      "epoch": 4.692275919041718,
      "grad_norm": 474.15350341796875,
      "learning_rate": 0.00071,
      "loss": 25.802,
      "step": 710
    },
    {
      "epoch": 4.698884758364312,
      "grad_norm": 571.1240844726562,
      "learning_rate": 0.0007109999999999999,
      "loss": 43.3884,
      "step": 711
    },
    {
      "epoch": 4.705493597686906,
      "grad_norm": 206.8074188232422,
      "learning_rate": 0.000712,
      "loss": 12.5261,
      "step": 712
    },
    {
      "epoch": 4.7121024370095,
      "grad_norm": 290.71490478515625,
      "learning_rate": 0.000713,
      "loss": 12.1857,
      "step": 713
    },
    {
      "epoch": 4.718711276332094,
      "grad_norm": 449.28076171875,
      "learning_rate": 0.000714,
      "loss": 24.883,
      "step": 714
    },
    {
      "epoch": 4.7253201156546885,
      "grad_norm": 29.260711669921875,
      "learning_rate": 0.000715,
      "loss": 4.9266,
      "step": 715
    },
    {
      "epoch": 4.731928954977282,
      "grad_norm": 211.64593505859375,
      "learning_rate": 0.000716,
      "loss": 15.4532,
      "step": 716
    },
    {
      "epoch": 4.7385377942998765,
      "grad_norm": 67.66960144042969,
      "learning_rate": 0.000717,
      "loss": 8.8457,
      "step": 717
    },
    {
      "epoch": 4.74514663362247,
      "grad_norm": 104.52184295654297,
      "learning_rate": 0.000718,
      "loss": 9.1612,
      "step": 718
    },
    {
      "epoch": 4.751755472945064,
      "grad_norm": 405.9548034667969,
      "learning_rate": 0.000719,
      "loss": 19.8248,
      "step": 719
    },
    {
      "epoch": 4.758364312267658,
      "grad_norm": 574.8738403320312,
      "learning_rate": 0.0007199999999999999,
      "loss": 24.2197,
      "step": 720
    },
    {
      "epoch": 4.764973151590252,
      "grad_norm": 120.14276123046875,
      "learning_rate": 0.000721,
      "loss": 9.4871,
      "step": 721
    },
    {
      "epoch": 4.771581990912846,
      "grad_norm": 138.9852752685547,
      "learning_rate": 0.000722,
      "loss": 3.7904,
      "step": 722
    },
    {
      "epoch": 4.77819083023544,
      "grad_norm": 437.1851806640625,
      "learning_rate": 0.000723,
      "loss": 16.3752,
      "step": 723
    },
    {
      "epoch": 4.7847996695580335,
      "grad_norm": 365.1669921875,
      "learning_rate": 0.000724,
      "loss": 16.2908,
      "step": 724
    },
    {
      "epoch": 4.791408508880628,
      "grad_norm": 566.8656005859375,
      "learning_rate": 0.000725,
      "loss": 26.4677,
      "step": 725
    },
    {
      "epoch": 4.798017348203222,
      "grad_norm": 180.4246063232422,
      "learning_rate": 0.000726,
      "loss": 11.6276,
      "step": 726
    },
    {
      "epoch": 4.804626187525816,
      "grad_norm": 147.82037353515625,
      "learning_rate": 0.000727,
      "loss": 3.7292,
      "step": 727
    },
    {
      "epoch": 4.81123502684841,
      "grad_norm": 257.3363037109375,
      "learning_rate": 0.000728,
      "loss": 14.5792,
      "step": 728
    },
    {
      "epoch": 4.817843866171003,
      "grad_norm": 267.217041015625,
      "learning_rate": 0.000729,
      "loss": 8.9596,
      "step": 729
    },
    {
      "epoch": 4.824452705493598,
      "grad_norm": 239.07876586914062,
      "learning_rate": 0.00073,
      "loss": 8.3872,
      "step": 730
    },
    {
      "epoch": 4.831061544816191,
      "grad_norm": 109.87870025634766,
      "learning_rate": 0.000731,
      "loss": 6.8874,
      "step": 731
    },
    {
      "epoch": 4.837670384138786,
      "grad_norm": 43.21452331542969,
      "learning_rate": 0.000732,
      "loss": 4.5458,
      "step": 732
    },
    {
      "epoch": 4.844279223461379,
      "grad_norm": 62.66123962402344,
      "learning_rate": 0.000733,
      "loss": 5.2664,
      "step": 733
    },
    {
      "epoch": 4.850888062783974,
      "grad_norm": 522.2291259765625,
      "learning_rate": 0.000734,
      "loss": 30.8406,
      "step": 734
    },
    {
      "epoch": 4.8574969021065675,
      "grad_norm": 455.4945373535156,
      "learning_rate": 0.000735,
      "loss": 22.0465,
      "step": 735
    },
    {
      "epoch": 4.864105741429162,
      "grad_norm": 160.51007080078125,
      "learning_rate": 0.000736,
      "loss": 5.9221,
      "step": 736
    },
    {
      "epoch": 4.870714580751756,
      "grad_norm": 272.92437744140625,
      "learning_rate": 0.000737,
      "loss": 8.8287,
      "step": 737
    },
    {
      "epoch": 4.877323420074349,
      "grad_norm": 378.6394958496094,
      "learning_rate": 0.000738,
      "loss": 20.2818,
      "step": 738
    },
    {
      "epoch": 4.883932259396944,
      "grad_norm": 225.633056640625,
      "learning_rate": 0.000739,
      "loss": 7.923,
      "step": 739
    },
    {
      "epoch": 4.890541098719537,
      "grad_norm": 111.32180786132812,
      "learning_rate": 0.00074,
      "loss": 6.3717,
      "step": 740
    },
    {
      "epoch": 4.897149938042132,
      "grad_norm": 241.57029724121094,
      "learning_rate": 0.000741,
      "loss": 9.1832,
      "step": 741
    },
    {
      "epoch": 4.903758777364725,
      "grad_norm": 59.56508255004883,
      "learning_rate": 0.000742,
      "loss": 4.1416,
      "step": 742
    },
    {
      "epoch": 4.910367616687319,
      "grad_norm": 155.44322204589844,
      "learning_rate": 0.0007430000000000001,
      "loss": 7.1677,
      "step": 743
    },
    {
      "epoch": 4.916976456009913,
      "grad_norm": 299.5372009277344,
      "learning_rate": 0.000744,
      "loss": 19.334,
      "step": 744
    },
    {
      "epoch": 4.923585295332507,
      "grad_norm": 126.15562438964844,
      "learning_rate": 0.000745,
      "loss": 6.139,
      "step": 745
    },
    {
      "epoch": 4.9301941346551015,
      "grad_norm": 26.998918533325195,
      "learning_rate": 0.000746,
      "loss": 2.5915,
      "step": 746
    },
    {
      "epoch": 4.936802973977695,
      "grad_norm": 82.82616424560547,
      "learning_rate": 0.000747,
      "loss": 2.0254,
      "step": 747
    },
    {
      "epoch": 4.943411813300289,
      "grad_norm": 47.181732177734375,
      "learning_rate": 0.000748,
      "loss": 4.922,
      "step": 748
    },
    {
      "epoch": 4.950020652622883,
      "grad_norm": 89.6666488647461,
      "learning_rate": 0.000749,
      "loss": 5.1468,
      "step": 749
    },
    {
      "epoch": 4.956629491945477,
      "grad_norm": 90.81011199951172,
      "learning_rate": 0.00075,
      "loss": 5.2069,
      "step": 750
    },
    {
      "epoch": 4.963238331268071,
      "grad_norm": 19.004528045654297,
      "learning_rate": 0.000751,
      "loss": 7.9167,
      "step": 751
    },
    {
      "epoch": 4.969847170590665,
      "grad_norm": 44.968116760253906,
      "learning_rate": 0.0007520000000000001,
      "loss": 5.1341,
      "step": 752
    },
    {
      "epoch": 4.976456009913259,
      "grad_norm": 137.14219665527344,
      "learning_rate": 0.000753,
      "loss": 6.5846,
      "step": 753
    },
    {
      "epoch": 4.983064849235853,
      "grad_norm": 64.890380859375,
      "learning_rate": 0.000754,
      "loss": 7.1514,
      "step": 754
    },
    {
      "epoch": 4.989673688558447,
      "grad_norm": 154.4868621826172,
      "learning_rate": 0.000755,
      "loss": 6.4622,
      "step": 755
    },
    {
      "epoch": 4.996282527881041,
      "grad_norm": 157.69398498535156,
      "learning_rate": 0.000756,
      "loss": 7.2434,
      "step": 756
    },
    {
      "epoch": 4.996282527881041,
      "eval_validation_error_bar": 0.054904883086517496,
      "eval_validation_loss": 6.653159141540527,
      "eval_validation_pearsonr": 0.4627311636690168,
      "eval_validation_rmse": 2.579371929168701,
      "eval_validation_runtime": 41.136,
      "eval_validation_samples_per_second": 4.935,
      "eval_validation_spearman": 0.5209510830605288,
      "eval_validation_steps_per_second": 4.935,
      "step": 756
    },
    {
      "epoch": 4.996282527881041,
      "eval_test_error_bar": 0.047549806582387916,
      "eval_test_loss": 9.163093566894531,
      "eval_test_pearsonr": 0.3923108997584562,
      "eval_test_rmse": 3.0270602703094482,
      "eval_test_runtime": 55.9493,
      "eval_test_samples_per_second": 5.827,
      "eval_test_spearman": 0.4263853604124666,
      "eval_test_steps_per_second": 5.827,
      "step": 756
    },
    {
      "epoch": 5.002891367203635,
      "grad_norm": 147.464599609375,
      "learning_rate": 0.000757,
      "loss": 9.2752,
      "step": 757
    },
    {
      "epoch": 5.009500206526229,
      "grad_norm": 108.15895080566406,
      "learning_rate": 0.000758,
      "loss": 3.8223,
      "step": 758
    },
    {
      "epoch": 5.016109045848823,
      "grad_norm": 181.36045837402344,
      "learning_rate": 0.000759,
      "loss": 8.8517,
      "step": 759
    },
    {
      "epoch": 5.022717885171417,
      "grad_norm": 125.04558563232422,
      "learning_rate": 0.00076,
      "loss": 5.036,
      "step": 760
    },
    {
      "epoch": 5.029326724494011,
      "grad_norm": 95.6645736694336,
      "learning_rate": 0.0007610000000000001,
      "loss": 5.4324,
      "step": 761
    },
    {
      "epoch": 5.035935563816604,
      "grad_norm": 68.8681640625,
      "learning_rate": 0.000762,
      "loss": 5.7703,
      "step": 762
    },
    {
      "epoch": 5.042544403139199,
      "grad_norm": 209.6075897216797,
      "learning_rate": 0.000763,
      "loss": 9.659,
      "step": 763
    },
    {
      "epoch": 5.049153242461792,
      "grad_norm": 124.07701110839844,
      "learning_rate": 0.000764,
      "loss": 3.9171,
      "step": 764
    },
    {
      "epoch": 5.055762081784387,
      "grad_norm": 55.94570541381836,
      "learning_rate": 0.0007650000000000001,
      "loss": 3.9458,
      "step": 765
    },
    {
      "epoch": 5.0623709211069805,
      "grad_norm": 238.1616973876953,
      "learning_rate": 0.0007660000000000001,
      "loss": 14.8044,
      "step": 766
    },
    {
      "epoch": 5.068979760429575,
      "grad_norm": 356.2200927734375,
      "learning_rate": 0.000767,
      "loss": 29.1272,
      "step": 767
    },
    {
      "epoch": 5.075588599752169,
      "grad_norm": 160.32359313964844,
      "learning_rate": 0.000768,
      "loss": 9.3348,
      "step": 768
    },
    {
      "epoch": 5.082197439074762,
      "grad_norm": 88.0669174194336,
      "learning_rate": 0.000769,
      "loss": 5.3076,
      "step": 769
    },
    {
      "epoch": 5.088806278397357,
      "grad_norm": 136.6840057373047,
      "learning_rate": 0.0007700000000000001,
      "loss": 8.5541,
      "step": 770
    },
    {
      "epoch": 5.09541511771995,
      "grad_norm": 191.47012329101562,
      "learning_rate": 0.000771,
      "loss": 9.2477,
      "step": 771
    },
    {
      "epoch": 5.102023957042545,
      "grad_norm": 65.79733276367188,
      "learning_rate": 0.000772,
      "loss": 3.1279,
      "step": 772
    },
    {
      "epoch": 5.108632796365138,
      "grad_norm": 60.83855438232422,
      "learning_rate": 0.000773,
      "loss": 2.8166,
      "step": 773
    },
    {
      "epoch": 5.115241635687732,
      "grad_norm": 189.45404052734375,
      "learning_rate": 0.0007740000000000001,
      "loss": 8.4591,
      "step": 774
    },
    {
      "epoch": 5.121850475010326,
      "grad_norm": 133.91432189941406,
      "learning_rate": 0.0007750000000000001,
      "loss": 7.4598,
      "step": 775
    },
    {
      "epoch": 5.12845931433292,
      "grad_norm": 231.4960479736328,
      "learning_rate": 0.000776,
      "loss": 14.0139,
      "step": 776
    },
    {
      "epoch": 5.1350681536555145,
      "grad_norm": 235.16131591796875,
      "learning_rate": 0.000777,
      "loss": 10.0727,
      "step": 777
    },
    {
      "epoch": 5.141676992978108,
      "grad_norm": 112.07946014404297,
      "learning_rate": 0.000778,
      "loss": 5.1096,
      "step": 778
    },
    {
      "epoch": 5.148285832300703,
      "grad_norm": 75.57762145996094,
      "learning_rate": 0.0007790000000000001,
      "loss": 3.2019,
      "step": 779
    },
    {
      "epoch": 5.154894671623296,
      "grad_norm": 85.04512023925781,
      "learning_rate": 0.0007800000000000001,
      "loss": 5.0449,
      "step": 780
    },
    {
      "epoch": 5.16150351094589,
      "grad_norm": 35.16396713256836,
      "learning_rate": 0.000781,
      "loss": 4.2239,
      "step": 781
    },
    {
      "epoch": 5.168112350268484,
      "grad_norm": 11.164974212646484,
      "learning_rate": 0.000782,
      "loss": 2.8165,
      "step": 782
    },
    {
      "epoch": 5.174721189591078,
      "grad_norm": 17.53265380859375,
      "learning_rate": 0.0007830000000000001,
      "loss": 3.1381,
      "step": 783
    },
    {
      "epoch": 5.181330028913672,
      "grad_norm": 139.33758544921875,
      "learning_rate": 0.0007840000000000001,
      "loss": 7.2304,
      "step": 784
    },
    {
      "epoch": 5.187938868236266,
      "grad_norm": 12.018011093139648,
      "learning_rate": 0.000785,
      "loss": 2.3229,
      "step": 785
    },
    {
      "epoch": 5.1945477075588595,
      "grad_norm": 237.16331481933594,
      "learning_rate": 0.000786,
      "loss": 11.3192,
      "step": 786
    },
    {
      "epoch": 5.201156546881454,
      "grad_norm": 57.31698226928711,
      "learning_rate": 0.000787,
      "loss": 10.262,
      "step": 787
    },
    {
      "epoch": 5.207765386204048,
      "grad_norm": 34.61659240722656,
      "learning_rate": 0.0007880000000000001,
      "loss": 5.1761,
      "step": 788
    },
    {
      "epoch": 5.214374225526642,
      "grad_norm": 460.2771301269531,
      "learning_rate": 0.0007890000000000001,
      "loss": 23.9546,
      "step": 789
    },
    {
      "epoch": 5.220983064849236,
      "grad_norm": 646.3961791992188,
      "learning_rate": 0.00079,
      "loss": 40.2598,
      "step": 790
    },
    {
      "epoch": 5.22759190417183,
      "grad_norm": 304.0741882324219,
      "learning_rate": 0.000791,
      "loss": 11.8541,
      "step": 791
    },
    {
      "epoch": 5.234200743494424,
      "grad_norm": 139.0092315673828,
      "learning_rate": 0.0007920000000000001,
      "loss": 7.4946,
      "step": 792
    },
    {
      "epoch": 5.240809582817017,
      "grad_norm": 226.7144012451172,
      "learning_rate": 0.0007930000000000001,
      "loss": 14.646,
      "step": 793
    },
    {
      "epoch": 5.247418422139612,
      "grad_norm": 38.23784255981445,
      "learning_rate": 0.0007940000000000001,
      "loss": 4.4974,
      "step": 794
    },
    {
      "epoch": 5.2540272614622054,
      "grad_norm": 222.02090454101562,
      "learning_rate": 0.000795,
      "loss": 13.2531,
      "step": 795
    },
    {
      "epoch": 5.2606361007848,
      "grad_norm": 245.58912658691406,
      "learning_rate": 0.000796,
      "loss": 15.9806,
      "step": 796
    },
    {
      "epoch": 5.2672449401073935,
      "grad_norm": 115.18850708007812,
      "learning_rate": 0.0007970000000000001,
      "loss": 6.3739,
      "step": 797
    },
    {
      "epoch": 5.273853779429988,
      "grad_norm": 124.87824249267578,
      "learning_rate": 0.0007980000000000001,
      "loss": 9.6348,
      "step": 798
    },
    {
      "epoch": 5.280462618752582,
      "grad_norm": 229.28855895996094,
      "learning_rate": 0.000799,
      "loss": 13.3102,
      "step": 799
    },
    {
      "epoch": 5.287071458075175,
      "grad_norm": 97.08261108398438,
      "learning_rate": 0.0008,
      "loss": 6.3009,
      "step": 800
    },
    {
      "epoch": 5.29368029739777,
      "grad_norm": 52.8040771484375,
      "learning_rate": 0.0008010000000000001,
      "loss": 2.9091,
      "step": 801
    },
    {
      "epoch": 5.300289136720363,
      "grad_norm": 193.0927734375,
      "learning_rate": 0.0008020000000000001,
      "loss": 10.3787,
      "step": 802
    },
    {
      "epoch": 5.306897976042958,
      "grad_norm": 107.78668975830078,
      "learning_rate": 0.0008030000000000001,
      "loss": 6.1084,
      "step": 803
    },
    {
      "epoch": 5.313506815365551,
      "grad_norm": 36.17817306518555,
      "learning_rate": 0.000804,
      "loss": 4.3166,
      "step": 804
    },
    {
      "epoch": 5.320115654688145,
      "grad_norm": 95.23432159423828,
      "learning_rate": 0.000805,
      "loss": 6.8257,
      "step": 805
    },
    {
      "epoch": 5.326724494010739,
      "grad_norm": 143.1351776123047,
      "learning_rate": 0.0008060000000000001,
      "loss": 8.8622,
      "step": 806
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 119.36624908447266,
      "learning_rate": 0.0008070000000000001,
      "loss": 10.2488,
      "step": 807
    },
    {
      "epoch": 5.3399421726559275,
      "grad_norm": 12.641338348388672,
      "learning_rate": 0.000808,
      "loss": 2.219,
      "step": 808
    },
    {
      "epoch": 5.346551011978521,
      "grad_norm": 183.77255249023438,
      "learning_rate": 0.000809,
      "loss": 12.6289,
      "step": 809
    },
    {
      "epoch": 5.353159851301116,
      "grad_norm": 270.02288818359375,
      "learning_rate": 0.0008100000000000001,
      "loss": 14.9255,
      "step": 810
    },
    {
      "epoch": 5.359768690623709,
      "grad_norm": 177.5732879638672,
      "learning_rate": 0.0008110000000000001,
      "loss": 10.3481,
      "step": 811
    },
    {
      "epoch": 5.366377529946303,
      "grad_norm": 94.91606140136719,
      "learning_rate": 0.0008120000000000001,
      "loss": 6.1374,
      "step": 812
    },
    {
      "epoch": 5.372986369268897,
      "grad_norm": 109.59383392333984,
      "learning_rate": 0.0008129999999999999,
      "loss": 8.7541,
      "step": 813
    },
    {
      "epoch": 5.379595208591491,
      "grad_norm": 55.87261199951172,
      "learning_rate": 0.0008139999999999999,
      "loss": 2.3468,
      "step": 814
    },
    {
      "epoch": 5.386204047914085,
      "grad_norm": 32.73454284667969,
      "learning_rate": 0.000815,
      "loss": 4.951,
      "step": 815
    },
    {
      "epoch": 5.392812887236679,
      "grad_norm": 144.8616180419922,
      "learning_rate": 0.000816,
      "loss": 5.981,
      "step": 816
    },
    {
      "epoch": 5.399421726559273,
      "grad_norm": 6.941190719604492,
      "learning_rate": 0.000817,
      "loss": 2.0456,
      "step": 817
    },
    {
      "epoch": 5.406030565881867,
      "grad_norm": 95.07264709472656,
      "learning_rate": 0.0008179999999999999,
      "loss": 7.3639,
      "step": 818
    },
    {
      "epoch": 5.412639405204461,
      "grad_norm": 46.92625045776367,
      "learning_rate": 0.000819,
      "loss": 11.5045,
      "step": 819
    },
    {
      "epoch": 5.419248244527055,
      "grad_norm": 38.86576843261719,
      "learning_rate": 0.00082,
      "loss": 3.4329,
      "step": 820
    },
    {
      "epoch": 5.425857083849649,
      "grad_norm": 68.16606903076172,
      "learning_rate": 0.000821,
      "loss": 3.1527,
      "step": 821
    },
    {
      "epoch": 5.432465923172243,
      "grad_norm": 151.68798828125,
      "learning_rate": 0.0008219999999999999,
      "loss": 8.4761,
      "step": 822
    },
    {
      "epoch": 5.439074762494837,
      "grad_norm": 17.68881607055664,
      "learning_rate": 0.000823,
      "loss": 3.5521,
      "step": 823
    },
    {
      "epoch": 5.44568360181743,
      "grad_norm": 16.843242645263672,
      "learning_rate": 0.000824,
      "loss": 5.0231,
      "step": 824
    },
    {
      "epoch": 5.452292441140025,
      "grad_norm": 161.23439025878906,
      "learning_rate": 0.000825,
      "loss": 8.223,
      "step": 825
    },
    {
      "epoch": 5.4589012804626185,
      "grad_norm": 69.8136978149414,
      "learning_rate": 0.000826,
      "loss": 5.2694,
      "step": 826
    },
    {
      "epoch": 5.465510119785213,
      "grad_norm": 28.034292221069336,
      "learning_rate": 0.0008269999999999999,
      "loss": 1.9767,
      "step": 827
    },
    {
      "epoch": 5.4721189591078065,
      "grad_norm": 41.55498504638672,
      "learning_rate": 0.000828,
      "loss": 3.9032,
      "step": 828
    },
    {
      "epoch": 5.478727798430401,
      "grad_norm": 31.100858688354492,
      "learning_rate": 0.000829,
      "loss": 4.8323,
      "step": 829
    },
    {
      "epoch": 5.485336637752995,
      "grad_norm": 143.88400268554688,
      "learning_rate": 0.00083,
      "loss": 4.0105,
      "step": 830
    },
    {
      "epoch": 5.491945477075588,
      "grad_norm": 122.696044921875,
      "learning_rate": 0.0008309999999999999,
      "loss": 6.0693,
      "step": 831
    },
    {
      "epoch": 5.498554316398183,
      "grad_norm": 153.6737823486328,
      "learning_rate": 0.000832,
      "loss": 8.1395,
      "step": 832
    },
    {
      "epoch": 5.505163155720776,
      "grad_norm": 40.47103500366211,
      "learning_rate": 0.000833,
      "loss": 3.4565,
      "step": 833
    },
    {
      "epoch": 5.511771995043371,
      "grad_norm": 23.4041805267334,
      "learning_rate": 0.000834,
      "loss": 1.7698,
      "step": 834
    },
    {
      "epoch": 5.518380834365964,
      "grad_norm": 185.20062255859375,
      "learning_rate": 0.000835,
      "loss": 10.5203,
      "step": 835
    },
    {
      "epoch": 5.524989673688559,
      "grad_norm": 207.48374938964844,
      "learning_rate": 0.0008359999999999999,
      "loss": 13.4557,
      "step": 836
    },
    {
      "epoch": 5.5315985130111525,
      "grad_norm": 92.40452575683594,
      "learning_rate": 0.000837,
      "loss": 3.6338,
      "step": 837
    },
    {
      "epoch": 5.538207352333746,
      "grad_norm": 190.8096160888672,
      "learning_rate": 0.000838,
      "loss": 9.6491,
      "step": 838
    },
    {
      "epoch": 5.5448161916563405,
      "grad_norm": 172.6166229248047,
      "learning_rate": 0.000839,
      "loss": 6.0619,
      "step": 839
    },
    {
      "epoch": 5.551425030978934,
      "grad_norm": 72.78270721435547,
      "learning_rate": 0.00084,
      "loss": 2.9425,
      "step": 840
    },
    {
      "epoch": 5.558033870301529,
      "grad_norm": 39.95254135131836,
      "learning_rate": 0.000841,
      "loss": 2.9062,
      "step": 841
    },
    {
      "epoch": 5.564642709624122,
      "grad_norm": 106.69712829589844,
      "learning_rate": 0.000842,
      "loss": 10.5943,
      "step": 842
    },
    {
      "epoch": 5.571251548946716,
      "grad_norm": 86.524169921875,
      "learning_rate": 0.000843,
      "loss": 3.5819,
      "step": 843
    },
    {
      "epoch": 5.57786038826931,
      "grad_norm": 75.52234649658203,
      "learning_rate": 0.000844,
      "loss": 6.0102,
      "step": 844
    },
    {
      "epoch": 5.584469227591904,
      "grad_norm": 56.90443801879883,
      "learning_rate": 0.0008449999999999999,
      "loss": 6.9682,
      "step": 845
    },
    {
      "epoch": 5.591078066914498,
      "grad_norm": 81.32128143310547,
      "learning_rate": 0.000846,
      "loss": 3.387,
      "step": 846
    },
    {
      "epoch": 5.597686906237092,
      "grad_norm": 36.392940521240234,
      "learning_rate": 0.000847,
      "loss": 2.0049,
      "step": 847
    },
    {
      "epoch": 5.6042957455596865,
      "grad_norm": 41.401161193847656,
      "learning_rate": 0.000848,
      "loss": 5.468,
      "step": 848
    },
    {
      "epoch": 5.61090458488228,
      "grad_norm": 18.581058502197266,
      "learning_rate": 0.000849,
      "loss": 4.7529,
      "step": 849
    },
    {
      "epoch": 5.617513424204874,
      "grad_norm": 15.004202842712402,
      "learning_rate": 0.00085,
      "loss": 1.9046,
      "step": 850
    },
    {
      "epoch": 5.624122263527468,
      "grad_norm": 44.484107971191406,
      "learning_rate": 0.000851,
      "loss": 3.5966,
      "step": 851
    },
    {
      "epoch": 5.630731102850062,
      "grad_norm": 127.05042266845703,
      "learning_rate": 0.000852,
      "loss": 6.4419,
      "step": 852
    },
    {
      "epoch": 5.637339942172656,
      "grad_norm": 90.30733489990234,
      "learning_rate": 0.000853,
      "loss": 3.6487,
      "step": 853
    },
    {
      "epoch": 5.64394878149525,
      "grad_norm": 23.760087966918945,
      "learning_rate": 0.000854,
      "loss": 5.3052,
      "step": 854
    },
    {
      "epoch": 5.650557620817844,
      "grad_norm": 80.9106216430664,
      "learning_rate": 0.000855,
      "loss": 3.6274,
      "step": 855
    },
    {
      "epoch": 5.657166460140438,
      "grad_norm": 201.14141845703125,
      "learning_rate": 0.000856,
      "loss": 9.9603,
      "step": 856
    },
    {
      "epoch": 5.6637752994630315,
      "grad_norm": 135.01312255859375,
      "learning_rate": 0.000857,
      "loss": 6.6629,
      "step": 857
    },
    {
      "epoch": 5.670384138785626,
      "grad_norm": 82.0435791015625,
      "learning_rate": 0.000858,
      "loss": 3.9015,
      "step": 858
    },
    {
      "epoch": 5.67699297810822,
      "grad_norm": 240.0865020751953,
      "learning_rate": 0.000859,
      "loss": 15.7497,
      "step": 859
    },
    {
      "epoch": 5.683601817430814,
      "grad_norm": 88.8701171875,
      "learning_rate": 0.00086,
      "loss": 5.9541,
      "step": 860
    },
    {
      "epoch": 5.690210656753408,
      "grad_norm": 96.6608657836914,
      "learning_rate": 0.000861,
      "loss": 6.4146,
      "step": 861
    },
    {
      "epoch": 5.696819496076001,
      "grad_norm": 173.94520568847656,
      "learning_rate": 0.000862,
      "loss": 9.3723,
      "step": 862
    },
    {
      "epoch": 5.703428335398596,
      "grad_norm": 34.073028564453125,
      "learning_rate": 0.000863,
      "loss": 4.5621,
      "step": 863
    },
    {
      "epoch": 5.710037174721189,
      "grad_norm": 28.367971420288086,
      "learning_rate": 0.000864,
      "loss": 6.5848,
      "step": 864
    },
    {
      "epoch": 5.716646014043784,
      "grad_norm": 109.01390075683594,
      "learning_rate": 0.000865,
      "loss": 11.1406,
      "step": 865
    },
    {
      "epoch": 5.723254853366377,
      "grad_norm": 48.593040466308594,
      "learning_rate": 0.000866,
      "loss": 5.0141,
      "step": 866
    },
    {
      "epoch": 5.729863692688972,
      "grad_norm": 47.09342956542969,
      "learning_rate": 0.000867,
      "loss": 5.3203,
      "step": 867
    },
    {
      "epoch": 5.7364725320115655,
      "grad_norm": 136.78176879882812,
      "learning_rate": 0.0008680000000000001,
      "loss": 5.4258,
      "step": 868
    },
    {
      "epoch": 5.743081371334159,
      "grad_norm": 205.52687072753906,
      "learning_rate": 0.000869,
      "loss": 9.1058,
      "step": 869
    },
    {
      "epoch": 5.749690210656754,
      "grad_norm": 108.05927276611328,
      "learning_rate": 0.00087,
      "loss": 3.264,
      "step": 870
    },
    {
      "epoch": 5.756299049979347,
      "grad_norm": 125.77503204345703,
      "learning_rate": 0.000871,
      "loss": 8.3637,
      "step": 871
    },
    {
      "epoch": 5.762907889301942,
      "grad_norm": 132.63221740722656,
      "learning_rate": 0.000872,
      "loss": 7.92,
      "step": 872
    },
    {
      "epoch": 5.769516728624535,
      "grad_norm": 131.6206512451172,
      "learning_rate": 0.000873,
      "loss": 11.7846,
      "step": 873
    },
    {
      "epoch": 5.77612556794713,
      "grad_norm": 70.4365005493164,
      "learning_rate": 0.000874,
      "loss": 5.1258,
      "step": 874
    },
    {
      "epoch": 5.782734407269723,
      "grad_norm": 17.72467041015625,
      "learning_rate": 0.000875,
      "loss": 3.209,
      "step": 875
    },
    {
      "epoch": 5.789343246592317,
      "grad_norm": 53.40114974975586,
      "learning_rate": 0.000876,
      "loss": 2.895,
      "step": 876
    },
    {
      "epoch": 5.795952085914911,
      "grad_norm": 31.43816375732422,
      "learning_rate": 0.0008770000000000001,
      "loss": 3.5622,
      "step": 877
    },
    {
      "epoch": 5.802560925237505,
      "grad_norm": 167.02919006347656,
      "learning_rate": 0.000878,
      "loss": 7.3422,
      "step": 878
    },
    {
      "epoch": 5.8091697645600995,
      "grad_norm": 126.96000671386719,
      "learning_rate": 0.000879,
      "loss": 4.2755,
      "step": 879
    },
    {
      "epoch": 5.815778603882693,
      "grad_norm": 138.120849609375,
      "learning_rate": 0.00088,
      "loss": 5.3893,
      "step": 880
    },
    {
      "epoch": 5.822387443205287,
      "grad_norm": 60.340599060058594,
      "learning_rate": 0.0008810000000000001,
      "loss": 2.704,
      "step": 881
    },
    {
      "epoch": 5.828996282527881,
      "grad_norm": 138.960693359375,
      "learning_rate": 0.000882,
      "loss": 7.772,
      "step": 882
    },
    {
      "epoch": 5.835605121850475,
      "grad_norm": 36.971893310546875,
      "learning_rate": 0.000883,
      "loss": 3.6687,
      "step": 883
    },
    {
      "epoch": 5.842213961173069,
      "grad_norm": 22.16175079345703,
      "learning_rate": 0.000884,
      "loss": 4.0253,
      "step": 884
    },
    {
      "epoch": 5.848822800495663,
      "grad_norm": 36.42028045654297,
      "learning_rate": 0.000885,
      "loss": 1.859,
      "step": 885
    },
    {
      "epoch": 5.855431639818256,
      "grad_norm": 20.205303192138672,
      "learning_rate": 0.0008860000000000001,
      "loss": 2.5279,
      "step": 886
    },
    {
      "epoch": 5.862040479140851,
      "grad_norm": 126.60774230957031,
      "learning_rate": 0.000887,
      "loss": 5.3202,
      "step": 887
    },
    {
      "epoch": 5.8686493184634445,
      "grad_norm": 94.64606475830078,
      "learning_rate": 0.000888,
      "loss": 3.64,
      "step": 888
    },
    {
      "epoch": 5.875258157786039,
      "grad_norm": 233.24070739746094,
      "learning_rate": 0.000889,
      "loss": 12.1333,
      "step": 889
    },
    {
      "epoch": 5.881866997108633,
      "grad_norm": 175.06478881835938,
      "learning_rate": 0.0008900000000000001,
      "loss": 7.6739,
      "step": 890
    },
    {
      "epoch": 5.888475836431227,
      "grad_norm": 93.75464630126953,
      "learning_rate": 0.0008910000000000001,
      "loss": 5.9478,
      "step": 891
    },
    {
      "epoch": 5.895084675753821,
      "grad_norm": 43.727657318115234,
      "learning_rate": 0.000892,
      "loss": 4.8928,
      "step": 892
    },
    {
      "epoch": 5.901693515076415,
      "grad_norm": 162.26564025878906,
      "learning_rate": 0.000893,
      "loss": 10.1673,
      "step": 893
    },
    {
      "epoch": 5.908302354399009,
      "grad_norm": 73.9069595336914,
      "learning_rate": 0.000894,
      "loss": 4.5476,
      "step": 894
    },
    {
      "epoch": 5.914911193721602,
      "grad_norm": 25.750173568725586,
      "learning_rate": 0.0008950000000000001,
      "loss": 5.5554,
      "step": 895
    },
    {
      "epoch": 5.921520033044197,
      "grad_norm": 209.72824096679688,
      "learning_rate": 0.000896,
      "loss": 9.7347,
      "step": 896
    },
    {
      "epoch": 5.92812887236679,
      "grad_norm": 61.666908264160156,
      "learning_rate": 0.000897,
      "loss": 1.936,
      "step": 897
    },
    {
      "epoch": 5.934737711689385,
      "grad_norm": 67.62091827392578,
      "learning_rate": 0.000898,
      "loss": 7.4749,
      "step": 898
    },
    {
      "epoch": 5.9413465510119785,
      "grad_norm": 8.949772834777832,
      "learning_rate": 0.0008990000000000001,
      "loss": 1.8768,
      "step": 899
    },
    {
      "epoch": 5.947955390334572,
      "grad_norm": 39.904151916503906,
      "learning_rate": 0.0009000000000000001,
      "loss": 5.1279,
      "step": 900
    },
    {
      "epoch": 5.954564229657167,
      "grad_norm": 181.41421508789062,
      "learning_rate": 0.000901,
      "loss": 8.9095,
      "step": 901
    },
    {
      "epoch": 5.96117306897976,
      "grad_norm": 129.6778106689453,
      "learning_rate": 0.000902,
      "loss": 9.1584,
      "step": 902
    },
    {
      "epoch": 5.967781908302355,
      "grad_norm": 34.8974609375,
      "learning_rate": 0.000903,
      "loss": 6.0781,
      "step": 903
    },
    {
      "epoch": 5.974390747624948,
      "grad_norm": 118.309326171875,
      "learning_rate": 0.0009040000000000001,
      "loss": 3.9467,
      "step": 904
    },
    {
      "epoch": 5.980999586947542,
      "grad_norm": 79.15198516845703,
      "learning_rate": 0.0009050000000000001,
      "loss": 3.8643,
      "step": 905
    },
    {
      "epoch": 5.987608426270136,
      "grad_norm": 122.15381622314453,
      "learning_rate": 0.000906,
      "loss": 5.7766,
      "step": 906
    },
    {
      "epoch": 5.99421726559273,
      "grad_norm": 150.60159301757812,
      "learning_rate": 0.000907,
      "loss": 7.3747,
      "step": 907
    },
    {
      "epoch": 5.99421726559273,
      "eval_validation_error_bar": 0.052715003439434555,
      "eval_validation_loss": 7.192422866821289,
      "eval_validation_pearsonr": 0.5526615358621444,
      "eval_validation_rmse": 2.6818692684173584,
      "eval_validation_runtime": 41.8723,
      "eval_validation_samples_per_second": 4.848,
      "eval_validation_spearman": 0.5528747668936826,
      "eval_validation_steps_per_second": 4.848,
      "step": 907
    },
    {
      "epoch": 5.99421726559273,
      "eval_test_error_bar": 0.054441968854618265,
      "eval_test_loss": 11.138614654541016,
      "eval_test_pearsonr": 0.183583055189835,
      "eval_test_rmse": 3.337456226348877,
      "eval_test_runtime": 57.4749,
      "eval_test_samples_per_second": 5.672,
      "eval_test_spearman": 0.1686471042334658,
      "eval_test_steps_per_second": 5.672,
      "step": 907
    },
    {
      "epoch": 6.000826104915324,
      "grad_norm": 28.584787368774414,
      "learning_rate": 0.0009080000000000001,
      "loss": 6.0573,
      "step": 908
    },
    {
      "epoch": 6.007434944237918,
      "grad_norm": 182.46316528320312,
      "learning_rate": 0.0009090000000000001,
      "loss": 6.0173,
      "step": 909
    },
    {
      "epoch": 6.0140437835605125,
      "grad_norm": 117.32075500488281,
      "learning_rate": 0.00091,
      "loss": 5.7218,
      "step": 910
    },
    {
      "epoch": 6.020652622883106,
      "grad_norm": 65.68175506591797,
      "learning_rate": 0.000911,
      "loss": 3.0906,
      "step": 911
    },
    {
      "epoch": 6.0272614622057,
      "grad_norm": 267.23590087890625,
      "learning_rate": 0.000912,
      "loss": 11.9416,
      "step": 912
    },
    {
      "epoch": 6.033870301528294,
      "grad_norm": 398.51898193359375,
      "learning_rate": 0.0009130000000000001,
      "loss": 22.3757,
      "step": 913
    },
    {
      "epoch": 6.040479140850888,
      "grad_norm": 434.9415588378906,
      "learning_rate": 0.0009140000000000001,
      "loss": 22.5615,
      "step": 914
    },
    {
      "epoch": 6.047087980173482,
      "grad_norm": 35.28641128540039,
      "learning_rate": 0.000915,
      "loss": 2.2786,
      "step": 915
    },
    {
      "epoch": 6.053696819496076,
      "grad_norm": 32.243831634521484,
      "learning_rate": 0.000916,
      "loss": 4.4499,
      "step": 916
    },
    {
      "epoch": 6.06030565881867,
      "grad_norm": 46.22231674194336,
      "learning_rate": 0.0009170000000000001,
      "loss": 3.3202,
      "step": 917
    },
    {
      "epoch": 6.066914498141264,
      "grad_norm": 135.6619110107422,
      "learning_rate": 0.0009180000000000001,
      "loss": 4.0421,
      "step": 918
    },
    {
      "epoch": 6.0735233374638575,
      "grad_norm": 159.81341552734375,
      "learning_rate": 0.0009190000000000001,
      "loss": 6.5687,
      "step": 919
    },
    {
      "epoch": 6.080132176786452,
      "grad_norm": 126.06529998779297,
      "learning_rate": 0.00092,
      "loss": 6.5169,
      "step": 920
    },
    {
      "epoch": 6.086741016109046,
      "grad_norm": 175.9188232421875,
      "learning_rate": 0.000921,
      "loss": 7.6897,
      "step": 921
    },
    {
      "epoch": 6.09334985543164,
      "grad_norm": 302.37066650390625,
      "learning_rate": 0.0009220000000000001,
      "loss": 15.5026,
      "step": 922
    },
    {
      "epoch": 6.099958694754234,
      "grad_norm": 225.7633514404297,
      "learning_rate": 0.0009230000000000001,
      "loss": 9.9402,
      "step": 923
    },
    {
      "epoch": 6.106567534076828,
      "grad_norm": 71.81246185302734,
      "learning_rate": 0.000924,
      "loss": 7.0046,
      "step": 924
    },
    {
      "epoch": 6.113176373399422,
      "grad_norm": 61.27899169921875,
      "learning_rate": 0.000925,
      "loss": 3.2569,
      "step": 925
    },
    {
      "epoch": 6.119785212722015,
      "grad_norm": 40.44935989379883,
      "learning_rate": 0.0009260000000000001,
      "loss": 2.7777,
      "step": 926
    },
    {
      "epoch": 6.12639405204461,
      "grad_norm": 73.20319366455078,
      "learning_rate": 0.0009270000000000001,
      "loss": 3.727,
      "step": 927
    },
    {
      "epoch": 6.1330028913672034,
      "grad_norm": 53.72190856933594,
      "learning_rate": 0.0009280000000000001,
      "loss": 4.5629,
      "step": 928
    },
    {
      "epoch": 6.139611730689798,
      "grad_norm": 74.21076965332031,
      "learning_rate": 0.000929,
      "loss": 5.5693,
      "step": 929
    },
    {
      "epoch": 6.1462205700123915,
      "grad_norm": 174.49795532226562,
      "learning_rate": 0.00093,
      "loss": 10.2259,
      "step": 930
    },
    {
      "epoch": 6.152829409334985,
      "grad_norm": 18.793384552001953,
      "learning_rate": 0.0009310000000000001,
      "loss": 2.2545,
      "step": 931
    },
    {
      "epoch": 6.15943824865758,
      "grad_norm": 218.87254333496094,
      "learning_rate": 0.0009320000000000001,
      "loss": 8.1395,
      "step": 932
    },
    {
      "epoch": 6.166047087980173,
      "grad_norm": 240.5160675048828,
      "learning_rate": 0.000933,
      "loss": 7.6544,
      "step": 933
    },
    {
      "epoch": 6.172655927302768,
      "grad_norm": 178.21636962890625,
      "learning_rate": 0.000934,
      "loss": 10.1188,
      "step": 934
    },
    {
      "epoch": 6.179264766625361,
      "grad_norm": 90.88908386230469,
      "learning_rate": 0.0009350000000000001,
      "loss": 2.8381,
      "step": 935
    },
    {
      "epoch": 6.185873605947956,
      "grad_norm": 79.73834991455078,
      "learning_rate": 0.0009360000000000001,
      "loss": 3.6924,
      "step": 936
    },
    {
      "epoch": 6.192482445270549,
      "grad_norm": 23.32715606689453,
      "learning_rate": 0.0009370000000000001,
      "loss": 3.9304,
      "step": 937
    },
    {
      "epoch": 6.199091284593143,
      "grad_norm": 16.59770965576172,
      "learning_rate": 0.0009379999999999999,
      "loss": 2.4741,
      "step": 938
    },
    {
      "epoch": 6.205700123915737,
      "grad_norm": 35.130348205566406,
      "learning_rate": 0.000939,
      "loss": 1.7534,
      "step": 939
    },
    {
      "epoch": 6.212308963238331,
      "grad_norm": 43.210628509521484,
      "learning_rate": 0.00094,
      "loss": 2.144,
      "step": 940
    },
    {
      "epoch": 6.2189178025609255,
      "grad_norm": 49.04817581176758,
      "learning_rate": 0.000941,
      "loss": 2.2438,
      "step": 941
    },
    {
      "epoch": 6.225526641883519,
      "grad_norm": 213.70326232910156,
      "learning_rate": 0.000942,
      "loss": 9.4177,
      "step": 942
    },
    {
      "epoch": 6.232135481206114,
      "grad_norm": 321.99627685546875,
      "learning_rate": 0.0009429999999999999,
      "loss": 17.5407,
      "step": 943
    },
    {
      "epoch": 6.238744320528707,
      "grad_norm": 251.361572265625,
      "learning_rate": 0.000944,
      "loss": 12.6092,
      "step": 944
    },
    {
      "epoch": 6.245353159851301,
      "grad_norm": 60.89594268798828,
      "learning_rate": 0.000945,
      "loss": 5.1452,
      "step": 945
    },
    {
      "epoch": 6.251961999173895,
      "grad_norm": 89.90880584716797,
      "learning_rate": 0.000946,
      "loss": 6.2073,
      "step": 946
    },
    {
      "epoch": 6.258570838496489,
      "grad_norm": 121.8410873413086,
      "learning_rate": 0.0009469999999999999,
      "loss": 5.3977,
      "step": 947
    },
    {
      "epoch": 6.265179677819083,
      "grad_norm": 160.87591552734375,
      "learning_rate": 0.000948,
      "loss": 6.1421,
      "step": 948
    },
    {
      "epoch": 6.271788517141677,
      "grad_norm": 10.344257354736328,
      "learning_rate": 0.000949,
      "loss": 3.5216,
      "step": 949
    },
    {
      "epoch": 6.2783973564642706,
      "grad_norm": 22.42462730407715,
      "learning_rate": 0.00095,
      "loss": 4.0445,
      "step": 950
    },
    {
      "epoch": 6.285006195786865,
      "grad_norm": 48.575950622558594,
      "learning_rate": 0.000951,
      "loss": 8.0903,
      "step": 951
    },
    {
      "epoch": 6.291615035109459,
      "grad_norm": 7.691715240478516,
      "learning_rate": 0.0009519999999999999,
      "loss": 1.2903,
      "step": 952
    },
    {
      "epoch": 6.298223874432053,
      "grad_norm": 97.01478576660156,
      "learning_rate": 0.000953,
      "loss": 4.0389,
      "step": 953
    },
    {
      "epoch": 6.304832713754647,
      "grad_norm": 33.3328742980957,
      "learning_rate": 0.000954,
      "loss": 4.1503,
      "step": 954
    },
    {
      "epoch": 6.311441553077241,
      "grad_norm": 64.588134765625,
      "learning_rate": 0.000955,
      "loss": 3.083,
      "step": 955
    },
    {
      "epoch": 6.318050392399835,
      "grad_norm": 157.15597534179688,
      "learning_rate": 0.0009559999999999999,
      "loss": 8.0601,
      "step": 956
    },
    {
      "epoch": 6.324659231722428,
      "grad_norm": 29.83464813232422,
      "learning_rate": 0.000957,
      "loss": 2.667,
      "step": 957
    },
    {
      "epoch": 6.331268071045023,
      "grad_norm": 96.81453704833984,
      "learning_rate": 0.000958,
      "loss": 3.4772,
      "step": 958
    },
    {
      "epoch": 6.3378769103676165,
      "grad_norm": 155.015380859375,
      "learning_rate": 0.000959,
      "loss": 6.74,
      "step": 959
    },
    {
      "epoch": 6.344485749690211,
      "grad_norm": 55.95624542236328,
      "learning_rate": 0.00096,
      "loss": 4.612,
      "step": 960
    },
    {
      "epoch": 6.3510945890128045,
      "grad_norm": 186.30213928222656,
      "learning_rate": 0.0009609999999999999,
      "loss": 7.4352,
      "step": 961
    },
    {
      "epoch": 6.357703428335398,
      "grad_norm": 119.0326919555664,
      "learning_rate": 0.000962,
      "loss": 4.0986,
      "step": 962
    },
    {
      "epoch": 6.364312267657993,
      "grad_norm": 25.556222915649414,
      "learning_rate": 0.000963,
      "loss": 4.1273,
      "step": 963
    },
    {
      "epoch": 6.370921106980586,
      "grad_norm": 20.944814682006836,
      "learning_rate": 0.000964,
      "loss": 2.5579,
      "step": 964
    },
    {
      "epoch": 6.377529946303181,
      "grad_norm": 56.61045455932617,
      "learning_rate": 0.000965,
      "loss": 6.6135,
      "step": 965
    },
    {
      "epoch": 6.384138785625774,
      "grad_norm": 169.6251220703125,
      "learning_rate": 0.000966,
      "loss": 9.5038,
      "step": 966
    },
    {
      "epoch": 6.390747624948369,
      "grad_norm": 71.57643127441406,
      "learning_rate": 0.000967,
      "loss": 2.8415,
      "step": 967
    },
    {
      "epoch": 6.397356464270962,
      "grad_norm": 42.0539665222168,
      "learning_rate": 0.000968,
      "loss": 2.8569,
      "step": 968
    },
    {
      "epoch": 6.403965303593556,
      "grad_norm": 124.30549621582031,
      "learning_rate": 0.000969,
      "loss": 5.7112,
      "step": 969
    },
    {
      "epoch": 6.4105741429161505,
      "grad_norm": 108.23068237304688,
      "learning_rate": 0.0009699999999999999,
      "loss": 3.5551,
      "step": 970
    },
    {
      "epoch": 6.417182982238744,
      "grad_norm": 57.21381759643555,
      "learning_rate": 0.000971,
      "loss": 4.8029,
      "step": 971
    },
    {
      "epoch": 6.4237918215613385,
      "grad_norm": 86.26477813720703,
      "learning_rate": 0.000972,
      "loss": 4.3638,
      "step": 972
    },
    {
      "epoch": 6.430400660883932,
      "grad_norm": 89.82382202148438,
      "learning_rate": 0.000973,
      "loss": 3.1413,
      "step": 973
    },
    {
      "epoch": 6.437009500206527,
      "grad_norm": 64.6541976928711,
      "learning_rate": 0.000974,
      "loss": 3.8729,
      "step": 974
    },
    {
      "epoch": 6.44361833952912,
      "grad_norm": 147.20883178710938,
      "learning_rate": 0.000975,
      "loss": 10.9784,
      "step": 975
    },
    {
      "epoch": 6.450227178851714,
      "grad_norm": 45.105674743652344,
      "learning_rate": 0.000976,
      "loss": 2.6131,
      "step": 976
    },
    {
      "epoch": 6.456836018174308,
      "grad_norm": 22.892168045043945,
      "learning_rate": 0.000977,
      "loss": 2.9797,
      "step": 977
    },
    {
      "epoch": 6.463444857496902,
      "grad_norm": 53.56193923950195,
      "learning_rate": 0.000978,
      "loss": 3.7211,
      "step": 978
    },
    {
      "epoch": 6.470053696819496,
      "grad_norm": 59.66447067260742,
      "learning_rate": 0.000979,
      "loss": 2.9806,
      "step": 979
    },
    {
      "epoch": 6.47666253614209,
      "grad_norm": 9.264906883239746,
      "learning_rate": 0.00098,
      "loss": 0.7869,
      "step": 980
    },
    {
      "epoch": 6.483271375464684,
      "grad_norm": 44.43029022216797,
      "learning_rate": 0.000981,
      "loss": 4.8419,
      "step": 981
    },
    {
      "epoch": 6.489880214787278,
      "grad_norm": 53.20136642456055,
      "learning_rate": 0.000982,
      "loss": 6.323,
      "step": 982
    },
    {
      "epoch": 6.496489054109872,
      "grad_norm": 105.28695678710938,
      "learning_rate": 0.000983,
      "loss": 3.349,
      "step": 983
    },
    {
      "epoch": 6.503097893432466,
      "grad_norm": 128.58116149902344,
      "learning_rate": 0.000984,
      "loss": 6.2636,
      "step": 984
    },
    {
      "epoch": 6.50970673275506,
      "grad_norm": 85.75123596191406,
      "learning_rate": 0.000985,
      "loss": 4.9889,
      "step": 985
    },
    {
      "epoch": 6.516315572077654,
      "grad_norm": 108.41010284423828,
      "learning_rate": 0.0009860000000000001,
      "loss": 3.7041,
      "step": 986
    },
    {
      "epoch": 6.522924411400248,
      "grad_norm": 137.5587921142578,
      "learning_rate": 0.000987,
      "loss": 6.2073,
      "step": 987
    },
    {
      "epoch": 6.529533250722842,
      "grad_norm": 134.371826171875,
      "learning_rate": 0.000988,
      "loss": 5.1841,
      "step": 988
    },
    {
      "epoch": 6.536142090045436,
      "grad_norm": 83.6728515625,
      "learning_rate": 0.000989,
      "loss": 5.9191,
      "step": 989
    },
    {
      "epoch": 6.5427509293680295,
      "grad_norm": 106.80011749267578,
      "learning_rate": 0.00099,
      "loss": 3.1876,
      "step": 990
    },
    {
      "epoch": 6.549359768690624,
      "grad_norm": 39.17029571533203,
      "learning_rate": 0.000991,
      "loss": 2.7814,
      "step": 991
    },
    {
      "epoch": 6.555968608013218,
      "grad_norm": 19.79191780090332,
      "learning_rate": 0.000992,
      "loss": 2.5352,
      "step": 992
    },
    {
      "epoch": 6.562577447335812,
      "grad_norm": 150.4922332763672,
      "learning_rate": 0.000993,
      "loss": 5.8438,
      "step": 993
    },
    {
      "epoch": 6.569186286658406,
      "grad_norm": 23.069589614868164,
      "learning_rate": 0.000994,
      "loss": 4.1304,
      "step": 994
    },
    {
      "epoch": 6.575795125980999,
      "grad_norm": 22.882993698120117,
      "learning_rate": 0.000995,
      "loss": 2.347,
      "step": 995
    },
    {
      "epoch": 6.582403965303594,
      "grad_norm": 143.5899200439453,
      "learning_rate": 0.000996,
      "loss": 6.0125,
      "step": 996
    },
    {
      "epoch": 6.589012804626187,
      "grad_norm": 140.5593719482422,
      "learning_rate": 0.000997,
      "loss": 7.3321,
      "step": 997
    },
    {
      "epoch": 6.595621643948782,
      "grad_norm": 143.7288818359375,
      "learning_rate": 0.000998,
      "loss": 4.5179,
      "step": 998
    },
    {
      "epoch": 6.602230483271375,
      "grad_norm": 19.295019149780273,
      "learning_rate": 0.000999,
      "loss": 3.8145,
      "step": 999
    },
    {
      "epoch": 6.608839322593969,
      "grad_norm": 27.63590431213379,
      "learning_rate": 0.001,
      "loss": 2.4181,
      "step": 1000
    },
    {
      "epoch": 6.6154481619165635,
      "grad_norm": 302.5634460449219,
      "learning_rate": 0.0009995049504950494,
      "loss": 14.0626,
      "step": 1001
    },
    {
      "epoch": 6.622057001239157,
      "grad_norm": 225.69558715820312,
      "learning_rate": 0.000999009900990099,
      "loss": 9.2534,
      "step": 1002
    },
    {
      "epoch": 6.628665840561752,
      "grad_norm": 49.6151237487793,
      "learning_rate": 0.0009985148514851485,
      "loss": 7.7315,
      "step": 1003
    },
    {
      "epoch": 6.635274679884345,
      "grad_norm": 61.5510368347168,
      "learning_rate": 0.0009980198019801981,
      "loss": 6.1016,
      "step": 1004
    },
    {
      "epoch": 6.64188351920694,
      "grad_norm": 114.24559020996094,
      "learning_rate": 0.0009975247524752475,
      "loss": 6.0074,
      "step": 1005
    },
    {
      "epoch": 6.648492358529533,
      "grad_norm": 27.45611572265625,
      "learning_rate": 0.000997029702970297,
      "loss": 4.2876,
      "step": 1006
    },
    {
      "epoch": 6.655101197852127,
      "grad_norm": 41.48487091064453,
      "learning_rate": 0.0009965346534653466,
      "loss": 2.5544,
      "step": 1007
    },
    {
      "epoch": 6.661710037174721,
      "grad_norm": 96.13870239257812,
      "learning_rate": 0.000996039603960396,
      "loss": 3.2886,
      "step": 1008
    },
    {
      "epoch": 6.668318876497315,
      "grad_norm": 43.743865966796875,
      "learning_rate": 0.0009955445544554456,
      "loss": 3.6099,
      "step": 1009
    },
    {
      "epoch": 6.674927715819909,
      "grad_norm": 54.09553146362305,
      "learning_rate": 0.000995049504950495,
      "loss": 6.4613,
      "step": 1010
    },
    {
      "epoch": 6.681536555142503,
      "grad_norm": 43.497901916503906,
      "learning_rate": 0.0009945544554455445,
      "loss": 4.1515,
      "step": 1011
    },
    {
      "epoch": 6.6881453944650975,
      "grad_norm": 213.772216796875,
      "learning_rate": 0.0009940594059405941,
      "loss": 7.9253,
      "step": 1012
    },
    {
      "epoch": 6.694754233787691,
      "grad_norm": 43.047550201416016,
      "learning_rate": 0.0009935643564356435,
      "loss": 5.8284,
      "step": 1013
    },
    {
      "epoch": 6.701363073110285,
      "grad_norm": 64.76728057861328,
      "learning_rate": 0.0009930693069306932,
      "loss": 3.6733,
      "step": 1014
    },
    {
      "epoch": 6.707971912432879,
      "grad_norm": 254.5758514404297,
      "learning_rate": 0.0009925742574257426,
      "loss": 12.5942,
      "step": 1015
    },
    {
      "epoch": 6.714580751755473,
      "grad_norm": 119.85667419433594,
      "learning_rate": 0.000992079207920792,
      "loss": 4.4112,
      "step": 1016
    },
    {
      "epoch": 6.721189591078067,
      "grad_norm": 289.20892333984375,
      "learning_rate": 0.0009915841584158416,
      "loss": 10.7893,
      "step": 1017
    },
    {
      "epoch": 6.727798430400661,
      "grad_norm": 256.78662109375,
      "learning_rate": 0.000991089108910891,
      "loss": 9.1103,
      "step": 1018
    },
    {
      "epoch": 6.734407269723254,
      "grad_norm": 22.410337448120117,
      "learning_rate": 0.0009905940594059407,
      "loss": 2.0014,
      "step": 1019
    },
    {
      "epoch": 6.741016109045849,
      "grad_norm": 79.6662826538086,
      "learning_rate": 0.0009900990099009901,
      "loss": 2.5945,
      "step": 1020
    },
    {
      "epoch": 6.7476249483684425,
      "grad_norm": 18.701871871948242,
      "learning_rate": 0.0009896039603960395,
      "loss": 2.4543,
      "step": 1021
    },
    {
      "epoch": 6.754233787691037,
      "grad_norm": 110.33828735351562,
      "learning_rate": 0.0009891089108910892,
      "loss": 4.3202,
      "step": 1022
    },
    {
      "epoch": 6.760842627013631,
      "grad_norm": 25.98370361328125,
      "learning_rate": 0.0009886138613861386,
      "loss": 2.5645,
      "step": 1023
    },
    {
      "epoch": 6.767451466336225,
      "grad_norm": 74.94462585449219,
      "learning_rate": 0.0009881188118811882,
      "loss": 5.5359,
      "step": 1024
    },
    {
      "epoch": 6.774060305658819,
      "grad_norm": 304.95770263671875,
      "learning_rate": 0.0009876237623762376,
      "loss": 9.0081,
      "step": 1025
    },
    {
      "epoch": 6.780669144981412,
      "grad_norm": 256.7262878417969,
      "learning_rate": 0.000987128712871287,
      "loss": 7.9861,
      "step": 1026
    },
    {
      "epoch": 6.787277984304007,
      "grad_norm": 20.33481788635254,
      "learning_rate": 0.0009866336633663367,
      "loss": 1.8492,
      "step": 1027
    },
    {
      "epoch": 6.7938868236266,
      "grad_norm": 56.890228271484375,
      "learning_rate": 0.000986138613861386,
      "loss": 2.0603,
      "step": 1028
    },
    {
      "epoch": 6.800495662949195,
      "grad_norm": 251.86444091796875,
      "learning_rate": 0.0009856435643564357,
      "loss": 9.2705,
      "step": 1029
    },
    {
      "epoch": 6.807104502271788,
      "grad_norm": 160.57620239257812,
      "learning_rate": 0.0009851485148514852,
      "loss": 5.8842,
      "step": 1030
    },
    {
      "epoch": 6.813713341594383,
      "grad_norm": 109.9579849243164,
      "learning_rate": 0.0009846534653465346,
      "loss": 2.7562,
      "step": 1031
    },
    {
      "epoch": 6.8203221809169765,
      "grad_norm": 202.38552856445312,
      "learning_rate": 0.0009841584158415842,
      "loss": 7.6626,
      "step": 1032
    },
    {
      "epoch": 6.82693102023957,
      "grad_norm": 122.42387390136719,
      "learning_rate": 0.0009836633663366336,
      "loss": 3.6923,
      "step": 1033
    },
    {
      "epoch": 6.833539859562165,
      "grad_norm": 150.1290283203125,
      "learning_rate": 0.0009831683168316833,
      "loss": 3.9701,
      "step": 1034
    },
    {
      "epoch": 6.840148698884758,
      "grad_norm": 172.50917053222656,
      "learning_rate": 0.0009826732673267327,
      "loss": 3.9992,
      "step": 1035
    },
    {
      "epoch": 6.846757538207353,
      "grad_norm": 85.28710174560547,
      "learning_rate": 0.000982178217821782,
      "loss": 2.7805,
      "step": 1036
    },
    {
      "epoch": 6.853366377529946,
      "grad_norm": 111.19486236572266,
      "learning_rate": 0.0009816831683168317,
      "loss": 3.6265,
      "step": 1037
    },
    {
      "epoch": 6.85997521685254,
      "grad_norm": 120.92337036132812,
      "learning_rate": 0.0009811881188118811,
      "loss": 4.7854,
      "step": 1038
    },
    {
      "epoch": 6.866584056175134,
      "grad_norm": 86.03235626220703,
      "learning_rate": 0.0009806930693069308,
      "loss": 2.0631,
      "step": 1039
    },
    {
      "epoch": 6.873192895497728,
      "grad_norm": 175.54624938964844,
      "learning_rate": 0.0009801980198019802,
      "loss": 14.4942,
      "step": 1040
    },
    {
      "epoch": 6.879801734820322,
      "grad_norm": 21.01283073425293,
      "learning_rate": 0.0009797029702970296,
      "loss": 2.2346,
      "step": 1041
    },
    {
      "epoch": 6.886410574142916,
      "grad_norm": 210.68006896972656,
      "learning_rate": 0.0009792079207920793,
      "loss": 7.8533,
      "step": 1042
    },
    {
      "epoch": 6.8930194134655105,
      "grad_norm": 300.7787170410156,
      "learning_rate": 0.0009787128712871287,
      "loss": 13.2473,
      "step": 1043
    },
    {
      "epoch": 6.899628252788104,
      "grad_norm": 124.48393249511719,
      "learning_rate": 0.0009782178217821783,
      "loss": 4.2768,
      "step": 1044
    },
    {
      "epoch": 6.906237092110698,
      "grad_norm": 110.85132598876953,
      "learning_rate": 0.0009777227722772277,
      "loss": 11.0576,
      "step": 1045
    },
    {
      "epoch": 6.912845931433292,
      "grad_norm": 167.32284545898438,
      "learning_rate": 0.0009772277227722771,
      "loss": 5.7327,
      "step": 1046
    },
    {
      "epoch": 6.919454770755886,
      "grad_norm": 26.92586898803711,
      "learning_rate": 0.0009767326732673268,
      "loss": 2.8748,
      "step": 1047
    },
    {
      "epoch": 6.92606361007848,
      "grad_norm": 156.5403594970703,
      "learning_rate": 0.0009762376237623762,
      "loss": 4.8682,
      "step": 1048
    },
    {
      "epoch": 6.932672449401074,
      "grad_norm": 163.44190979003906,
      "learning_rate": 0.0009757425742574257,
      "loss": 5.3648,
      "step": 1049
    },
    {
      "epoch": 6.939281288723668,
      "grad_norm": 115.5523452758789,
      "learning_rate": 0.0009752475247524752,
      "loss": 5.999,
      "step": 1050
    },
    {
      "epoch": 6.945890128046262,
      "grad_norm": 49.70206832885742,
      "learning_rate": 0.0009747524752475248,
      "loss": 2.9665,
      "step": 1051
    },
    {
      "epoch": 6.9524989673688555,
      "grad_norm": 144.28277587890625,
      "learning_rate": 0.0009742574257425743,
      "loss": 7.9313,
      "step": 1052
    },
    {
      "epoch": 6.95910780669145,
      "grad_norm": 185.16622924804688,
      "learning_rate": 0.0009737623762376237,
      "loss": 7.5034,
      "step": 1053
    },
    {
      "epoch": 6.965716646014044,
      "grad_norm": 156.97793579101562,
      "learning_rate": 0.0009732673267326732,
      "loss": 6.8619,
      "step": 1054
    },
    {
      "epoch": 6.972325485336638,
      "grad_norm": 14.38571834564209,
      "learning_rate": 0.0009727722772277228,
      "loss": 4.0315,
      "step": 1055
    },
    {
      "epoch": 6.978934324659232,
      "grad_norm": 233.8118133544922,
      "learning_rate": 0.0009722772277227723,
      "loss": 10.8053,
      "step": 1056
    },
    {
      "epoch": 6.985543163981825,
      "grad_norm": 205.53457641601562,
      "learning_rate": 0.0009717821782178218,
      "loss": 6.2613,
      "step": 1057
    },
    {
      "epoch": 6.99215200330442,
      "grad_norm": 232.01113891601562,
      "learning_rate": 0.0009712871287128712,
      "loss": 10.18,
      "step": 1058
    },
    {
      "epoch": 6.998760842627013,
      "grad_norm": 19.810283660888672,
      "learning_rate": 0.0009707920792079208,
      "loss": 3.6983,
      "step": 1059
    },
    {
      "epoch": 6.998760842627013,
      "eval_validation_error_bar": 0.047944445584716326,
      "eval_validation_loss": 5.298923015594482,
      "eval_validation_pearsonr": 0.573812477198051,
      "eval_validation_rmse": 2.301939010620117,
      "eval_validation_runtime": 41.5759,
      "eval_validation_samples_per_second": 4.883,
      "eval_validation_spearman": 0.6149885238871963,
      "eval_validation_steps_per_second": 4.883,
      "step": 1059
    },
    {
      "epoch": 6.998760842627013,
      "eval_test_error_bar": 0.04301049778241024,
      "eval_test_loss": 7.3385090827941895,
      "eval_test_pearsonr": 0.4631799804793534,
      "eval_test_rmse": 2.708968162536621,
      "eval_test_runtime": 55.8469,
      "eval_test_samples_per_second": 5.837,
      "eval_test_spearman": 0.5246655441092021,
      "eval_test_steps_per_second": 5.837,
      "step": 1059
    },
    {
      "epoch": 7.005369681949608,
      "grad_norm": 94.63087463378906,
      "learning_rate": 0.0009702970297029703,
      "loss": 4.082,
      "step": 1060
    },
    {
      "epoch": 7.0119785212722014,
      "grad_norm": 104.30607604980469,
      "learning_rate": 0.0009698019801980198,
      "loss": 4.5668,
      "step": 1061
    },
    {
      "epoch": 7.018587360594796,
      "grad_norm": 24.0036563873291,
      "learning_rate": 0.0009693069306930693,
      "loss": 3.3368,
      "step": 1062
    },
    {
      "epoch": 7.0251961999173895,
      "grad_norm": 46.44252395629883,
      "learning_rate": 0.0009688118811881188,
      "loss": 4.1579,
      "step": 1063
    },
    {
      "epoch": 7.031805039239983,
      "grad_norm": 19.36917495727539,
      "learning_rate": 0.0009683168316831683,
      "loss": 1.6655,
      "step": 1064
    },
    {
      "epoch": 7.038413878562578,
      "grad_norm": 70.1456298828125,
      "learning_rate": 0.0009678217821782178,
      "loss": 3.4866,
      "step": 1065
    },
    {
      "epoch": 7.045022717885171,
      "grad_norm": 21.197864532470703,
      "learning_rate": 0.0009673267326732673,
      "loss": 2.2895,
      "step": 1066
    },
    {
      "epoch": 7.051631557207766,
      "grad_norm": 197.1727752685547,
      "learning_rate": 0.0009668316831683169,
      "loss": 7.3365,
      "step": 1067
    },
    {
      "epoch": 7.058240396530359,
      "grad_norm": 188.78408813476562,
      "learning_rate": 0.0009663366336633663,
      "loss": 8.9096,
      "step": 1068
    },
    {
      "epoch": 7.064849235852954,
      "grad_norm": 146.90487670898438,
      "learning_rate": 0.0009658415841584158,
      "loss": 3.4332,
      "step": 1069
    },
    {
      "epoch": 7.071458075175547,
      "grad_norm": 339.5042419433594,
      "learning_rate": 0.0009653465346534653,
      "loss": 15.417,
      "step": 1070
    },
    {
      "epoch": 7.078066914498141,
      "grad_norm": 151.47781372070312,
      "learning_rate": 0.0009648514851485149,
      "loss": 5.2461,
      "step": 1071
    },
    {
      "epoch": 7.0846757538207354,
      "grad_norm": 125.30036926269531,
      "learning_rate": 0.0009643564356435644,
      "loss": 5.0503,
      "step": 1072
    },
    {
      "epoch": 7.091284593143329,
      "grad_norm": 238.86053466796875,
      "learning_rate": 0.0009638613861386138,
      "loss": 13.2926,
      "step": 1073
    },
    {
      "epoch": 7.0978934324659235,
      "grad_norm": 114.02025604248047,
      "learning_rate": 0.0009633663366336633,
      "loss": 6.2727,
      "step": 1074
    },
    {
      "epoch": 7.104502271788517,
      "grad_norm": 50.221412658691406,
      "learning_rate": 0.0009628712871287129,
      "loss": 3.0009,
      "step": 1075
    },
    {
      "epoch": 7.111111111111111,
      "grad_norm": 26.747509002685547,
      "learning_rate": 0.0009623762376237624,
      "loss": 2.4252,
      "step": 1076
    },
    {
      "epoch": 7.117719950433705,
      "grad_norm": 14.061624526977539,
      "learning_rate": 0.0009618811881188119,
      "loss": 0.776,
      "step": 1077
    },
    {
      "epoch": 7.124328789756299,
      "grad_norm": 25.031143188476562,
      "learning_rate": 0.0009613861386138613,
      "loss": 3.4946,
      "step": 1078
    },
    {
      "epoch": 7.130937629078893,
      "grad_norm": 97.15469360351562,
      "learning_rate": 0.0009608910891089109,
      "loss": 9.9317,
      "step": 1079
    },
    {
      "epoch": 7.137546468401487,
      "grad_norm": 140.94683837890625,
      "learning_rate": 0.0009603960396039604,
      "loss": 4.9085,
      "step": 1080
    },
    {
      "epoch": 7.144155307724081,
      "grad_norm": 139.2787628173828,
      "learning_rate": 0.0009599009900990099,
      "loss": 3.6775,
      "step": 1081
    },
    {
      "epoch": 7.150764147046675,
      "grad_norm": 230.44105529785156,
      "learning_rate": 0.0009594059405940594,
      "loss": 8.7138,
      "step": 1082
    },
    {
      "epoch": 7.1573729863692686,
      "grad_norm": 221.95433044433594,
      "learning_rate": 0.0009589108910891089,
      "loss": 8.5311,
      "step": 1083
    },
    {
      "epoch": 7.163981825691863,
      "grad_norm": 275.470703125,
      "learning_rate": 0.0009584158415841584,
      "loss": 12.122,
      "step": 1084
    },
    {
      "epoch": 7.170590665014457,
      "grad_norm": 23.410751342773438,
      "learning_rate": 0.0009579207920792079,
      "loss": 2.8457,
      "step": 1085
    },
    {
      "epoch": 7.177199504337051,
      "grad_norm": 374.1790466308594,
      "learning_rate": 0.0009574257425742574,
      "loss": 19.3105,
      "step": 1086
    },
    {
      "epoch": 7.183808343659645,
      "grad_norm": 300.2003479003906,
      "learning_rate": 0.000956930693069307,
      "loss": 19.2795,
      "step": 1087
    },
    {
      "epoch": 7.190417182982239,
      "grad_norm": 81.46321868896484,
      "learning_rate": 0.0009564356435643564,
      "loss": 2.8501,
      "step": 1088
    },
    {
      "epoch": 7.197026022304833,
      "grad_norm": 86.6207504272461,
      "learning_rate": 0.0009559405940594059,
      "loss": 2.6541,
      "step": 1089
    },
    {
      "epoch": 7.203634861627426,
      "grad_norm": 207.53759765625,
      "learning_rate": 0.0009554455445544554,
      "loss": 7.2211,
      "step": 1090
    },
    {
      "epoch": 7.210243700950021,
      "grad_norm": 102.80878448486328,
      "learning_rate": 0.000954950495049505,
      "loss": 3.1431,
      "step": 1091
    },
    {
      "epoch": 7.2168525402726145,
      "grad_norm": 82.97293853759766,
      "learning_rate": 0.0009544554455445545,
      "loss": 3.568,
      "step": 1092
    },
    {
      "epoch": 7.223461379595209,
      "grad_norm": 279.0906066894531,
      "learning_rate": 0.0009539603960396039,
      "loss": 9.7474,
      "step": 1093
    },
    {
      "epoch": 7.2300702189178025,
      "grad_norm": 147.10089111328125,
      "learning_rate": 0.0009534653465346534,
      "loss": 6.4035,
      "step": 1094
    },
    {
      "epoch": 7.236679058240396,
      "grad_norm": 92.31007385253906,
      "learning_rate": 0.000952970297029703,
      "loss": 3.0352,
      "step": 1095
    },
    {
      "epoch": 7.243287897562991,
      "grad_norm": 167.56982421875,
      "learning_rate": 0.0009524752475247525,
      "loss": 7.5874,
      "step": 1096
    },
    {
      "epoch": 7.249896736885584,
      "grad_norm": 163.56060791015625,
      "learning_rate": 0.000951980198019802,
      "loss": 5.8649,
      "step": 1097
    },
    {
      "epoch": 7.256505576208179,
      "grad_norm": 91.5966796875,
      "learning_rate": 0.0009514851485148514,
      "loss": 3.1855,
      "step": 1098
    },
    {
      "epoch": 7.263114415530772,
      "grad_norm": 51.28579330444336,
      "learning_rate": 0.0009509900990099009,
      "loss": 2.5795,
      "step": 1099
    },
    {
      "epoch": 7.269723254853367,
      "grad_norm": 162.6549072265625,
      "learning_rate": 0.0009504950495049505,
      "loss": 5.9062,
      "step": 1100
    },
    {
      "epoch": 7.27633209417596,
      "grad_norm": 144.720703125,
      "learning_rate": 0.00095,
      "loss": 6.9593,
      "step": 1101
    },
    {
      "epoch": 7.282940933498554,
      "grad_norm": 148.50259399414062,
      "learning_rate": 0.0009495049504950495,
      "loss": 4.7341,
      "step": 1102
    },
    {
      "epoch": 7.2895497728211485,
      "grad_norm": 65.05645751953125,
      "learning_rate": 0.0009490099009900989,
      "loss": 5.3272,
      "step": 1103
    },
    {
      "epoch": 7.296158612143742,
      "grad_norm": 88.96312713623047,
      "learning_rate": 0.0009485148514851485,
      "loss": 4.4804,
      "step": 1104
    },
    {
      "epoch": 7.3027674514663365,
      "grad_norm": 157.16122436523438,
      "learning_rate": 0.000948019801980198,
      "loss": 5.1239,
      "step": 1105
    },
    {
      "epoch": 7.30937629078893,
      "grad_norm": 151.50271606445312,
      "learning_rate": 0.0009475247524752475,
      "loss": 4.9225,
      "step": 1106
    },
    {
      "epoch": 7.315985130111525,
      "grad_norm": 56.49740219116211,
      "learning_rate": 0.000947029702970297,
      "loss": 4.2179,
      "step": 1107
    },
    {
      "epoch": 7.322593969434118,
      "grad_norm": 28.431812286376953,
      "learning_rate": 0.0009465346534653465,
      "loss": 0.9454,
      "step": 1108
    },
    {
      "epoch": 7.329202808756712,
      "grad_norm": 12.102254867553711,
      "learning_rate": 0.000946039603960396,
      "loss": 1.6042,
      "step": 1109
    },
    {
      "epoch": 7.335811648079306,
      "grad_norm": 74.44225311279297,
      "learning_rate": 0.0009455445544554455,
      "loss": 5.4869,
      "step": 1110
    },
    {
      "epoch": 7.3424204874019,
      "grad_norm": 29.007497787475586,
      "learning_rate": 0.000945049504950495,
      "loss": 3.2827,
      "step": 1111
    },
    {
      "epoch": 7.349029326724494,
      "grad_norm": 58.78839111328125,
      "learning_rate": 0.0009445544554455446,
      "loss": 2.0785,
      "step": 1112
    },
    {
      "epoch": 7.355638166047088,
      "grad_norm": 44.241798400878906,
      "learning_rate": 0.000944059405940594,
      "loss": 2.0668,
      "step": 1113
    },
    {
      "epoch": 7.362247005369682,
      "grad_norm": 54.42753219604492,
      "learning_rate": 0.0009435643564356435,
      "loss": 1.3847,
      "step": 1114
    },
    {
      "epoch": 7.368855844692276,
      "grad_norm": 155.88568115234375,
      "learning_rate": 0.000943069306930693,
      "loss": 4.2409,
      "step": 1115
    },
    {
      "epoch": 7.37546468401487,
      "grad_norm": 96.12556457519531,
      "learning_rate": 0.0009425742574257426,
      "loss": 6.2509,
      "step": 1116
    },
    {
      "epoch": 7.382073523337464,
      "grad_norm": 35.54032516479492,
      "learning_rate": 0.0009420792079207921,
      "loss": 1.9819,
      "step": 1117
    },
    {
      "epoch": 7.388682362660058,
      "grad_norm": 74.57850646972656,
      "learning_rate": 0.0009415841584158415,
      "loss": 4.9352,
      "step": 1118
    },
    {
      "epoch": 7.395291201982651,
      "grad_norm": 26.849241256713867,
      "learning_rate": 0.000941089108910891,
      "loss": 1.7404,
      "step": 1119
    },
    {
      "epoch": 7.401900041305246,
      "grad_norm": 59.9149055480957,
      "learning_rate": 0.0009405940594059406,
      "loss": 2.2401,
      "step": 1120
    },
    {
      "epoch": 7.408508880627839,
      "grad_norm": 81.7316665649414,
      "learning_rate": 0.0009400990099009901,
      "loss": 3.9212,
      "step": 1121
    },
    {
      "epoch": 7.415117719950434,
      "grad_norm": 84.33329772949219,
      "learning_rate": 0.0009396039603960396,
      "loss": 3.5745,
      "step": 1122
    },
    {
      "epoch": 7.4217265592730275,
      "grad_norm": 39.723026275634766,
      "learning_rate": 0.000939108910891089,
      "loss": 0.8882,
      "step": 1123
    },
    {
      "epoch": 7.428335398595622,
      "grad_norm": 47.56547164916992,
      "learning_rate": 0.0009386138613861386,
      "loss": 2.3608,
      "step": 1124
    },
    {
      "epoch": 7.434944237918216,
      "grad_norm": 17.89818000793457,
      "learning_rate": 0.0009381188118811881,
      "loss": 2.1728,
      "step": 1125
    },
    {
      "epoch": 7.44155307724081,
      "grad_norm": 125.10851287841797,
      "learning_rate": 0.0009376237623762376,
      "loss": 5.115,
      "step": 1126
    },
    {
      "epoch": 7.448161916563404,
      "grad_norm": 56.815738677978516,
      "learning_rate": 0.0009371287128712872,
      "loss": 3.7377,
      "step": 1127
    },
    {
      "epoch": 7.454770755885997,
      "grad_norm": 28.398033142089844,
      "learning_rate": 0.0009366336633663367,
      "loss": 3.0266,
      "step": 1128
    },
    {
      "epoch": 7.461379595208592,
      "grad_norm": 29.311641693115234,
      "learning_rate": 0.0009361386138613862,
      "loss": 1.1242,
      "step": 1129
    },
    {
      "epoch": 7.467988434531185,
      "grad_norm": 47.3956184387207,
      "learning_rate": 0.0009356435643564357,
      "loss": 3.8691,
      "step": 1130
    },
    {
      "epoch": 7.47459727385378,
      "grad_norm": 12.711986541748047,
      "learning_rate": 0.0009351485148514852,
      "loss": 1.748,
      "step": 1131
    },
    {
      "epoch": 7.481206113176373,
      "grad_norm": 31.386579513549805,
      "learning_rate": 0.0009346534653465348,
      "loss": 3.1558,
      "step": 1132
    },
    {
      "epoch": 7.487814952498967,
      "grad_norm": 179.63111877441406,
      "learning_rate": 0.0009341584158415842,
      "loss": 6.2703,
      "step": 1133
    },
    {
      "epoch": 7.4944237918215615,
      "grad_norm": 73.2593002319336,
      "learning_rate": 0.0009336633663366337,
      "loss": 1.6133,
      "step": 1134
    },
    {
      "epoch": 7.501032631144155,
      "grad_norm": 10.629886627197266,
      "learning_rate": 0.0009331683168316832,
      "loss": 1.4391,
      "step": 1135
    },
    {
      "epoch": 7.50764147046675,
      "grad_norm": 175.422607421875,
      "learning_rate": 0.0009326732673267328,
      "loss": 11.3904,
      "step": 1136
    },
    {
      "epoch": 7.514250309789343,
      "grad_norm": 159.28697204589844,
      "learning_rate": 0.0009321782178217823,
      "loss": 5.72,
      "step": 1137
    },
    {
      "epoch": 7.520859149111937,
      "grad_norm": 30.126956939697266,
      "learning_rate": 0.0009316831683168317,
      "loss": 5.3143,
      "step": 1138
    },
    {
      "epoch": 7.527467988434531,
      "grad_norm": 173.43661499023438,
      "learning_rate": 0.0009311881188118812,
      "loss": 6.2491,
      "step": 1139
    },
    {
      "epoch": 7.534076827757125,
      "grad_norm": 149.10601806640625,
      "learning_rate": 0.0009306930693069308,
      "loss": 5.683,
      "step": 1140
    },
    {
      "epoch": 7.540685667079719,
      "grad_norm": 431.78143310546875,
      "learning_rate": 0.0009301980198019803,
      "loss": 7.5947,
      "step": 1141
    },
    {
      "epoch": 7.547294506402313,
      "grad_norm": 188.9920654296875,
      "learning_rate": 0.0009297029702970298,
      "loss": 6.2526,
      "step": 1142
    },
    {
      "epoch": 7.553903345724907,
      "grad_norm": 211.46066284179688,
      "learning_rate": 0.0009292079207920792,
      "loss": 11.138,
      "step": 1143
    },
    {
      "epoch": 7.560512185047501,
      "grad_norm": 110.7258071899414,
      "learning_rate": 0.0009287128712871288,
      "loss": 5.1274,
      "step": 1144
    },
    {
      "epoch": 7.5671210243700955,
      "grad_norm": 117.27498626708984,
      "learning_rate": 0.0009282178217821783,
      "loss": 2.9319,
      "step": 1145
    },
    {
      "epoch": 7.573729863692689,
      "grad_norm": 68.6858139038086,
      "learning_rate": 0.0009277227722772278,
      "loss": 4.1561,
      "step": 1146
    },
    {
      "epoch": 7.580338703015283,
      "grad_norm": 107.70834350585938,
      "learning_rate": 0.0009272277227722773,
      "loss": 3.4535,
      "step": 1147
    },
    {
      "epoch": 7.586947542337877,
      "grad_norm": 47.81814193725586,
      "learning_rate": 0.0009267326732673268,
      "loss": 3.7619,
      "step": 1148
    },
    {
      "epoch": 7.593556381660471,
      "grad_norm": 14.964991569519043,
      "learning_rate": 0.0009262376237623763,
      "loss": 2.7459,
      "step": 1149
    },
    {
      "epoch": 7.600165220983065,
      "grad_norm": 91.6819076538086,
      "learning_rate": 0.0009257425742574258,
      "loss": 5.2067,
      "step": 1150
    },
    {
      "epoch": 7.606774060305659,
      "grad_norm": 45.23051452636719,
      "learning_rate": 0.0009252475247524753,
      "loss": 3.8751,
      "step": 1151
    },
    {
      "epoch": 7.613382899628252,
      "grad_norm": 46.03357696533203,
      "learning_rate": 0.0009247524752475249,
      "loss": 3.2565,
      "step": 1152
    },
    {
      "epoch": 7.619991738950847,
      "grad_norm": 83.41910552978516,
      "learning_rate": 0.0009242574257425743,
      "loss": 3.3722,
      "step": 1153
    },
    {
      "epoch": 7.6266005782734405,
      "grad_norm": 60.653717041015625,
      "learning_rate": 0.0009237623762376238,
      "loss": 4.711,
      "step": 1154
    },
    {
      "epoch": 7.633209417596035,
      "grad_norm": 69.21495819091797,
      "learning_rate": 0.0009232673267326733,
      "loss": 3.152,
      "step": 1155
    },
    {
      "epoch": 7.639818256918629,
      "grad_norm": 39.255889892578125,
      "learning_rate": 0.0009227722772277229,
      "loss": 4.2811,
      "step": 1156
    },
    {
      "epoch": 7.646427096241222,
      "grad_norm": 212.31668090820312,
      "learning_rate": 0.0009222772277227724,
      "loss": 8.6945,
      "step": 1157
    },
    {
      "epoch": 7.653035935563817,
      "grad_norm": 209.49200439453125,
      "learning_rate": 0.0009217821782178218,
      "loss": 8.2942,
      "step": 1158
    },
    {
      "epoch": 7.65964477488641,
      "grad_norm": 27.8507022857666,
      "learning_rate": 0.0009212871287128713,
      "loss": 3.1416,
      "step": 1159
    },
    {
      "epoch": 7.666253614209005,
      "grad_norm": 264.76812744140625,
      "learning_rate": 0.0009207920792079209,
      "loss": 12.1334,
      "step": 1160
    },
    {
      "epoch": 7.672862453531598,
      "grad_norm": 236.11209106445312,
      "learning_rate": 0.0009202970297029704,
      "loss": 14.2586,
      "step": 1161
    },
    {
      "epoch": 7.679471292854193,
      "grad_norm": 128.4434814453125,
      "learning_rate": 0.0009198019801980199,
      "loss": 3.2553,
      "step": 1162
    },
    {
      "epoch": 7.686080132176786,
      "grad_norm": 45.8895378112793,
      "learning_rate": 0.0009193069306930693,
      "loss": 5.8077,
      "step": 1163
    },
    {
      "epoch": 7.692688971499381,
      "grad_norm": 259.4551086425781,
      "learning_rate": 0.0009188118811881188,
      "loss": 11.3618,
      "step": 1164
    },
    {
      "epoch": 7.6992978108219745,
      "grad_norm": 218.2784423828125,
      "learning_rate": 0.0009183168316831684,
      "loss": 7.2664,
      "step": 1165
    },
    {
      "epoch": 7.705906650144568,
      "grad_norm": 258.2804870605469,
      "learning_rate": 0.0009178217821782179,
      "loss": 10.0579,
      "step": 1166
    },
    {
      "epoch": 7.712515489467163,
      "grad_norm": 10.319395065307617,
      "learning_rate": 0.0009173267326732674,
      "loss": 0.5589,
      "step": 1167
    },
    {
      "epoch": 7.719124328789756,
      "grad_norm": 40.04070281982422,
      "learning_rate": 0.0009168316831683168,
      "loss": 3.4839,
      "step": 1168
    },
    {
      "epoch": 7.725733168112351,
      "grad_norm": 42.26692581176758,
      "learning_rate": 0.0009163366336633664,
      "loss": 1.2923,
      "step": 1169
    },
    {
      "epoch": 7.732342007434944,
      "grad_norm": 35.72503662109375,
      "learning_rate": 0.0009158415841584159,
      "loss": 2.5135,
      "step": 1170
    },
    {
      "epoch": 7.738950846757538,
      "grad_norm": 35.2846565246582,
      "learning_rate": 0.0009153465346534654,
      "loss": 3.5207,
      "step": 1171
    },
    {
      "epoch": 7.745559686080132,
      "grad_norm": 141.1378936767578,
      "learning_rate": 0.000914851485148515,
      "loss": 3.2498,
      "step": 1172
    },
    {
      "epoch": 7.752168525402726,
      "grad_norm": 132.3695526123047,
      "learning_rate": 0.0009143564356435644,
      "loss": 3.5351,
      "step": 1173
    },
    {
      "epoch": 7.75877736472532,
      "grad_norm": 35.436737060546875,
      "learning_rate": 0.0009138613861386139,
      "loss": 3.3156,
      "step": 1174
    },
    {
      "epoch": 7.765386204047914,
      "grad_norm": 117.20899963378906,
      "learning_rate": 0.0009133663366336634,
      "loss": 6.1379,
      "step": 1175
    },
    {
      "epoch": 7.771995043370508,
      "grad_norm": 194.36172485351562,
      "learning_rate": 0.0009128712871287129,
      "loss": 5.9959,
      "step": 1176
    },
    {
      "epoch": 7.778603882693102,
      "grad_norm": 101.7241439819336,
      "learning_rate": 0.0009123762376237625,
      "loss": 9.2362,
      "step": 1177
    },
    {
      "epoch": 7.785212722015696,
      "grad_norm": 11.690951347351074,
      "learning_rate": 0.0009118811881188119,
      "loss": 1.565,
      "step": 1178
    },
    {
      "epoch": 7.79182156133829,
      "grad_norm": 133.57162475585938,
      "learning_rate": 0.0009113861386138614,
      "loss": 5.8548,
      "step": 1179
    },
    {
      "epoch": 7.798430400660884,
      "grad_norm": 216.5995635986328,
      "learning_rate": 0.0009108910891089109,
      "loss": 7.9515,
      "step": 1180
    },
    {
      "epoch": 7.805039239983478,
      "grad_norm": 231.87042236328125,
      "learning_rate": 0.0009103960396039605,
      "loss": 9.1887,
      "step": 1181
    },
    {
      "epoch": 7.811648079306072,
      "grad_norm": 81.6447982788086,
      "learning_rate": 0.00090990099009901,
      "loss": 4.4713,
      "step": 1182
    },
    {
      "epoch": 7.8182569186286655,
      "grad_norm": 74.0012435913086,
      "learning_rate": 0.0009094059405940594,
      "loss": 2.9198,
      "step": 1183
    },
    {
      "epoch": 7.82486575795126,
      "grad_norm": 72.6045913696289,
      "learning_rate": 0.0009089108910891089,
      "loss": 5.4117,
      "step": 1184
    },
    {
      "epoch": 7.8314745972738535,
      "grad_norm": 183.9560089111328,
      "learning_rate": 0.0009084158415841585,
      "loss": 5.3031,
      "step": 1185
    },
    {
      "epoch": 7.838083436596448,
      "grad_norm": 17.677831649780273,
      "learning_rate": 0.000907920792079208,
      "loss": 4.0554,
      "step": 1186
    },
    {
      "epoch": 7.844692275919042,
      "grad_norm": 187.260009765625,
      "learning_rate": 0.0009074257425742575,
      "loss": 4.6983,
      "step": 1187
    },
    {
      "epoch": 7.851301115241636,
      "grad_norm": 199.2642059326172,
      "learning_rate": 0.0009069306930693069,
      "loss": 6.2895,
      "step": 1188
    },
    {
      "epoch": 7.85790995456423,
      "grad_norm": 131.20535278320312,
      "learning_rate": 0.0009064356435643565,
      "loss": 2.706,
      "step": 1189
    },
    {
      "epoch": 7.864518793886823,
      "grad_norm": 26.516700744628906,
      "learning_rate": 0.000905940594059406,
      "loss": 2.4768,
      "step": 1190
    },
    {
      "epoch": 7.871127633209418,
      "grad_norm": 62.002323150634766,
      "learning_rate": 0.0009054455445544555,
      "loss": 3.4791,
      "step": 1191
    },
    {
      "epoch": 7.877736472532011,
      "grad_norm": 32.82963180541992,
      "learning_rate": 0.000904950495049505,
      "loss": 2.7756,
      "step": 1192
    },
    {
      "epoch": 7.884345311854606,
      "grad_norm": 51.13398742675781,
      "learning_rate": 0.0009044554455445545,
      "loss": 3.0621,
      "step": 1193
    },
    {
      "epoch": 7.8909541511771994,
      "grad_norm": 53.73846435546875,
      "learning_rate": 0.000903960396039604,
      "loss": 2.8149,
      "step": 1194
    },
    {
      "epoch": 7.897562990499793,
      "grad_norm": 65.9740219116211,
      "learning_rate": 0.0009034653465346535,
      "loss": 1.5042,
      "step": 1195
    },
    {
      "epoch": 7.9041718298223875,
      "grad_norm": 27.523862838745117,
      "learning_rate": 0.000902970297029703,
      "loss": 2.6014,
      "step": 1196
    },
    {
      "epoch": 7.910780669144981,
      "grad_norm": 22.36268424987793,
      "learning_rate": 0.0009024752475247526,
      "loss": 2.054,
      "step": 1197
    },
    {
      "epoch": 7.917389508467576,
      "grad_norm": 27.817001342773438,
      "learning_rate": 0.000901980198019802,
      "loss": 1.9967,
      "step": 1198
    },
    {
      "epoch": 7.923998347790169,
      "grad_norm": 132.0524139404297,
      "learning_rate": 0.0009014851485148515,
      "loss": 8.4258,
      "step": 1199
    },
    {
      "epoch": 7.930607187112764,
      "grad_norm": 68.58419799804688,
      "learning_rate": 0.000900990099009901,
      "loss": 3.5351,
      "step": 1200
    },
    {
      "epoch": 7.937216026435357,
      "grad_norm": 50.30961608886719,
      "learning_rate": 0.0009004950495049506,
      "loss": 1.8582,
      "step": 1201
    },
    {
      "epoch": 7.943824865757951,
      "grad_norm": 14.146714210510254,
      "learning_rate": 0.0009000000000000001,
      "loss": 2.484,
      "step": 1202
    },
    {
      "epoch": 7.950433705080545,
      "grad_norm": 38.55032730102539,
      "learning_rate": 0.0008995049504950495,
      "loss": 2.4513,
      "step": 1203
    },
    {
      "epoch": 7.957042544403139,
      "grad_norm": 17.20556640625,
      "learning_rate": 0.000899009900990099,
      "loss": 1.4169,
      "step": 1204
    },
    {
      "epoch": 7.9636513837257334,
      "grad_norm": 60.49942398071289,
      "learning_rate": 0.0008985148514851486,
      "loss": 1.9932,
      "step": 1205
    },
    {
      "epoch": 7.970260223048327,
      "grad_norm": 66.13858795166016,
      "learning_rate": 0.0008980198019801981,
      "loss": 4.94,
      "step": 1206
    },
    {
      "epoch": 7.9768690623709215,
      "grad_norm": 71.9854965209961,
      "learning_rate": 0.0008975247524752476,
      "loss": 2.045,
      "step": 1207
    },
    {
      "epoch": 7.983477901693515,
      "grad_norm": 171.5890350341797,
      "learning_rate": 0.000897029702970297,
      "loss": 5.4084,
      "step": 1208
    },
    {
      "epoch": 7.990086741016109,
      "grad_norm": 12.983114242553711,
      "learning_rate": 0.0008965346534653466,
      "loss": 1.3452,
      "step": 1209
    },
    {
      "epoch": 7.996695580338703,
      "grad_norm": 112.94025421142578,
      "learning_rate": 0.0008960396039603961,
      "loss": 4.3102,
      "step": 1210
    },
    {
      "epoch": 7.996695580338703,
      "eval_validation_error_bar": 0.05334770472421313,
      "eval_validation_loss": 6.353264331817627,
      "eval_validation_pearsonr": 0.5622381857145158,
      "eval_validation_rmse": 2.520568370819092,
      "eval_validation_runtime": 41.4516,
      "eval_validation_samples_per_second": 4.897,
      "eval_validation_spearman": 0.5439054655021669,
      "eval_validation_steps_per_second": 4.897,
      "step": 1210
    },
    {
      "epoch": 7.996695580338703,
      "eval_test_error_bar": 0.04236982282625504,
      "eval_test_loss": 7.970125198364258,
      "eval_test_pearsonr": 0.5157382707367177,
      "eval_test_rmse": 2.823141098022461,
      "eval_test_runtime": 55.8908,
      "eval_test_samples_per_second": 5.833,
      "eval_test_spearman": 0.5367197941228763,
      "eval_test_steps_per_second": 5.833,
      "step": 1210
    },
    {
      "epoch": 8.003304419661298,
      "grad_norm": 12.198339462280273,
      "learning_rate": 0.0008955445544554456,
      "loss": 1.8859,
      "step": 1211
    },
    {
      "epoch": 8.009913258983891,
      "grad_norm": 20.737300872802734,
      "learning_rate": 0.0008950495049504951,
      "loss": 1.1204,
      "step": 1212
    },
    {
      "epoch": 8.016522098306485,
      "grad_norm": 63.287410736083984,
      "learning_rate": 0.0008945544554455445,
      "loss": 1.7732,
      "step": 1213
    },
    {
      "epoch": 8.023130937629078,
      "grad_norm": 75.64056396484375,
      "learning_rate": 0.0008940594059405941,
      "loss": 1.9293,
      "step": 1214
    },
    {
      "epoch": 8.029739776951672,
      "grad_norm": 74.27472686767578,
      "learning_rate": 0.0008935643564356436,
      "loss": 7.1634,
      "step": 1215
    },
    {
      "epoch": 8.036348616274267,
      "grad_norm": 137.79769897460938,
      "learning_rate": 0.0008930693069306931,
      "loss": 5.0403,
      "step": 1216
    },
    {
      "epoch": 8.042957455596861,
      "grad_norm": 101.37418365478516,
      "learning_rate": 0.0008925742574257427,
      "loss": 1.4476,
      "step": 1217
    },
    {
      "epoch": 8.049566294919455,
      "grad_norm": 110.31449890136719,
      "learning_rate": 0.0008920792079207921,
      "loss": 3.7345,
      "step": 1218
    },
    {
      "epoch": 8.056175134242048,
      "grad_norm": 69.47413635253906,
      "learning_rate": 0.0008915841584158416,
      "loss": 2.2696,
      "step": 1219
    },
    {
      "epoch": 8.062783973564642,
      "grad_norm": 73.69338989257812,
      "learning_rate": 0.0008910891089108911,
      "loss": 2.5804,
      "step": 1220
    },
    {
      "epoch": 8.069392812887237,
      "grad_norm": 79.98526763916016,
      "learning_rate": 0.0008905940594059406,
      "loss": 2.0605,
      "step": 1221
    },
    {
      "epoch": 8.07600165220983,
      "grad_norm": 85.9984130859375,
      "learning_rate": 0.0008900990099009902,
      "loss": 4.9628,
      "step": 1222
    },
    {
      "epoch": 8.082610491532424,
      "grad_norm": 36.242942810058594,
      "learning_rate": 0.0008896039603960396,
      "loss": 2.748,
      "step": 1223
    },
    {
      "epoch": 8.089219330855018,
      "grad_norm": 125.88824462890625,
      "learning_rate": 0.0008891089108910891,
      "loss": 2.3782,
      "step": 1224
    },
    {
      "epoch": 8.095828170177613,
      "grad_norm": 192.9302520751953,
      "learning_rate": 0.0008886138613861386,
      "loss": 4.7726,
      "step": 1225
    },
    {
      "epoch": 8.102437009500207,
      "grad_norm": 29.61511993408203,
      "learning_rate": 0.0008881188118811882,
      "loss": 5.3304,
      "step": 1226
    },
    {
      "epoch": 8.1090458488228,
      "grad_norm": 60.17367172241211,
      "learning_rate": 0.0008876237623762377,
      "loss": 4.4465,
      "step": 1227
    },
    {
      "epoch": 8.115654688145394,
      "grad_norm": 69.48474884033203,
      "learning_rate": 0.0008871287128712871,
      "loss": 1.9642,
      "step": 1228
    },
    {
      "epoch": 8.122263527467988,
      "grad_norm": 67.25358581542969,
      "learning_rate": 0.0008866336633663366,
      "loss": 2.5663,
      "step": 1229
    },
    {
      "epoch": 8.128872366790583,
      "grad_norm": 91.18682861328125,
      "learning_rate": 0.0008861386138613862,
      "loss": 3.1799,
      "step": 1230
    },
    {
      "epoch": 8.135481206113177,
      "grad_norm": 141.7796630859375,
      "learning_rate": 0.0008856435643564357,
      "loss": 3.7318,
      "step": 1231
    },
    {
      "epoch": 8.14209004543577,
      "grad_norm": 272.6880187988281,
      "learning_rate": 0.0008851485148514852,
      "loss": 7.9614,
      "step": 1232
    },
    {
      "epoch": 8.148698884758364,
      "grad_norm": 52.20597457885742,
      "learning_rate": 0.0008846534653465346,
      "loss": 2.9156,
      "step": 1233
    },
    {
      "epoch": 8.155307724080957,
      "grad_norm": 194.12083435058594,
      "learning_rate": 0.0008841584158415842,
      "loss": 4.7888,
      "step": 1234
    },
    {
      "epoch": 8.161916563403553,
      "grad_norm": 412.8237609863281,
      "learning_rate": 0.0008836633663366337,
      "loss": 15.2925,
      "step": 1235
    },
    {
      "epoch": 8.168525402726146,
      "grad_norm": 136.5679168701172,
      "learning_rate": 0.0008831683168316832,
      "loss": 7.9335,
      "step": 1236
    },
    {
      "epoch": 8.17513424204874,
      "grad_norm": 9.07100772857666,
      "learning_rate": 0.0008826732673267327,
      "loss": 3.0956,
      "step": 1237
    },
    {
      "epoch": 8.181743081371334,
      "grad_norm": 82.06086730957031,
      "learning_rate": 0.0008821782178217822,
      "loss": 3.8902,
      "step": 1238
    },
    {
      "epoch": 8.188351920693927,
      "grad_norm": 77.38521575927734,
      "learning_rate": 0.0008816831683168317,
      "loss": 2.9643,
      "step": 1239
    },
    {
      "epoch": 8.194960760016523,
      "grad_norm": 50.802825927734375,
      "learning_rate": 0.0008811881188118812,
      "loss": 5.2218,
      "step": 1240
    },
    {
      "epoch": 8.201569599339116,
      "grad_norm": 95.0369644165039,
      "learning_rate": 0.0008806930693069307,
      "loss": 2.3171,
      "step": 1241
    },
    {
      "epoch": 8.20817843866171,
      "grad_norm": 108.32603454589844,
      "learning_rate": 0.0008801980198019803,
      "loss": 3.2297,
      "step": 1242
    },
    {
      "epoch": 8.214787277984303,
      "grad_norm": 36.799198150634766,
      "learning_rate": 0.0008797029702970297,
      "loss": 1.6953,
      "step": 1243
    },
    {
      "epoch": 8.221396117306899,
      "grad_norm": 43.886512756347656,
      "learning_rate": 0.0008792079207920792,
      "loss": 3.3657,
      "step": 1244
    },
    {
      "epoch": 8.228004956629492,
      "grad_norm": 202.3041534423828,
      "learning_rate": 0.0008787128712871287,
      "loss": 5.8875,
      "step": 1245
    },
    {
      "epoch": 8.234613795952086,
      "grad_norm": 99.9104232788086,
      "learning_rate": 0.0008782178217821783,
      "loss": 6.7024,
      "step": 1246
    },
    {
      "epoch": 8.24122263527468,
      "grad_norm": 28.670909881591797,
      "learning_rate": 0.0008777227722772278,
      "loss": 1.6372,
      "step": 1247
    },
    {
      "epoch": 8.247831474597273,
      "grad_norm": 102.04975128173828,
      "learning_rate": 0.0008772277227722772,
      "loss": 2.0803,
      "step": 1248
    },
    {
      "epoch": 8.254440313919869,
      "grad_norm": 111.34516906738281,
      "learning_rate": 0.0008767326732673267,
      "loss": 3.0153,
      "step": 1249
    },
    {
      "epoch": 8.261049153242462,
      "grad_norm": 65.6198959350586,
      "learning_rate": 0.0008762376237623763,
      "loss": 4.0463,
      "step": 1250
    },
    {
      "epoch": 8.267657992565056,
      "grad_norm": 46.89817810058594,
      "learning_rate": 0.0008757425742574258,
      "loss": 3.9628,
      "step": 1251
    },
    {
      "epoch": 8.27426683188765,
      "grad_norm": 57.55622863769531,
      "learning_rate": 0.0008752475247524753,
      "loss": 2.5631,
      "step": 1252
    },
    {
      "epoch": 8.280875671210243,
      "grad_norm": 18.053218841552734,
      "learning_rate": 0.0008747524752475247,
      "loss": 2.3486,
      "step": 1253
    },
    {
      "epoch": 8.287484510532838,
      "grad_norm": 85.22308349609375,
      "learning_rate": 0.0008742574257425743,
      "loss": 2.7059,
      "step": 1254
    },
    {
      "epoch": 8.294093349855432,
      "grad_norm": 63.53398895263672,
      "learning_rate": 0.0008737623762376238,
      "loss": 2.6362,
      "step": 1255
    },
    {
      "epoch": 8.300702189178025,
      "grad_norm": 65.56134796142578,
      "learning_rate": 0.0008732673267326733,
      "loss": 2.8997,
      "step": 1256
    },
    {
      "epoch": 8.307311028500619,
      "grad_norm": 46.036216735839844,
      "learning_rate": 0.0008727722772277228,
      "loss": 3.6379,
      "step": 1257
    },
    {
      "epoch": 8.313919867823213,
      "grad_norm": 30.691465377807617,
      "learning_rate": 0.0008722772277227722,
      "loss": 4.0275,
      "step": 1258
    },
    {
      "epoch": 8.320528707145808,
      "grad_norm": 91.93486785888672,
      "learning_rate": 0.0008717821782178218,
      "loss": 3.0384,
      "step": 1259
    },
    {
      "epoch": 8.327137546468402,
      "grad_norm": 159.51162719726562,
      "learning_rate": 0.0008712871287128713,
      "loss": 4.8428,
      "step": 1260
    },
    {
      "epoch": 8.333746385790995,
      "grad_norm": 100.74397277832031,
      "learning_rate": 0.0008707920792079208,
      "loss": 6.6016,
      "step": 1261
    },
    {
      "epoch": 8.340355225113589,
      "grad_norm": 43.79194641113281,
      "learning_rate": 0.0008702970297029704,
      "loss": 3.3283,
      "step": 1262
    },
    {
      "epoch": 8.346964064436184,
      "grad_norm": 96.1287612915039,
      "learning_rate": 0.0008698019801980198,
      "loss": 3.8278,
      "step": 1263
    },
    {
      "epoch": 8.353572903758778,
      "grad_norm": 112.23068237304688,
      "learning_rate": 0.0008693069306930693,
      "loss": 5.3181,
      "step": 1264
    },
    {
      "epoch": 8.360181743081371,
      "grad_norm": 67.56631469726562,
      "learning_rate": 0.0008688118811881188,
      "loss": 1.3782,
      "step": 1265
    },
    {
      "epoch": 8.366790582403965,
      "grad_norm": 96.09868621826172,
      "learning_rate": 0.0008683168316831684,
      "loss": 4.24,
      "step": 1266
    },
    {
      "epoch": 8.373399421726559,
      "grad_norm": 20.115556716918945,
      "learning_rate": 0.0008678217821782179,
      "loss": 2.4158,
      "step": 1267
    },
    {
      "epoch": 8.380008261049154,
      "grad_norm": 24.043790817260742,
      "learning_rate": 0.0008673267326732673,
      "loss": 4.3125,
      "step": 1268
    },
    {
      "epoch": 8.386617100371748,
      "grad_norm": 24.605941772460938,
      "learning_rate": 0.0008668316831683168,
      "loss": 1.3259,
      "step": 1269
    },
    {
      "epoch": 8.393225939694341,
      "grad_norm": 38.18592834472656,
      "learning_rate": 0.0008663366336633663,
      "loss": 1.5368,
      "step": 1270
    },
    {
      "epoch": 8.399834779016935,
      "grad_norm": 54.02500915527344,
      "learning_rate": 0.0008658415841584159,
      "loss": 5.5083,
      "step": 1271
    },
    {
      "epoch": 8.406443618339528,
      "grad_norm": 92.34902954101562,
      "learning_rate": 0.0008653465346534654,
      "loss": 4.3331,
      "step": 1272
    },
    {
      "epoch": 8.413052457662124,
      "grad_norm": 47.498634338378906,
      "learning_rate": 0.0008648514851485148,
      "loss": 3.3889,
      "step": 1273
    },
    {
      "epoch": 8.419661296984717,
      "grad_norm": 70.02619934082031,
      "learning_rate": 0.0008643564356435643,
      "loss": 1.6423,
      "step": 1274
    },
    {
      "epoch": 8.426270136307311,
      "grad_norm": 11.876730918884277,
      "learning_rate": 0.0008638613861386139,
      "loss": 1.7852,
      "step": 1275
    },
    {
      "epoch": 8.432878975629905,
      "grad_norm": 18.049686431884766,
      "learning_rate": 0.0008633663366336634,
      "loss": 1.3866,
      "step": 1276
    },
    {
      "epoch": 8.439487814952498,
      "grad_norm": 126.86504364013672,
      "learning_rate": 0.0008628712871287129,
      "loss": 4.2738,
      "step": 1277
    },
    {
      "epoch": 8.446096654275093,
      "grad_norm": 122.3291244506836,
      "learning_rate": 0.0008623762376237623,
      "loss": 3.8723,
      "step": 1278
    },
    {
      "epoch": 8.452705493597687,
      "grad_norm": 67.81814575195312,
      "learning_rate": 0.0008618811881188119,
      "loss": 1.7328,
      "step": 1279
    },
    {
      "epoch": 8.45931433292028,
      "grad_norm": 191.07809448242188,
      "learning_rate": 0.0008613861386138614,
      "loss": 3.8586,
      "step": 1280
    },
    {
      "epoch": 8.465923172242874,
      "grad_norm": 221.1978302001953,
      "learning_rate": 0.0008608910891089109,
      "loss": 7.4728,
      "step": 1281
    },
    {
      "epoch": 8.47253201156547,
      "grad_norm": 251.535888671875,
      "learning_rate": 0.0008603960396039604,
      "loss": 9.9477,
      "step": 1282
    },
    {
      "epoch": 8.479140850888063,
      "grad_norm": 79.21601867675781,
      "learning_rate": 0.0008599009900990099,
      "loss": 3.9044,
      "step": 1283
    },
    {
      "epoch": 8.485749690210657,
      "grad_norm": 133.1772918701172,
      "learning_rate": 0.0008594059405940594,
      "loss": 4.5627,
      "step": 1284
    },
    {
      "epoch": 8.49235852953325,
      "grad_norm": 193.98715209960938,
      "learning_rate": 0.0008589108910891089,
      "loss": 7.1271,
      "step": 1285
    },
    {
      "epoch": 8.498967368855844,
      "grad_norm": 159.16845703125,
      "learning_rate": 0.0008584158415841584,
      "loss": 6.2394,
      "step": 1286
    },
    {
      "epoch": 8.50557620817844,
      "grad_norm": 80.07244873046875,
      "learning_rate": 0.000857920792079208,
      "loss": 4.28,
      "step": 1287
    },
    {
      "epoch": 8.512185047501033,
      "grad_norm": 123.87759399414062,
      "learning_rate": 0.0008574257425742574,
      "loss": 5.657,
      "step": 1288
    },
    {
      "epoch": 8.518793886823627,
      "grad_norm": 162.63096618652344,
      "learning_rate": 0.0008569306930693069,
      "loss": 6.4079,
      "step": 1289
    },
    {
      "epoch": 8.52540272614622,
      "grad_norm": 80.95651245117188,
      "learning_rate": 0.0008564356435643564,
      "loss": 3.6143,
      "step": 1290
    },
    {
      "epoch": 8.532011565468814,
      "grad_norm": 63.754608154296875,
      "learning_rate": 0.000855940594059406,
      "loss": 2.4626,
      "step": 1291
    },
    {
      "epoch": 8.53862040479141,
      "grad_norm": 163.94369506835938,
      "learning_rate": 0.0008554455445544555,
      "loss": 5.8562,
      "step": 1292
    },
    {
      "epoch": 8.545229244114003,
      "grad_norm": 296.9505310058594,
      "learning_rate": 0.0008549504950495049,
      "loss": 12.3647,
      "step": 1293
    },
    {
      "epoch": 8.551838083436596,
      "grad_norm": 204.19253540039062,
      "learning_rate": 0.0008544554455445544,
      "loss": 9.8113,
      "step": 1294
    },
    {
      "epoch": 8.55844692275919,
      "grad_norm": 69.72960662841797,
      "learning_rate": 0.000853960396039604,
      "loss": 2.0062,
      "step": 1295
    },
    {
      "epoch": 8.565055762081784,
      "grad_norm": 23.523540496826172,
      "learning_rate": 0.0008534653465346535,
      "loss": 2.1587,
      "step": 1296
    },
    {
      "epoch": 8.571664601404379,
      "grad_norm": 115.19713592529297,
      "learning_rate": 0.000852970297029703,
      "loss": 2.8566,
      "step": 1297
    },
    {
      "epoch": 8.578273440726973,
      "grad_norm": 196.341064453125,
      "learning_rate": 0.0008524752475247524,
      "loss": 8.7606,
      "step": 1298
    },
    {
      "epoch": 8.584882280049566,
      "grad_norm": 69.54576110839844,
      "learning_rate": 0.000851980198019802,
      "loss": 3.927,
      "step": 1299
    },
    {
      "epoch": 8.59149111937216,
      "grad_norm": 70.63814544677734,
      "learning_rate": 0.0008514851485148515,
      "loss": 7.0587,
      "step": 1300
    },
    {
      "epoch": 8.598099958694753,
      "grad_norm": 83.71603393554688,
      "learning_rate": 0.000850990099009901,
      "loss": 2.7499,
      "step": 1301
    },
    {
      "epoch": 8.604708798017349,
      "grad_norm": 119.59053802490234,
      "learning_rate": 0.0008504950495049505,
      "loss": 2.1164,
      "step": 1302
    },
    {
      "epoch": 8.611317637339942,
      "grad_norm": 75.3429183959961,
      "learning_rate": 0.00085,
      "loss": 2.552,
      "step": 1303
    },
    {
      "epoch": 8.617926476662536,
      "grad_norm": 76.0988540649414,
      "learning_rate": 0.0008495049504950495,
      "loss": 2.1167,
      "step": 1304
    },
    {
      "epoch": 8.62453531598513,
      "grad_norm": 105.00193786621094,
      "learning_rate": 0.000849009900990099,
      "loss": 2.6248,
      "step": 1305
    },
    {
      "epoch": 8.631144155307725,
      "grad_norm": 163.9490509033203,
      "learning_rate": 0.0008485148514851485,
      "loss": 5.8259,
      "step": 1306
    },
    {
      "epoch": 8.637752994630318,
      "grad_norm": 155.38438415527344,
      "learning_rate": 0.0008480198019801981,
      "loss": 5.9613,
      "step": 1307
    },
    {
      "epoch": 8.644361833952912,
      "grad_norm": 27.02068328857422,
      "learning_rate": 0.0008475247524752475,
      "loss": 3.2768,
      "step": 1308
    },
    {
      "epoch": 8.650970673275506,
      "grad_norm": 118.59034729003906,
      "learning_rate": 0.000847029702970297,
      "loss": 3.2729,
      "step": 1309
    },
    {
      "epoch": 8.6575795125981,
      "grad_norm": 221.53036499023438,
      "learning_rate": 0.0008465346534653465,
      "loss": 9.5031,
      "step": 1310
    },
    {
      "epoch": 8.664188351920695,
      "grad_norm": 243.74606323242188,
      "learning_rate": 0.000846039603960396,
      "loss": 9.1084,
      "step": 1311
    },
    {
      "epoch": 8.670797191243288,
      "grad_norm": 81.54178619384766,
      "learning_rate": 0.0008455445544554456,
      "loss": 3.5681,
      "step": 1312
    },
    {
      "epoch": 8.677406030565882,
      "grad_norm": 47.4783935546875,
      "learning_rate": 0.000845049504950495,
      "loss": 4.1934,
      "step": 1313
    },
    {
      "epoch": 8.684014869888475,
      "grad_norm": 262.1927795410156,
      "learning_rate": 0.0008445544554455445,
      "loss": 10.5789,
      "step": 1314
    },
    {
      "epoch": 8.69062370921107,
      "grad_norm": 129.16073608398438,
      "learning_rate": 0.000844059405940594,
      "loss": 3.1421,
      "step": 1315
    },
    {
      "epoch": 8.697232548533664,
      "grad_norm": 78.93140411376953,
      "learning_rate": 0.0008435643564356436,
      "loss": 3.2578,
      "step": 1316
    },
    {
      "epoch": 8.703841387856258,
      "grad_norm": 9.39886474609375,
      "learning_rate": 0.0008430693069306931,
      "loss": 2.3198,
      "step": 1317
    },
    {
      "epoch": 8.710450227178852,
      "grad_norm": 87.05545806884766,
      "learning_rate": 0.0008425742574257425,
      "loss": 2.6881,
      "step": 1318
    },
    {
      "epoch": 8.717059066501445,
      "grad_norm": 115.3886947631836,
      "learning_rate": 0.000842079207920792,
      "loss": 4.2385,
      "step": 1319
    },
    {
      "epoch": 8.72366790582404,
      "grad_norm": 13.589828491210938,
      "learning_rate": 0.0008415841584158416,
      "loss": 1.24,
      "step": 1320
    },
    {
      "epoch": 8.730276745146634,
      "grad_norm": 58.274662017822266,
      "learning_rate": 0.0008410891089108911,
      "loss": 1.8786,
      "step": 1321
    },
    {
      "epoch": 8.736885584469228,
      "grad_norm": 25.9556884765625,
      "learning_rate": 0.0008405940594059406,
      "loss": 4.2619,
      "step": 1322
    },
    {
      "epoch": 8.743494423791821,
      "grad_norm": 199.65194702148438,
      "learning_rate": 0.00084009900990099,
      "loss": 7.6319,
      "step": 1323
    },
    {
      "epoch": 8.750103263114415,
      "grad_norm": 126.56298828125,
      "learning_rate": 0.0008396039603960396,
      "loss": 3.9156,
      "step": 1324
    },
    {
      "epoch": 8.75671210243701,
      "grad_norm": 78.2716064453125,
      "learning_rate": 0.0008391089108910891,
      "loss": 3.4299,
      "step": 1325
    },
    {
      "epoch": 8.763320941759604,
      "grad_norm": 186.697509765625,
      "learning_rate": 0.0008386138613861386,
      "loss": 5.4057,
      "step": 1326
    },
    {
      "epoch": 8.769929781082197,
      "grad_norm": 86.63140106201172,
      "learning_rate": 0.0008381188118811881,
      "loss": 2.2045,
      "step": 1327
    },
    {
      "epoch": 8.776538620404791,
      "grad_norm": 10.577582359313965,
      "learning_rate": 0.0008376237623762376,
      "loss": 3.0626,
      "step": 1328
    },
    {
      "epoch": 8.783147459727385,
      "grad_norm": 72.40198516845703,
      "learning_rate": 0.0008371287128712871,
      "loss": 2.2687,
      "step": 1329
    },
    {
      "epoch": 8.78975629904998,
      "grad_norm": 158.07260131835938,
      "learning_rate": 0.0008366336633663366,
      "loss": 6.0819,
      "step": 1330
    },
    {
      "epoch": 8.796365138372574,
      "grad_norm": 108.66163635253906,
      "learning_rate": 0.0008361386138613861,
      "loss": 2.8464,
      "step": 1331
    },
    {
      "epoch": 8.802973977695167,
      "grad_norm": 64.87642669677734,
      "learning_rate": 0.0008356435643564357,
      "loss": 3.9206,
      "step": 1332
    },
    {
      "epoch": 8.80958281701776,
      "grad_norm": 85.96782684326172,
      "learning_rate": 0.0008351485148514851,
      "loss": 2.0556,
      "step": 1333
    },
    {
      "epoch": 8.816191656340354,
      "grad_norm": 124.7884521484375,
      "learning_rate": 0.0008346534653465346,
      "loss": 7.3586,
      "step": 1334
    },
    {
      "epoch": 8.82280049566295,
      "grad_norm": 67.9814224243164,
      "learning_rate": 0.0008341584158415841,
      "loss": 1.8716,
      "step": 1335
    },
    {
      "epoch": 8.829409334985543,
      "grad_norm": 52.96091842651367,
      "learning_rate": 0.0008336633663366337,
      "loss": 4.0779,
      "step": 1336
    },
    {
      "epoch": 8.836018174308137,
      "grad_norm": 144.1977081298828,
      "learning_rate": 0.0008331683168316832,
      "loss": 4.4434,
      "step": 1337
    },
    {
      "epoch": 8.84262701363073,
      "grad_norm": 128.089599609375,
      "learning_rate": 0.0008326732673267326,
      "loss": 4.2464,
      "step": 1338
    },
    {
      "epoch": 8.849235852953324,
      "grad_norm": 24.38066291809082,
      "learning_rate": 0.0008321782178217821,
      "loss": 1.8423,
      "step": 1339
    },
    {
      "epoch": 8.85584469227592,
      "grad_norm": 62.6090202331543,
      "learning_rate": 0.0008316831683168317,
      "loss": 2.0855,
      "step": 1340
    },
    {
      "epoch": 8.862453531598513,
      "grad_norm": 185.91624450683594,
      "learning_rate": 0.0008311881188118812,
      "loss": 5.1361,
      "step": 1341
    },
    {
      "epoch": 8.869062370921107,
      "grad_norm": 28.793121337890625,
      "learning_rate": 0.0008306930693069307,
      "loss": 2.4443,
      "step": 1342
    },
    {
      "epoch": 8.8756712102437,
      "grad_norm": 50.26575469970703,
      "learning_rate": 0.0008301980198019801,
      "loss": 3.6525,
      "step": 1343
    },
    {
      "epoch": 8.882280049566296,
      "grad_norm": 84.8299789428711,
      "learning_rate": 0.0008297029702970297,
      "loss": 5.9368,
      "step": 1344
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 114.70349884033203,
      "learning_rate": 0.0008292079207920792,
      "loss": 3.7515,
      "step": 1345
    },
    {
      "epoch": 8.895497728211483,
      "grad_norm": 58.20437240600586,
      "learning_rate": 0.0008287128712871287,
      "loss": 3.2503,
      "step": 1346
    },
    {
      "epoch": 8.902106567534076,
      "grad_norm": 50.44749069213867,
      "learning_rate": 0.0008282178217821782,
      "loss": 3.1807,
      "step": 1347
    },
    {
      "epoch": 8.90871540685667,
      "grad_norm": 21.021209716796875,
      "learning_rate": 0.0008277227722772277,
      "loss": 4.951,
      "step": 1348
    },
    {
      "epoch": 8.915324246179265,
      "grad_norm": 19.595930099487305,
      "learning_rate": 0.0008272277227722772,
      "loss": 2.0105,
      "step": 1349
    },
    {
      "epoch": 8.921933085501859,
      "grad_norm": 86.77294158935547,
      "learning_rate": 0.0008267326732673267,
      "loss": 3.7371,
      "step": 1350
    },
    {
      "epoch": 8.928541924824453,
      "grad_norm": 43.528961181640625,
      "learning_rate": 0.0008262376237623762,
      "loss": 1.3919,
      "step": 1351
    },
    {
      "epoch": 8.935150764147046,
      "grad_norm": 17.664445877075195,
      "learning_rate": 0.0008257425742574258,
      "loss": 1.7531,
      "step": 1352
    },
    {
      "epoch": 8.94175960346964,
      "grad_norm": 78.855224609375,
      "learning_rate": 0.0008252475247524752,
      "loss": 5.2613,
      "step": 1353
    },
    {
      "epoch": 8.948368442792235,
      "grad_norm": 31.54233741760254,
      "learning_rate": 0.0008247524752475247,
      "loss": 1.5057,
      "step": 1354
    },
    {
      "epoch": 8.954977282114829,
      "grad_norm": 30.014413833618164,
      "learning_rate": 0.0008242574257425742,
      "loss": 6.1025,
      "step": 1355
    },
    {
      "epoch": 8.961586121437422,
      "grad_norm": 32.90418243408203,
      "learning_rate": 0.0008237623762376238,
      "loss": 1.3899,
      "step": 1356
    },
    {
      "epoch": 8.968194960760016,
      "grad_norm": 20.413837432861328,
      "learning_rate": 0.0008232673267326733,
      "loss": 3.03,
      "step": 1357
    },
    {
      "epoch": 8.974803800082611,
      "grad_norm": 100.27903747558594,
      "learning_rate": 0.0008227722772277227,
      "loss": 2.3159,
      "step": 1358
    },
    {
      "epoch": 8.981412639405205,
      "grad_norm": 86.3998794555664,
      "learning_rate": 0.0008222772277227722,
      "loss": 4.8271,
      "step": 1359
    },
    {
      "epoch": 8.988021478727799,
      "grad_norm": 41.711116790771484,
      "learning_rate": 0.0008217821782178218,
      "loss": 4.8313,
      "step": 1360
    },
    {
      "epoch": 8.994630318050392,
      "grad_norm": 114.50227355957031,
      "learning_rate": 0.0008212871287128713,
      "loss": 3.7457,
      "step": 1361
    },
    {
      "epoch": 8.994630318050392,
      "eval_validation_error_bar": 0.04911301211583739,
      "eval_validation_loss": 7.954598426818848,
      "eval_validation_pearsonr": 0.6047762690907232,
      "eval_validation_rmse": 2.820389747619629,
      "eval_validation_runtime": 41.5383,
      "eval_validation_samples_per_second": 4.887,
      "eval_validation_spearman": 0.6005809783413041,
      "eval_validation_steps_per_second": 4.887,
      "step": 1361
    },
    {
      "epoch": 8.994630318050392,
      "eval_test_error_bar": 0.04449815343826335,
      "eval_test_loss": 7.987256050109863,
      "eval_test_pearsonr": 0.5008075434105544,
      "eval_test_rmse": 2.8261733055114746,
      "eval_test_runtime": 57.5534,
      "eval_test_samples_per_second": 5.664,
      "eval_test_spearman": 0.49517918324519694,
      "eval_test_steps_per_second": 5.664,
      "step": 1361
    },
    {
      "epoch": 9.001239157372986,
      "grad_norm": 120.94200897216797,
      "learning_rate": 0.0008207920792079208,
      "loss": 3.0933,
      "step": 1362
    },
    {
      "epoch": 9.007847996695581,
      "grad_norm": 55.37635040283203,
      "learning_rate": 0.0008202970297029702,
      "loss": 1.3279,
      "step": 1363
    },
    {
      "epoch": 9.014456836018175,
      "grad_norm": 56.50992202758789,
      "learning_rate": 0.0008198019801980197,
      "loss": 4.2431,
      "step": 1364
    },
    {
      "epoch": 9.021065675340768,
      "grad_norm": 231.53199768066406,
      "learning_rate": 0.0008193069306930693,
      "loss": 7.652,
      "step": 1365
    },
    {
      "epoch": 9.027674514663362,
      "grad_norm": 141.5421142578125,
      "learning_rate": 0.0008188118811881188,
      "loss": 5.3771,
      "step": 1366
    },
    {
      "epoch": 9.034283353985956,
      "grad_norm": 139.22183227539062,
      "learning_rate": 0.0008183168316831683,
      "loss": 7.5725,
      "step": 1367
    },
    {
      "epoch": 9.04089219330855,
      "grad_norm": 101.1108627319336,
      "learning_rate": 0.0008178217821782177,
      "loss": 5.3536,
      "step": 1368
    },
    {
      "epoch": 9.047501032631144,
      "grad_norm": 162.8556671142578,
      "learning_rate": 0.0008173267326732673,
      "loss": 5.2506,
      "step": 1369
    },
    {
      "epoch": 9.054109871953738,
      "grad_norm": 71.544921875,
      "learning_rate": 0.0008168316831683168,
      "loss": 4.4055,
      "step": 1370
    },
    {
      "epoch": 9.060718711276332,
      "grad_norm": 76.72731018066406,
      "learning_rate": 0.0008163366336633663,
      "loss": 2.8077,
      "step": 1371
    },
    {
      "epoch": 9.067327550598925,
      "grad_norm": 18.12366485595703,
      "learning_rate": 0.0008158415841584159,
      "loss": 2.0792,
      "step": 1372
    },
    {
      "epoch": 9.07393638992152,
      "grad_norm": 19.805509567260742,
      "learning_rate": 0.0008153465346534653,
      "loss": 3.6721,
      "step": 1373
    },
    {
      "epoch": 9.080545229244114,
      "grad_norm": 42.15361022949219,
      "learning_rate": 0.0008148514851485148,
      "loss": 5.0375,
      "step": 1374
    },
    {
      "epoch": 9.087154068566708,
      "grad_norm": 12.176229476928711,
      "learning_rate": 0.0008143564356435643,
      "loss": 2.55,
      "step": 1375
    },
    {
      "epoch": 9.093762907889301,
      "grad_norm": 29.376989364624023,
      "learning_rate": 0.0008138613861386138,
      "loss": 1.7965,
      "step": 1376
    },
    {
      "epoch": 9.100371747211897,
      "grad_norm": 26.300687789916992,
      "learning_rate": 0.0008133663366336634,
      "loss": 4.0138,
      "step": 1377
    },
    {
      "epoch": 9.10698058653449,
      "grad_norm": 34.297447204589844,
      "learning_rate": 0.0008128712871287128,
      "loss": 3.493,
      "step": 1378
    },
    {
      "epoch": 9.113589425857084,
      "grad_norm": 12.679130554199219,
      "learning_rate": 0.0008123762376237624,
      "loss": 1.5988,
      "step": 1379
    },
    {
      "epoch": 9.120198265179678,
      "grad_norm": 43.57777786254883,
      "learning_rate": 0.000811881188118812,
      "loss": 2.7957,
      "step": 1380
    },
    {
      "epoch": 9.126807104502271,
      "grad_norm": 100.63510131835938,
      "learning_rate": 0.0008113861386138615,
      "loss": 4.6303,
      "step": 1381
    },
    {
      "epoch": 9.133415943824867,
      "grad_norm": 97.47472381591797,
      "learning_rate": 0.000810891089108911,
      "loss": 6.7959,
      "step": 1382
    },
    {
      "epoch": 9.14002478314746,
      "grad_norm": 127.75544738769531,
      "learning_rate": 0.0008103960396039604,
      "loss": 4.9443,
      "step": 1383
    },
    {
      "epoch": 9.146633622470054,
      "grad_norm": 69.64998626708984,
      "learning_rate": 0.00080990099009901,
      "loss": 3.7041,
      "step": 1384
    },
    {
      "epoch": 9.153242461792647,
      "grad_norm": 38.77183532714844,
      "learning_rate": 0.0008094059405940595,
      "loss": 4.9016,
      "step": 1385
    },
    {
      "epoch": 9.159851301115241,
      "grad_norm": 133.8131866455078,
      "learning_rate": 0.000808910891089109,
      "loss": 5.2339,
      "step": 1386
    },
    {
      "epoch": 9.166460140437836,
      "grad_norm": 14.175559997558594,
      "learning_rate": 0.0008084158415841585,
      "loss": 2.7024,
      "step": 1387
    },
    {
      "epoch": 9.17306897976043,
      "grad_norm": 34.597450256347656,
      "learning_rate": 0.0008079207920792079,
      "loss": 5.0723,
      "step": 1388
    },
    {
      "epoch": 9.179677819083023,
      "grad_norm": 115.2420883178711,
      "learning_rate": 0.0008074257425742575,
      "loss": 6.1911,
      "step": 1389
    },
    {
      "epoch": 9.186286658405617,
      "grad_norm": 34.93359375,
      "learning_rate": 0.000806930693069307,
      "loss": 2.2516,
      "step": 1390
    },
    {
      "epoch": 9.19289549772821,
      "grad_norm": 164.21852111816406,
      "learning_rate": 0.0008064356435643565,
      "loss": 8.0722,
      "step": 1391
    },
    {
      "epoch": 9.199504337050806,
      "grad_norm": 62.16498565673828,
      "learning_rate": 0.000805940594059406,
      "loss": 4.2515,
      "step": 1392
    },
    {
      "epoch": 9.2061131763734,
      "grad_norm": 145.16867065429688,
      "learning_rate": 0.0008054455445544555,
      "loss": 4.4648,
      "step": 1393
    },
    {
      "epoch": 9.212722015695993,
      "grad_norm": 131.7073211669922,
      "learning_rate": 0.000804950495049505,
      "loss": 5.8819,
      "step": 1394
    },
    {
      "epoch": 9.219330855018587,
      "grad_norm": 32.275421142578125,
      "learning_rate": 0.0008044554455445545,
      "loss": 7.0002,
      "step": 1395
    },
    {
      "epoch": 9.225939694341182,
      "grad_norm": 83.49662017822266,
      "learning_rate": 0.000803960396039604,
      "loss": 3.391,
      "step": 1396
    },
    {
      "epoch": 9.232548533663776,
      "grad_norm": 57.80185317993164,
      "learning_rate": 0.0008034653465346536,
      "loss": 2.4618,
      "step": 1397
    },
    {
      "epoch": 9.23915737298637,
      "grad_norm": 78.24537658691406,
      "learning_rate": 0.000802970297029703,
      "loss": 2.5673,
      "step": 1398
    },
    {
      "epoch": 9.245766212308963,
      "grad_norm": 53.473323822021484,
      "learning_rate": 0.0008024752475247525,
      "loss": 2.3674,
      "step": 1399
    },
    {
      "epoch": 9.252375051631557,
      "grad_norm": 105.40129852294922,
      "learning_rate": 0.000801980198019802,
      "loss": 3.6815,
      "step": 1400
    },
    {
      "epoch": 9.258983890954152,
      "grad_norm": 40.076168060302734,
      "learning_rate": 0.0008014851485148516,
      "loss": 3.5832,
      "step": 1401
    },
    {
      "epoch": 9.265592730276746,
      "grad_norm": 128.73851013183594,
      "learning_rate": 0.0008009900990099011,
      "loss": 4.6713,
      "step": 1402
    },
    {
      "epoch": 9.27220156959934,
      "grad_norm": 100.2632827758789,
      "learning_rate": 0.0008004950495049505,
      "loss": 5.1866,
      "step": 1403
    },
    {
      "epoch": 9.278810408921933,
      "grad_norm": 26.688438415527344,
      "learning_rate": 0.0008,
      "loss": 4.2267,
      "step": 1404
    },
    {
      "epoch": 9.285419248244526,
      "grad_norm": 10.855306625366211,
      "learning_rate": 0.0007995049504950496,
      "loss": 0.7306,
      "step": 1405
    },
    {
      "epoch": 9.292028087567122,
      "grad_norm": 60.02293395996094,
      "learning_rate": 0.0007990099009900991,
      "loss": 2.4344,
      "step": 1406
    },
    {
      "epoch": 9.298636926889715,
      "grad_norm": 126.77301025390625,
      "learning_rate": 0.0007985148514851486,
      "loss": 2.6456,
      "step": 1407
    },
    {
      "epoch": 9.305245766212309,
      "grad_norm": 39.828495025634766,
      "learning_rate": 0.000798019801980198,
      "loss": 4.6538,
      "step": 1408
    },
    {
      "epoch": 9.311854605534903,
      "grad_norm": 18.924835205078125,
      "learning_rate": 0.0007975247524752476,
      "loss": 2.5974,
      "step": 1409
    },
    {
      "epoch": 9.318463444857496,
      "grad_norm": 146.26605224609375,
      "learning_rate": 0.0007970297029702971,
      "loss": 5.6618,
      "step": 1410
    },
    {
      "epoch": 9.325072284180091,
      "grad_norm": 98.52484130859375,
      "learning_rate": 0.0007965346534653466,
      "loss": 2.0418,
      "step": 1411
    },
    {
      "epoch": 9.331681123502685,
      "grad_norm": 22.925134658813477,
      "learning_rate": 0.0007960396039603961,
      "loss": 1.6607,
      "step": 1412
    },
    {
      "epoch": 9.338289962825279,
      "grad_norm": 45.34625244140625,
      "learning_rate": 0.0007955445544554456,
      "loss": 2.2894,
      "step": 1413
    },
    {
      "epoch": 9.344898802147872,
      "grad_norm": 73.96153259277344,
      "learning_rate": 0.0007950495049504951,
      "loss": 2.6889,
      "step": 1414
    },
    {
      "epoch": 9.351507641470466,
      "grad_norm": 57.20615768432617,
      "learning_rate": 0.0007945544554455446,
      "loss": 4.4836,
      "step": 1415
    },
    {
      "epoch": 9.358116480793061,
      "grad_norm": 42.16598129272461,
      "learning_rate": 0.0007940594059405941,
      "loss": 1.3302,
      "step": 1416
    },
    {
      "epoch": 9.364725320115655,
      "grad_norm": 67.16082000732422,
      "learning_rate": 0.0007935643564356437,
      "loss": 2.806,
      "step": 1417
    },
    {
      "epoch": 9.371334159438248,
      "grad_norm": 121.97310638427734,
      "learning_rate": 0.0007930693069306931,
      "loss": 4.9845,
      "step": 1418
    },
    {
      "epoch": 9.377942998760842,
      "grad_norm": 114.67115783691406,
      "learning_rate": 0.0007925742574257426,
      "loss": 3.7141,
      "step": 1419
    },
    {
      "epoch": 9.384551838083437,
      "grad_norm": 10.838750839233398,
      "learning_rate": 0.0007920792079207921,
      "loss": 2.3163,
      "step": 1420
    },
    {
      "epoch": 9.391160677406031,
      "grad_norm": 50.8521728515625,
      "learning_rate": 0.0007915841584158417,
      "loss": 2.3147,
      "step": 1421
    },
    {
      "epoch": 9.397769516728625,
      "grad_norm": 33.302490234375,
      "learning_rate": 0.0007910891089108912,
      "loss": 3.074,
      "step": 1422
    },
    {
      "epoch": 9.404378356051218,
      "grad_norm": 19.37384796142578,
      "learning_rate": 0.0007905940594059406,
      "loss": 1.388,
      "step": 1423
    },
    {
      "epoch": 9.410987195373812,
      "grad_norm": 89.37250518798828,
      "learning_rate": 0.0007900990099009901,
      "loss": 4.5457,
      "step": 1424
    },
    {
      "epoch": 9.417596034696407,
      "grad_norm": 50.81482696533203,
      "learning_rate": 0.0007896039603960397,
      "loss": 3.1522,
      "step": 1425
    },
    {
      "epoch": 9.424204874019,
      "grad_norm": 20.71288299560547,
      "learning_rate": 0.0007891089108910892,
      "loss": 2.4776,
      "step": 1426
    },
    {
      "epoch": 9.430813713341594,
      "grad_norm": 19.397260665893555,
      "learning_rate": 0.0007886138613861387,
      "loss": 3.64,
      "step": 1427
    },
    {
      "epoch": 9.437422552664188,
      "grad_norm": 54.549461364746094,
      "learning_rate": 0.0007881188118811881,
      "loss": 1.9738,
      "step": 1428
    },
    {
      "epoch": 9.444031391986782,
      "grad_norm": 117.67857360839844,
      "learning_rate": 0.0007876237623762377,
      "loss": 3.367,
      "step": 1429
    },
    {
      "epoch": 9.450640231309377,
      "grad_norm": 25.619491577148438,
      "learning_rate": 0.0007871287128712872,
      "loss": 0.7972,
      "step": 1430
    },
    {
      "epoch": 9.45724907063197,
      "grad_norm": 38.327999114990234,
      "learning_rate": 0.0007866336633663367,
      "loss": 3.0329,
      "step": 1431
    },
    {
      "epoch": 9.463857909954564,
      "grad_norm": 87.32584381103516,
      "learning_rate": 0.0007861386138613862,
      "loss": 3.3452,
      "step": 1432
    },
    {
      "epoch": 9.470466749277158,
      "grad_norm": 58.4837760925293,
      "learning_rate": 0.0007856435643564356,
      "loss": 2.5409,
      "step": 1433
    },
    {
      "epoch": 9.477075588599753,
      "grad_norm": 9.399603843688965,
      "learning_rate": 0.0007851485148514852,
      "loss": 1.1702,
      "step": 1434
    },
    {
      "epoch": 9.483684427922347,
      "grad_norm": 38.21787643432617,
      "learning_rate": 0.0007846534653465347,
      "loss": 2.065,
      "step": 1435
    },
    {
      "epoch": 9.49029326724494,
      "grad_norm": 63.14256286621094,
      "learning_rate": 0.0007841584158415842,
      "loss": 3.9336,
      "step": 1436
    },
    {
      "epoch": 9.496902106567534,
      "grad_norm": 29.378400802612305,
      "learning_rate": 0.0007836633663366338,
      "loss": 1.0546,
      "step": 1437
    },
    {
      "epoch": 9.503510945890127,
      "grad_norm": 85.56237030029297,
      "learning_rate": 0.0007831683168316832,
      "loss": 4.2171,
      "step": 1438
    },
    {
      "epoch": 9.510119785212723,
      "grad_norm": 62.6343994140625,
      "learning_rate": 0.0007826732673267327,
      "loss": 1.6479,
      "step": 1439
    },
    {
      "epoch": 9.516728624535316,
      "grad_norm": 85.50389099121094,
      "learning_rate": 0.0007821782178217822,
      "loss": 4.9396,
      "step": 1440
    },
    {
      "epoch": 9.52333746385791,
      "grad_norm": 104.71247863769531,
      "learning_rate": 0.0007816831683168317,
      "loss": 4.4489,
      "step": 1441
    },
    {
      "epoch": 9.529946303180504,
      "grad_norm": 116.3734359741211,
      "learning_rate": 0.0007811881188118813,
      "loss": 3.1016,
      "step": 1442
    },
    {
      "epoch": 9.536555142503097,
      "grad_norm": 64.8331069946289,
      "learning_rate": 0.0007806930693069307,
      "loss": 3.8645,
      "step": 1443
    },
    {
      "epoch": 9.543163981825693,
      "grad_norm": 27.673784255981445,
      "learning_rate": 0.0007801980198019802,
      "loss": 1.9848,
      "step": 1444
    },
    {
      "epoch": 9.549772821148286,
      "grad_norm": 26.72773551940918,
      "learning_rate": 0.0007797029702970297,
      "loss": 3.1147,
      "step": 1445
    },
    {
      "epoch": 9.55638166047088,
      "grad_norm": 135.5281982421875,
      "learning_rate": 0.0007792079207920793,
      "loss": 3.5751,
      "step": 1446
    },
    {
      "epoch": 9.562990499793473,
      "grad_norm": 65.79557037353516,
      "learning_rate": 0.0007787128712871288,
      "loss": 2.9075,
      "step": 1447
    },
    {
      "epoch": 9.569599339116067,
      "grad_norm": 77.8201904296875,
      "learning_rate": 0.0007782178217821782,
      "loss": 2.0319,
      "step": 1448
    },
    {
      "epoch": 9.576208178438662,
      "grad_norm": 51.031585693359375,
      "learning_rate": 0.0007777227722772277,
      "loss": 3.3085,
      "step": 1449
    },
    {
      "epoch": 9.582817017761256,
      "grad_norm": 72.06746673583984,
      "learning_rate": 0.0007772277227722773,
      "loss": 4.2415,
      "step": 1450
    },
    {
      "epoch": 9.58942585708385,
      "grad_norm": 32.30062484741211,
      "learning_rate": 0.0007767326732673268,
      "loss": 3.612,
      "step": 1451
    },
    {
      "epoch": 9.596034696406443,
      "grad_norm": 30.905973434448242,
      "learning_rate": 0.0007762376237623763,
      "loss": 0.9679,
      "step": 1452
    },
    {
      "epoch": 9.602643535729037,
      "grad_norm": 113.79874420166016,
      "learning_rate": 0.0007757425742574257,
      "loss": 5.7461,
      "step": 1453
    },
    {
      "epoch": 9.609252375051632,
      "grad_norm": 98.60191345214844,
      "learning_rate": 0.0007752475247524753,
      "loss": 3.4869,
      "step": 1454
    },
    {
      "epoch": 9.615861214374226,
      "grad_norm": 80.17718505859375,
      "learning_rate": 0.0007747524752475248,
      "loss": 1.703,
      "step": 1455
    },
    {
      "epoch": 9.62247005369682,
      "grad_norm": 18.016826629638672,
      "learning_rate": 0.0007742574257425743,
      "loss": 2.5203,
      "step": 1456
    },
    {
      "epoch": 9.629078893019413,
      "grad_norm": 87.01956176757812,
      "learning_rate": 0.0007737623762376238,
      "loss": 2.3125,
      "step": 1457
    },
    {
      "epoch": 9.635687732342008,
      "grad_norm": 71.21034240722656,
      "learning_rate": 0.0007732673267326733,
      "loss": 2.8367,
      "step": 1458
    },
    {
      "epoch": 9.642296571664602,
      "grad_norm": 7.3924407958984375,
      "learning_rate": 0.0007727722772277228,
      "loss": 2.0084,
      "step": 1459
    },
    {
      "epoch": 9.648905410987195,
      "grad_norm": 25.221744537353516,
      "learning_rate": 0.0007722772277227723,
      "loss": 1.8394,
      "step": 1460
    },
    {
      "epoch": 9.655514250309789,
      "grad_norm": 94.56704711914062,
      "learning_rate": 0.0007717821782178218,
      "loss": 3.5899,
      "step": 1461
    },
    {
      "epoch": 9.662123089632383,
      "grad_norm": 46.48428726196289,
      "learning_rate": 0.0007712871287128714,
      "loss": 3.9443,
      "step": 1462
    },
    {
      "epoch": 9.668731928954978,
      "grad_norm": 102.69456481933594,
      "learning_rate": 0.0007707920792079208,
      "loss": 4.4148,
      "step": 1463
    },
    {
      "epoch": 9.675340768277572,
      "grad_norm": 38.79684066772461,
      "learning_rate": 0.0007702970297029703,
      "loss": 4.2773,
      "step": 1464
    },
    {
      "epoch": 9.681949607600165,
      "grad_norm": 106.59615325927734,
      "learning_rate": 0.0007698019801980198,
      "loss": 2.3246,
      "step": 1465
    },
    {
      "epoch": 9.688558446922759,
      "grad_norm": 33.73439025878906,
      "learning_rate": 0.0007693069306930694,
      "loss": 1.4377,
      "step": 1466
    },
    {
      "epoch": 9.695167286245352,
      "grad_norm": 14.650755882263184,
      "learning_rate": 0.0007688118811881189,
      "loss": 0.8379,
      "step": 1467
    },
    {
      "epoch": 9.701776125567948,
      "grad_norm": 111.47930908203125,
      "learning_rate": 0.0007683168316831683,
      "loss": 3.7392,
      "step": 1468
    },
    {
      "epoch": 9.708384964890541,
      "grad_norm": 107.2160415649414,
      "learning_rate": 0.0007678217821782178,
      "loss": 3.6067,
      "step": 1469
    },
    {
      "epoch": 9.714993804213135,
      "grad_norm": 35.07353591918945,
      "learning_rate": 0.0007673267326732674,
      "loss": 3.2163,
      "step": 1470
    },
    {
      "epoch": 9.721602643535729,
      "grad_norm": 63.60392379760742,
      "learning_rate": 0.0007668316831683169,
      "loss": 2.2071,
      "step": 1471
    },
    {
      "epoch": 9.728211482858324,
      "grad_norm": 111.94567108154297,
      "learning_rate": 0.0007663366336633664,
      "loss": 3.3359,
      "step": 1472
    },
    {
      "epoch": 9.734820322180918,
      "grad_norm": 42.989139556884766,
      "learning_rate": 0.0007658415841584158,
      "loss": 3.1401,
      "step": 1473
    },
    {
      "epoch": 9.741429161503511,
      "grad_norm": 52.592857360839844,
      "learning_rate": 0.0007653465346534654,
      "loss": 3.0095,
      "step": 1474
    },
    {
      "epoch": 9.748038000826105,
      "grad_norm": 45.58799743652344,
      "learning_rate": 0.0007648514851485149,
      "loss": 1.3023,
      "step": 1475
    },
    {
      "epoch": 9.754646840148698,
      "grad_norm": 55.27009201049805,
      "learning_rate": 0.0007643564356435644,
      "loss": 1.9767,
      "step": 1476
    },
    {
      "epoch": 9.761255679471294,
      "grad_norm": 35.17814254760742,
      "learning_rate": 0.0007638613861386139,
      "loss": 2.6282,
      "step": 1477
    },
    {
      "epoch": 9.767864518793887,
      "grad_norm": 73.505615234375,
      "learning_rate": 0.0007633663366336634,
      "loss": 1.3799,
      "step": 1478
    },
    {
      "epoch": 9.77447335811648,
      "grad_norm": 11.775114059448242,
      "learning_rate": 0.0007628712871287129,
      "loss": 3.5697,
      "step": 1479
    },
    {
      "epoch": 9.781082197439074,
      "grad_norm": 88.55591583251953,
      "learning_rate": 0.0007623762376237624,
      "loss": 3.3129,
      "step": 1480
    },
    {
      "epoch": 9.787691036761668,
      "grad_norm": 24.880638122558594,
      "learning_rate": 0.0007618811881188119,
      "loss": 3.8257,
      "step": 1481
    },
    {
      "epoch": 9.794299876084263,
      "grad_norm": 32.24994659423828,
      "learning_rate": 0.0007613861386138615,
      "loss": 4.3575,
      "step": 1482
    },
    {
      "epoch": 9.800908715406857,
      "grad_norm": 34.24153137207031,
      "learning_rate": 0.0007608910891089109,
      "loss": 1.2781,
      "step": 1483
    },
    {
      "epoch": 9.80751755472945,
      "grad_norm": 29.58368682861328,
      "learning_rate": 0.0007603960396039604,
      "loss": 2.084,
      "step": 1484
    },
    {
      "epoch": 9.814126394052044,
      "grad_norm": 37.07946014404297,
      "learning_rate": 0.0007599009900990099,
      "loss": 1.4018,
      "step": 1485
    },
    {
      "epoch": 9.820735233374638,
      "grad_norm": 28.415435791015625,
      "learning_rate": 0.0007594059405940595,
      "loss": 1.7502,
      "step": 1486
    },
    {
      "epoch": 9.827344072697233,
      "grad_norm": 92.6664810180664,
      "learning_rate": 0.000758910891089109,
      "loss": 2.2052,
      "step": 1487
    },
    {
      "epoch": 9.833952912019827,
      "grad_norm": 170.35316467285156,
      "learning_rate": 0.0007584158415841584,
      "loss": 6.1455,
      "step": 1488
    },
    {
      "epoch": 9.84056175134242,
      "grad_norm": 126.75304412841797,
      "learning_rate": 0.0007579207920792079,
      "loss": 3.6218,
      "step": 1489
    },
    {
      "epoch": 9.847170590665014,
      "grad_norm": 67.32160186767578,
      "learning_rate": 0.0007574257425742574,
      "loss": 1.222,
      "step": 1490
    },
    {
      "epoch": 9.853779429987608,
      "grad_norm": 20.400968551635742,
      "learning_rate": 0.000756930693069307,
      "loss": 1.7383,
      "step": 1491
    },
    {
      "epoch": 9.860388269310203,
      "grad_norm": 27.66414451599121,
      "learning_rate": 0.0007564356435643565,
      "loss": 4.5229,
      "step": 1492
    },
    {
      "epoch": 9.866997108632797,
      "grad_norm": 83.1675796508789,
      "learning_rate": 0.0007559405940594059,
      "loss": 1.601,
      "step": 1493
    },
    {
      "epoch": 9.87360594795539,
      "grad_norm": 146.39385986328125,
      "learning_rate": 0.0007554455445544554,
      "loss": 5.8499,
      "step": 1494
    },
    {
      "epoch": 9.880214787277984,
      "grad_norm": 86.14166259765625,
      "learning_rate": 0.000754950495049505,
      "loss": 6.5909,
      "step": 1495
    },
    {
      "epoch": 9.886823626600577,
      "grad_norm": 29.99368667602539,
      "learning_rate": 0.0007544554455445545,
      "loss": 2.7368,
      "step": 1496
    },
    {
      "epoch": 9.893432465923173,
      "grad_norm": 45.611873626708984,
      "learning_rate": 0.000753960396039604,
      "loss": 1.1796,
      "step": 1497
    },
    {
      "epoch": 9.900041305245766,
      "grad_norm": 23.563018798828125,
      "learning_rate": 0.0007534653465346534,
      "loss": 1.1523,
      "step": 1498
    },
    {
      "epoch": 9.90665014456836,
      "grad_norm": 35.69770431518555,
      "learning_rate": 0.000752970297029703,
      "loss": 2.6671,
      "step": 1499
    },
    {
      "epoch": 9.913258983890954,
      "grad_norm": 48.44951629638672,
      "learning_rate": 0.0007524752475247525,
      "loss": 1.4291,
      "step": 1500
    },
    {
      "epoch": 9.919867823213549,
      "grad_norm": 7.548491477966309,
      "learning_rate": 0.000751980198019802,
      "loss": 0.934,
      "step": 1501
    },
    {
      "epoch": 9.926476662536142,
      "grad_norm": 76.24830627441406,
      "learning_rate": 0.0007514851485148515,
      "loss": 3.0521,
      "step": 1502
    },
    {
      "epoch": 9.933085501858736,
      "grad_norm": 54.32974624633789,
      "learning_rate": 0.000750990099009901,
      "loss": 2.1026,
      "step": 1503
    },
    {
      "epoch": 9.93969434118133,
      "grad_norm": 103.63563537597656,
      "learning_rate": 0.0007504950495049505,
      "loss": 2.8332,
      "step": 1504
    },
    {
      "epoch": 9.946303180503923,
      "grad_norm": 152.1211700439453,
      "learning_rate": 0.00075,
      "loss": 3.7798,
      "step": 1505
    },
    {
      "epoch": 9.952912019826519,
      "grad_norm": 118.43317413330078,
      "learning_rate": 0.0007495049504950495,
      "loss": 4.6835,
      "step": 1506
    },
    {
      "epoch": 9.959520859149112,
      "grad_norm": 60.23274230957031,
      "learning_rate": 0.0007490099009900991,
      "loss": 2.5938,
      "step": 1507
    },
    {
      "epoch": 9.966129698471706,
      "grad_norm": 14.591599464416504,
      "learning_rate": 0.0007485148514851485,
      "loss": 1.3567,
      "step": 1508
    },
    {
      "epoch": 9.9727385377943,
      "grad_norm": 92.02881622314453,
      "learning_rate": 0.000748019801980198,
      "loss": 4.6173,
      "step": 1509
    },
    {
      "epoch": 9.979347377116893,
      "grad_norm": 53.728023529052734,
      "learning_rate": 0.0007475247524752475,
      "loss": 2.7328,
      "step": 1510
    },
    {
      "epoch": 9.985956216439488,
      "grad_norm": 45.697479248046875,
      "learning_rate": 0.0007470297029702971,
      "loss": 1.506,
      "step": 1511
    },
    {
      "epoch": 9.992565055762082,
      "grad_norm": 21.40484619140625,
      "learning_rate": 0.0007465346534653466,
      "loss": 1.7448,
      "step": 1512
    },
    {
      "epoch": 9.999173895084676,
      "grad_norm": 124.78858947753906,
      "learning_rate": 0.000746039603960396,
      "loss": 5.0986,
      "step": 1513
    },
    {
      "epoch": 9.999173895084676,
      "eval_validation_error_bar": 0.04616640115987612,
      "eval_validation_loss": 6.594452381134033,
      "eval_validation_pearsonr": 0.5879122658349799,
      "eval_validation_rmse": 2.5679666996002197,
      "eval_validation_runtime": 41.5216,
      "eval_validation_samples_per_second": 4.889,
      "eval_validation_spearman": 0.6360500645557665,
      "eval_validation_steps_per_second": 4.889,
      "step": 1513
    },
    {
      "epoch": 9.999173895084676,
      "eval_test_error_bar": 0.04004821758311836,
      "eval_test_loss": 6.6895833015441895,
      "eval_test_pearsonr": 0.5977823581375415,
      "eval_test_rmse": 2.586422920227051,
      "eval_test_runtime": 56.1748,
      "eval_test_samples_per_second": 5.803,
      "eval_test_spearman": 0.5776566880367653,
      "eval_test_steps_per_second": 5.803,
      "step": 1513
    },
    {
      "epoch": 10.00578273440727,
      "grad_norm": 116.60145568847656,
      "learning_rate": 0.0007455445544554455,
      "loss": 4.7511,
      "step": 1514
    },
    {
      "epoch": 10.012391573729865,
      "grad_norm": 102.0093994140625,
      "learning_rate": 0.0007450495049504951,
      "loss": 2.8878,
      "step": 1515
    },
    {
      "epoch": 10.019000413052458,
      "grad_norm": 15.986715316772461,
      "learning_rate": 0.0007445544554455446,
      "loss": 1.1258,
      "step": 1516
    },
    {
      "epoch": 10.025609252375052,
      "grad_norm": 80.40503692626953,
      "learning_rate": 0.0007440594059405941,
      "loss": 4.8291,
      "step": 1517
    },
    {
      "epoch": 10.032218091697645,
      "grad_norm": 45.712284088134766,
      "learning_rate": 0.0007435643564356435,
      "loss": 3.3587,
      "step": 1518
    },
    {
      "epoch": 10.038826931020239,
      "grad_norm": 104.4583511352539,
      "learning_rate": 0.0007430693069306931,
      "loss": 4.7964,
      "step": 1519
    },
    {
      "epoch": 10.045435770342834,
      "grad_norm": 46.973533630371094,
      "learning_rate": 0.0007425742574257426,
      "loss": 2.1503,
      "step": 1520
    },
    {
      "epoch": 10.052044609665428,
      "grad_norm": 17.753265380859375,
      "learning_rate": 0.0007420792079207921,
      "loss": 2.4101,
      "step": 1521
    },
    {
      "epoch": 10.058653448988021,
      "grad_norm": 24.898149490356445,
      "learning_rate": 0.0007415841584158416,
      "loss": 1.2499,
      "step": 1522
    },
    {
      "epoch": 10.065262288310615,
      "grad_norm": 16.78569221496582,
      "learning_rate": 0.000741089108910891,
      "loss": 1.2216,
      "step": 1523
    },
    {
      "epoch": 10.071871127633209,
      "grad_norm": 10.704718589782715,
      "learning_rate": 0.0007405940594059406,
      "loss": 1.4597,
      "step": 1524
    },
    {
      "epoch": 10.078479966955804,
      "grad_norm": 29.922794342041016,
      "learning_rate": 0.0007400990099009901,
      "loss": 2.4693,
      "step": 1525
    },
    {
      "epoch": 10.085088806278398,
      "grad_norm": 84.12014770507812,
      "learning_rate": 0.0007396039603960396,
      "loss": 5.6343,
      "step": 1526
    },
    {
      "epoch": 10.091697645600991,
      "grad_norm": 51.09169006347656,
      "learning_rate": 0.0007391089108910892,
      "loss": 2.3057,
      "step": 1527
    },
    {
      "epoch": 10.098306484923585,
      "grad_norm": 87.74652862548828,
      "learning_rate": 0.0007386138613861386,
      "loss": 6.4251,
      "step": 1528
    },
    {
      "epoch": 10.104915324246178,
      "grad_norm": 62.83537673950195,
      "learning_rate": 0.0007381188118811881,
      "loss": 2.2211,
      "step": 1529
    },
    {
      "epoch": 10.111524163568774,
      "grad_norm": 28.454174041748047,
      "learning_rate": 0.0007376237623762376,
      "loss": 1.7172,
      "step": 1530
    },
    {
      "epoch": 10.118133002891367,
      "grad_norm": 18.627613067626953,
      "learning_rate": 0.0007371287128712872,
      "loss": 3.8039,
      "step": 1531
    },
    {
      "epoch": 10.124741842213961,
      "grad_norm": 23.701297760009766,
      "learning_rate": 0.0007366336633663367,
      "loss": 2.5712,
      "step": 1532
    },
    {
      "epoch": 10.131350681536555,
      "grad_norm": 20.37432861328125,
      "learning_rate": 0.0007361386138613861,
      "loss": 2.737,
      "step": 1533
    },
    {
      "epoch": 10.13795952085915,
      "grad_norm": 32.15165328979492,
      "learning_rate": 0.0007356435643564356,
      "loss": 2.9448,
      "step": 1534
    },
    {
      "epoch": 10.144568360181744,
      "grad_norm": 28.011869430541992,
      "learning_rate": 0.0007351485148514852,
      "loss": 1.5058,
      "step": 1535
    },
    {
      "epoch": 10.151177199504337,
      "grad_norm": 46.31565856933594,
      "learning_rate": 0.0007346534653465347,
      "loss": 1.8982,
      "step": 1536
    },
    {
      "epoch": 10.15778603882693,
      "grad_norm": 66.26496887207031,
      "learning_rate": 0.0007341584158415842,
      "loss": 3.0418,
      "step": 1537
    },
    {
      "epoch": 10.164394878149524,
      "grad_norm": 68.29379272460938,
      "learning_rate": 0.0007336633663366336,
      "loss": 2.8302,
      "step": 1538
    },
    {
      "epoch": 10.17100371747212,
      "grad_norm": 52.6671028137207,
      "learning_rate": 0.0007331683168316831,
      "loss": 3.4193,
      "step": 1539
    },
    {
      "epoch": 10.177612556794713,
      "grad_norm": 73.328369140625,
      "learning_rate": 0.0007326732673267327,
      "loss": 1.9275,
      "step": 1540
    },
    {
      "epoch": 10.184221396117307,
      "grad_norm": 10.301726341247559,
      "learning_rate": 0.0007321782178217822,
      "loss": 1.349,
      "step": 1541
    },
    {
      "epoch": 10.1908302354399,
      "grad_norm": 55.63716506958008,
      "learning_rate": 0.0007316831683168317,
      "loss": 1.5645,
      "step": 1542
    },
    {
      "epoch": 10.197439074762494,
      "grad_norm": 27.81719970703125,
      "learning_rate": 0.0007311881188118811,
      "loss": 3.9229,
      "step": 1543
    },
    {
      "epoch": 10.20404791408509,
      "grad_norm": 65.35004425048828,
      "learning_rate": 0.0007306930693069307,
      "loss": 2.5251,
      "step": 1544
    },
    {
      "epoch": 10.210656753407683,
      "grad_norm": 7.512425422668457,
      "learning_rate": 0.0007301980198019802,
      "loss": 0.6409,
      "step": 1545
    },
    {
      "epoch": 10.217265592730277,
      "grad_norm": 90.15099334716797,
      "learning_rate": 0.0007297029702970297,
      "loss": 3.3026,
      "step": 1546
    },
    {
      "epoch": 10.22387443205287,
      "grad_norm": 101.29730224609375,
      "learning_rate": 0.0007292079207920792,
      "loss": 6.0358,
      "step": 1547
    },
    {
      "epoch": 10.230483271375464,
      "grad_norm": 48.79708480834961,
      "learning_rate": 0.0007287128712871287,
      "loss": 1.6507,
      "step": 1548
    },
    {
      "epoch": 10.23709211069806,
      "grad_norm": 116.85540771484375,
      "learning_rate": 0.0007282178217821782,
      "loss": 2.8734,
      "step": 1549
    },
    {
      "epoch": 10.243700950020653,
      "grad_norm": 170.46994018554688,
      "learning_rate": 0.0007277227722772277,
      "loss": 8.5316,
      "step": 1550
    },
    {
      "epoch": 10.250309789343246,
      "grad_norm": 101.75498962402344,
      "learning_rate": 0.0007272277227722772,
      "loss": 4.1054,
      "step": 1551
    },
    {
      "epoch": 10.25691862866584,
      "grad_norm": 44.20949935913086,
      "learning_rate": 0.0007267326732673268,
      "loss": 2.2337,
      "step": 1552
    },
    {
      "epoch": 10.263527467988435,
      "grad_norm": 14.67134952545166,
      "learning_rate": 0.0007262376237623762,
      "loss": 1.1874,
      "step": 1553
    },
    {
      "epoch": 10.270136307311029,
      "grad_norm": 116.9957504272461,
      "learning_rate": 0.0007257425742574257,
      "loss": 3.6503,
      "step": 1554
    },
    {
      "epoch": 10.276745146633623,
      "grad_norm": 136.6094970703125,
      "learning_rate": 0.0007252475247524752,
      "loss": 3.618,
      "step": 1555
    },
    {
      "epoch": 10.283353985956216,
      "grad_norm": 34.767234802246094,
      "learning_rate": 0.0007247524752475248,
      "loss": 2.385,
      "step": 1556
    },
    {
      "epoch": 10.28996282527881,
      "grad_norm": 9.87596607208252,
      "learning_rate": 0.0007242574257425743,
      "loss": 2.2806,
      "step": 1557
    },
    {
      "epoch": 10.296571664601405,
      "grad_norm": 32.89215087890625,
      "learning_rate": 0.0007237623762376237,
      "loss": 1.6009,
      "step": 1558
    },
    {
      "epoch": 10.303180503923999,
      "grad_norm": 80.50263977050781,
      "learning_rate": 0.0007232673267326732,
      "loss": 2.8911,
      "step": 1559
    },
    {
      "epoch": 10.309789343246592,
      "grad_norm": 18.993648529052734,
      "learning_rate": 0.0007227722772277228,
      "loss": 1.0732,
      "step": 1560
    },
    {
      "epoch": 10.316398182569186,
      "grad_norm": 27.12692642211914,
      "learning_rate": 0.0007222772277227723,
      "loss": 1.7622,
      "step": 1561
    },
    {
      "epoch": 10.32300702189178,
      "grad_norm": 19.68292999267578,
      "learning_rate": 0.0007217821782178218,
      "loss": 1.4503,
      "step": 1562
    },
    {
      "epoch": 10.329615861214375,
      "grad_norm": 54.693153381347656,
      "learning_rate": 0.0007212871287128712,
      "loss": 1.3116,
      "step": 1563
    },
    {
      "epoch": 10.336224700536969,
      "grad_norm": 11.630834579467773,
      "learning_rate": 0.0007207920792079208,
      "loss": 1.764,
      "step": 1564
    },
    {
      "epoch": 10.342833539859562,
      "grad_norm": 20.83820343017578,
      "learning_rate": 0.0007202970297029703,
      "loss": 3.4126,
      "step": 1565
    },
    {
      "epoch": 10.349442379182156,
      "grad_norm": 26.97938346862793,
      "learning_rate": 0.0007198019801980198,
      "loss": 3.1628,
      "step": 1566
    },
    {
      "epoch": 10.35605121850475,
      "grad_norm": 7.977200031280518,
      "learning_rate": 0.0007193069306930693,
      "loss": 1.7261,
      "step": 1567
    },
    {
      "epoch": 10.362660057827345,
      "grad_norm": 69.77830505371094,
      "learning_rate": 0.0007188118811881188,
      "loss": 4.2239,
      "step": 1568
    },
    {
      "epoch": 10.369268897149938,
      "grad_norm": 56.080780029296875,
      "learning_rate": 0.0007183168316831683,
      "loss": 2.0215,
      "step": 1569
    },
    {
      "epoch": 10.375877736472532,
      "grad_norm": 13.399615287780762,
      "learning_rate": 0.0007178217821782178,
      "loss": 1.1123,
      "step": 1570
    },
    {
      "epoch": 10.382486575795125,
      "grad_norm": 113.13765716552734,
      "learning_rate": 0.0007173267326732673,
      "loss": 4.743,
      "step": 1571
    },
    {
      "epoch": 10.389095415117719,
      "grad_norm": 131.80593872070312,
      "learning_rate": 0.0007168316831683169,
      "loss": 4.2372,
      "step": 1572
    },
    {
      "epoch": 10.395704254440314,
      "grad_norm": 131.71847534179688,
      "learning_rate": 0.0007163366336633663,
      "loss": 7.1225,
      "step": 1573
    },
    {
      "epoch": 10.402313093762908,
      "grad_norm": 96.37246704101562,
      "learning_rate": 0.0007158415841584158,
      "loss": 5.588,
      "step": 1574
    },
    {
      "epoch": 10.408921933085502,
      "grad_norm": 58.81143569946289,
      "learning_rate": 0.0007153465346534653,
      "loss": 1.5562,
      "step": 1575
    },
    {
      "epoch": 10.415530772408095,
      "grad_norm": 136.5065460205078,
      "learning_rate": 0.0007148514851485149,
      "loss": 3.701,
      "step": 1576
    },
    {
      "epoch": 10.42213961173069,
      "grad_norm": 159.23309326171875,
      "learning_rate": 0.0007143564356435644,
      "loss": 7.5878,
      "step": 1577
    },
    {
      "epoch": 10.428748451053284,
      "grad_norm": 104.8891372680664,
      "learning_rate": 0.0007138613861386138,
      "loss": 3.3851,
      "step": 1578
    },
    {
      "epoch": 10.435357290375878,
      "grad_norm": 34.09869384765625,
      "learning_rate": 0.0007133663366336633,
      "loss": 3.7482,
      "step": 1579
    },
    {
      "epoch": 10.441966129698471,
      "grad_norm": 188.0407257080078,
      "learning_rate": 0.0007128712871287129,
      "loss": 6.4221,
      "step": 1580
    },
    {
      "epoch": 10.448574969021065,
      "grad_norm": 204.4790802001953,
      "learning_rate": 0.0007123762376237624,
      "loss": 6.0757,
      "step": 1581
    },
    {
      "epoch": 10.45518380834366,
      "grad_norm": 195.7115936279297,
      "learning_rate": 0.0007118811881188119,
      "loss": 6.4326,
      "step": 1582
    },
    {
      "epoch": 10.461792647666254,
      "grad_norm": 241.8831787109375,
      "learning_rate": 0.0007113861386138613,
      "loss": 10.2192,
      "step": 1583
    },
    {
      "epoch": 10.468401486988848,
      "grad_norm": 10.177613258361816,
      "learning_rate": 0.0007108910891089109,
      "loss": 1.3534,
      "step": 1584
    },
    {
      "epoch": 10.475010326311441,
      "grad_norm": 59.469791412353516,
      "learning_rate": 0.0007103960396039604,
      "loss": 3.4991,
      "step": 1585
    },
    {
      "epoch": 10.481619165634035,
      "grad_norm": 97.59352111816406,
      "learning_rate": 0.0007099009900990099,
      "loss": 2.5306,
      "step": 1586
    },
    {
      "epoch": 10.48822800495663,
      "grad_norm": 136.7332763671875,
      "learning_rate": 0.0007094059405940594,
      "loss": 5.1811,
      "step": 1587
    },
    {
      "epoch": 10.494836844279224,
      "grad_norm": 78.90132141113281,
      "learning_rate": 0.0007089108910891088,
      "loss": 2.2661,
      "step": 1588
    },
    {
      "epoch": 10.501445683601817,
      "grad_norm": 30.679885864257812,
      "learning_rate": 0.0007084158415841584,
      "loss": 1.9356,
      "step": 1589
    },
    {
      "epoch": 10.508054522924411,
      "grad_norm": 8.655635833740234,
      "learning_rate": 0.0007079207920792079,
      "loss": 1.4911,
      "step": 1590
    },
    {
      "epoch": 10.514663362247006,
      "grad_norm": 68.13990783691406,
      "learning_rate": 0.0007074257425742574,
      "loss": 2.1235,
      "step": 1591
    },
    {
      "epoch": 10.5212722015696,
      "grad_norm": 119.87772369384766,
      "learning_rate": 0.000706930693069307,
      "loss": 4.5511,
      "step": 1592
    },
    {
      "epoch": 10.527881040892193,
      "grad_norm": 95.20030975341797,
      "learning_rate": 0.0007064356435643564,
      "loss": 2.2673,
      "step": 1593
    },
    {
      "epoch": 10.534489880214787,
      "grad_norm": 17.293224334716797,
      "learning_rate": 0.0007059405940594059,
      "loss": 1.7095,
      "step": 1594
    },
    {
      "epoch": 10.54109871953738,
      "grad_norm": 85.75330352783203,
      "learning_rate": 0.0007054455445544554,
      "loss": 4.3485,
      "step": 1595
    },
    {
      "epoch": 10.547707558859976,
      "grad_norm": 47.233524322509766,
      "learning_rate": 0.000704950495049505,
      "loss": 2.8522,
      "step": 1596
    },
    {
      "epoch": 10.55431639818257,
      "grad_norm": 80.80838775634766,
      "learning_rate": 0.0007044554455445545,
      "loss": 4.402,
      "step": 1597
    },
    {
      "epoch": 10.560925237505163,
      "grad_norm": 10.56323528289795,
      "learning_rate": 0.0007039603960396039,
      "loss": 1.4469,
      "step": 1598
    },
    {
      "epoch": 10.567534076827757,
      "grad_norm": 37.41816711425781,
      "learning_rate": 0.0007034653465346534,
      "loss": 2.4835,
      "step": 1599
    },
    {
      "epoch": 10.57414291615035,
      "grad_norm": 90.51225280761719,
      "learning_rate": 0.0007029702970297029,
      "loss": 3.2514,
      "step": 1600
    },
    {
      "epoch": 10.580751755472946,
      "grad_norm": 7.11812162399292,
      "learning_rate": 0.0007024752475247525,
      "loss": 1.9864,
      "step": 1601
    },
    {
      "epoch": 10.58736059479554,
      "grad_norm": 17.638671875,
      "learning_rate": 0.000701980198019802,
      "loss": 2.3303,
      "step": 1602
    },
    {
      "epoch": 10.593969434118133,
      "grad_norm": 59.46797180175781,
      "learning_rate": 0.0007014851485148514,
      "loss": 5.1577,
      "step": 1603
    },
    {
      "epoch": 10.600578273440727,
      "grad_norm": 84.54402923583984,
      "learning_rate": 0.0007009900990099009,
      "loss": 5.0734,
      "step": 1604
    },
    {
      "epoch": 10.60718711276332,
      "grad_norm": 18.102294921875,
      "learning_rate": 0.0007004950495049505,
      "loss": 1.4917,
      "step": 1605
    },
    {
      "epoch": 10.613795952085916,
      "grad_norm": 31.49028205871582,
      "learning_rate": 0.0007,
      "loss": 5.7222,
      "step": 1606
    },
    {
      "epoch": 10.62040479140851,
      "grad_norm": 41.56855010986328,
      "learning_rate": 0.0006995049504950495,
      "loss": 2.3879,
      "step": 1607
    },
    {
      "epoch": 10.627013630731103,
      "grad_norm": 16.40414047241211,
      "learning_rate": 0.0006990099009900989,
      "loss": 1.5318,
      "step": 1608
    },
    {
      "epoch": 10.633622470053696,
      "grad_norm": 60.77911376953125,
      "learning_rate": 0.0006985148514851485,
      "loss": 2.4655,
      "step": 1609
    },
    {
      "epoch": 10.64023130937629,
      "grad_norm": 65.67982482910156,
      "learning_rate": 0.000698019801980198,
      "loss": 2.056,
      "step": 1610
    },
    {
      "epoch": 10.646840148698885,
      "grad_norm": 34.360931396484375,
      "learning_rate": 0.0006975247524752475,
      "loss": 1.2241,
      "step": 1611
    },
    {
      "epoch": 10.653448988021479,
      "grad_norm": 145.2487335205078,
      "learning_rate": 0.000697029702970297,
      "loss": 5.1658,
      "step": 1612
    },
    {
      "epoch": 10.660057827344072,
      "grad_norm": 80.88067626953125,
      "learning_rate": 0.0006965346534653465,
      "loss": 3.715,
      "step": 1613
    },
    {
      "epoch": 10.666666666666666,
      "grad_norm": 37.272216796875,
      "learning_rate": 0.000696039603960396,
      "loss": 3.6755,
      "step": 1614
    },
    {
      "epoch": 10.673275505989261,
      "grad_norm": 14.746503829956055,
      "learning_rate": 0.0006955445544554455,
      "loss": 1.3058,
      "step": 1615
    },
    {
      "epoch": 10.679884345311855,
      "grad_norm": 72.37814331054688,
      "learning_rate": 0.000695049504950495,
      "loss": 1.6905,
      "step": 1616
    },
    {
      "epoch": 10.686493184634449,
      "grad_norm": 51.2813606262207,
      "learning_rate": 0.0006945544554455446,
      "loss": 3.4051,
      "step": 1617
    },
    {
      "epoch": 10.693102023957042,
      "grad_norm": 38.94575119018555,
      "learning_rate": 0.000694059405940594,
      "loss": 1.5125,
      "step": 1618
    },
    {
      "epoch": 10.699710863279636,
      "grad_norm": 21.981447219848633,
      "learning_rate": 0.0006935643564356435,
      "loss": 2.5845,
      "step": 1619
    },
    {
      "epoch": 10.706319702602231,
      "grad_norm": 71.8622055053711,
      "learning_rate": 0.000693069306930693,
      "loss": 3.9125,
      "step": 1620
    },
    {
      "epoch": 10.712928541924825,
      "grad_norm": 148.021728515625,
      "learning_rate": 0.0006925742574257426,
      "loss": 6.1805,
      "step": 1621
    },
    {
      "epoch": 10.719537381247418,
      "grad_norm": 89.64459991455078,
      "learning_rate": 0.0006920792079207921,
      "loss": 3.183,
      "step": 1622
    },
    {
      "epoch": 10.726146220570012,
      "grad_norm": 62.62015914916992,
      "learning_rate": 0.0006915841584158415,
      "loss": 5.8043,
      "step": 1623
    },
    {
      "epoch": 10.732755059892606,
      "grad_norm": 123.25149536132812,
      "learning_rate": 0.000691089108910891,
      "loss": 2.5085,
      "step": 1624
    },
    {
      "epoch": 10.739363899215201,
      "grad_norm": 163.5395050048828,
      "learning_rate": 0.0006905940594059406,
      "loss": 4.8895,
      "step": 1625
    },
    {
      "epoch": 10.745972738537795,
      "grad_norm": 38.0631103515625,
      "learning_rate": 0.0006900990099009901,
      "loss": 1.3125,
      "step": 1626
    },
    {
      "epoch": 10.752581577860388,
      "grad_norm": 78.812255859375,
      "learning_rate": 0.0006896039603960396,
      "loss": 2.2729,
      "step": 1627
    },
    {
      "epoch": 10.759190417182982,
      "grad_norm": 45.263084411621094,
      "learning_rate": 0.000689108910891089,
      "loss": 1.9329,
      "step": 1628
    },
    {
      "epoch": 10.765799256505577,
      "grad_norm": 120.40347290039062,
      "learning_rate": 0.0006886138613861386,
      "loss": 3.9203,
      "step": 1629
    },
    {
      "epoch": 10.77240809582817,
      "grad_norm": 57.005306243896484,
      "learning_rate": 0.0006881188118811881,
      "loss": 2.0439,
      "step": 1630
    },
    {
      "epoch": 10.779016935150764,
      "grad_norm": 62.45833969116211,
      "learning_rate": 0.0006876237623762376,
      "loss": 2.0804,
      "step": 1631
    },
    {
      "epoch": 10.785625774473358,
      "grad_norm": 138.17103576660156,
      "learning_rate": 0.0006871287128712872,
      "loss": 6.0794,
      "step": 1632
    },
    {
      "epoch": 10.792234613795952,
      "grad_norm": 145.69654846191406,
      "learning_rate": 0.0006866336633663367,
      "loss": 4.3638,
      "step": 1633
    },
    {
      "epoch": 10.798843453118547,
      "grad_norm": 65.45797729492188,
      "learning_rate": 0.0006861386138613862,
      "loss": 1.9326,
      "step": 1634
    },
    {
      "epoch": 10.80545229244114,
      "grad_norm": 82.64165496826172,
      "learning_rate": 0.0006856435643564357,
      "loss": 2.778,
      "step": 1635
    },
    {
      "epoch": 10.812061131763734,
      "grad_norm": 27.44509506225586,
      "learning_rate": 0.0006851485148514852,
      "loss": 2.0948,
      "step": 1636
    },
    {
      "epoch": 10.818669971086328,
      "grad_norm": 20.62077522277832,
      "learning_rate": 0.0006846534653465348,
      "loss": 1.7978,
      "step": 1637
    },
    {
      "epoch": 10.825278810408921,
      "grad_norm": 88.34024810791016,
      "learning_rate": 0.0006841584158415842,
      "loss": 2.9167,
      "step": 1638
    },
    {
      "epoch": 10.831887649731517,
      "grad_norm": 13.508591651916504,
      "learning_rate": 0.0006836633663366337,
      "loss": 1.0554,
      "step": 1639
    },
    {
      "epoch": 10.83849648905411,
      "grad_norm": 61.26825714111328,
      "learning_rate": 0.0006831683168316832,
      "loss": 3.855,
      "step": 1640
    },
    {
      "epoch": 10.845105328376704,
      "grad_norm": 4.581418514251709,
      "learning_rate": 0.0006826732673267328,
      "loss": 0.856,
      "step": 1641
    },
    {
      "epoch": 10.851714167699297,
      "grad_norm": 15.968372344970703,
      "learning_rate": 0.0006821782178217823,
      "loss": 2.7053,
      "step": 1642
    },
    {
      "epoch": 10.858323007021891,
      "grad_norm": 49.41922378540039,
      "learning_rate": 0.0006816831683168317,
      "loss": 1.9245,
      "step": 1643
    },
    {
      "epoch": 10.864931846344486,
      "grad_norm": 28.322765350341797,
      "learning_rate": 0.0006811881188118812,
      "loss": 1.8589,
      "step": 1644
    },
    {
      "epoch": 10.87154068566708,
      "grad_norm": 67.85193634033203,
      "learning_rate": 0.0006806930693069308,
      "loss": 3.5366,
      "step": 1645
    },
    {
      "epoch": 10.878149524989674,
      "grad_norm": 26.58844566345215,
      "learning_rate": 0.0006801980198019803,
      "loss": 3.5217,
      "step": 1646
    },
    {
      "epoch": 10.884758364312267,
      "grad_norm": 36.9504508972168,
      "learning_rate": 0.0006797029702970298,
      "loss": 5.3322,
      "step": 1647
    },
    {
      "epoch": 10.89136720363486,
      "grad_norm": 91.2510757446289,
      "learning_rate": 0.0006792079207920792,
      "loss": 4.9111,
      "step": 1648
    },
    {
      "epoch": 10.897976042957456,
      "grad_norm": 62.58045959472656,
      "learning_rate": 0.0006787128712871288,
      "loss": 3.179,
      "step": 1649
    },
    {
      "epoch": 10.90458488228005,
      "grad_norm": 29.209598541259766,
      "learning_rate": 0.0006782178217821783,
      "loss": 0.9857,
      "step": 1650
    },
    {
      "epoch": 10.911193721602643,
      "grad_norm": 12.716878890991211,
      "learning_rate": 0.0006777227722772278,
      "loss": 4.2088,
      "step": 1651
    },
    {
      "epoch": 10.917802560925237,
      "grad_norm": 108.05899047851562,
      "learning_rate": 0.0006772277227722773,
      "loss": 2.331,
      "step": 1652
    },
    {
      "epoch": 10.924411400247832,
      "grad_norm": 14.43465805053711,
      "learning_rate": 0.0006767326732673267,
      "loss": 4.0599,
      "step": 1653
    },
    {
      "epoch": 10.931020239570426,
      "grad_norm": 46.00893783569336,
      "learning_rate": 0.0006762376237623763,
      "loss": 2.7993,
      "step": 1654
    },
    {
      "epoch": 10.93762907889302,
      "grad_norm": 32.0063591003418,
      "learning_rate": 0.0006757425742574258,
      "loss": 1.3978,
      "step": 1655
    },
    {
      "epoch": 10.944237918215613,
      "grad_norm": 17.36453628540039,
      "learning_rate": 0.0006752475247524753,
      "loss": 2.4124,
      "step": 1656
    },
    {
      "epoch": 10.950846757538207,
      "grad_norm": 92.29547882080078,
      "learning_rate": 0.0006747524752475249,
      "loss": 2.7389,
      "step": 1657
    },
    {
      "epoch": 10.957455596860802,
      "grad_norm": 116.27371215820312,
      "learning_rate": 0.0006742574257425743,
      "loss": 3.7495,
      "step": 1658
    },
    {
      "epoch": 10.964064436183396,
      "grad_norm": 78.3723373413086,
      "learning_rate": 0.0006737623762376238,
      "loss": 4.4152,
      "step": 1659
    },
    {
      "epoch": 10.97067327550599,
      "grad_norm": 26.12206268310547,
      "learning_rate": 0.0006732673267326733,
      "loss": 1.9386,
      "step": 1660
    },
    {
      "epoch": 10.977282114828583,
      "grad_norm": 26.66843032836914,
      "learning_rate": 0.0006727722772277228,
      "loss": 4.2027,
      "step": 1661
    },
    {
      "epoch": 10.983890954151176,
      "grad_norm": 5.436038017272949,
      "learning_rate": 0.0006722772277227724,
      "loss": 1.615,
      "step": 1662
    },
    {
      "epoch": 10.990499793473772,
      "grad_norm": 58.70123291015625,
      "learning_rate": 0.0006717821782178218,
      "loss": 4.72,
      "step": 1663
    },
    {
      "epoch": 10.997108632796365,
      "grad_norm": 29.036945343017578,
      "learning_rate": 0.0006712871287128713,
      "loss": 0.9207,
      "step": 1664
    },
    {
      "epoch": 10.997108632796365,
      "eval_validation_error_bar": 0.044874796651251575,
      "eval_validation_loss": 6.228747844696045,
      "eval_validation_pearsonr": 0.6231014437097723,
      "eval_validation_rmse": 2.495745897293091,
      "eval_validation_runtime": 41.0884,
      "eval_validation_samples_per_second": 4.941,
      "eval_validation_spearman": 0.6507552718431603,
      "eval_validation_steps_per_second": 4.941,
      "step": 1664
    },
    {
      "epoch": 10.997108632796365,
      "eval_test_error_bar": 0.044161494698316366,
      "eval_test_loss": 7.991769790649414,
      "eval_test_pearsonr": 0.5196801153199585,
      "eval_test_rmse": 2.8269717693328857,
      "eval_test_runtime": 55.7317,
      "eval_test_samples_per_second": 5.849,
      "eval_test_spearman": 0.5020490181619035,
      "eval_test_steps_per_second": 5.849,
      "step": 1664
    },
    {
      "epoch": 11.003717472118959,
      "grad_norm": 15.373574256896973,
      "learning_rate": 0.0006707920792079208,
      "loss": 1.3064,
      "step": 1665
    },
    {
      "epoch": 11.010326311441553,
      "grad_norm": 11.684125900268555,
      "learning_rate": 0.0006702970297029704,
      "loss": 2.1952,
      "step": 1666
    },
    {
      "epoch": 11.016935150764146,
      "grad_norm": 23.921106338500977,
      "learning_rate": 0.0006698019801980199,
      "loss": 1.275,
      "step": 1667
    },
    {
      "epoch": 11.023543990086742,
      "grad_norm": 34.7713508605957,
      "learning_rate": 0.0006693069306930693,
      "loss": 3.0722,
      "step": 1668
    },
    {
      "epoch": 11.030152829409335,
      "grad_norm": 56.30560302734375,
      "learning_rate": 0.0006688118811881188,
      "loss": 1.374,
      "step": 1669
    },
    {
      "epoch": 11.036761668731929,
      "grad_norm": 32.534873962402344,
      "learning_rate": 0.0006683168316831684,
      "loss": 4.8417,
      "step": 1670
    },
    {
      "epoch": 11.043370508054522,
      "grad_norm": 64.86687469482422,
      "learning_rate": 0.0006678217821782179,
      "loss": 7.5686,
      "step": 1671
    },
    {
      "epoch": 11.049979347377118,
      "grad_norm": 12.89223575592041,
      "learning_rate": 0.0006673267326732674,
      "loss": 2.3603,
      "step": 1672
    },
    {
      "epoch": 11.056588186699711,
      "grad_norm": 167.88009643554688,
      "learning_rate": 0.0006668316831683168,
      "loss": 4.1338,
      "step": 1673
    },
    {
      "epoch": 11.063197026022305,
      "grad_norm": 151.61795043945312,
      "learning_rate": 0.0006663366336633664,
      "loss": 4.7397,
      "step": 1674
    },
    {
      "epoch": 11.069805865344899,
      "grad_norm": 14.259291648864746,
      "learning_rate": 0.0006658415841584159,
      "loss": 3.126,
      "step": 1675
    },
    {
      "epoch": 11.076414704667492,
      "grad_norm": 49.17695236206055,
      "learning_rate": 0.0006653465346534654,
      "loss": 0.9134,
      "step": 1676
    },
    {
      "epoch": 11.083023543990087,
      "grad_norm": 83.16047668457031,
      "learning_rate": 0.0006648514851485149,
      "loss": 2.8328,
      "step": 1677
    },
    {
      "epoch": 11.089632383312681,
      "grad_norm": 16.228225708007812,
      "learning_rate": 0.0006643564356435644,
      "loss": 1.2286,
      "step": 1678
    },
    {
      "epoch": 11.096241222635275,
      "grad_norm": 37.893184661865234,
      "learning_rate": 0.0006638613861386139,
      "loss": 3.4952,
      "step": 1679
    },
    {
      "epoch": 11.102850061957868,
      "grad_norm": 33.08158493041992,
      "learning_rate": 0.0006633663366336634,
      "loss": 5.5011,
      "step": 1680
    },
    {
      "epoch": 11.109458901280462,
      "grad_norm": 5.536898136138916,
      "learning_rate": 0.0006628712871287129,
      "loss": 0.7434,
      "step": 1681
    },
    {
      "epoch": 11.116067740603057,
      "grad_norm": 18.084442138671875,
      "learning_rate": 0.0006623762376237625,
      "loss": 1.0051,
      "step": 1682
    },
    {
      "epoch": 11.12267657992565,
      "grad_norm": 20.806419372558594,
      "learning_rate": 0.0006618811881188119,
      "loss": 1.0875,
      "step": 1683
    },
    {
      "epoch": 11.129285419248244,
      "grad_norm": 21.930192947387695,
      "learning_rate": 0.0006613861386138614,
      "loss": 2.6637,
      "step": 1684
    },
    {
      "epoch": 11.135894258570838,
      "grad_norm": 61.46574783325195,
      "learning_rate": 0.0006608910891089109,
      "loss": 3.3109,
      "step": 1685
    },
    {
      "epoch": 11.142503097893432,
      "grad_norm": 63.49894332885742,
      "learning_rate": 0.0006603960396039605,
      "loss": 3.2034,
      "step": 1686
    },
    {
      "epoch": 11.149111937216027,
      "grad_norm": 14.912200927734375,
      "learning_rate": 0.00065990099009901,
      "loss": 0.9642,
      "step": 1687
    },
    {
      "epoch": 11.15572077653862,
      "grad_norm": 75.9540023803711,
      "learning_rate": 0.0006594059405940594,
      "loss": 2.4252,
      "step": 1688
    },
    {
      "epoch": 11.162329615861214,
      "grad_norm": 40.49853515625,
      "learning_rate": 0.0006589108910891089,
      "loss": 2.8795,
      "step": 1689
    },
    {
      "epoch": 11.168938455183808,
      "grad_norm": 44.4713134765625,
      "learning_rate": 0.0006584158415841585,
      "loss": 5.3118,
      "step": 1690
    },
    {
      "epoch": 11.175547294506403,
      "grad_norm": 19.430150985717773,
      "learning_rate": 0.000657920792079208,
      "loss": 1.9967,
      "step": 1691
    },
    {
      "epoch": 11.182156133828997,
      "grad_norm": 78.38783264160156,
      "learning_rate": 0.0006574257425742575,
      "loss": 4.3888,
      "step": 1692
    },
    {
      "epoch": 11.18876497315159,
      "grad_norm": 59.7778434753418,
      "learning_rate": 0.0006569306930693069,
      "loss": 1.9255,
      "step": 1693
    },
    {
      "epoch": 11.195373812474184,
      "grad_norm": 24.506450653076172,
      "learning_rate": 0.0006564356435643565,
      "loss": 2.1244,
      "step": 1694
    },
    {
      "epoch": 11.201982651796778,
      "grad_norm": 88.42913818359375,
      "learning_rate": 0.000655940594059406,
      "loss": 1.7055,
      "step": 1695
    },
    {
      "epoch": 11.208591491119373,
      "grad_norm": 108.42884063720703,
      "learning_rate": 0.0006554455445544555,
      "loss": 3.7528,
      "step": 1696
    },
    {
      "epoch": 11.215200330441967,
      "grad_norm": 118.00109100341797,
      "learning_rate": 0.000654950495049505,
      "loss": 3.6552,
      "step": 1697
    },
    {
      "epoch": 11.22180916976456,
      "grad_norm": 15.500316619873047,
      "learning_rate": 0.0006544554455445545,
      "loss": 1.1402,
      "step": 1698
    },
    {
      "epoch": 11.228418009087154,
      "grad_norm": 14.729310989379883,
      "learning_rate": 0.000653960396039604,
      "loss": 2.7138,
      "step": 1699
    },
    {
      "epoch": 11.235026848409747,
      "grad_norm": 55.733543395996094,
      "learning_rate": 0.0006534653465346535,
      "loss": 1.0208,
      "step": 1700
    },
    {
      "epoch": 11.241635687732343,
      "grad_norm": 26.431894302368164,
      "learning_rate": 0.000652970297029703,
      "loss": 3.8439,
      "step": 1701
    },
    {
      "epoch": 11.248244527054936,
      "grad_norm": 82.7760009765625,
      "learning_rate": 0.0006524752475247526,
      "loss": 1.7461,
      "step": 1702
    },
    {
      "epoch": 11.25485336637753,
      "grad_norm": 29.46917724609375,
      "learning_rate": 0.000651980198019802,
      "loss": 2.8141,
      "step": 1703
    },
    {
      "epoch": 11.261462205700123,
      "grad_norm": 21.152002334594727,
      "learning_rate": 0.0006514851485148515,
      "loss": 2.8749,
      "step": 1704
    },
    {
      "epoch": 11.268071045022717,
      "grad_norm": 26.578977584838867,
      "learning_rate": 0.000650990099009901,
      "loss": 3.6226,
      "step": 1705
    },
    {
      "epoch": 11.274679884345312,
      "grad_norm": 39.68631362915039,
      "learning_rate": 0.0006504950495049506,
      "loss": 2.0998,
      "step": 1706
    },
    {
      "epoch": 11.281288723667906,
      "grad_norm": 55.48933029174805,
      "learning_rate": 0.0006500000000000001,
      "loss": 1.2049,
      "step": 1707
    },
    {
      "epoch": 11.2878975629905,
      "grad_norm": 51.41273880004883,
      "learning_rate": 0.0006495049504950495,
      "loss": 2.8975,
      "step": 1708
    },
    {
      "epoch": 11.294506402313093,
      "grad_norm": 8.529869079589844,
      "learning_rate": 0.000649009900990099,
      "loss": 1.6714,
      "step": 1709
    },
    {
      "epoch": 11.301115241635689,
      "grad_norm": 100.97884368896484,
      "learning_rate": 0.0006485148514851485,
      "loss": 3.9969,
      "step": 1710
    },
    {
      "epoch": 11.307724080958282,
      "grad_norm": 89.12629699707031,
      "learning_rate": 0.0006480198019801981,
      "loss": 4.6903,
      "step": 1711
    },
    {
      "epoch": 11.314332920280876,
      "grad_norm": 107.49467468261719,
      "learning_rate": 0.0006475247524752476,
      "loss": 4.52,
      "step": 1712
    },
    {
      "epoch": 11.32094175960347,
      "grad_norm": 61.734107971191406,
      "learning_rate": 0.000647029702970297,
      "loss": 2.3517,
      "step": 1713
    },
    {
      "epoch": 11.327550598926063,
      "grad_norm": 61.26095962524414,
      "learning_rate": 0.0006465346534653465,
      "loss": 1.6815,
      "step": 1714
    },
    {
      "epoch": 11.334159438248658,
      "grad_norm": 87.56617736816406,
      "learning_rate": 0.0006460396039603961,
      "loss": 2.4153,
      "step": 1715
    },
    {
      "epoch": 11.340768277571252,
      "grad_norm": 111.18087005615234,
      "learning_rate": 0.0006455445544554456,
      "loss": 3.4577,
      "step": 1716
    },
    {
      "epoch": 11.347377116893846,
      "grad_norm": 47.69509506225586,
      "learning_rate": 0.0006450495049504951,
      "loss": 2.5343,
      "step": 1717
    },
    {
      "epoch": 11.35398595621644,
      "grad_norm": 33.44697952270508,
      "learning_rate": 0.0006445544554455445,
      "loss": 3.9796,
      "step": 1718
    },
    {
      "epoch": 11.360594795539033,
      "grad_norm": 30.043209075927734,
      "learning_rate": 0.0006440594059405941,
      "loss": 2.1465,
      "step": 1719
    },
    {
      "epoch": 11.367203634861628,
      "grad_norm": 91.72718048095703,
      "learning_rate": 0.0006435643564356436,
      "loss": 3.7484,
      "step": 1720
    },
    {
      "epoch": 11.373812474184222,
      "grad_norm": 86.7269515991211,
      "learning_rate": 0.0006430693069306931,
      "loss": 2.4585,
      "step": 1721
    },
    {
      "epoch": 11.380421313506815,
      "grad_norm": 56.62742233276367,
      "learning_rate": 0.0006425742574257426,
      "loss": 1.6533,
      "step": 1722
    },
    {
      "epoch": 11.387030152829409,
      "grad_norm": 16.889965057373047,
      "learning_rate": 0.0006420792079207921,
      "loss": 1.8827,
      "step": 1723
    },
    {
      "epoch": 11.393638992152002,
      "grad_norm": 21.471582412719727,
      "learning_rate": 0.0006415841584158416,
      "loss": 4.6773,
      "step": 1724
    },
    {
      "epoch": 11.400247831474598,
      "grad_norm": 14.151169776916504,
      "learning_rate": 0.0006410891089108911,
      "loss": 2.4537,
      "step": 1725
    },
    {
      "epoch": 11.406856670797191,
      "grad_norm": 109.88481903076172,
      "learning_rate": 0.0006405940594059406,
      "loss": 2.6664,
      "step": 1726
    },
    {
      "epoch": 11.413465510119785,
      "grad_norm": 19.054235458374023,
      "learning_rate": 0.0006400990099009902,
      "loss": 1.4568,
      "step": 1727
    },
    {
      "epoch": 11.420074349442379,
      "grad_norm": 66.41513061523438,
      "learning_rate": 0.0006396039603960396,
      "loss": 2.2371,
      "step": 1728
    },
    {
      "epoch": 11.426683188764972,
      "grad_norm": 8.412555694580078,
      "learning_rate": 0.0006391089108910891,
      "loss": 2.3445,
      "step": 1729
    },
    {
      "epoch": 11.433292028087568,
      "grad_norm": 12.648505210876465,
      "learning_rate": 0.0006386138613861386,
      "loss": 2.359,
      "step": 1730
    },
    {
      "epoch": 11.439900867410161,
      "grad_norm": 61.555999755859375,
      "learning_rate": 0.0006381188118811882,
      "loss": 4.7316,
      "step": 1731
    },
    {
      "epoch": 11.446509706732755,
      "grad_norm": 13.320592880249023,
      "learning_rate": 0.0006376237623762377,
      "loss": 1.3251,
      "step": 1732
    },
    {
      "epoch": 11.453118546055348,
      "grad_norm": 15.314577102661133,
      "learning_rate": 0.0006371287128712871,
      "loss": 1.7202,
      "step": 1733
    },
    {
      "epoch": 11.459727385377944,
      "grad_norm": 85.30278015136719,
      "learning_rate": 0.0006366336633663366,
      "loss": 2.6679,
      "step": 1734
    },
    {
      "epoch": 11.466336224700537,
      "grad_norm": 64.00231170654297,
      "learning_rate": 0.0006361386138613862,
      "loss": 2.8436,
      "step": 1735
    },
    {
      "epoch": 11.472945064023131,
      "grad_norm": 23.866622924804688,
      "learning_rate": 0.0006356435643564357,
      "loss": 0.7526,
      "step": 1736
    },
    {
      "epoch": 11.479553903345725,
      "grad_norm": 16.168869018554688,
      "learning_rate": 0.0006351485148514852,
      "loss": 0.7681,
      "step": 1737
    },
    {
      "epoch": 11.486162742668318,
      "grad_norm": 36.336219787597656,
      "learning_rate": 0.0006346534653465346,
      "loss": 1.2653,
      "step": 1738
    },
    {
      "epoch": 11.492771581990914,
      "grad_norm": 31.348711013793945,
      "learning_rate": 0.0006341584158415842,
      "loss": 2.0764,
      "step": 1739
    },
    {
      "epoch": 11.499380421313507,
      "grad_norm": 14.332822799682617,
      "learning_rate": 0.0006336633663366337,
      "loss": 1.1673,
      "step": 1740
    },
    {
      "epoch": 11.5059892606361,
      "grad_norm": 9.811062812805176,
      "learning_rate": 0.0006331683168316832,
      "loss": 2.0646,
      "step": 1741
    },
    {
      "epoch": 11.512598099958694,
      "grad_norm": 66.220947265625,
      "learning_rate": 0.0006326732673267327,
      "loss": 3.9974,
      "step": 1742
    },
    {
      "epoch": 11.519206939281288,
      "grad_norm": 84.19441986083984,
      "learning_rate": 0.0006321782178217822,
      "loss": 4.8724,
      "step": 1743
    },
    {
      "epoch": 11.525815778603883,
      "grad_norm": 80.5758285522461,
      "learning_rate": 0.0006316831683168317,
      "loss": 3.0926,
      "step": 1744
    },
    {
      "epoch": 11.532424617926477,
      "grad_norm": 27.563974380493164,
      "learning_rate": 0.0006311881188118812,
      "loss": 1.7765,
      "step": 1745
    },
    {
      "epoch": 11.53903345724907,
      "grad_norm": 70.43550872802734,
      "learning_rate": 0.0006306930693069307,
      "loss": 2.3033,
      "step": 1746
    },
    {
      "epoch": 11.545642296571664,
      "grad_norm": 98.24855041503906,
      "learning_rate": 0.0006301980198019803,
      "loss": 2.6597,
      "step": 1747
    },
    {
      "epoch": 11.55225113589426,
      "grad_norm": 129.28919982910156,
      "learning_rate": 0.0006297029702970297,
      "loss": 5.2961,
      "step": 1748
    },
    {
      "epoch": 11.558859975216853,
      "grad_norm": 39.06121063232422,
      "learning_rate": 0.0006292079207920792,
      "loss": 1.169,
      "step": 1749
    },
    {
      "epoch": 11.565468814539447,
      "grad_norm": 15.888877868652344,
      "learning_rate": 0.0006287128712871287,
      "loss": 2.4261,
      "step": 1750
    },
    {
      "epoch": 11.57207765386204,
      "grad_norm": 57.02961349487305,
      "learning_rate": 0.0006282178217821783,
      "loss": 1.7869,
      "step": 1751
    },
    {
      "epoch": 11.578686493184634,
      "grad_norm": 62.758182525634766,
      "learning_rate": 0.0006277227722772278,
      "loss": 3.5881,
      "step": 1752
    },
    {
      "epoch": 11.58529533250723,
      "grad_norm": 42.028587341308594,
      "learning_rate": 0.0006272277227722772,
      "loss": 1.9621,
      "step": 1753
    },
    {
      "epoch": 11.591904171829823,
      "grad_norm": 109.5397720336914,
      "learning_rate": 0.0006267326732673267,
      "loss": 4.3775,
      "step": 1754
    },
    {
      "epoch": 11.598513011152416,
      "grad_norm": 25.184541702270508,
      "learning_rate": 0.0006262376237623763,
      "loss": 3.2864,
      "step": 1755
    },
    {
      "epoch": 11.60512185047501,
      "grad_norm": 43.123809814453125,
      "learning_rate": 0.0006257425742574258,
      "loss": 1.681,
      "step": 1756
    },
    {
      "epoch": 11.611730689797604,
      "grad_norm": 18.048484802246094,
      "learning_rate": 0.0006252475247524753,
      "loss": 3.2606,
      "step": 1757
    },
    {
      "epoch": 11.618339529120199,
      "grad_norm": 42.49939727783203,
      "learning_rate": 0.0006247524752475247,
      "loss": 0.8536,
      "step": 1758
    },
    {
      "epoch": 11.624948368442793,
      "grad_norm": 73.48106384277344,
      "learning_rate": 0.0006242574257425742,
      "loss": 1.5158,
      "step": 1759
    },
    {
      "epoch": 11.631557207765386,
      "grad_norm": 17.80710220336914,
      "learning_rate": 0.0006237623762376238,
      "loss": 1.2222,
      "step": 1760
    },
    {
      "epoch": 11.63816604708798,
      "grad_norm": 27.83279800415039,
      "learning_rate": 0.0006232673267326733,
      "loss": 1.2756,
      "step": 1761
    },
    {
      "epoch": 11.644774886410573,
      "grad_norm": 4.978246688842773,
      "learning_rate": 0.0006227722772277228,
      "loss": 0.8619,
      "step": 1762
    },
    {
      "epoch": 11.651383725733169,
      "grad_norm": 15.290023803710938,
      "learning_rate": 0.0006222772277227722,
      "loss": 1.4545,
      "step": 1763
    },
    {
      "epoch": 11.657992565055762,
      "grad_norm": 10.628005027770996,
      "learning_rate": 0.0006217821782178218,
      "loss": 3.1253,
      "step": 1764
    },
    {
      "epoch": 11.664601404378356,
      "grad_norm": 76.48329162597656,
      "learning_rate": 0.0006212871287128713,
      "loss": 1.6444,
      "step": 1765
    },
    {
      "epoch": 11.67121024370095,
      "grad_norm": 62.56631851196289,
      "learning_rate": 0.0006207920792079208,
      "loss": 4.2556,
      "step": 1766
    },
    {
      "epoch": 11.677819083023543,
      "grad_norm": 51.38034439086914,
      "learning_rate": 0.0006202970297029703,
      "loss": 2.4408,
      "step": 1767
    },
    {
      "epoch": 11.684427922346138,
      "grad_norm": 29.268510818481445,
      "learning_rate": 0.0006198019801980198,
      "loss": 4.1185,
      "step": 1768
    },
    {
      "epoch": 11.691036761668732,
      "grad_norm": 18.80729103088379,
      "learning_rate": 0.0006193069306930693,
      "loss": 2.7664,
      "step": 1769
    },
    {
      "epoch": 11.697645600991326,
      "grad_norm": 92.17796325683594,
      "learning_rate": 0.0006188118811881188,
      "loss": 1.8458,
      "step": 1770
    },
    {
      "epoch": 11.70425444031392,
      "grad_norm": 122.65437316894531,
      "learning_rate": 0.0006183168316831683,
      "loss": 3.1702,
      "step": 1771
    },
    {
      "epoch": 11.710863279636515,
      "grad_norm": 73.54268646240234,
      "learning_rate": 0.0006178217821782179,
      "loss": 1.548,
      "step": 1772
    },
    {
      "epoch": 11.717472118959108,
      "grad_norm": 181.70140075683594,
      "learning_rate": 0.0006173267326732673,
      "loss": 7.457,
      "step": 1773
    },
    {
      "epoch": 11.724080958281702,
      "grad_norm": 54.075138092041016,
      "learning_rate": 0.0006168316831683168,
      "loss": 3.1925,
      "step": 1774
    },
    {
      "epoch": 11.730689797604295,
      "grad_norm": 38.81941223144531,
      "learning_rate": 0.0006163366336633663,
      "loss": 2.6629,
      "step": 1775
    },
    {
      "epoch": 11.737298636926889,
      "grad_norm": 12.47268009185791,
      "learning_rate": 0.0006158415841584159,
      "loss": 1.2905,
      "step": 1776
    },
    {
      "epoch": 11.743907476249484,
      "grad_norm": 105.66841888427734,
      "learning_rate": 0.0006153465346534654,
      "loss": 2.5032,
      "step": 1777
    },
    {
      "epoch": 11.750516315572078,
      "grad_norm": 64.8477783203125,
      "learning_rate": 0.0006148514851485148,
      "loss": 2.3981,
      "step": 1778
    },
    {
      "epoch": 11.757125154894672,
      "grad_norm": 85.31120300292969,
      "learning_rate": 0.0006143564356435643,
      "loss": 2.352,
      "step": 1779
    },
    {
      "epoch": 11.763733994217265,
      "grad_norm": 31.614578247070312,
      "learning_rate": 0.0006138613861386139,
      "loss": 3.1086,
      "step": 1780
    },
    {
      "epoch": 11.770342833539859,
      "grad_norm": 44.911529541015625,
      "learning_rate": 0.0006133663366336634,
      "loss": 1.7223,
      "step": 1781
    },
    {
      "epoch": 11.776951672862454,
      "grad_norm": 25.943437576293945,
      "learning_rate": 0.0006128712871287129,
      "loss": 1.2069,
      "step": 1782
    },
    {
      "epoch": 11.783560512185048,
      "grad_norm": 9.907483100891113,
      "learning_rate": 0.0006123762376237623,
      "loss": 1.2116,
      "step": 1783
    },
    {
      "epoch": 11.790169351507641,
      "grad_norm": 150.88148498535156,
      "learning_rate": 0.0006118811881188119,
      "loss": 5.2989,
      "step": 1784
    },
    {
      "epoch": 11.796778190830235,
      "grad_norm": 92.04889678955078,
      "learning_rate": 0.0006113861386138614,
      "loss": 3.1049,
      "step": 1785
    },
    {
      "epoch": 11.80338703015283,
      "grad_norm": 64.5028305053711,
      "learning_rate": 0.0006108910891089109,
      "loss": 2.452,
      "step": 1786
    },
    {
      "epoch": 11.809995869475424,
      "grad_norm": 26.405216217041016,
      "learning_rate": 0.0006103960396039604,
      "loss": 0.9748,
      "step": 1787
    },
    {
      "epoch": 11.816604708798017,
      "grad_norm": 21.23863983154297,
      "learning_rate": 0.0006099009900990099,
      "loss": 0.9821,
      "step": 1788
    },
    {
      "epoch": 11.823213548120611,
      "grad_norm": 9.471922874450684,
      "learning_rate": 0.0006094059405940594,
      "loss": 4.0935,
      "step": 1789
    },
    {
      "epoch": 11.829822387443205,
      "grad_norm": 13.844460487365723,
      "learning_rate": 0.0006089108910891089,
      "loss": 3.6364,
      "step": 1790
    },
    {
      "epoch": 11.8364312267658,
      "grad_norm": 36.34906005859375,
      "learning_rate": 0.0006084158415841584,
      "loss": 0.7326,
      "step": 1791
    },
    {
      "epoch": 11.843040066088394,
      "grad_norm": 43.12820816040039,
      "learning_rate": 0.000607920792079208,
      "loss": 1.9471,
      "step": 1792
    },
    {
      "epoch": 11.849648905410987,
      "grad_norm": 24.559572219848633,
      "learning_rate": 0.0006074257425742574,
      "loss": 2.0155,
      "step": 1793
    },
    {
      "epoch": 11.85625774473358,
      "grad_norm": 28.67435073852539,
      "learning_rate": 0.0006069306930693069,
      "loss": 3.5616,
      "step": 1794
    },
    {
      "epoch": 11.862866584056174,
      "grad_norm": 19.66285514831543,
      "learning_rate": 0.0006064356435643564,
      "loss": 1.7147,
      "step": 1795
    },
    {
      "epoch": 11.86947542337877,
      "grad_norm": 72.16288757324219,
      "learning_rate": 0.000605940594059406,
      "loss": 1.9797,
      "step": 1796
    },
    {
      "epoch": 11.876084262701363,
      "grad_norm": 48.28480529785156,
      "learning_rate": 0.0006054455445544555,
      "loss": 2.4861,
      "step": 1797
    },
    {
      "epoch": 11.882693102023957,
      "grad_norm": 49.38190841674805,
      "learning_rate": 0.0006049504950495049,
      "loss": 2.0597,
      "step": 1798
    },
    {
      "epoch": 11.88930194134655,
      "grad_norm": 75.92481231689453,
      "learning_rate": 0.0006044554455445544,
      "loss": 4.5267,
      "step": 1799
    },
    {
      "epoch": 11.895910780669144,
      "grad_norm": 17.660921096801758,
      "learning_rate": 0.000603960396039604,
      "loss": 2.2375,
      "step": 1800
    },
    {
      "epoch": 11.90251961999174,
      "grad_norm": 137.05775451660156,
      "learning_rate": 0.0006034653465346535,
      "loss": 6.5387,
      "step": 1801
    },
    {
      "epoch": 11.909128459314333,
      "grad_norm": 36.07633972167969,
      "learning_rate": 0.000602970297029703,
      "loss": 4.4381,
      "step": 1802
    },
    {
      "epoch": 11.915737298636927,
      "grad_norm": 24.54136848449707,
      "learning_rate": 0.0006024752475247524,
      "loss": 3.9322,
      "step": 1803
    },
    {
      "epoch": 11.92234613795952,
      "grad_norm": 93.4291000366211,
      "learning_rate": 0.000601980198019802,
      "loss": 3.4062,
      "step": 1804
    },
    {
      "epoch": 11.928954977282114,
      "grad_norm": 77.29443359375,
      "learning_rate": 0.0006014851485148515,
      "loss": 4.4744,
      "step": 1805
    },
    {
      "epoch": 11.93556381660471,
      "grad_norm": 37.22896957397461,
      "learning_rate": 0.000600990099009901,
      "loss": 4.3871,
      "step": 1806
    },
    {
      "epoch": 11.942172655927303,
      "grad_norm": 24.1289119720459,
      "learning_rate": 0.0006004950495049505,
      "loss": 1.4713,
      "step": 1807
    },
    {
      "epoch": 11.948781495249897,
      "grad_norm": 29.91655731201172,
      "learning_rate": 0.0006,
      "loss": 1.375,
      "step": 1808
    },
    {
      "epoch": 11.95539033457249,
      "grad_norm": 79.33383178710938,
      "learning_rate": 0.0005995049504950495,
      "loss": 3.5767,
      "step": 1809
    },
    {
      "epoch": 11.961999173895085,
      "grad_norm": 48.83225631713867,
      "learning_rate": 0.000599009900990099,
      "loss": 1.5221,
      "step": 1810
    },
    {
      "epoch": 11.968608013217679,
      "grad_norm": 12.269808769226074,
      "learning_rate": 0.0005985148514851485,
      "loss": 5.0861,
      "step": 1811
    },
    {
      "epoch": 11.975216852540273,
      "grad_norm": 13.730574607849121,
      "learning_rate": 0.000598019801980198,
      "loss": 2.3529,
      "step": 1812
    },
    {
      "epoch": 11.981825691862866,
      "grad_norm": 31.235368728637695,
      "learning_rate": 0.0005975247524752475,
      "loss": 3.8892,
      "step": 1813
    },
    {
      "epoch": 11.98843453118546,
      "grad_norm": 90.08906555175781,
      "learning_rate": 0.000597029702970297,
      "loss": 5.9141,
      "step": 1814
    },
    {
      "epoch": 11.995043370508055,
      "grad_norm": 45.45256805419922,
      "learning_rate": 0.0005965346534653465,
      "loss": 2.9166,
      "step": 1815
    },
    {
      "epoch": 11.995043370508055,
      "eval_validation_error_bar": 0.0429136578483723,
      "eval_validation_loss": 5.4868011474609375,
      "eval_validation_pearsonr": 0.6148119712754433,
      "eval_validation_rmse": 2.3423922061920166,
      "eval_validation_runtime": 41.2397,
      "eval_validation_samples_per_second": 4.922,
      "eval_validation_spearman": 0.6722199110628728,
      "eval_validation_steps_per_second": 4.922,
      "step": 1815
    },
    {
      "epoch": 11.995043370508055,
      "eval_test_error_bar": 0.04405471023731668,
      "eval_test_loss": 6.92346715927124,
      "eval_test_pearsonr": 0.515003552527855,
      "eval_test_rmse": 2.6312482357025146,
      "eval_test_runtime": 57.3707,
      "eval_test_samples_per_second": 5.682,
      "eval_test_spearman": 0.5042028987926153,
      "eval_test_steps_per_second": 5.682,
      "step": 1815
    },
    {
      "epoch": 12.001652209830649,
      "grad_norm": 35.28476333618164,
      "learning_rate": 0.000596039603960396,
      "loss": 4.922,
      "step": 1816
    },
    {
      "epoch": 12.008261049153242,
      "grad_norm": 7.197840213775635,
      "learning_rate": 0.0005955445544554456,
      "loss": 1.5596,
      "step": 1817
    },
    {
      "epoch": 12.014869888475836,
      "grad_norm": 61.32830810546875,
      "learning_rate": 0.000595049504950495,
      "loss": 2.6784,
      "step": 1818
    },
    {
      "epoch": 12.02147872779843,
      "grad_norm": 46.44101333618164,
      "learning_rate": 0.0005945544554455445,
      "loss": 1.7706,
      "step": 1819
    },
    {
      "epoch": 12.028087567121025,
      "grad_norm": 16.248550415039062,
      "learning_rate": 0.000594059405940594,
      "loss": 1.1104,
      "step": 1820
    },
    {
      "epoch": 12.034696406443619,
      "grad_norm": 59.06447219848633,
      "learning_rate": 0.0005935643564356436,
      "loss": 2.5114,
      "step": 1821
    },
    {
      "epoch": 12.041305245766212,
      "grad_norm": 27.39954948425293,
      "learning_rate": 0.0005930693069306931,
      "loss": 2.9224,
      "step": 1822
    },
    {
      "epoch": 12.047914085088806,
      "grad_norm": 55.88585662841797,
      "learning_rate": 0.0005925742574257425,
      "loss": 2.8947,
      "step": 1823
    },
    {
      "epoch": 12.0545229244114,
      "grad_norm": 15.336053848266602,
      "learning_rate": 0.000592079207920792,
      "loss": 0.969,
      "step": 1824
    },
    {
      "epoch": 12.061131763733995,
      "grad_norm": 45.131900787353516,
      "learning_rate": 0.0005915841584158416,
      "loss": 3.8056,
      "step": 1825
    },
    {
      "epoch": 12.067740603056588,
      "grad_norm": 36.239681243896484,
      "learning_rate": 0.0005910891089108911,
      "loss": 2.184,
      "step": 1826
    },
    {
      "epoch": 12.074349442379182,
      "grad_norm": 39.958621978759766,
      "learning_rate": 0.0005905940594059406,
      "loss": 2.381,
      "step": 1827
    },
    {
      "epoch": 12.080958281701776,
      "grad_norm": 50.99067306518555,
      "learning_rate": 0.00059009900990099,
      "loss": 1.3728,
      "step": 1828
    },
    {
      "epoch": 12.087567121024371,
      "grad_norm": 107.78157043457031,
      "learning_rate": 0.0005896039603960396,
      "loss": 3.7725,
      "step": 1829
    },
    {
      "epoch": 12.094175960346965,
      "grad_norm": 56.55675506591797,
      "learning_rate": 0.0005891089108910891,
      "loss": 2.101,
      "step": 1830
    },
    {
      "epoch": 12.100784799669558,
      "grad_norm": 30.754106521606445,
      "learning_rate": 0.0005886138613861386,
      "loss": 0.8143,
      "step": 1831
    },
    {
      "epoch": 12.107393638992152,
      "grad_norm": 11.086119651794434,
      "learning_rate": 0.0005881188118811881,
      "loss": 2.1655,
      "step": 1832
    },
    {
      "epoch": 12.114002478314745,
      "grad_norm": 59.09221649169922,
      "learning_rate": 0.0005876237623762376,
      "loss": 1.8895,
      "step": 1833
    },
    {
      "epoch": 12.12061131763734,
      "grad_norm": 100.12252044677734,
      "learning_rate": 0.0005871287128712871,
      "loss": 2.3908,
      "step": 1834
    },
    {
      "epoch": 12.127220156959934,
      "grad_norm": 84.5468521118164,
      "learning_rate": 0.0005866336633663366,
      "loss": 2.1256,
      "step": 1835
    },
    {
      "epoch": 12.133828996282528,
      "grad_norm": 82.43091583251953,
      "learning_rate": 0.0005861386138613861,
      "loss": 2.3806,
      "step": 1836
    },
    {
      "epoch": 12.140437835605121,
      "grad_norm": 56.3489990234375,
      "learning_rate": 0.0005856435643564357,
      "loss": 2.6154,
      "step": 1837
    },
    {
      "epoch": 12.147046674927715,
      "grad_norm": 91.4767837524414,
      "learning_rate": 0.0005851485148514851,
      "loss": 4.1743,
      "step": 1838
    },
    {
      "epoch": 12.15365551425031,
      "grad_norm": 99.0755844116211,
      "learning_rate": 0.0005846534653465346,
      "loss": 6.5532,
      "step": 1839
    },
    {
      "epoch": 12.160264353572904,
      "grad_norm": 6.903715133666992,
      "learning_rate": 0.0005841584158415841,
      "loss": 1.1385,
      "step": 1840
    },
    {
      "epoch": 12.166873192895498,
      "grad_norm": 22.50112533569336,
      "learning_rate": 0.0005836633663366337,
      "loss": 1.995,
      "step": 1841
    },
    {
      "epoch": 12.173482032218091,
      "grad_norm": 36.93925094604492,
      "learning_rate": 0.0005831683168316832,
      "loss": 2.864,
      "step": 1842
    },
    {
      "epoch": 12.180090871540685,
      "grad_norm": 34.5824089050293,
      "learning_rate": 0.0005826732673267326,
      "loss": 2.3314,
      "step": 1843
    },
    {
      "epoch": 12.18669971086328,
      "grad_norm": 35.32653045654297,
      "learning_rate": 0.0005821782178217821,
      "loss": 0.6878,
      "step": 1844
    },
    {
      "epoch": 12.193308550185874,
      "grad_norm": 60.632911682128906,
      "learning_rate": 0.0005816831683168317,
      "loss": 2.3736,
      "step": 1845
    },
    {
      "epoch": 12.199917389508467,
      "grad_norm": 69.55500793457031,
      "learning_rate": 0.0005811881188118812,
      "loss": 2.5039,
      "step": 1846
    },
    {
      "epoch": 12.206526228831061,
      "grad_norm": 65.02449798583984,
      "learning_rate": 0.0005806930693069307,
      "loss": 1.869,
      "step": 1847
    },
    {
      "epoch": 12.213135068153656,
      "grad_norm": 7.263566970825195,
      "learning_rate": 0.0005801980198019801,
      "loss": 1.3327,
      "step": 1848
    },
    {
      "epoch": 12.21974390747625,
      "grad_norm": 45.295677185058594,
      "learning_rate": 0.0005797029702970297,
      "loss": 2.7383,
      "step": 1849
    },
    {
      "epoch": 12.226352746798844,
      "grad_norm": 32.77147674560547,
      "learning_rate": 0.0005792079207920792,
      "loss": 1.5863,
      "step": 1850
    },
    {
      "epoch": 12.232961586121437,
      "grad_norm": 72.39997100830078,
      "learning_rate": 0.0005787128712871287,
      "loss": 1.4598,
      "step": 1851
    },
    {
      "epoch": 12.23957042544403,
      "grad_norm": 9.749828338623047,
      "learning_rate": 0.0005782178217821782,
      "loss": 3.063,
      "step": 1852
    },
    {
      "epoch": 12.246179264766626,
      "grad_norm": 28.872804641723633,
      "learning_rate": 0.0005777227722772277,
      "loss": 4.0744,
      "step": 1853
    },
    {
      "epoch": 12.25278810408922,
      "grad_norm": 16.28211784362793,
      "learning_rate": 0.0005772277227722772,
      "loss": 5.2812,
      "step": 1854
    },
    {
      "epoch": 12.259396943411813,
      "grad_norm": 12.823482513427734,
      "learning_rate": 0.0005767326732673267,
      "loss": 2.6452,
      "step": 1855
    },
    {
      "epoch": 12.266005782734407,
      "grad_norm": 14.572850227355957,
      "learning_rate": 0.0005762376237623762,
      "loss": 0.7648,
      "step": 1856
    },
    {
      "epoch": 12.272614622057,
      "grad_norm": 12.348003387451172,
      "learning_rate": 0.0005757425742574258,
      "loss": 0.8143,
      "step": 1857
    },
    {
      "epoch": 12.279223461379596,
      "grad_norm": 5.781181812286377,
      "learning_rate": 0.0005752475247524752,
      "loss": 1.1471,
      "step": 1858
    },
    {
      "epoch": 12.28583230070219,
      "grad_norm": 26.18886375427246,
      "learning_rate": 0.0005747524752475247,
      "loss": 1.5889,
      "step": 1859
    },
    {
      "epoch": 12.292441140024783,
      "grad_norm": 22.98641014099121,
      "learning_rate": 0.0005742574257425742,
      "loss": 1.416,
      "step": 1860
    },
    {
      "epoch": 12.299049979347377,
      "grad_norm": 17.57547950744629,
      "learning_rate": 0.0005737623762376238,
      "loss": 3.1831,
      "step": 1861
    },
    {
      "epoch": 12.30565881866997,
      "grad_norm": 36.06324768066406,
      "learning_rate": 0.0005732673267326733,
      "loss": 3.0942,
      "step": 1862
    },
    {
      "epoch": 12.312267657992566,
      "grad_norm": 18.783069610595703,
      "learning_rate": 0.0005727722772277227,
      "loss": 4.8387,
      "step": 1863
    },
    {
      "epoch": 12.31887649731516,
      "grad_norm": 23.1441650390625,
      "learning_rate": 0.0005722772277227722,
      "loss": 2.9867,
      "step": 1864
    },
    {
      "epoch": 12.325485336637753,
      "grad_norm": 17.60257911682129,
      "learning_rate": 0.0005717821782178217,
      "loss": 2.898,
      "step": 1865
    },
    {
      "epoch": 12.332094175960346,
      "grad_norm": 62.630306243896484,
      "learning_rate": 0.0005712871287128713,
      "loss": 3.0202,
      "step": 1866
    },
    {
      "epoch": 12.338703015282942,
      "grad_norm": 11.27349853515625,
      "learning_rate": 0.0005707920792079208,
      "loss": 3.2745,
      "step": 1867
    },
    {
      "epoch": 12.345311854605535,
      "grad_norm": 5.554133892059326,
      "learning_rate": 0.0005702970297029702,
      "loss": 1.948,
      "step": 1868
    },
    {
      "epoch": 12.351920693928129,
      "grad_norm": 21.75140953063965,
      "learning_rate": 0.0005698019801980197,
      "loss": 1.5586,
      "step": 1869
    },
    {
      "epoch": 12.358529533250723,
      "grad_norm": 26.52618408203125,
      "learning_rate": 0.0005693069306930693,
      "loss": 1.4087,
      "step": 1870
    },
    {
      "epoch": 12.365138372573316,
      "grad_norm": 13.050800323486328,
      "learning_rate": 0.0005688118811881188,
      "loss": 2.6226,
      "step": 1871
    },
    {
      "epoch": 12.371747211895912,
      "grad_norm": 108.48847198486328,
      "learning_rate": 0.0005683168316831683,
      "loss": 4.1955,
      "step": 1872
    },
    {
      "epoch": 12.378356051218505,
      "grad_norm": 133.369140625,
      "learning_rate": 0.0005678217821782177,
      "loss": 3.3881,
      "step": 1873
    },
    {
      "epoch": 12.384964890541099,
      "grad_norm": 93.07398986816406,
      "learning_rate": 0.0005673267326732673,
      "loss": 3.8492,
      "step": 1874
    },
    {
      "epoch": 12.391573729863692,
      "grad_norm": 14.903844833374023,
      "learning_rate": 0.0005668316831683168,
      "loss": 2.0067,
      "step": 1875
    },
    {
      "epoch": 12.398182569186286,
      "grad_norm": 14.315467834472656,
      "learning_rate": 0.0005663366336633663,
      "loss": 1.8036,
      "step": 1876
    },
    {
      "epoch": 12.404791408508881,
      "grad_norm": 40.62840270996094,
      "learning_rate": 0.0005658415841584158,
      "loss": 2.8837,
      "step": 1877
    },
    {
      "epoch": 12.411400247831475,
      "grad_norm": 44.75944519042969,
      "learning_rate": 0.0005653465346534653,
      "loss": 3.0619,
      "step": 1878
    },
    {
      "epoch": 12.418009087154068,
      "grad_norm": 14.346134185791016,
      "learning_rate": 0.0005648514851485148,
      "loss": 4.2121,
      "step": 1879
    },
    {
      "epoch": 12.424617926476662,
      "grad_norm": 32.20661544799805,
      "learning_rate": 0.0005643564356435643,
      "loss": 1.3314,
      "step": 1880
    },
    {
      "epoch": 12.431226765799256,
      "grad_norm": 6.605449199676514,
      "learning_rate": 0.0005638613861386138,
      "loss": 0.8344,
      "step": 1881
    },
    {
      "epoch": 12.437835605121851,
      "grad_norm": 51.97883605957031,
      "learning_rate": 0.0005633663366336634,
      "loss": 6.8124,
      "step": 1882
    },
    {
      "epoch": 12.444444444444445,
      "grad_norm": 110.0973892211914,
      "learning_rate": 0.0005628712871287128,
      "loss": 5.2515,
      "step": 1883
    },
    {
      "epoch": 12.451053283767038,
      "grad_norm": 34.73683166503906,
      "learning_rate": 0.0005623762376237624,
      "loss": 1.2428,
      "step": 1884
    },
    {
      "epoch": 12.457662123089632,
      "grad_norm": 20.486980438232422,
      "learning_rate": 0.000561881188118812,
      "loss": 1.0566,
      "step": 1885
    },
    {
      "epoch": 12.464270962412227,
      "grad_norm": 82.46639251708984,
      "learning_rate": 0.0005613861386138615,
      "loss": 2.7786,
      "step": 1886
    },
    {
      "epoch": 12.47087980173482,
      "grad_norm": 107.69097137451172,
      "learning_rate": 0.000560891089108911,
      "loss": 2.0541,
      "step": 1887
    },
    {
      "epoch": 12.477488641057414,
      "grad_norm": 46.280311584472656,
      "learning_rate": 0.0005603960396039604,
      "loss": 1.4622,
      "step": 1888
    },
    {
      "epoch": 12.484097480380008,
      "grad_norm": 59.853721618652344,
      "learning_rate": 0.0005599009900990099,
      "loss": 2.7672,
      "step": 1889
    },
    {
      "epoch": 12.490706319702602,
      "grad_norm": 84.04627227783203,
      "learning_rate": 0.0005594059405940595,
      "loss": 1.3973,
      "step": 1890
    },
    {
      "epoch": 12.497315159025197,
      "grad_norm": 79.59456634521484,
      "learning_rate": 0.000558910891089109,
      "loss": 1.8897,
      "step": 1891
    },
    {
      "epoch": 12.50392399834779,
      "grad_norm": 24.597679138183594,
      "learning_rate": 0.0005584158415841585,
      "loss": 1.0561,
      "step": 1892
    },
    {
      "epoch": 12.510532837670384,
      "grad_norm": 83.57942199707031,
      "learning_rate": 0.0005579207920792079,
      "loss": 2.3473,
      "step": 1893
    },
    {
      "epoch": 12.517141676992978,
      "grad_norm": 165.0086669921875,
      "learning_rate": 0.0005574257425742575,
      "loss": 4.984,
      "step": 1894
    },
    {
      "epoch": 12.523750516315571,
      "grad_norm": 161.63046264648438,
      "learning_rate": 0.000556930693069307,
      "loss": 4.501,
      "step": 1895
    },
    {
      "epoch": 12.530359355638167,
      "grad_norm": 180.96075439453125,
      "learning_rate": 0.0005564356435643565,
      "loss": 5.3417,
      "step": 1896
    },
    {
      "epoch": 12.53696819496076,
      "grad_norm": 96.60145568847656,
      "learning_rate": 0.000555940594059406,
      "loss": 3.8129,
      "step": 1897
    },
    {
      "epoch": 12.543577034283354,
      "grad_norm": 6.466059684753418,
      "learning_rate": 0.0005554455445544555,
      "loss": 1.434,
      "step": 1898
    },
    {
      "epoch": 12.550185873605948,
      "grad_norm": 63.03997039794922,
      "learning_rate": 0.000554950495049505,
      "loss": 4.3795,
      "step": 1899
    },
    {
      "epoch": 12.556794712928541,
      "grad_norm": 84.30438232421875,
      "learning_rate": 0.0005544554455445545,
      "loss": 2.2428,
      "step": 1900
    },
    {
      "epoch": 12.563403552251136,
      "grad_norm": 89.69343566894531,
      "learning_rate": 0.000553960396039604,
      "loss": 3.8493,
      "step": 1901
    },
    {
      "epoch": 12.57001239157373,
      "grad_norm": 100.23600769042969,
      "learning_rate": 0.0005534653465346536,
      "loss": 4.3428,
      "step": 1902
    },
    {
      "epoch": 12.576621230896324,
      "grad_norm": 95.54960632324219,
      "learning_rate": 0.000552970297029703,
      "loss": 3.9158,
      "step": 1903
    },
    {
      "epoch": 12.583230070218917,
      "grad_norm": 155.72592163085938,
      "learning_rate": 0.0005524752475247525,
      "loss": 4.21,
      "step": 1904
    },
    {
      "epoch": 12.589838909541513,
      "grad_norm": 92.68743896484375,
      "learning_rate": 0.000551980198019802,
      "loss": 1.9651,
      "step": 1905
    },
    {
      "epoch": 12.596447748864106,
      "grad_norm": 135.01959228515625,
      "learning_rate": 0.0005514851485148516,
      "loss": 2.9761,
      "step": 1906
    },
    {
      "epoch": 12.6030565881867,
      "grad_norm": 29.984596252441406,
      "learning_rate": 0.0005509900990099011,
      "loss": 2.531,
      "step": 1907
    },
    {
      "epoch": 12.609665427509293,
      "grad_norm": 47.362388610839844,
      "learning_rate": 0.0005504950495049505,
      "loss": 3.6839,
      "step": 1908
    },
    {
      "epoch": 12.616274266831887,
      "grad_norm": 57.419681549072266,
      "learning_rate": 0.00055,
      "loss": 2.2939,
      "step": 1909
    },
    {
      "epoch": 12.622883106154482,
      "grad_norm": 149.56256103515625,
      "learning_rate": 0.0005495049504950496,
      "loss": 4.2341,
      "step": 1910
    },
    {
      "epoch": 12.629491945477076,
      "grad_norm": 153.06617736816406,
      "learning_rate": 0.0005490099009900991,
      "loss": 6.3402,
      "step": 1911
    },
    {
      "epoch": 12.63610078479967,
      "grad_norm": 111.94528198242188,
      "learning_rate": 0.0005485148514851486,
      "loss": 4.128,
      "step": 1912
    },
    {
      "epoch": 12.642709624122263,
      "grad_norm": 24.25397300720215,
      "learning_rate": 0.000548019801980198,
      "loss": 1.5786,
      "step": 1913
    },
    {
      "epoch": 12.649318463444857,
      "grad_norm": 37.539981842041016,
      "learning_rate": 0.0005475247524752476,
      "loss": 1.4928,
      "step": 1914
    },
    {
      "epoch": 12.655927302767452,
      "grad_norm": 46.74106979370117,
      "learning_rate": 0.0005470297029702971,
      "loss": 2.945,
      "step": 1915
    },
    {
      "epoch": 12.662536142090046,
      "grad_norm": 174.7044677734375,
      "learning_rate": 0.0005465346534653466,
      "loss": 5.3845,
      "step": 1916
    },
    {
      "epoch": 12.66914498141264,
      "grad_norm": 29.735748291015625,
      "learning_rate": 0.0005460396039603961,
      "loss": 1.9483,
      "step": 1917
    },
    {
      "epoch": 12.675753820735233,
      "grad_norm": 18.858951568603516,
      "learning_rate": 0.0005455445544554456,
      "loss": 2.4914,
      "step": 1918
    },
    {
      "epoch": 12.682362660057827,
      "grad_norm": 16.09600067138672,
      "learning_rate": 0.0005450495049504951,
      "loss": 1.053,
      "step": 1919
    },
    {
      "epoch": 12.688971499380422,
      "grad_norm": 7.934563636779785,
      "learning_rate": 0.0005445544554455446,
      "loss": 0.7041,
      "step": 1920
    },
    {
      "epoch": 12.695580338703015,
      "grad_norm": 61.46696090698242,
      "learning_rate": 0.0005440594059405941,
      "loss": 1.7447,
      "step": 1921
    },
    {
      "epoch": 12.702189178025609,
      "grad_norm": 12.15015983581543,
      "learning_rate": 0.0005435643564356437,
      "loss": 2.3949,
      "step": 1922
    },
    {
      "epoch": 12.708798017348203,
      "grad_norm": 91.05009460449219,
      "learning_rate": 0.0005430693069306931,
      "loss": 2.5775,
      "step": 1923
    },
    {
      "epoch": 12.715406856670796,
      "grad_norm": 28.530498504638672,
      "learning_rate": 0.0005425742574257426,
      "loss": 1.2572,
      "step": 1924
    },
    {
      "epoch": 12.722015695993392,
      "grad_norm": 79.12566375732422,
      "learning_rate": 0.0005420792079207921,
      "loss": 2.3171,
      "step": 1925
    },
    {
      "epoch": 12.728624535315985,
      "grad_norm": 38.41445541381836,
      "learning_rate": 0.0005415841584158417,
      "loss": 1.5842,
      "step": 1926
    },
    {
      "epoch": 12.735233374638579,
      "grad_norm": 95.50450897216797,
      "learning_rate": 0.0005410891089108912,
      "loss": 4.0063,
      "step": 1927
    },
    {
      "epoch": 12.741842213961172,
      "grad_norm": 76.92662048339844,
      "learning_rate": 0.0005405940594059406,
      "loss": 3.7196,
      "step": 1928
    },
    {
      "epoch": 12.748451053283768,
      "grad_norm": 31.99884605407715,
      "learning_rate": 0.0005400990099009901,
      "loss": 5.6405,
      "step": 1929
    },
    {
      "epoch": 12.755059892606361,
      "grad_norm": 45.75730895996094,
      "learning_rate": 0.0005396039603960396,
      "loss": 2.8187,
      "step": 1930
    },
    {
      "epoch": 12.761668731928955,
      "grad_norm": 34.5562744140625,
      "learning_rate": 0.0005391089108910892,
      "loss": 2.2059,
      "step": 1931
    },
    {
      "epoch": 12.768277571251549,
      "grad_norm": 30.690031051635742,
      "learning_rate": 0.0005386138613861387,
      "loss": 2.8068,
      "step": 1932
    },
    {
      "epoch": 12.774886410574142,
      "grad_norm": 53.5255241394043,
      "learning_rate": 0.0005381188118811881,
      "loss": 1.9826,
      "step": 1933
    },
    {
      "epoch": 12.781495249896738,
      "grad_norm": 66.78388214111328,
      "learning_rate": 0.0005376237623762376,
      "loss": 2.4071,
      "step": 1934
    },
    {
      "epoch": 12.788104089219331,
      "grad_norm": 116.53108978271484,
      "learning_rate": 0.0005371287128712872,
      "loss": 3.484,
      "step": 1935
    },
    {
      "epoch": 12.794712928541925,
      "grad_norm": 66.84646606445312,
      "learning_rate": 0.0005366336633663367,
      "loss": 3.7099,
      "step": 1936
    },
    {
      "epoch": 12.801321767864518,
      "grad_norm": 35.674320220947266,
      "learning_rate": 0.0005361386138613862,
      "loss": 1.795,
      "step": 1937
    },
    {
      "epoch": 12.807930607187112,
      "grad_norm": 11.871634483337402,
      "learning_rate": 0.0005356435643564356,
      "loss": 1.1391,
      "step": 1938
    },
    {
      "epoch": 12.814539446509707,
      "grad_norm": 90.7860336303711,
      "learning_rate": 0.0005351485148514852,
      "loss": 3.3736,
      "step": 1939
    },
    {
      "epoch": 12.821148285832301,
      "grad_norm": 44.9174919128418,
      "learning_rate": 0.0005346534653465347,
      "loss": 1.5944,
      "step": 1940
    },
    {
      "epoch": 12.827757125154895,
      "grad_norm": 46.41839599609375,
      "learning_rate": 0.0005341584158415842,
      "loss": 2.49,
      "step": 1941
    },
    {
      "epoch": 12.834365964477488,
      "grad_norm": 12.491741180419922,
      "learning_rate": 0.0005336633663366337,
      "loss": 1.1875,
      "step": 1942
    },
    {
      "epoch": 12.840974803800083,
      "grad_norm": 6.630578994750977,
      "learning_rate": 0.0005331683168316832,
      "loss": 1.0067,
      "step": 1943
    },
    {
      "epoch": 12.847583643122677,
      "grad_norm": 101.16893768310547,
      "learning_rate": 0.0005326732673267327,
      "loss": 2.6751,
      "step": 1944
    },
    {
      "epoch": 12.85419248244527,
      "grad_norm": 137.51519775390625,
      "learning_rate": 0.0005321782178217822,
      "loss": 5.0892,
      "step": 1945
    },
    {
      "epoch": 12.860801321767864,
      "grad_norm": 93.04642486572266,
      "learning_rate": 0.0005316831683168317,
      "loss": 3.7602,
      "step": 1946
    },
    {
      "epoch": 12.867410161090458,
      "grad_norm": 103.45702362060547,
      "learning_rate": 0.0005311881188118813,
      "loss": 5.032,
      "step": 1947
    },
    {
      "epoch": 12.874019000413053,
      "grad_norm": 18.97295570373535,
      "learning_rate": 0.0005306930693069307,
      "loss": 2.181,
      "step": 1948
    },
    {
      "epoch": 12.880627839735647,
      "grad_norm": 16.10651969909668,
      "learning_rate": 0.0005301980198019802,
      "loss": 2.9237,
      "step": 1949
    },
    {
      "epoch": 12.88723667905824,
      "grad_norm": 8.202417373657227,
      "learning_rate": 0.0005297029702970297,
      "loss": 1.1518,
      "step": 1950
    },
    {
      "epoch": 12.893845518380834,
      "grad_norm": 33.322261810302734,
      "learning_rate": 0.0005292079207920793,
      "loss": 1.7649,
      "step": 1951
    },
    {
      "epoch": 12.900454357703428,
      "grad_norm": 70.37805938720703,
      "learning_rate": 0.0005287128712871288,
      "loss": 3.3661,
      "step": 1952
    },
    {
      "epoch": 12.907063197026023,
      "grad_norm": 16.14853286743164,
      "learning_rate": 0.0005282178217821782,
      "loss": 2.4887,
      "step": 1953
    },
    {
      "epoch": 12.913672036348617,
      "grad_norm": 29.4912166595459,
      "learning_rate": 0.0005277227722772277,
      "loss": 1.3126,
      "step": 1954
    },
    {
      "epoch": 12.92028087567121,
      "grad_norm": 72.87864685058594,
      "learning_rate": 0.0005272277227722773,
      "loss": 1.3177,
      "step": 1955
    },
    {
      "epoch": 12.926889714993804,
      "grad_norm": 16.739330291748047,
      "learning_rate": 0.0005267326732673268,
      "loss": 2.955,
      "step": 1956
    },
    {
      "epoch": 12.933498554316397,
      "grad_norm": 22.940439224243164,
      "learning_rate": 0.0005262376237623763,
      "loss": 2.5033,
      "step": 1957
    },
    {
      "epoch": 12.940107393638993,
      "grad_norm": 83.52787017822266,
      "learning_rate": 0.0005257425742574257,
      "loss": 3.4649,
      "step": 1958
    },
    {
      "epoch": 12.946716232961586,
      "grad_norm": 52.93300247192383,
      "learning_rate": 0.0005252475247524753,
      "loss": 1.7266,
      "step": 1959
    },
    {
      "epoch": 12.95332507228418,
      "grad_norm": 22.136781692504883,
      "learning_rate": 0.0005247524752475248,
      "loss": 4.0898,
      "step": 1960
    },
    {
      "epoch": 12.959933911606774,
      "grad_norm": 12.143190383911133,
      "learning_rate": 0.0005242574257425743,
      "loss": 2.9688,
      "step": 1961
    },
    {
      "epoch": 12.966542750929367,
      "grad_norm": 15.867552757263184,
      "learning_rate": 0.0005237623762376238,
      "loss": 0.8428,
      "step": 1962
    },
    {
      "epoch": 12.973151590251963,
      "grad_norm": 74.6546630859375,
      "learning_rate": 0.0005232673267326733,
      "loss": 4.8571,
      "step": 1963
    },
    {
      "epoch": 12.979760429574556,
      "grad_norm": 28.510814666748047,
      "learning_rate": 0.0005227722772277228,
      "loss": 2.9171,
      "step": 1964
    },
    {
      "epoch": 12.98636926889715,
      "grad_norm": 16.12480926513672,
      "learning_rate": 0.0005222772277227723,
      "loss": 2.8539,
      "step": 1965
    },
    {
      "epoch": 12.992978108219743,
      "grad_norm": 38.47309875488281,
      "learning_rate": 0.0005217821782178218,
      "loss": 3.3432,
      "step": 1966
    },
    {
      "epoch": 12.999586947542339,
      "grad_norm": 124.55120086669922,
      "learning_rate": 0.0005212871287128714,
      "loss": 2.6728,
      "step": 1967
    },
    {
      "epoch": 12.999586947542339,
      "eval_validation_error_bar": 0.03942965794514822,
      "eval_validation_loss": 8.556132316589355,
      "eval_validation_pearsonr": 0.6691133198476519,
      "eval_validation_rmse": 2.925086736679077,
      "eval_validation_runtime": 40.7096,
      "eval_validation_samples_per_second": 4.987,
      "eval_validation_spearman": 0.7080877922852268,
      "eval_validation_steps_per_second": 4.987,
      "step": 1967
    },
    {
      "epoch": 12.999586947542339,
      "eval_test_error_bar": 0.03927320264085606,
      "eval_test_loss": 10.268919944763184,
      "eval_test_pearsonr": 0.613427566935619,
      "eval_test_rmse": 3.2045154571533203,
      "eval_test_runtime": 55.6728,
      "eval_test_samples_per_second": 5.856,
      "eval_test_spearman": 0.5904821232597794,
      "eval_test_steps_per_second": 5.856,
      "step": 1967
    }
  ],
  "logging_steps": 1,
  "max_steps": 3020,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
