{
  "best_metric": 0.6469552431528565,
  "best_model_checkpoint": "backbone_ElnaggarLab/ankh-large-setup_convbert_sequence_concat_weights/checkpoint-3020",
  "epoch": 19.958694754233786,
  "eval_steps": 500,
  "global_step": 3020,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00660883932259397,
      "grad_norm": 2616.621337890625,
      "learning_rate": 1e-06,
      "loss": 152.8199,
      "step": 1
    },
    {
      "epoch": 0.01321767864518794,
      "grad_norm": 2801.03662109375,
      "learning_rate": 2e-06,
      "loss": 167.9539,
      "step": 2
    },
    {
      "epoch": 0.01982651796778191,
      "grad_norm": 2972.306884765625,
      "learning_rate": 3e-06,
      "loss": 187.214,
      "step": 3
    },
    {
      "epoch": 0.02643535729037588,
      "grad_norm": 2480.713623046875,
      "learning_rate": 4e-06,
      "loss": 128.0887,
      "step": 4
    },
    {
      "epoch": 0.033044196612969846,
      "grad_norm": 2758.615478515625,
      "learning_rate": 5e-06,
      "loss": 160.4231,
      "step": 5
    },
    {
      "epoch": 0.03965303593556382,
      "grad_norm": 2316.766357421875,
      "learning_rate": 6e-06,
      "loss": 120.2126,
      "step": 6
    },
    {
      "epoch": 0.04626187525815779,
      "grad_norm": 2436.71044921875,
      "learning_rate": 7e-06,
      "loss": 129.1543,
      "step": 7
    },
    {
      "epoch": 0.05287071458075176,
      "grad_norm": 2332.418701171875,
      "learning_rate": 8e-06,
      "loss": 119.3719,
      "step": 8
    },
    {
      "epoch": 0.05947955390334572,
      "grad_norm": 2137.4296875,
      "learning_rate": 9e-06,
      "loss": 98.4683,
      "step": 9
    },
    {
      "epoch": 0.06608839322593969,
      "grad_norm": 1806.785400390625,
      "learning_rate": 1e-05,
      "loss": 77.0213,
      "step": 10
    },
    {
      "epoch": 0.07269723254853366,
      "grad_norm": 1721.1634521484375,
      "learning_rate": 1.1e-05,
      "loss": 65.878,
      "step": 11
    },
    {
      "epoch": 0.07930607187112763,
      "grad_norm": 1413.2364501953125,
      "learning_rate": 1.2e-05,
      "loss": 52.4789,
      "step": 12
    },
    {
      "epoch": 0.0859149111937216,
      "grad_norm": 1591.82763671875,
      "learning_rate": 1.3e-05,
      "loss": 59.4181,
      "step": 13
    },
    {
      "epoch": 0.09252375051631558,
      "grad_norm": 1214.371826171875,
      "learning_rate": 1.4e-05,
      "loss": 35.4369,
      "step": 14
    },
    {
      "epoch": 0.09913258983890955,
      "grad_norm": 724.7294311523438,
      "learning_rate": 1.5e-05,
      "loss": 22.234,
      "step": 15
    },
    {
      "epoch": 0.10574142916150352,
      "grad_norm": 744.7297973632812,
      "learning_rate": 1.6e-05,
      "loss": 20.7407,
      "step": 16
    },
    {
      "epoch": 0.11235026848409747,
      "grad_norm": 618.2769165039062,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 15.57,
      "step": 17
    },
    {
      "epoch": 0.11895910780669144,
      "grad_norm": 204.1542205810547,
      "learning_rate": 1.8e-05,
      "loss": 8.7486,
      "step": 18
    },
    {
      "epoch": 0.12556794712928543,
      "grad_norm": 377.5178527832031,
      "learning_rate": 1.9e-05,
      "loss": 9.5741,
      "step": 19
    },
    {
      "epoch": 0.13217678645187939,
      "grad_norm": 457.14239501953125,
      "learning_rate": 2e-05,
      "loss": 10.3471,
      "step": 20
    },
    {
      "epoch": 0.13878562577447337,
      "grad_norm": 581.4067993164062,
      "learning_rate": 2.1000000000000002e-05,
      "loss": 23.2803,
      "step": 21
    },
    {
      "epoch": 0.14539446509706733,
      "grad_norm": 418.4539489746094,
      "learning_rate": 2.2e-05,
      "loss": 13.4248,
      "step": 22
    },
    {
      "epoch": 0.15200330441966128,
      "grad_norm": 502.15716552734375,
      "learning_rate": 2.3e-05,
      "loss": 14.4886,
      "step": 23
    },
    {
      "epoch": 0.15861214374225527,
      "grad_norm": 1024.7540283203125,
      "learning_rate": 2.4e-05,
      "loss": 32.3381,
      "step": 24
    },
    {
      "epoch": 0.16522098306484923,
      "grad_norm": 800.9833984375,
      "learning_rate": 2.5e-05,
      "loss": 24.2374,
      "step": 25
    },
    {
      "epoch": 0.1718298223874432,
      "grad_norm": 541.6886596679688,
      "learning_rate": 2.6e-05,
      "loss": 13.828,
      "step": 26
    },
    {
      "epoch": 0.17843866171003717,
      "grad_norm": 832.6576538085938,
      "learning_rate": 2.7e-05,
      "loss": 31.8313,
      "step": 27
    },
    {
      "epoch": 0.18504750103263115,
      "grad_norm": 591.6522827148438,
      "learning_rate": 2.8e-05,
      "loss": 11.3937,
      "step": 28
    },
    {
      "epoch": 0.1916563403552251,
      "grad_norm": 654.2435302734375,
      "learning_rate": 2.9000000000000004e-05,
      "loss": 14.1781,
      "step": 29
    },
    {
      "epoch": 0.1982651796778191,
      "grad_norm": 139.17552185058594,
      "learning_rate": 3e-05,
      "loss": 9.5634,
      "step": 30
    },
    {
      "epoch": 0.20487401900041305,
      "grad_norm": 139.20849609375,
      "learning_rate": 3.1e-05,
      "loss": 4.7401,
      "step": 31
    },
    {
      "epoch": 0.21148285832300703,
      "grad_norm": 206.370849609375,
      "learning_rate": 3.2e-05,
      "loss": 10.4904,
      "step": 32
    },
    {
      "epoch": 0.218091697645601,
      "grad_norm": 296.2991638183594,
      "learning_rate": 3.3e-05,
      "loss": 11.8923,
      "step": 33
    },
    {
      "epoch": 0.22470053696819495,
      "grad_norm": 382.9721374511719,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 19.0043,
      "step": 34
    },
    {
      "epoch": 0.23130937629078893,
      "grad_norm": 502.4991455078125,
      "learning_rate": 3.5000000000000004e-05,
      "loss": 12.7029,
      "step": 35
    },
    {
      "epoch": 0.2379182156133829,
      "grad_norm": 411.9459533691406,
      "learning_rate": 3.6e-05,
      "loss": 10.4968,
      "step": 36
    },
    {
      "epoch": 0.24452705493597687,
      "grad_norm": 227.8806610107422,
      "learning_rate": 3.7e-05,
      "loss": 14.7919,
      "step": 37
    },
    {
      "epoch": 0.25113589425857086,
      "grad_norm": 287.57806396484375,
      "learning_rate": 3.8e-05,
      "loss": 12.6328,
      "step": 38
    },
    {
      "epoch": 0.2577447335811648,
      "grad_norm": 97.86067199707031,
      "learning_rate": 3.9e-05,
      "loss": 6.1839,
      "step": 39
    },
    {
      "epoch": 0.26435357290375877,
      "grad_norm": 230.90664672851562,
      "learning_rate": 4e-05,
      "loss": 10.8955,
      "step": 40
    },
    {
      "epoch": 0.27096241222635276,
      "grad_norm": 54.73957824707031,
      "learning_rate": 4.1e-05,
      "loss": 4.847,
      "step": 41
    },
    {
      "epoch": 0.27757125154894674,
      "grad_norm": 128.16090393066406,
      "learning_rate": 4.2000000000000004e-05,
      "loss": 3.1402,
      "step": 42
    },
    {
      "epoch": 0.28418009087154067,
      "grad_norm": 517.3259887695312,
      "learning_rate": 4.2999999999999995e-05,
      "loss": 10.9456,
      "step": 43
    },
    {
      "epoch": 0.29078893019413465,
      "grad_norm": 430.6957092285156,
      "learning_rate": 4.4e-05,
      "loss": 15.5636,
      "step": 44
    },
    {
      "epoch": 0.29739776951672864,
      "grad_norm": 267.322998046875,
      "learning_rate": 4.4999999999999996e-05,
      "loss": 11.1609,
      "step": 45
    },
    {
      "epoch": 0.30400660883932257,
      "grad_norm": 27.609107971191406,
      "learning_rate": 4.6e-05,
      "loss": 13.6256,
      "step": 46
    },
    {
      "epoch": 0.31061544816191655,
      "grad_norm": 19.654386520385742,
      "learning_rate": 4.7000000000000004e-05,
      "loss": 4.2132,
      "step": 47
    },
    {
      "epoch": 0.31722428748451054,
      "grad_norm": 129.8475799560547,
      "learning_rate": 4.8e-05,
      "loss": 11.1008,
      "step": 48
    },
    {
      "epoch": 0.3238331268071045,
      "grad_norm": 184.08526611328125,
      "learning_rate": 4.9000000000000005e-05,
      "loss": 7.4277,
      "step": 49
    },
    {
      "epoch": 0.33044196612969845,
      "grad_norm": 244.8865203857422,
      "learning_rate": 5e-05,
      "loss": 8.3496,
      "step": 50
    },
    {
      "epoch": 0.33705080545229243,
      "grad_norm": 620.2802124023438,
      "learning_rate": 5.1e-05,
      "loss": 16.9172,
      "step": 51
    },
    {
      "epoch": 0.3436596447748864,
      "grad_norm": 155.14573669433594,
      "learning_rate": 5.2e-05,
      "loss": 7.4699,
      "step": 52
    },
    {
      "epoch": 0.3502684840974804,
      "grad_norm": 22.47500991821289,
      "learning_rate": 5.3e-05,
      "loss": 5.0731,
      "step": 53
    },
    {
      "epoch": 0.35687732342007433,
      "grad_norm": 176.92340087890625,
      "learning_rate": 5.4e-05,
      "loss": 12.0986,
      "step": 54
    },
    {
      "epoch": 0.3634861627426683,
      "grad_norm": 136.7953338623047,
      "learning_rate": 5.5e-05,
      "loss": 8.0766,
      "step": 55
    },
    {
      "epoch": 0.3700950020652623,
      "grad_norm": 87.22740173339844,
      "learning_rate": 5.6e-05,
      "loss": 12.0728,
      "step": 56
    },
    {
      "epoch": 0.37670384138785623,
      "grad_norm": 233.38308715820312,
      "learning_rate": 5.7e-05,
      "loss": 9.0352,
      "step": 57
    },
    {
      "epoch": 0.3833126807104502,
      "grad_norm": 614.2805786132812,
      "learning_rate": 5.800000000000001e-05,
      "loss": 13.2613,
      "step": 58
    },
    {
      "epoch": 0.3899215200330442,
      "grad_norm": 465.9129943847656,
      "learning_rate": 5.9e-05,
      "loss": 9.8807,
      "step": 59
    },
    {
      "epoch": 0.3965303593556382,
      "grad_norm": 600.592529296875,
      "learning_rate": 6e-05,
      "loss": 12.8631,
      "step": 60
    },
    {
      "epoch": 0.4031391986782321,
      "grad_norm": 353.62689208984375,
      "learning_rate": 6.1e-05,
      "loss": 8.7226,
      "step": 61
    },
    {
      "epoch": 0.4097480380008261,
      "grad_norm": 122.79306030273438,
      "learning_rate": 6.2e-05,
      "loss": 10.3505,
      "step": 62
    },
    {
      "epoch": 0.4163568773234201,
      "grad_norm": 392.6168518066406,
      "learning_rate": 6.3e-05,
      "loss": 5.9884,
      "step": 63
    },
    {
      "epoch": 0.42296571664601407,
      "grad_norm": 628.1493530273438,
      "learning_rate": 6.4e-05,
      "loss": 10.6208,
      "step": 64
    },
    {
      "epoch": 0.429574555968608,
      "grad_norm": 1019.9105834960938,
      "learning_rate": 6.500000000000001e-05,
      "loss": 22.0901,
      "step": 65
    },
    {
      "epoch": 0.436183395291202,
      "grad_norm": 678.256591796875,
      "learning_rate": 6.6e-05,
      "loss": 10.8272,
      "step": 66
    },
    {
      "epoch": 0.44279223461379597,
      "grad_norm": 733.4603881835938,
      "learning_rate": 6.7e-05,
      "loss": 13.9654,
      "step": 67
    },
    {
      "epoch": 0.4494010739363899,
      "grad_norm": 187.40017700195312,
      "learning_rate": 6.800000000000001e-05,
      "loss": 2.6777,
      "step": 68
    },
    {
      "epoch": 0.4560099132589839,
      "grad_norm": 269.89923095703125,
      "learning_rate": 6.900000000000001e-05,
      "loss": 7.0768,
      "step": 69
    },
    {
      "epoch": 0.46261875258157786,
      "grad_norm": 219.03419494628906,
      "learning_rate": 7.000000000000001e-05,
      "loss": 7.4447,
      "step": 70
    },
    {
      "epoch": 0.46922759190417185,
      "grad_norm": 269.6797180175781,
      "learning_rate": 7.099999999999999e-05,
      "loss": 8.3358,
      "step": 71
    },
    {
      "epoch": 0.4758364312267658,
      "grad_norm": 307.2607421875,
      "learning_rate": 7.2e-05,
      "loss": 5.5449,
      "step": 72
    },
    {
      "epoch": 0.48244527054935976,
      "grad_norm": 126.11002349853516,
      "learning_rate": 7.3e-05,
      "loss": 6.444,
      "step": 73
    },
    {
      "epoch": 0.48905410987195375,
      "grad_norm": 382.566162109375,
      "learning_rate": 7.4e-05,
      "loss": 6.3765,
      "step": 74
    },
    {
      "epoch": 0.49566294919454773,
      "grad_norm": 176.9502410888672,
      "learning_rate": 7.5e-05,
      "loss": 5.1256,
      "step": 75
    },
    {
      "epoch": 0.5022717885171417,
      "grad_norm": 527.7574462890625,
      "learning_rate": 7.6e-05,
      "loss": 12.3648,
      "step": 76
    },
    {
      "epoch": 0.5088806278397356,
      "grad_norm": 332.6683654785156,
      "learning_rate": 7.7e-05,
      "loss": 5.4812,
      "step": 77
    },
    {
      "epoch": 0.5154894671623296,
      "grad_norm": 86.38098907470703,
      "learning_rate": 7.8e-05,
      "loss": 4.7292,
      "step": 78
    },
    {
      "epoch": 0.5220983064849236,
      "grad_norm": 369.2254943847656,
      "learning_rate": 7.9e-05,
      "loss": 9.2847,
      "step": 79
    },
    {
      "epoch": 0.5287071458075175,
      "grad_norm": 121.63153839111328,
      "learning_rate": 8e-05,
      "loss": 10.3226,
      "step": 80
    },
    {
      "epoch": 0.5353159851301115,
      "grad_norm": 238.15371704101562,
      "learning_rate": 8.1e-05,
      "loss": 4.6112,
      "step": 81
    },
    {
      "epoch": 0.5419248244527055,
      "grad_norm": 313.69989013671875,
      "learning_rate": 8.2e-05,
      "loss": 6.2305,
      "step": 82
    },
    {
      "epoch": 0.5485336637752994,
      "grad_norm": 144.74191284179688,
      "learning_rate": 8.300000000000001e-05,
      "loss": 4.9041,
      "step": 83
    },
    {
      "epoch": 0.5551425030978935,
      "grad_norm": 111.78086853027344,
      "learning_rate": 8.400000000000001e-05,
      "loss": 5.1738,
      "step": 84
    },
    {
      "epoch": 0.5617513424204874,
      "grad_norm": 371.1761779785156,
      "learning_rate": 8.5e-05,
      "loss": 7.3776,
      "step": 85
    },
    {
      "epoch": 0.5683601817430813,
      "grad_norm": 416.8386535644531,
      "learning_rate": 8.599999999999999e-05,
      "loss": 5.8428,
      "step": 86
    },
    {
      "epoch": 0.5749690210656754,
      "grad_norm": 78.52389526367188,
      "learning_rate": 8.7e-05,
      "loss": 5.9072,
      "step": 87
    },
    {
      "epoch": 0.5815778603882693,
      "grad_norm": 73.09709167480469,
      "learning_rate": 8.8e-05,
      "loss": 6.3985,
      "step": 88
    },
    {
      "epoch": 0.5881866997108632,
      "grad_norm": 67.98403930664062,
      "learning_rate": 8.9e-05,
      "loss": 5.2188,
      "step": 89
    },
    {
      "epoch": 0.5947955390334573,
      "grad_norm": 44.194908142089844,
      "learning_rate": 8.999999999999999e-05,
      "loss": 2.7715,
      "step": 90
    },
    {
      "epoch": 0.6014043783560512,
      "grad_norm": 85.70745086669922,
      "learning_rate": 9.1e-05,
      "loss": 2.1067,
      "step": 91
    },
    {
      "epoch": 0.6080132176786451,
      "grad_norm": 436.33050537109375,
      "learning_rate": 9.2e-05,
      "loss": 7.7903,
      "step": 92
    },
    {
      "epoch": 0.6146220570012392,
      "grad_norm": 495.8853454589844,
      "learning_rate": 9.3e-05,
      "loss": 9.0313,
      "step": 93
    },
    {
      "epoch": 0.6212308963238331,
      "grad_norm": 666.75439453125,
      "learning_rate": 9.400000000000001e-05,
      "loss": 14.6662,
      "step": 94
    },
    {
      "epoch": 0.6278397356464271,
      "grad_norm": 710.986083984375,
      "learning_rate": 9.5e-05,
      "loss": 10.6762,
      "step": 95
    },
    {
      "epoch": 0.6344485749690211,
      "grad_norm": 119.62244415283203,
      "learning_rate": 9.6e-05,
      "loss": 3.5983,
      "step": 96
    },
    {
      "epoch": 0.641057414291615,
      "grad_norm": 351.8896789550781,
      "learning_rate": 9.7e-05,
      "loss": 9.8982,
      "step": 97
    },
    {
      "epoch": 0.647666253614209,
      "grad_norm": 46.014373779296875,
      "learning_rate": 9.800000000000001e-05,
      "loss": 6.1773,
      "step": 98
    },
    {
      "epoch": 0.654275092936803,
      "grad_norm": 72.896240234375,
      "learning_rate": 9.900000000000001e-05,
      "loss": 4.101,
      "step": 99
    },
    {
      "epoch": 0.6608839322593969,
      "grad_norm": 41.09584426879883,
      "learning_rate": 0.0001,
      "loss": 6.7843,
      "step": 100
    },
    {
      "epoch": 0.6674927715819909,
      "grad_norm": 175.04946899414062,
      "learning_rate": 0.000101,
      "loss": 1.9942,
      "step": 101
    },
    {
      "epoch": 0.6741016109045849,
      "grad_norm": 178.0556182861328,
      "learning_rate": 0.000102,
      "loss": 7.5316,
      "step": 102
    },
    {
      "epoch": 0.6807104502271788,
      "grad_norm": 10.939657211303711,
      "learning_rate": 0.000103,
      "loss": 2.7714,
      "step": 103
    },
    {
      "epoch": 0.6873192895497728,
      "grad_norm": 238.927734375,
      "learning_rate": 0.000104,
      "loss": 3.8029,
      "step": 104
    },
    {
      "epoch": 0.6939281288723668,
      "grad_norm": 363.8180847167969,
      "learning_rate": 0.000105,
      "loss": 7.7594,
      "step": 105
    },
    {
      "epoch": 0.7005369681949608,
      "grad_norm": 489.4647216796875,
      "learning_rate": 0.000106,
      "loss": 8.5453,
      "step": 106
    },
    {
      "epoch": 0.7071458075175547,
      "grad_norm": 55.49309158325195,
      "learning_rate": 0.000107,
      "loss": 1.6085,
      "step": 107
    },
    {
      "epoch": 0.7137546468401487,
      "grad_norm": 640.0635986328125,
      "learning_rate": 0.000108,
      "loss": 8.103,
      "step": 108
    },
    {
      "epoch": 0.7203634861627427,
      "grad_norm": 977.0732421875,
      "learning_rate": 0.000109,
      "loss": 17.3397,
      "step": 109
    },
    {
      "epoch": 0.7269723254853366,
      "grad_norm": 1200.011962890625,
      "learning_rate": 0.00011,
      "loss": 24.3551,
      "step": 110
    },
    {
      "epoch": 0.7335811648079306,
      "grad_norm": 1096.177734375,
      "learning_rate": 0.000111,
      "loss": 18.215,
      "step": 111
    },
    {
      "epoch": 0.7401900041305246,
      "grad_norm": 177.87057495117188,
      "learning_rate": 0.000112,
      "loss": 3.1665,
      "step": 112
    },
    {
      "epoch": 0.7467988434531185,
      "grad_norm": 36.511043548583984,
      "learning_rate": 0.00011300000000000001,
      "loss": 5.8098,
      "step": 113
    },
    {
      "epoch": 0.7534076827757125,
      "grad_norm": 580.4600830078125,
      "learning_rate": 0.000114,
      "loss": 10.0559,
      "step": 114
    },
    {
      "epoch": 0.7600165220983065,
      "grad_norm": 625.5357666015625,
      "learning_rate": 0.000115,
      "loss": 8.374,
      "step": 115
    },
    {
      "epoch": 0.7666253614209004,
      "grad_norm": 780.8634643554688,
      "learning_rate": 0.00011600000000000001,
      "loss": 12.7487,
      "step": 116
    },
    {
      "epoch": 0.7732342007434945,
      "grad_norm": 138.80780029296875,
      "learning_rate": 0.00011700000000000001,
      "loss": 10.3133,
      "step": 117
    },
    {
      "epoch": 0.7798430400660884,
      "grad_norm": 376.63531494140625,
      "learning_rate": 0.000118,
      "loss": 7.9029,
      "step": 118
    },
    {
      "epoch": 0.7864518793886823,
      "grad_norm": 363.9743957519531,
      "learning_rate": 0.00011899999999999999,
      "loss": 4.3179,
      "step": 119
    },
    {
      "epoch": 0.7930607187112764,
      "grad_norm": 672.121826171875,
      "learning_rate": 0.00012,
      "loss": 12.3702,
      "step": 120
    },
    {
      "epoch": 0.7996695580338703,
      "grad_norm": 251.39540100097656,
      "learning_rate": 0.000121,
      "loss": 2.2867,
      "step": 121
    },
    {
      "epoch": 0.8062783973564642,
      "grad_norm": 432.3846130371094,
      "learning_rate": 0.000122,
      "loss": 5.1752,
      "step": 122
    },
    {
      "epoch": 0.8128872366790583,
      "grad_norm": 327.61370849609375,
      "learning_rate": 0.000123,
      "loss": 7.1016,
      "step": 123
    },
    {
      "epoch": 0.8194960760016522,
      "grad_norm": 191.96319580078125,
      "learning_rate": 0.000124,
      "loss": 2.7987,
      "step": 124
    },
    {
      "epoch": 0.8261049153242461,
      "grad_norm": 92.9054946899414,
      "learning_rate": 0.000125,
      "loss": 2.6309,
      "step": 125
    },
    {
      "epoch": 0.8327137546468402,
      "grad_norm": 44.767112731933594,
      "learning_rate": 0.000126,
      "loss": 3.5324,
      "step": 126
    },
    {
      "epoch": 0.8393225939694341,
      "grad_norm": 217.57467651367188,
      "learning_rate": 0.000127,
      "loss": 4.8945,
      "step": 127
    },
    {
      "epoch": 0.8459314332920281,
      "grad_norm": 82.86463165283203,
      "learning_rate": 0.000128,
      "loss": 6.8995,
      "step": 128
    },
    {
      "epoch": 0.8525402726146221,
      "grad_norm": 158.455322265625,
      "learning_rate": 0.00012900000000000002,
      "loss": 8.8572,
      "step": 129
    },
    {
      "epoch": 0.859149111937216,
      "grad_norm": 16.058849334716797,
      "learning_rate": 0.00013000000000000002,
      "loss": 3.7405,
      "step": 130
    },
    {
      "epoch": 0.86575795125981,
      "grad_norm": 56.40597152709961,
      "learning_rate": 0.000131,
      "loss": 4.8571,
      "step": 131
    },
    {
      "epoch": 0.872366790582404,
      "grad_norm": 170.1154022216797,
      "learning_rate": 0.000132,
      "loss": 3.8122,
      "step": 132
    },
    {
      "epoch": 0.8789756299049979,
      "grad_norm": 78.4056625366211,
      "learning_rate": 0.000133,
      "loss": 4.1909,
      "step": 133
    },
    {
      "epoch": 0.8855844692275919,
      "grad_norm": 133.2696533203125,
      "learning_rate": 0.000134,
      "loss": 6.2527,
      "step": 134
    },
    {
      "epoch": 0.8921933085501859,
      "grad_norm": 53.59644317626953,
      "learning_rate": 0.000135,
      "loss": 2.3116,
      "step": 135
    },
    {
      "epoch": 0.8988021478727798,
      "grad_norm": 117.67897033691406,
      "learning_rate": 0.00013600000000000003,
      "loss": 5.0416,
      "step": 136
    },
    {
      "epoch": 0.9054109871953738,
      "grad_norm": 291.4381103515625,
      "learning_rate": 0.00013700000000000002,
      "loss": 7.3539,
      "step": 137
    },
    {
      "epoch": 0.9120198265179678,
      "grad_norm": 32.87957763671875,
      "learning_rate": 0.00013800000000000002,
      "loss": 6.4701,
      "step": 138
    },
    {
      "epoch": 0.9186286658405618,
      "grad_norm": 99.759033203125,
      "learning_rate": 0.00013900000000000002,
      "loss": 4.3092,
      "step": 139
    },
    {
      "epoch": 0.9252375051631557,
      "grad_norm": 103.78515625,
      "learning_rate": 0.00014000000000000001,
      "loss": 2.6022,
      "step": 140
    },
    {
      "epoch": 0.9318463444857497,
      "grad_norm": 160.11480712890625,
      "learning_rate": 0.00014099999999999998,
      "loss": 4.5849,
      "step": 141
    },
    {
      "epoch": 0.9384551838083437,
      "grad_norm": 208.3599853515625,
      "learning_rate": 0.00014199999999999998,
      "loss": 3.5063,
      "step": 142
    },
    {
      "epoch": 0.9450640231309376,
      "grad_norm": 112.62290954589844,
      "learning_rate": 0.00014299999999999998,
      "loss": 4.8487,
      "step": 143
    },
    {
      "epoch": 0.9516728624535316,
      "grad_norm": 316.4012145996094,
      "learning_rate": 0.000144,
      "loss": 9.2024,
      "step": 144
    },
    {
      "epoch": 0.9582817017761256,
      "grad_norm": 537.76220703125,
      "learning_rate": 0.000145,
      "loss": 11.9709,
      "step": 145
    },
    {
      "epoch": 0.9648905410987195,
      "grad_norm": 295.58465576171875,
      "learning_rate": 0.000146,
      "loss": 6.7116,
      "step": 146
    },
    {
      "epoch": 0.9714993804213135,
      "grad_norm": 98.4529037475586,
      "learning_rate": 0.000147,
      "loss": 3.5379,
      "step": 147
    },
    {
      "epoch": 0.9781082197439075,
      "grad_norm": 331.7806091308594,
      "learning_rate": 0.000148,
      "loss": 5.8032,
      "step": 148
    },
    {
      "epoch": 0.9847170590665014,
      "grad_norm": 700.6492309570312,
      "learning_rate": 0.000149,
      "loss": 11.5118,
      "step": 149
    },
    {
      "epoch": 0.9913258983890955,
      "grad_norm": 759.9554443359375,
      "learning_rate": 0.00015,
      "loss": 16.236,
      "step": 150
    },
    {
      "epoch": 0.9979347377116894,
      "grad_norm": 410.8774108886719,
      "learning_rate": 0.000151,
      "loss": 7.3795,
      "step": 151
    },
    {
      "epoch": 0.9979347377116894,
      "eval_validation_error_bar": 0.06265821595919367,
      "eval_validation_loss": 9.035340309143066,
      "eval_validation_pearsonr": 0.3988151502886683,
      "eval_validation_rmse": 3.0058841705322266,
      "eval_validation_runtime": 33.4457,
      "eval_validation_samples_per_second": 6.07,
      "eval_validation_spearman": 0.37972672500514915,
      "eval_validation_steps_per_second": 6.07,
      "step": 151
    },
    {
      "epoch": 0.9979347377116894,
      "eval_test_error_bar": 0.054375425998358734,
      "eval_test_loss": 14.689474105834961,
      "eval_test_pearsonr": 0.12450893466036023,
      "eval_test_rmse": 3.8326849937438965,
      "eval_test_runtime": 41.4679,
      "eval_test_samples_per_second": 7.862,
      "eval_test_spearman": 0.17321204268686557,
      "eval_test_steps_per_second": 7.862,
      "step": 151
    },
    {
      "epoch": 1.0045435770342834,
      "grad_norm": 80.6015853881836,
      "learning_rate": 0.000152,
      "loss": 6.874,
      "step": 152
    },
    {
      "epoch": 1.0111524163568772,
      "grad_norm": 110.82610321044922,
      "learning_rate": 0.000153,
      "loss": 3.9632,
      "step": 153
    },
    {
      "epoch": 1.0177612556794713,
      "grad_norm": 9.587516784667969,
      "learning_rate": 0.000154,
      "loss": 3.4974,
      "step": 154
    },
    {
      "epoch": 1.0243700950020653,
      "grad_norm": 59.63758850097656,
      "learning_rate": 0.000155,
      "loss": 1.9668,
      "step": 155
    },
    {
      "epoch": 1.0309789343246591,
      "grad_norm": 399.2044982910156,
      "learning_rate": 0.000156,
      "loss": 10.0349,
      "step": 156
    },
    {
      "epoch": 1.0375877736472532,
      "grad_norm": 394.01617431640625,
      "learning_rate": 0.000157,
      "loss": 5.5038,
      "step": 157
    },
    {
      "epoch": 1.0441966129698472,
      "grad_norm": 27.98029136657715,
      "learning_rate": 0.000158,
      "loss": 8.7213,
      "step": 158
    },
    {
      "epoch": 1.050805452292441,
      "grad_norm": 18.00185775756836,
      "learning_rate": 0.00015900000000000002,
      "loss": 6.7603,
      "step": 159
    },
    {
      "epoch": 1.057414291615035,
      "grad_norm": 482.7797546386719,
      "learning_rate": 0.00016,
      "loss": 8.4375,
      "step": 160
    },
    {
      "epoch": 1.0640231309376291,
      "grad_norm": 872.88916015625,
      "learning_rate": 0.000161,
      "loss": 22.871,
      "step": 161
    },
    {
      "epoch": 1.070631970260223,
      "grad_norm": 726.8242797851562,
      "learning_rate": 0.000162,
      "loss": 18.5175,
      "step": 162
    },
    {
      "epoch": 1.077240809582817,
      "grad_norm": 457.1524353027344,
      "learning_rate": 0.000163,
      "loss": 7.2767,
      "step": 163
    },
    {
      "epoch": 1.083849648905411,
      "grad_norm": 157.50729370117188,
      "learning_rate": 0.000164,
      "loss": 7.7665,
      "step": 164
    },
    {
      "epoch": 1.090458488228005,
      "grad_norm": 568.9497680664062,
      "learning_rate": 0.000165,
      "loss": 13.796,
      "step": 165
    },
    {
      "epoch": 1.0970673275505989,
      "grad_norm": 652.7288208007812,
      "learning_rate": 0.00016600000000000002,
      "loss": 15.1482,
      "step": 166
    },
    {
      "epoch": 1.103676166873193,
      "grad_norm": 1136.1712646484375,
      "learning_rate": 0.00016700000000000002,
      "loss": 32.7826,
      "step": 167
    },
    {
      "epoch": 1.110285006195787,
      "grad_norm": 498.8573913574219,
      "learning_rate": 0.00016800000000000002,
      "loss": 9.477,
      "step": 168
    },
    {
      "epoch": 1.1168938455183808,
      "grad_norm": 319.7702331542969,
      "learning_rate": 0.00016900000000000002,
      "loss": 7.7587,
      "step": 169
    },
    {
      "epoch": 1.1235026848409748,
      "grad_norm": 212.1654052734375,
      "learning_rate": 0.00017,
      "loss": 6.0406,
      "step": 170
    },
    {
      "epoch": 1.1301115241635689,
      "grad_norm": 457.03631591796875,
      "learning_rate": 0.000171,
      "loss": 10.9688,
      "step": 171
    },
    {
      "epoch": 1.1367203634861627,
      "grad_norm": 371.3103942871094,
      "learning_rate": 0.00017199999999999998,
      "loss": 9.0632,
      "step": 172
    },
    {
      "epoch": 1.1433292028087567,
      "grad_norm": 453.39044189453125,
      "learning_rate": 0.000173,
      "loss": 12.2029,
      "step": 173
    },
    {
      "epoch": 1.1499380421313508,
      "grad_norm": 83.6697006225586,
      "learning_rate": 0.000174,
      "loss": 4.4977,
      "step": 174
    },
    {
      "epoch": 1.1565468814539446,
      "grad_norm": 107.82612609863281,
      "learning_rate": 0.000175,
      "loss": 2.5508,
      "step": 175
    },
    {
      "epoch": 1.1631557207765386,
      "grad_norm": 185.48985290527344,
      "learning_rate": 0.000176,
      "loss": 6.1322,
      "step": 176
    },
    {
      "epoch": 1.1697645600991327,
      "grad_norm": 240.71633911132812,
      "learning_rate": 0.000177,
      "loss": 6.922,
      "step": 177
    },
    {
      "epoch": 1.1763733994217265,
      "grad_norm": 247.32827758789062,
      "learning_rate": 0.000178,
      "loss": 4.0278,
      "step": 178
    },
    {
      "epoch": 1.1829822387443205,
      "grad_norm": 178.55752563476562,
      "learning_rate": 0.000179,
      "loss": 6.7534,
      "step": 179
    },
    {
      "epoch": 1.1895910780669146,
      "grad_norm": 125.75406646728516,
      "learning_rate": 0.00017999999999999998,
      "loss": 3.5487,
      "step": 180
    },
    {
      "epoch": 1.1961999173895084,
      "grad_norm": 353.8265686035156,
      "learning_rate": 0.000181,
      "loss": 8.4808,
      "step": 181
    },
    {
      "epoch": 1.2028087567121024,
      "grad_norm": 159.58062744140625,
      "learning_rate": 0.000182,
      "loss": 6.8993,
      "step": 182
    },
    {
      "epoch": 1.2094175960346965,
      "grad_norm": 196.92860412597656,
      "learning_rate": 0.000183,
      "loss": 4.9631,
      "step": 183
    },
    {
      "epoch": 1.2160264353572905,
      "grad_norm": 40.74089050292969,
      "learning_rate": 0.000184,
      "loss": 7.8256,
      "step": 184
    },
    {
      "epoch": 1.2226352746798843,
      "grad_norm": 162.2694091796875,
      "learning_rate": 0.000185,
      "loss": 3.1681,
      "step": 185
    },
    {
      "epoch": 1.2292441140024783,
      "grad_norm": 206.8298797607422,
      "learning_rate": 0.000186,
      "loss": 3.7772,
      "step": 186
    },
    {
      "epoch": 1.2358529533250722,
      "grad_norm": 129.50425720214844,
      "learning_rate": 0.000187,
      "loss": 5.3386,
      "step": 187
    },
    {
      "epoch": 1.2424617926476662,
      "grad_norm": 64.11201477050781,
      "learning_rate": 0.00018800000000000002,
      "loss": 4.021,
      "step": 188
    },
    {
      "epoch": 1.2490706319702602,
      "grad_norm": 230.2142333984375,
      "learning_rate": 0.000189,
      "loss": 6.937,
      "step": 189
    },
    {
      "epoch": 1.2556794712928543,
      "grad_norm": 241.172607421875,
      "learning_rate": 0.00019,
      "loss": 6.2996,
      "step": 190
    },
    {
      "epoch": 1.262288310615448,
      "grad_norm": 73.90281677246094,
      "learning_rate": 0.000191,
      "loss": 2.1921,
      "step": 191
    },
    {
      "epoch": 1.2688971499380421,
      "grad_norm": 258.5501403808594,
      "learning_rate": 0.000192,
      "loss": 6.9659,
      "step": 192
    },
    {
      "epoch": 1.275505989260636,
      "grad_norm": 259.863037109375,
      "learning_rate": 0.000193,
      "loss": 3.8497,
      "step": 193
    },
    {
      "epoch": 1.28211482858323,
      "grad_norm": 231.0640411376953,
      "learning_rate": 0.000194,
      "loss": 7.4297,
      "step": 194
    },
    {
      "epoch": 1.288723667905824,
      "grad_norm": 5.470278263092041,
      "learning_rate": 0.00019500000000000002,
      "loss": 1.3227,
      "step": 195
    },
    {
      "epoch": 1.295332507228418,
      "grad_norm": 116.59984588623047,
      "learning_rate": 0.00019600000000000002,
      "loss": 2.342,
      "step": 196
    },
    {
      "epoch": 1.301941346551012,
      "grad_norm": 28.678791046142578,
      "learning_rate": 0.00019700000000000002,
      "loss": 3.5361,
      "step": 197
    },
    {
      "epoch": 1.308550185873606,
      "grad_norm": 78.28119659423828,
      "learning_rate": 0.00019800000000000002,
      "loss": 4.7597,
      "step": 198
    },
    {
      "epoch": 1.3151590251962,
      "grad_norm": 194.1317596435547,
      "learning_rate": 0.000199,
      "loss": 3.6326,
      "step": 199
    },
    {
      "epoch": 1.3217678645187938,
      "grad_norm": 52.47499465942383,
      "learning_rate": 0.0002,
      "loss": 3.7729,
      "step": 200
    },
    {
      "epoch": 1.3283767038413878,
      "grad_norm": 291.4162902832031,
      "learning_rate": 0.000201,
      "loss": 6.1947,
      "step": 201
    },
    {
      "epoch": 1.3349855431639819,
      "grad_norm": 403.8627014160156,
      "learning_rate": 0.000202,
      "loss": 7.4844,
      "step": 202
    },
    {
      "epoch": 1.341594382486576,
      "grad_norm": 420.8442687988281,
      "learning_rate": 0.00020300000000000003,
      "loss": 9.7204,
      "step": 203
    },
    {
      "epoch": 1.3482032218091697,
      "grad_norm": 114.43852996826172,
      "learning_rate": 0.000204,
      "loss": 4.0248,
      "step": 204
    },
    {
      "epoch": 1.3548120611317638,
      "grad_norm": 447.4477844238281,
      "learning_rate": 0.000205,
      "loss": 10.7833,
      "step": 205
    },
    {
      "epoch": 1.3614209004543576,
      "grad_norm": 554.31201171875,
      "learning_rate": 0.000206,
      "loss": 14.5203,
      "step": 206
    },
    {
      "epoch": 1.3680297397769516,
      "grad_norm": 519.5962524414062,
      "learning_rate": 0.000207,
      "loss": 14.0288,
      "step": 207
    },
    {
      "epoch": 1.3746385790995457,
      "grad_norm": 197.51231384277344,
      "learning_rate": 0.000208,
      "loss": 2.3548,
      "step": 208
    },
    {
      "epoch": 1.3812474184221397,
      "grad_norm": 38.40134048461914,
      "learning_rate": 0.00020899999999999998,
      "loss": 2.2552,
      "step": 209
    },
    {
      "epoch": 1.3878562577447335,
      "grad_norm": 483.0897216796875,
      "learning_rate": 0.00021,
      "loss": 12.5277,
      "step": 210
    },
    {
      "epoch": 1.3944650970673276,
      "grad_norm": 892.370361328125,
      "learning_rate": 0.000211,
      "loss": 33.857,
      "step": 211
    },
    {
      "epoch": 1.4010739363899214,
      "grad_norm": 782.2996826171875,
      "learning_rate": 0.000212,
      "loss": 24.3218,
      "step": 212
    },
    {
      "epoch": 1.4076827757125154,
      "grad_norm": 667.2808227539062,
      "learning_rate": 0.000213,
      "loss": 16.4261,
      "step": 213
    },
    {
      "epoch": 1.4142916150351095,
      "grad_norm": 410.96820068359375,
      "learning_rate": 0.000214,
      "loss": 8.1262,
      "step": 214
    },
    {
      "epoch": 1.4209004543577035,
      "grad_norm": 128.57655334472656,
      "learning_rate": 0.000215,
      "loss": 7.0348,
      "step": 215
    },
    {
      "epoch": 1.4275092936802973,
      "grad_norm": 279.083740234375,
      "learning_rate": 0.000216,
      "loss": 9.7256,
      "step": 216
    },
    {
      "epoch": 1.4341181330028914,
      "grad_norm": 418.63800048828125,
      "learning_rate": 0.00021700000000000002,
      "loss": 7.9779,
      "step": 217
    },
    {
      "epoch": 1.4407269723254854,
      "grad_norm": 233.05982971191406,
      "learning_rate": 0.000218,
      "loss": 7.1164,
      "step": 218
    },
    {
      "epoch": 1.4473358116480792,
      "grad_norm": 104.67221069335938,
      "learning_rate": 0.000219,
      "loss": 3.1047,
      "step": 219
    },
    {
      "epoch": 1.4539446509706733,
      "grad_norm": 308.23876953125,
      "learning_rate": 0.00022,
      "loss": 4.6674,
      "step": 220
    },
    {
      "epoch": 1.4605534902932673,
      "grad_norm": 106.04802703857422,
      "learning_rate": 0.000221,
      "loss": 2.5175,
      "step": 221
    },
    {
      "epoch": 1.4671623296158613,
      "grad_norm": 48.73169708251953,
      "learning_rate": 0.000222,
      "loss": 2.933,
      "step": 222
    },
    {
      "epoch": 1.4737711689384552,
      "grad_norm": 26.30081558227539,
      "learning_rate": 0.000223,
      "loss": 4.0876,
      "step": 223
    },
    {
      "epoch": 1.4803800082610492,
      "grad_norm": 102.2952651977539,
      "learning_rate": 0.000224,
      "loss": 5.2921,
      "step": 224
    },
    {
      "epoch": 1.486988847583643,
      "grad_norm": 89.15186309814453,
      "learning_rate": 0.00022500000000000002,
      "loss": 2.9986,
      "step": 225
    },
    {
      "epoch": 1.493597686906237,
      "grad_norm": 150.90542602539062,
      "learning_rate": 0.00022600000000000002,
      "loss": 8.8112,
      "step": 226
    },
    {
      "epoch": 1.500206526228831,
      "grad_norm": 79.18907928466797,
      "learning_rate": 0.00022700000000000002,
      "loss": 1.9747,
      "step": 227
    },
    {
      "epoch": 1.5068153655514251,
      "grad_norm": 212.4054718017578,
      "learning_rate": 0.000228,
      "loss": 2.4759,
      "step": 228
    },
    {
      "epoch": 1.513424204874019,
      "grad_norm": 319.5017395019531,
      "learning_rate": 0.000229,
      "loss": 7.0146,
      "step": 229
    },
    {
      "epoch": 1.520033044196613,
      "grad_norm": 17.972562789916992,
      "learning_rate": 0.00023,
      "loss": 3.4482,
      "step": 230
    },
    {
      "epoch": 1.5266418835192068,
      "grad_norm": 138.6988525390625,
      "learning_rate": 0.000231,
      "loss": 1.7869,
      "step": 231
    },
    {
      "epoch": 1.5332507228418009,
      "grad_norm": 57.40193557739258,
      "learning_rate": 0.00023200000000000003,
      "loss": 5.0238,
      "step": 232
    },
    {
      "epoch": 1.539859562164395,
      "grad_norm": 88.55693817138672,
      "learning_rate": 0.00023300000000000003,
      "loss": 3.5841,
      "step": 233
    },
    {
      "epoch": 1.546468401486989,
      "grad_norm": 164.7635498046875,
      "learning_rate": 0.00023400000000000002,
      "loss": 3.5974,
      "step": 234
    },
    {
      "epoch": 1.553077240809583,
      "grad_norm": 23.347488403320312,
      "learning_rate": 0.000235,
      "loss": 3.8769,
      "step": 235
    },
    {
      "epoch": 1.5596860801321768,
      "grad_norm": 28.498592376708984,
      "learning_rate": 0.000236,
      "loss": 2.926,
      "step": 236
    },
    {
      "epoch": 1.5662949194547706,
      "grad_norm": 130.35704040527344,
      "learning_rate": 0.000237,
      "loss": 3.4719,
      "step": 237
    },
    {
      "epoch": 1.5729037587773647,
      "grad_norm": 189.24160766601562,
      "learning_rate": 0.00023799999999999998,
      "loss": 3.6988,
      "step": 238
    },
    {
      "epoch": 1.5795125980999587,
      "grad_norm": 79.386474609375,
      "learning_rate": 0.00023899999999999998,
      "loss": 5.1581,
      "step": 239
    },
    {
      "epoch": 1.5861214374225527,
      "grad_norm": 456.49755859375,
      "learning_rate": 0.00024,
      "loss": 13.4393,
      "step": 240
    },
    {
      "epoch": 1.5927302767451468,
      "grad_norm": 599.3347778320312,
      "learning_rate": 0.000241,
      "loss": 19.7168,
      "step": 241
    },
    {
      "epoch": 1.5993391160677406,
      "grad_norm": 442.3421936035156,
      "learning_rate": 0.000242,
      "loss": 14.2224,
      "step": 242
    },
    {
      "epoch": 1.6059479553903344,
      "grad_norm": 279.3695373535156,
      "learning_rate": 0.000243,
      "loss": 11.0357,
      "step": 243
    },
    {
      "epoch": 1.6125567947129285,
      "grad_norm": 182.7035369873047,
      "learning_rate": 0.000244,
      "loss": 5.6438,
      "step": 244
    },
    {
      "epoch": 1.6191656340355225,
      "grad_norm": 186.70497131347656,
      "learning_rate": 0.000245,
      "loss": 9.7028,
      "step": 245
    },
    {
      "epoch": 1.6257744733581165,
      "grad_norm": 343.3663330078125,
      "learning_rate": 0.000246,
      "loss": 8.3664,
      "step": 246
    },
    {
      "epoch": 1.6323833126807106,
      "grad_norm": 37.759647369384766,
      "learning_rate": 0.000247,
      "loss": 9.2847,
      "step": 247
    },
    {
      "epoch": 1.6389921520033044,
      "grad_norm": 129.98094177246094,
      "learning_rate": 0.000248,
      "loss": 5.3028,
      "step": 248
    },
    {
      "epoch": 1.6456009913258984,
      "grad_norm": 97.20118713378906,
      "learning_rate": 0.000249,
      "loss": 8.4012,
      "step": 249
    },
    {
      "epoch": 1.6522098306484923,
      "grad_norm": 66.73204803466797,
      "learning_rate": 0.00025,
      "loss": 13.4364,
      "step": 250
    },
    {
      "epoch": 1.6588186699710863,
      "grad_norm": 128.4457244873047,
      "learning_rate": 0.00025100000000000003,
      "loss": 5.0324,
      "step": 251
    },
    {
      "epoch": 1.6654275092936803,
      "grad_norm": 442.8642883300781,
      "learning_rate": 0.000252,
      "loss": 13.0041,
      "step": 252
    },
    {
      "epoch": 1.6720363486162744,
      "grad_norm": 694.9381713867188,
      "learning_rate": 0.000253,
      "loss": 23.368,
      "step": 253
    },
    {
      "epoch": 1.6786451879388682,
      "grad_norm": 537.4754028320312,
      "learning_rate": 0.000254,
      "loss": 13.504,
      "step": 254
    },
    {
      "epoch": 1.6852540272614622,
      "grad_norm": 346.4310607910156,
      "learning_rate": 0.000255,
      "loss": 7.8778,
      "step": 255
    },
    {
      "epoch": 1.691862866584056,
      "grad_norm": 181.11767578125,
      "learning_rate": 0.000256,
      "loss": 7.7037,
      "step": 256
    },
    {
      "epoch": 1.69847170590665,
      "grad_norm": 433.926025390625,
      "learning_rate": 0.000257,
      "loss": 11.363,
      "step": 257
    },
    {
      "epoch": 1.7050805452292441,
      "grad_norm": 851.503662109375,
      "learning_rate": 0.00025800000000000004,
      "loss": 29.2824,
      "step": 258
    },
    {
      "epoch": 1.7116893845518382,
      "grad_norm": 791.3449096679688,
      "learning_rate": 0.000259,
      "loss": 26.5633,
      "step": 259
    },
    {
      "epoch": 1.7182982238744322,
      "grad_norm": 943.043701171875,
      "learning_rate": 0.00026000000000000003,
      "loss": 36.724,
      "step": 260
    },
    {
      "epoch": 1.724907063197026,
      "grad_norm": 567.7015991210938,
      "learning_rate": 0.000261,
      "loss": 14.9489,
      "step": 261
    },
    {
      "epoch": 1.7315159025196198,
      "grad_norm": 58.40654754638672,
      "learning_rate": 0.000262,
      "loss": 5.3561,
      "step": 262
    },
    {
      "epoch": 1.7381247418422139,
      "grad_norm": 17.473814010620117,
      "learning_rate": 0.000263,
      "loss": 1.1116,
      "step": 263
    },
    {
      "epoch": 1.744733581164808,
      "grad_norm": 161.69798278808594,
      "learning_rate": 0.000264,
      "loss": 3.5567,
      "step": 264
    },
    {
      "epoch": 1.751342420487402,
      "grad_norm": 146.65406799316406,
      "learning_rate": 0.00026500000000000004,
      "loss": 4.4131,
      "step": 265
    },
    {
      "epoch": 1.757951259809996,
      "grad_norm": 34.226036071777344,
      "learning_rate": 0.000266,
      "loss": 3.9398,
      "step": 266
    },
    {
      "epoch": 1.7645600991325898,
      "grad_norm": 50.616302490234375,
      "learning_rate": 0.00026700000000000004,
      "loss": 2.8881,
      "step": 267
    },
    {
      "epoch": 1.7711689384551839,
      "grad_norm": 81.48165893554688,
      "learning_rate": 0.000268,
      "loss": 6.915,
      "step": 268
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 140.77268981933594,
      "learning_rate": 0.00026900000000000003,
      "loss": 2.8582,
      "step": 269
    },
    {
      "epoch": 1.7843866171003717,
      "grad_norm": 161.00381469726562,
      "learning_rate": 0.00027,
      "loss": 5.8229,
      "step": 270
    },
    {
      "epoch": 1.7909954564229658,
      "grad_norm": 72.47505187988281,
      "learning_rate": 0.00027100000000000003,
      "loss": 3.2099,
      "step": 271
    },
    {
      "epoch": 1.7976042957455598,
      "grad_norm": 68.78705596923828,
      "learning_rate": 0.00027200000000000005,
      "loss": 6.7453,
      "step": 272
    },
    {
      "epoch": 1.8042131350681536,
      "grad_norm": 14.286944389343262,
      "learning_rate": 0.000273,
      "loss": 3.5945,
      "step": 273
    },
    {
      "epoch": 1.8108219743907477,
      "grad_norm": 228.90223693847656,
      "learning_rate": 0.00027400000000000005,
      "loss": 3.1902,
      "step": 274
    },
    {
      "epoch": 1.8174308137133415,
      "grad_norm": 493.6584167480469,
      "learning_rate": 0.000275,
      "loss": 8.1604,
      "step": 275
    },
    {
      "epoch": 1.8240396530359355,
      "grad_norm": 304.5910339355469,
      "learning_rate": 0.00027600000000000004,
      "loss": 4.0797,
      "step": 276
    },
    {
      "epoch": 1.8306484923585296,
      "grad_norm": 151.8524627685547,
      "learning_rate": 0.000277,
      "loss": 3.6776,
      "step": 277
    },
    {
      "epoch": 1.8372573316811236,
      "grad_norm": 135.0,
      "learning_rate": 0.00027800000000000004,
      "loss": 6.9384,
      "step": 278
    },
    {
      "epoch": 1.8438661710037176,
      "grad_norm": 74.865234375,
      "learning_rate": 0.000279,
      "loss": 2.9573,
      "step": 279
    },
    {
      "epoch": 1.8504750103263115,
      "grad_norm": 345.88604736328125,
      "learning_rate": 0.00028000000000000003,
      "loss": 5.779,
      "step": 280
    },
    {
      "epoch": 1.8570838496489053,
      "grad_norm": 431.28302001953125,
      "learning_rate": 0.00028100000000000005,
      "loss": 9.6517,
      "step": 281
    },
    {
      "epoch": 1.8636926889714993,
      "grad_norm": 290.30767822265625,
      "learning_rate": 0.00028199999999999997,
      "loss": 10.3216,
      "step": 282
    },
    {
      "epoch": 1.8703015282940934,
      "grad_norm": 60.81096267700195,
      "learning_rate": 0.000283,
      "loss": 10.1005,
      "step": 283
    },
    {
      "epoch": 1.8769103676166874,
      "grad_norm": 77.8653335571289,
      "learning_rate": 0.00028399999999999996,
      "loss": 7.2581,
      "step": 284
    },
    {
      "epoch": 1.8835192069392814,
      "grad_norm": 92.37955474853516,
      "learning_rate": 0.000285,
      "loss": 4.9237,
      "step": 285
    },
    {
      "epoch": 1.8901280462618752,
      "grad_norm": 83.38776397705078,
      "learning_rate": 0.00028599999999999996,
      "loss": 3.7171,
      "step": 286
    },
    {
      "epoch": 1.896736885584469,
      "grad_norm": 321.3521728515625,
      "learning_rate": 0.000287,
      "loss": 7.1628,
      "step": 287
    },
    {
      "epoch": 1.903345724907063,
      "grad_norm": 272.2088317871094,
      "learning_rate": 0.000288,
      "loss": 8.268,
      "step": 288
    },
    {
      "epoch": 1.9099545642296571,
      "grad_norm": 27.029510498046875,
      "learning_rate": 0.000289,
      "loss": 2.2617,
      "step": 289
    },
    {
      "epoch": 1.9165634035522512,
      "grad_norm": 304.70904541015625,
      "learning_rate": 0.00029,
      "loss": 7.6067,
      "step": 290
    },
    {
      "epoch": 1.9231722428748452,
      "grad_norm": 538.1708374023438,
      "learning_rate": 0.00029099999999999997,
      "loss": 16.2042,
      "step": 291
    },
    {
      "epoch": 1.929781082197439,
      "grad_norm": 572.1715698242188,
      "learning_rate": 0.000292,
      "loss": 22.1232,
      "step": 292
    },
    {
      "epoch": 1.936389921520033,
      "grad_norm": 178.97442626953125,
      "learning_rate": 0.00029299999999999997,
      "loss": 5.4662,
      "step": 293
    },
    {
      "epoch": 1.942998760842627,
      "grad_norm": 212.9838104248047,
      "learning_rate": 0.000294,
      "loss": 8.9282,
      "step": 294
    },
    {
      "epoch": 1.949607600165221,
      "grad_norm": 274.156494140625,
      "learning_rate": 0.000295,
      "loss": 6.6739,
      "step": 295
    },
    {
      "epoch": 1.956216439487815,
      "grad_norm": 523.0421752929688,
      "learning_rate": 0.000296,
      "loss": 15.7495,
      "step": 296
    },
    {
      "epoch": 1.962825278810409,
      "grad_norm": 397.1134033203125,
      "learning_rate": 0.000297,
      "loss": 14.9808,
      "step": 297
    },
    {
      "epoch": 1.9694341181330028,
      "grad_norm": 361.4122619628906,
      "learning_rate": 0.000298,
      "loss": 10.1239,
      "step": 298
    },
    {
      "epoch": 1.9760429574555969,
      "grad_norm": 67.83487701416016,
      "learning_rate": 0.000299,
      "loss": 2.9354,
      "step": 299
    },
    {
      "epoch": 1.9826517967781907,
      "grad_norm": 262.4185485839844,
      "learning_rate": 0.0003,
      "loss": 5.958,
      "step": 300
    },
    {
      "epoch": 1.9892606361007847,
      "grad_norm": 528.5928344726562,
      "learning_rate": 0.000301,
      "loss": 18.4255,
      "step": 301
    },
    {
      "epoch": 1.9958694754233788,
      "grad_norm": 296.7876281738281,
      "learning_rate": 0.000302,
      "loss": 7.6202,
      "step": 302
    },
    {
      "epoch": 1.9958694754233788,
      "eval_validation_error_bar": 0.06027870945623101,
      "eval_validation_loss": 15.641542434692383,
      "eval_validation_pearsonr": 0.47668820936636935,
      "eval_validation_rmse": 3.9549388885498047,
      "eval_validation_runtime": 33.7071,
      "eval_validation_samples_per_second": 6.022,
      "eval_validation_spearman": 0.42928202553612343,
      "eval_validation_steps_per_second": 6.022,
      "step": 302
    },
    {
      "epoch": 1.9958694754233788,
      "eval_test_error_bar": 0.051968622875626126,
      "eval_test_loss": 21.274654388427734,
      "eval_test_pearsonr": 0.14388701433224324,
      "eval_test_rmse": 4.612445831298828,
      "eval_test_runtime": 41.206,
      "eval_test_samples_per_second": 7.911,
      "eval_test_spearman": 0.2920869608831791,
      "eval_test_steps_per_second": 7.911,
      "step": 302
    },
    {
      "epoch": 2.002478314745973,
      "grad_norm": 277.5758056640625,
      "learning_rate": 0.000303,
      "loss": 5.9916,
      "step": 303
    },
    {
      "epoch": 2.009087154068567,
      "grad_norm": 117.85804748535156,
      "learning_rate": 0.000304,
      "loss": 4.3375,
      "step": 304
    },
    {
      "epoch": 2.0156959933911605,
      "grad_norm": 299.7850341796875,
      "learning_rate": 0.000305,
      "loss": 8.6532,
      "step": 305
    },
    {
      "epoch": 2.0223048327137545,
      "grad_norm": 490.0357360839844,
      "learning_rate": 0.000306,
      "loss": 16.0585,
      "step": 306
    },
    {
      "epoch": 2.0289136720363485,
      "grad_norm": 341.475341796875,
      "learning_rate": 0.000307,
      "loss": 14.4828,
      "step": 307
    },
    {
      "epoch": 2.0355225113589426,
      "grad_norm": 206.7185821533203,
      "learning_rate": 0.000308,
      "loss": 4.7735,
      "step": 308
    },
    {
      "epoch": 2.0421313506815366,
      "grad_norm": 194.9722137451172,
      "learning_rate": 0.00030900000000000003,
      "loss": 4.5304,
      "step": 309
    },
    {
      "epoch": 2.0487401900041307,
      "grad_norm": 48.21945571899414,
      "learning_rate": 0.00031,
      "loss": 4.5221,
      "step": 310
    },
    {
      "epoch": 2.0553490293267247,
      "grad_norm": 139.10623168945312,
      "learning_rate": 0.000311,
      "loss": 5.0284,
      "step": 311
    },
    {
      "epoch": 2.0619578686493183,
      "grad_norm": 196.27427673339844,
      "learning_rate": 0.000312,
      "loss": 5.9912,
      "step": 312
    },
    {
      "epoch": 2.0685667079719123,
      "grad_norm": 154.53009033203125,
      "learning_rate": 0.000313,
      "loss": 5.5641,
      "step": 313
    },
    {
      "epoch": 2.0751755472945064,
      "grad_norm": 73.76240539550781,
      "learning_rate": 0.000314,
      "loss": 11.9914,
      "step": 314
    },
    {
      "epoch": 2.0817843866171004,
      "grad_norm": 27.3389892578125,
      "learning_rate": 0.000315,
      "loss": 1.3992,
      "step": 315
    },
    {
      "epoch": 2.0883932259396945,
      "grad_norm": 390.0386962890625,
      "learning_rate": 0.000316,
      "loss": 15.8524,
      "step": 316
    },
    {
      "epoch": 2.0950020652622885,
      "grad_norm": 449.6129455566406,
      "learning_rate": 0.000317,
      "loss": 21.2521,
      "step": 317
    },
    {
      "epoch": 2.101610904584882,
      "grad_norm": 281.56243896484375,
      "learning_rate": 0.00031800000000000003,
      "loss": 8.7029,
      "step": 318
    },
    {
      "epoch": 2.108219743907476,
      "grad_norm": 127.91049194335938,
      "learning_rate": 0.000319,
      "loss": 4.6117,
      "step": 319
    },
    {
      "epoch": 2.11482858323007,
      "grad_norm": 199.79769897460938,
      "learning_rate": 0.00032,
      "loss": 5.7535,
      "step": 320
    },
    {
      "epoch": 2.121437422552664,
      "grad_norm": 370.0362243652344,
      "learning_rate": 0.000321,
      "loss": 13.2038,
      "step": 321
    },
    {
      "epoch": 2.1280462618752582,
      "grad_norm": 395.0120544433594,
      "learning_rate": 0.000322,
      "loss": 14.279,
      "step": 322
    },
    {
      "epoch": 2.1346551011978523,
      "grad_norm": 238.21641540527344,
      "learning_rate": 0.000323,
      "loss": 4.5367,
      "step": 323
    },
    {
      "epoch": 2.141263940520446,
      "grad_norm": 25.032609939575195,
      "learning_rate": 0.000324,
      "loss": 3.7171,
      "step": 324
    },
    {
      "epoch": 2.14787277984304,
      "grad_norm": 204.10829162597656,
      "learning_rate": 0.00032500000000000004,
      "loss": 5.5193,
      "step": 325
    },
    {
      "epoch": 2.154481619165634,
      "grad_norm": 343.17376708984375,
      "learning_rate": 0.000326,
      "loss": 10.8953,
      "step": 326
    },
    {
      "epoch": 2.161090458488228,
      "grad_norm": 419.6382751464844,
      "learning_rate": 0.00032700000000000003,
      "loss": 23.065,
      "step": 327
    },
    {
      "epoch": 2.167699297810822,
      "grad_norm": 302.8589172363281,
      "learning_rate": 0.000328,
      "loss": 7.2696,
      "step": 328
    },
    {
      "epoch": 2.174308137133416,
      "grad_norm": 260.6224670410156,
      "learning_rate": 0.00032900000000000003,
      "loss": 9.3642,
      "step": 329
    },
    {
      "epoch": 2.18091697645601,
      "grad_norm": 204.26087951660156,
      "learning_rate": 0.00033,
      "loss": 4.7732,
      "step": 330
    },
    {
      "epoch": 2.1875258157786037,
      "grad_norm": 189.25808715820312,
      "learning_rate": 0.000331,
      "loss": 5.5754,
      "step": 331
    },
    {
      "epoch": 2.1941346551011978,
      "grad_norm": 419.3990783691406,
      "learning_rate": 0.00033200000000000005,
      "loss": 16.7927,
      "step": 332
    },
    {
      "epoch": 2.200743494423792,
      "grad_norm": 238.6608123779297,
      "learning_rate": 0.000333,
      "loss": 5.8421,
      "step": 333
    },
    {
      "epoch": 2.207352333746386,
      "grad_norm": 96.87705993652344,
      "learning_rate": 0.00033400000000000004,
      "loss": 1.9062,
      "step": 334
    },
    {
      "epoch": 2.21396117306898,
      "grad_norm": 228.1727294921875,
      "learning_rate": 0.000335,
      "loss": 6.0952,
      "step": 335
    },
    {
      "epoch": 2.220570012391574,
      "grad_norm": 229.30746459960938,
      "learning_rate": 0.00033600000000000004,
      "loss": 4.495,
      "step": 336
    },
    {
      "epoch": 2.2271788517141675,
      "grad_norm": 443.1999206542969,
      "learning_rate": 0.000337,
      "loss": 11.5498,
      "step": 337
    },
    {
      "epoch": 2.2337876910367616,
      "grad_norm": 249.95578002929688,
      "learning_rate": 0.00033800000000000003,
      "loss": 5.4427,
      "step": 338
    },
    {
      "epoch": 2.2403965303593556,
      "grad_norm": 84.1009521484375,
      "learning_rate": 0.00033900000000000005,
      "loss": 2.5217,
      "step": 339
    },
    {
      "epoch": 2.2470053696819496,
      "grad_norm": 243.07846069335938,
      "learning_rate": 0.00034,
      "loss": 6.3643,
      "step": 340
    },
    {
      "epoch": 2.2536142090045437,
      "grad_norm": 357.47015380859375,
      "learning_rate": 0.00034100000000000005,
      "loss": 10.9676,
      "step": 341
    },
    {
      "epoch": 2.2602230483271377,
      "grad_norm": 391.9232177734375,
      "learning_rate": 0.000342,
      "loss": 10.9962,
      "step": 342
    },
    {
      "epoch": 2.2668318876497313,
      "grad_norm": 322.24334716796875,
      "learning_rate": 0.00034300000000000004,
      "loss": 7.6884,
      "step": 343
    },
    {
      "epoch": 2.2734407269723254,
      "grad_norm": 8.3538818359375,
      "learning_rate": 0.00034399999999999996,
      "loss": 1.9319,
      "step": 344
    },
    {
      "epoch": 2.2800495662949194,
      "grad_norm": 61.65977478027344,
      "learning_rate": 0.000345,
      "loss": 2.3189,
      "step": 345
    },
    {
      "epoch": 2.2866584056175134,
      "grad_norm": 55.898704528808594,
      "learning_rate": 0.000346,
      "loss": 3.5379,
      "step": 346
    },
    {
      "epoch": 2.2932672449401075,
      "grad_norm": 267.4873352050781,
      "learning_rate": 0.000347,
      "loss": 6.5673,
      "step": 347
    },
    {
      "epoch": 2.2998760842627015,
      "grad_norm": 399.689697265625,
      "learning_rate": 0.000348,
      "loss": 9.5363,
      "step": 348
    },
    {
      "epoch": 2.3064849235852956,
      "grad_norm": 251.1978302001953,
      "learning_rate": 0.00034899999999999997,
      "loss": 6.8973,
      "step": 349
    },
    {
      "epoch": 2.313093762907889,
      "grad_norm": 146.89756774902344,
      "learning_rate": 0.00035,
      "loss": 2.9583,
      "step": 350
    },
    {
      "epoch": 2.319702602230483,
      "grad_norm": 132.7383270263672,
      "learning_rate": 0.00035099999999999997,
      "loss": 3.645,
      "step": 351
    },
    {
      "epoch": 2.3263114415530772,
      "grad_norm": 52.423282623291016,
      "learning_rate": 0.000352,
      "loss": 7.5816,
      "step": 352
    },
    {
      "epoch": 2.3329202808756713,
      "grad_norm": 327.32391357421875,
      "learning_rate": 0.00035299999999999996,
      "loss": 9.8665,
      "step": 353
    },
    {
      "epoch": 2.3395291201982653,
      "grad_norm": 182.96441650390625,
      "learning_rate": 0.000354,
      "loss": 5.6249,
      "step": 354
    },
    {
      "epoch": 2.346137959520859,
      "grad_norm": 130.09666442871094,
      "learning_rate": 0.000355,
      "loss": 4.6751,
      "step": 355
    },
    {
      "epoch": 2.352746798843453,
      "grad_norm": 27.88547706604004,
      "learning_rate": 0.000356,
      "loss": 6.0371,
      "step": 356
    },
    {
      "epoch": 2.359355638166047,
      "grad_norm": 230.21426391601562,
      "learning_rate": 0.000357,
      "loss": 6.2495,
      "step": 357
    },
    {
      "epoch": 2.365964477488641,
      "grad_norm": 333.7673645019531,
      "learning_rate": 0.000358,
      "loss": 11.2895,
      "step": 358
    },
    {
      "epoch": 2.372573316811235,
      "grad_norm": 230.12156677246094,
      "learning_rate": 0.000359,
      "loss": 8.0586,
      "step": 359
    },
    {
      "epoch": 2.379182156133829,
      "grad_norm": 25.420516967773438,
      "learning_rate": 0.00035999999999999997,
      "loss": 3.7739,
      "step": 360
    },
    {
      "epoch": 2.385790995456423,
      "grad_norm": 56.43741989135742,
      "learning_rate": 0.000361,
      "loss": 2.7869,
      "step": 361
    },
    {
      "epoch": 2.3923998347790167,
      "grad_norm": 208.22535705566406,
      "learning_rate": 0.000362,
      "loss": 5.4641,
      "step": 362
    },
    {
      "epoch": 2.399008674101611,
      "grad_norm": 154.23272705078125,
      "learning_rate": 0.000363,
      "loss": 6.3211,
      "step": 363
    },
    {
      "epoch": 2.405617513424205,
      "grad_norm": 182.4716339111328,
      "learning_rate": 0.000364,
      "loss": 5.3365,
      "step": 364
    },
    {
      "epoch": 2.412226352746799,
      "grad_norm": 16.554244995117188,
      "learning_rate": 0.000365,
      "loss": 5.9323,
      "step": 365
    },
    {
      "epoch": 2.418835192069393,
      "grad_norm": 132.28018188476562,
      "learning_rate": 0.000366,
      "loss": 5.0372,
      "step": 366
    },
    {
      "epoch": 2.425444031391987,
      "grad_norm": 69.48743438720703,
      "learning_rate": 0.000367,
      "loss": 3.1784,
      "step": 367
    },
    {
      "epoch": 2.432052870714581,
      "grad_norm": 106.26897430419922,
      "learning_rate": 0.000368,
      "loss": 3.5208,
      "step": 368
    },
    {
      "epoch": 2.4386617100371746,
      "grad_norm": 125.20521545410156,
      "learning_rate": 0.000369,
      "loss": 4.4382,
      "step": 369
    },
    {
      "epoch": 2.4452705493597686,
      "grad_norm": 8.93178653717041,
      "learning_rate": 0.00037,
      "loss": 2.8301,
      "step": 370
    },
    {
      "epoch": 2.4518793886823627,
      "grad_norm": 123.68014526367188,
      "learning_rate": 0.000371,
      "loss": 4.2684,
      "step": 371
    },
    {
      "epoch": 2.4584882280049567,
      "grad_norm": 146.38320922851562,
      "learning_rate": 0.000372,
      "loss": 3.7038,
      "step": 372
    },
    {
      "epoch": 2.4650970673275507,
      "grad_norm": 383.7748107910156,
      "learning_rate": 0.000373,
      "loss": 11.7819,
      "step": 373
    },
    {
      "epoch": 2.4717059066501443,
      "grad_norm": 55.628353118896484,
      "learning_rate": 0.000374,
      "loss": 3.2972,
      "step": 374
    },
    {
      "epoch": 2.4783147459727384,
      "grad_norm": 90.39472961425781,
      "learning_rate": 0.000375,
      "loss": 2.4127,
      "step": 375
    },
    {
      "epoch": 2.4849235852953324,
      "grad_norm": 41.87309646606445,
      "learning_rate": 0.00037600000000000003,
      "loss": 2.2269,
      "step": 376
    },
    {
      "epoch": 2.4915324246179265,
      "grad_norm": 22.12057113647461,
      "learning_rate": 0.000377,
      "loss": 5.4831,
      "step": 377
    },
    {
      "epoch": 2.4981412639405205,
      "grad_norm": 59.74540328979492,
      "learning_rate": 0.000378,
      "loss": 7.7612,
      "step": 378
    },
    {
      "epoch": 2.5047501032631145,
      "grad_norm": 15.071894645690918,
      "learning_rate": 0.000379,
      "loss": 5.1621,
      "step": 379
    },
    {
      "epoch": 2.5113589425857086,
      "grad_norm": 303.027099609375,
      "learning_rate": 0.00038,
      "loss": 11.9021,
      "step": 380
    },
    {
      "epoch": 2.517967781908302,
      "grad_norm": 363.8250732421875,
      "learning_rate": 0.000381,
      "loss": 11.4455,
      "step": 381
    },
    {
      "epoch": 2.524576621230896,
      "grad_norm": 309.5527038574219,
      "learning_rate": 0.000382,
      "loss": 11.8285,
      "step": 382
    },
    {
      "epoch": 2.5311854605534903,
      "grad_norm": 259.48541259765625,
      "learning_rate": 0.00038300000000000004,
      "loss": 9.9285,
      "step": 383
    },
    {
      "epoch": 2.5377942998760843,
      "grad_norm": 142.23275756835938,
      "learning_rate": 0.000384,
      "loss": 4.0851,
      "step": 384
    },
    {
      "epoch": 2.5444031391986783,
      "grad_norm": 254.8813934326172,
      "learning_rate": 0.00038500000000000003,
      "loss": 8.0039,
      "step": 385
    },
    {
      "epoch": 2.551011978521272,
      "grad_norm": 9.817902565002441,
      "learning_rate": 0.000386,
      "loss": 3.9684,
      "step": 386
    },
    {
      "epoch": 2.5576208178438664,
      "grad_norm": 127.4610824584961,
      "learning_rate": 0.00038700000000000003,
      "loss": 5.183,
      "step": 387
    },
    {
      "epoch": 2.56422965716646,
      "grad_norm": 52.92441177368164,
      "learning_rate": 0.000388,
      "loss": 10.1279,
      "step": 388
    },
    {
      "epoch": 2.570838496489054,
      "grad_norm": 111.2695541381836,
      "learning_rate": 0.000389,
      "loss": 6.8074,
      "step": 389
    },
    {
      "epoch": 2.577447335811648,
      "grad_norm": 13.308998107910156,
      "learning_rate": 0.00039000000000000005,
      "loss": 2.3439,
      "step": 390
    },
    {
      "epoch": 2.584056175134242,
      "grad_norm": 155.99459838867188,
      "learning_rate": 0.000391,
      "loss": 3.4871,
      "step": 391
    },
    {
      "epoch": 2.590665014456836,
      "grad_norm": 211.2030029296875,
      "learning_rate": 0.00039200000000000004,
      "loss": 7.586,
      "step": 392
    },
    {
      "epoch": 2.5972738537794298,
      "grad_norm": 111.52770233154297,
      "learning_rate": 0.000393,
      "loss": 6.764,
      "step": 393
    },
    {
      "epoch": 2.603882693102024,
      "grad_norm": 47.640907287597656,
      "learning_rate": 0.00039400000000000004,
      "loss": 3.4476,
      "step": 394
    },
    {
      "epoch": 2.610491532424618,
      "grad_norm": 126.27403259277344,
      "learning_rate": 0.000395,
      "loss": 6.4258,
      "step": 395
    },
    {
      "epoch": 2.617100371747212,
      "grad_norm": 55.384239196777344,
      "learning_rate": 0.00039600000000000003,
      "loss": 4.7777,
      "step": 396
    },
    {
      "epoch": 2.623709211069806,
      "grad_norm": 196.55325317382812,
      "learning_rate": 0.00039700000000000005,
      "loss": 6.7794,
      "step": 397
    },
    {
      "epoch": 2.6303180503924,
      "grad_norm": 136.6420440673828,
      "learning_rate": 0.000398,
      "loss": 10.3357,
      "step": 398
    },
    {
      "epoch": 2.636926889714994,
      "grad_norm": 56.845462799072266,
      "learning_rate": 0.00039900000000000005,
      "loss": 2.3912,
      "step": 399
    },
    {
      "epoch": 2.6435357290375876,
      "grad_norm": 114.28972625732422,
      "learning_rate": 0.0004,
      "loss": 4.8267,
      "step": 400
    },
    {
      "epoch": 2.6501445683601816,
      "grad_norm": 99.16177368164062,
      "learning_rate": 0.00040100000000000004,
      "loss": 7.1975,
      "step": 401
    },
    {
      "epoch": 2.6567534076827757,
      "grad_norm": 136.93438720703125,
      "learning_rate": 0.000402,
      "loss": 4.2378,
      "step": 402
    },
    {
      "epoch": 2.6633622470053697,
      "grad_norm": 165.17140197753906,
      "learning_rate": 0.00040300000000000004,
      "loss": 5.2038,
      "step": 403
    },
    {
      "epoch": 2.6699710863279638,
      "grad_norm": 129.500732421875,
      "learning_rate": 0.000404,
      "loss": 4.0191,
      "step": 404
    },
    {
      "epoch": 2.6765799256505574,
      "grad_norm": 66.47267150878906,
      "learning_rate": 0.00040500000000000003,
      "loss": 3.6951,
      "step": 405
    },
    {
      "epoch": 2.683188764973152,
      "grad_norm": 60.97095489501953,
      "learning_rate": 0.00040600000000000006,
      "loss": 3.0409,
      "step": 406
    },
    {
      "epoch": 2.6897976042957454,
      "grad_norm": 20.10104751586914,
      "learning_rate": 0.00040699999999999997,
      "loss": 5.3893,
      "step": 407
    },
    {
      "epoch": 2.6964064436183395,
      "grad_norm": 22.493331909179688,
      "learning_rate": 0.000408,
      "loss": 1.6493,
      "step": 408
    },
    {
      "epoch": 2.7030152829409335,
      "grad_norm": 27.281112670898438,
      "learning_rate": 0.00040899999999999997,
      "loss": 6.5418,
      "step": 409
    },
    {
      "epoch": 2.7096241222635276,
      "grad_norm": 188.30186462402344,
      "learning_rate": 0.00041,
      "loss": 6.1242,
      "step": 410
    },
    {
      "epoch": 2.7162329615861216,
      "grad_norm": 194.5196075439453,
      "learning_rate": 0.00041099999999999996,
      "loss": 3.3898,
      "step": 411
    },
    {
      "epoch": 2.722841800908715,
      "grad_norm": 90.49066162109375,
      "learning_rate": 0.000412,
      "loss": 4.8523,
      "step": 412
    },
    {
      "epoch": 2.7294506402313092,
      "grad_norm": 180.1573944091797,
      "learning_rate": 0.000413,
      "loss": 6.4877,
      "step": 413
    },
    {
      "epoch": 2.7360594795539033,
      "grad_norm": 72.4202651977539,
      "learning_rate": 0.000414,
      "loss": 5.9405,
      "step": 414
    },
    {
      "epoch": 2.7426683188764973,
      "grad_norm": 75.87078094482422,
      "learning_rate": 0.000415,
      "loss": 5.2519,
      "step": 415
    },
    {
      "epoch": 2.7492771581990914,
      "grad_norm": 120.26215362548828,
      "learning_rate": 0.000416,
      "loss": 3.1978,
      "step": 416
    },
    {
      "epoch": 2.7558859975216854,
      "grad_norm": 262.8613586425781,
      "learning_rate": 0.000417,
      "loss": 6.0476,
      "step": 417
    },
    {
      "epoch": 2.7624948368442794,
      "grad_norm": 624.9620361328125,
      "learning_rate": 0.00041799999999999997,
      "loss": 24.4895,
      "step": 418
    },
    {
      "epoch": 2.769103676166873,
      "grad_norm": 530.3270874023438,
      "learning_rate": 0.000419,
      "loss": 16.1411,
      "step": 419
    },
    {
      "epoch": 2.775712515489467,
      "grad_norm": 310.16680908203125,
      "learning_rate": 0.00042,
      "loss": 9.6097,
      "step": 420
    },
    {
      "epoch": 2.782321354812061,
      "grad_norm": 115.74424743652344,
      "learning_rate": 0.000421,
      "loss": 3.1635,
      "step": 421
    },
    {
      "epoch": 2.788930194134655,
      "grad_norm": 163.4573211669922,
      "learning_rate": 0.000422,
      "loss": 11.8517,
      "step": 422
    },
    {
      "epoch": 2.795539033457249,
      "grad_norm": 381.6115417480469,
      "learning_rate": 0.000423,
      "loss": 13.1782,
      "step": 423
    },
    {
      "epoch": 2.802147872779843,
      "grad_norm": 258.71185302734375,
      "learning_rate": 0.000424,
      "loss": 5.8595,
      "step": 424
    },
    {
      "epoch": 2.8087567121024373,
      "grad_norm": 279.6826171875,
      "learning_rate": 0.000425,
      "loss": 7.9384,
      "step": 425
    },
    {
      "epoch": 2.815365551425031,
      "grad_norm": 23.939924240112305,
      "learning_rate": 0.000426,
      "loss": 4.3428,
      "step": 426
    },
    {
      "epoch": 2.821974390747625,
      "grad_norm": 66.66634368896484,
      "learning_rate": 0.000427,
      "loss": 7.0094,
      "step": 427
    },
    {
      "epoch": 2.828583230070219,
      "grad_norm": 193.0997314453125,
      "learning_rate": 0.000428,
      "loss": 9.7403,
      "step": 428
    },
    {
      "epoch": 2.835192069392813,
      "grad_norm": 205.02035522460938,
      "learning_rate": 0.000429,
      "loss": 8.2788,
      "step": 429
    },
    {
      "epoch": 2.841800908715407,
      "grad_norm": 193.86538696289062,
      "learning_rate": 0.00043,
      "loss": 6.9046,
      "step": 430
    },
    {
      "epoch": 2.8484097480380006,
      "grad_norm": 79.03099060058594,
      "learning_rate": 0.000431,
      "loss": 3.6242,
      "step": 431
    },
    {
      "epoch": 2.8550185873605947,
      "grad_norm": 189.9082489013672,
      "learning_rate": 0.000432,
      "loss": 12.8425,
      "step": 432
    },
    {
      "epoch": 2.8616274266831887,
      "grad_norm": 128.7954559326172,
      "learning_rate": 0.000433,
      "loss": 4.7456,
      "step": 433
    },
    {
      "epoch": 2.8682362660057827,
      "grad_norm": 81.5986099243164,
      "learning_rate": 0.00043400000000000003,
      "loss": 4.0783,
      "step": 434
    },
    {
      "epoch": 2.874845105328377,
      "grad_norm": 29.60869026184082,
      "learning_rate": 0.000435,
      "loss": 3.6228,
      "step": 435
    },
    {
      "epoch": 2.881453944650971,
      "grad_norm": 5.1300764083862305,
      "learning_rate": 0.000436,
      "loss": 2.8912,
      "step": 436
    },
    {
      "epoch": 2.888062783973565,
      "grad_norm": 246.86097717285156,
      "learning_rate": 0.000437,
      "loss": 6.0432,
      "step": 437
    },
    {
      "epoch": 2.8946716232961585,
      "grad_norm": 134.10055541992188,
      "learning_rate": 0.000438,
      "loss": 3.904,
      "step": 438
    },
    {
      "epoch": 2.9012804626187525,
      "grad_norm": 374.18170166015625,
      "learning_rate": 0.000439,
      "loss": 12.4831,
      "step": 439
    },
    {
      "epoch": 2.9078893019413465,
      "grad_norm": 207.35194396972656,
      "learning_rate": 0.00044,
      "loss": 5.974,
      "step": 440
    },
    {
      "epoch": 2.9144981412639406,
      "grad_norm": 26.143259048461914,
      "learning_rate": 0.000441,
      "loss": 3.998,
      "step": 441
    },
    {
      "epoch": 2.9211069805865346,
      "grad_norm": 174.06259155273438,
      "learning_rate": 0.000442,
      "loss": 8.4586,
      "step": 442
    },
    {
      "epoch": 2.927715819909128,
      "grad_norm": 287.9757995605469,
      "learning_rate": 0.00044300000000000003,
      "loss": 8.8068,
      "step": 443
    },
    {
      "epoch": 2.9343246592317227,
      "grad_norm": 82.73855590820312,
      "learning_rate": 0.000444,
      "loss": 5.176,
      "step": 444
    },
    {
      "epoch": 2.9409334985543163,
      "grad_norm": 65.71678161621094,
      "learning_rate": 0.00044500000000000003,
      "loss": 7.5282,
      "step": 445
    },
    {
      "epoch": 2.9475423378769103,
      "grad_norm": 108.19602966308594,
      "learning_rate": 0.000446,
      "loss": 2.6361,
      "step": 446
    },
    {
      "epoch": 2.9541511771995044,
      "grad_norm": 445.88970947265625,
      "learning_rate": 0.000447,
      "loss": 25.0473,
      "step": 447
    },
    {
      "epoch": 2.9607600165220984,
      "grad_norm": 266.4510803222656,
      "learning_rate": 0.000448,
      "loss": 7.4378,
      "step": 448
    },
    {
      "epoch": 2.9673688558446925,
      "grad_norm": 138.7958984375,
      "learning_rate": 0.000449,
      "loss": 5.5096,
      "step": 449
    },
    {
      "epoch": 2.973977695167286,
      "grad_norm": 8.37890911102295,
      "learning_rate": 0.00045000000000000004,
      "loss": 4.1402,
      "step": 450
    },
    {
      "epoch": 2.98058653448988,
      "grad_norm": 311.4175109863281,
      "learning_rate": 0.000451,
      "loss": 16.3818,
      "step": 451
    },
    {
      "epoch": 2.987195373812474,
      "grad_norm": 183.4537353515625,
      "learning_rate": 0.00045200000000000004,
      "loss": 5.6833,
      "step": 452
    },
    {
      "epoch": 2.993804213135068,
      "grad_norm": 234.6599884033203,
      "learning_rate": 0.000453,
      "loss": 8.4268,
      "step": 453
    },
    {
      "epoch": 2.993804213135068,
      "eval_validation_error_bar": 0.06179864906017124,
      "eval_validation_loss": 9.719583511352539,
      "eval_validation_pearsonr": 0.4223798833204352,
      "eval_validation_rmse": 3.1176247596740723,
      "eval_validation_runtime": 33.1706,
      "eval_validation_samples_per_second": 6.12,
      "eval_validation_spearman": 0.39848658729184233,
      "eval_validation_steps_per_second": 6.12,
      "step": 453
    },
    {
      "epoch": 2.993804213135068,
      "eval_test_error_bar": 0.05173008071773633,
      "eval_test_loss": 10.634119033813477,
      "eval_test_pearsonr": 0.26635945760608987,
      "eval_test_rmse": 3.2609996795654297,
      "eval_test_runtime": 41.5448,
      "eval_test_samples_per_second": 7.847,
      "eval_test_spearman": 0.3011361343748536,
      "eval_test_steps_per_second": 7.847,
      "step": 453
    },
    {
      "epoch": 3.000413052457662,
      "grad_norm": 132.58587646484375,
      "learning_rate": 0.00045400000000000003,
      "loss": 3.5633,
      "step": 454
    },
    {
      "epoch": 3.0070218917802563,
      "grad_norm": 9.966182708740234,
      "learning_rate": 0.000455,
      "loss": 4.2374,
      "step": 455
    },
    {
      "epoch": 3.01363073110285,
      "grad_norm": 208.7588348388672,
      "learning_rate": 0.000456,
      "loss": 7.9563,
      "step": 456
    },
    {
      "epoch": 3.020239570425444,
      "grad_norm": 207.7383270263672,
      "learning_rate": 0.00045700000000000005,
      "loss": 9.1474,
      "step": 457
    },
    {
      "epoch": 3.026848409748038,
      "grad_norm": 261.3671569824219,
      "learning_rate": 0.000458,
      "loss": 14.5659,
      "step": 458
    },
    {
      "epoch": 3.033457249070632,
      "grad_norm": 88.7845687866211,
      "learning_rate": 0.00045900000000000004,
      "loss": 3.2546,
      "step": 459
    },
    {
      "epoch": 3.040066088393226,
      "grad_norm": 40.81598663330078,
      "learning_rate": 0.00046,
      "loss": 3.9509,
      "step": 460
    },
    {
      "epoch": 3.04667492771582,
      "grad_norm": 158.4167022705078,
      "learning_rate": 0.00046100000000000004,
      "loss": 6.2323,
      "step": 461
    },
    {
      "epoch": 3.053283767038414,
      "grad_norm": 108.02507781982422,
      "learning_rate": 0.000462,
      "loss": 3.9393,
      "step": 462
    },
    {
      "epoch": 3.0598926063610077,
      "grad_norm": 96.0007553100586,
      "learning_rate": 0.00046300000000000003,
      "loss": 2.4882,
      "step": 463
    },
    {
      "epoch": 3.0665014456836017,
      "grad_norm": 6.113141059875488,
      "learning_rate": 0.00046400000000000006,
      "loss": 4.9,
      "step": 464
    },
    {
      "epoch": 3.0731102850061958,
      "grad_norm": 26.699621200561523,
      "learning_rate": 0.000465,
      "loss": 3.0151,
      "step": 465
    },
    {
      "epoch": 3.07971912432879,
      "grad_norm": 16.319347381591797,
      "learning_rate": 0.00046600000000000005,
      "loss": 3.1254,
      "step": 466
    },
    {
      "epoch": 3.086327963651384,
      "grad_norm": 99.43302917480469,
      "learning_rate": 0.000467,
      "loss": 8.2562,
      "step": 467
    },
    {
      "epoch": 3.092936802973978,
      "grad_norm": 97.99295806884766,
      "learning_rate": 0.00046800000000000005,
      "loss": 8.1122,
      "step": 468
    },
    {
      "epoch": 3.0995456422965715,
      "grad_norm": 135.6459503173828,
      "learning_rate": 0.00046899999999999996,
      "loss": 5.8996,
      "step": 469
    },
    {
      "epoch": 3.1061544816191655,
      "grad_norm": 171.0245361328125,
      "learning_rate": 0.00047,
      "loss": 7.8906,
      "step": 470
    },
    {
      "epoch": 3.1127633209417596,
      "grad_norm": 136.09933471679688,
      "learning_rate": 0.000471,
      "loss": 10.2013,
      "step": 471
    },
    {
      "epoch": 3.1193721602643536,
      "grad_norm": 152.0813446044922,
      "learning_rate": 0.000472,
      "loss": 7.5014,
      "step": 472
    },
    {
      "epoch": 3.1259809995869476,
      "grad_norm": 25.995948791503906,
      "learning_rate": 0.000473,
      "loss": 3.2882,
      "step": 473
    },
    {
      "epoch": 3.1325898389095417,
      "grad_norm": 309.9387512207031,
      "learning_rate": 0.000474,
      "loss": 12.7378,
      "step": 474
    },
    {
      "epoch": 3.1391986782321353,
      "grad_norm": 221.93505859375,
      "learning_rate": 0.000475,
      "loss": 8.1065,
      "step": 475
    },
    {
      "epoch": 3.1458075175547293,
      "grad_norm": 85.87372589111328,
      "learning_rate": 0.00047599999999999997,
      "loss": 5.2533,
      "step": 476
    },
    {
      "epoch": 3.1524163568773234,
      "grad_norm": 69.48931121826172,
      "learning_rate": 0.000477,
      "loss": 2.9146,
      "step": 477
    },
    {
      "epoch": 3.1590251961999174,
      "grad_norm": 131.65977478027344,
      "learning_rate": 0.00047799999999999996,
      "loss": 6.6316,
      "step": 478
    },
    {
      "epoch": 3.1656340355225114,
      "grad_norm": 94.98576354980469,
      "learning_rate": 0.000479,
      "loss": 5.6011,
      "step": 479
    },
    {
      "epoch": 3.1722428748451055,
      "grad_norm": 178.20269775390625,
      "learning_rate": 0.00048,
      "loss": 6.0911,
      "step": 480
    },
    {
      "epoch": 3.178851714167699,
      "grad_norm": 11.263922691345215,
      "learning_rate": 0.000481,
      "loss": 9.5274,
      "step": 481
    },
    {
      "epoch": 3.185460553490293,
      "grad_norm": 118.05341339111328,
      "learning_rate": 0.000482,
      "loss": 7.2764,
      "step": 482
    },
    {
      "epoch": 3.192069392812887,
      "grad_norm": 77.08304595947266,
      "learning_rate": 0.000483,
      "loss": 3.9973,
      "step": 483
    },
    {
      "epoch": 3.198678232135481,
      "grad_norm": 41.750301361083984,
      "learning_rate": 0.000484,
      "loss": 5.1225,
      "step": 484
    },
    {
      "epoch": 3.2052870714580752,
      "grad_norm": 205.43296813964844,
      "learning_rate": 0.00048499999999999997,
      "loss": 6.5652,
      "step": 485
    },
    {
      "epoch": 3.2118959107806693,
      "grad_norm": 39.5437126159668,
      "learning_rate": 0.000486,
      "loss": 4.263,
      "step": 486
    },
    {
      "epoch": 3.2185047501032633,
      "grad_norm": 57.70527648925781,
      "learning_rate": 0.000487,
      "loss": 2.9159,
      "step": 487
    },
    {
      "epoch": 3.225113589425857,
      "grad_norm": 3.461514949798584,
      "learning_rate": 0.000488,
      "loss": 1.3778,
      "step": 488
    },
    {
      "epoch": 3.231722428748451,
      "grad_norm": 92.55753326416016,
      "learning_rate": 0.000489,
      "loss": 3.2234,
      "step": 489
    },
    {
      "epoch": 3.238331268071045,
      "grad_norm": 15.408724784851074,
      "learning_rate": 0.00049,
      "loss": 4.2825,
      "step": 490
    },
    {
      "epoch": 3.244940107393639,
      "grad_norm": 84.30546569824219,
      "learning_rate": 0.000491,
      "loss": 4.9436,
      "step": 491
    },
    {
      "epoch": 3.251548946716233,
      "grad_norm": 9.699603080749512,
      "learning_rate": 0.000492,
      "loss": 3.3717,
      "step": 492
    },
    {
      "epoch": 3.258157786038827,
      "grad_norm": 36.39140701293945,
      "learning_rate": 0.0004930000000000001,
      "loss": 3.51,
      "step": 493
    },
    {
      "epoch": 3.264766625361421,
      "grad_norm": 88.40953063964844,
      "learning_rate": 0.000494,
      "loss": 8.7935,
      "step": 494
    },
    {
      "epoch": 3.2713754646840147,
      "grad_norm": 138.4902801513672,
      "learning_rate": 0.000495,
      "loss": 6.2951,
      "step": 495
    },
    {
      "epoch": 3.277984304006609,
      "grad_norm": 79.36653900146484,
      "learning_rate": 0.000496,
      "loss": 4.9381,
      "step": 496
    },
    {
      "epoch": 3.284593143329203,
      "grad_norm": 88.75106811523438,
      "learning_rate": 0.000497,
      "loss": 2.9154,
      "step": 497
    },
    {
      "epoch": 3.291201982651797,
      "grad_norm": 16.111928939819336,
      "learning_rate": 0.000498,
      "loss": 4.7043,
      "step": 498
    },
    {
      "epoch": 3.297810821974391,
      "grad_norm": 295.4071350097656,
      "learning_rate": 0.000499,
      "loss": 15.1232,
      "step": 499
    },
    {
      "epoch": 3.3044196612969845,
      "grad_norm": 283.2747802734375,
      "learning_rate": 0.0005,
      "loss": 11.1419,
      "step": 500
    },
    {
      "epoch": 3.3110285006195785,
      "grad_norm": 141.37538146972656,
      "learning_rate": 0.000501,
      "loss": 7.742,
      "step": 501
    },
    {
      "epoch": 3.3176373399421726,
      "grad_norm": 167.96827697753906,
      "learning_rate": 0.0005020000000000001,
      "loss": 7.5771,
      "step": 502
    },
    {
      "epoch": 3.3242461792647666,
      "grad_norm": 195.85055541992188,
      "learning_rate": 0.000503,
      "loss": 7.1997,
      "step": 503
    },
    {
      "epoch": 3.3308550185873607,
      "grad_norm": 274.0064697265625,
      "learning_rate": 0.000504,
      "loss": 11.1784,
      "step": 504
    },
    {
      "epoch": 3.3374638579099547,
      "grad_norm": 264.5513916015625,
      "learning_rate": 0.000505,
      "loss": 10.1735,
      "step": 505
    },
    {
      "epoch": 3.3440726972325487,
      "grad_norm": 168.2459259033203,
      "learning_rate": 0.000506,
      "loss": 5.6366,
      "step": 506
    },
    {
      "epoch": 3.3506815365551423,
      "grad_norm": 89.43494415283203,
      "learning_rate": 0.000507,
      "loss": 3.3193,
      "step": 507
    },
    {
      "epoch": 3.3572903758777364,
      "grad_norm": 158.7966766357422,
      "learning_rate": 0.000508,
      "loss": 5.579,
      "step": 508
    },
    {
      "epoch": 3.3638992152003304,
      "grad_norm": 47.61066818237305,
      "learning_rate": 0.000509,
      "loss": 6.6299,
      "step": 509
    },
    {
      "epoch": 3.3705080545229245,
      "grad_norm": 9.8734712600708,
      "learning_rate": 0.00051,
      "loss": 4.9577,
      "step": 510
    },
    {
      "epoch": 3.3771168938455185,
      "grad_norm": 265.4704284667969,
      "learning_rate": 0.0005110000000000001,
      "loss": 12.1088,
      "step": 511
    },
    {
      "epoch": 3.3837257331681125,
      "grad_norm": 224.827880859375,
      "learning_rate": 0.000512,
      "loss": 11.8712,
      "step": 512
    },
    {
      "epoch": 3.390334572490706,
      "grad_norm": 136.0146484375,
      "learning_rate": 0.000513,
      "loss": 4.9045,
      "step": 513
    },
    {
      "epoch": 3.3969434118133,
      "grad_norm": 114.25347900390625,
      "learning_rate": 0.000514,
      "loss": 4.6293,
      "step": 514
    },
    {
      "epoch": 3.403552251135894,
      "grad_norm": 34.321250915527344,
      "learning_rate": 0.000515,
      "loss": 3.247,
      "step": 515
    },
    {
      "epoch": 3.4101610904584883,
      "grad_norm": 153.1536865234375,
      "learning_rate": 0.0005160000000000001,
      "loss": 6.3951,
      "step": 516
    },
    {
      "epoch": 3.4167699297810823,
      "grad_norm": 86.357177734375,
      "learning_rate": 0.000517,
      "loss": 2.4071,
      "step": 517
    },
    {
      "epoch": 3.4233787691036763,
      "grad_norm": 3.780458688735962,
      "learning_rate": 0.000518,
      "loss": 2.403,
      "step": 518
    },
    {
      "epoch": 3.42998760842627,
      "grad_norm": 57.13691711425781,
      "learning_rate": 0.000519,
      "loss": 2.7446,
      "step": 519
    },
    {
      "epoch": 3.436596447748864,
      "grad_norm": 8.598409652709961,
      "learning_rate": 0.0005200000000000001,
      "loss": 2.3749,
      "step": 520
    },
    {
      "epoch": 3.443205287071458,
      "grad_norm": 72.7296142578125,
      "learning_rate": 0.000521,
      "loss": 5.3936,
      "step": 521
    },
    {
      "epoch": 3.449814126394052,
      "grad_norm": 100.93407440185547,
      "learning_rate": 0.000522,
      "loss": 5.1055,
      "step": 522
    },
    {
      "epoch": 3.456422965716646,
      "grad_norm": 177.02584838867188,
      "learning_rate": 0.000523,
      "loss": 5.7443,
      "step": 523
    },
    {
      "epoch": 3.46303180503924,
      "grad_norm": 7.166334629058838,
      "learning_rate": 0.000524,
      "loss": 3.1532,
      "step": 524
    },
    {
      "epoch": 3.469640644361834,
      "grad_norm": 134.06053161621094,
      "learning_rate": 0.0005250000000000001,
      "loss": 4.487,
      "step": 525
    },
    {
      "epoch": 3.4762494836844278,
      "grad_norm": 83.49424743652344,
      "learning_rate": 0.000526,
      "loss": 11.9203,
      "step": 526
    },
    {
      "epoch": 3.482858323007022,
      "grad_norm": 32.10511779785156,
      "learning_rate": 0.000527,
      "loss": 2.5661,
      "step": 527
    },
    {
      "epoch": 3.489467162329616,
      "grad_norm": 28.27445411682129,
      "learning_rate": 0.000528,
      "loss": 2.2581,
      "step": 528
    },
    {
      "epoch": 3.49607600165221,
      "grad_norm": 30.281124114990234,
      "learning_rate": 0.0005290000000000001,
      "loss": 3.9953,
      "step": 529
    },
    {
      "epoch": 3.502684840974804,
      "grad_norm": 59.99298095703125,
      "learning_rate": 0.0005300000000000001,
      "loss": 2.998,
      "step": 530
    },
    {
      "epoch": 3.5092936802973975,
      "grad_norm": 25.905094146728516,
      "learning_rate": 0.000531,
      "loss": 5.3248,
      "step": 531
    },
    {
      "epoch": 3.515902519619992,
      "grad_norm": 9.364774703979492,
      "learning_rate": 0.000532,
      "loss": 3.6381,
      "step": 532
    },
    {
      "epoch": 3.5225113589425856,
      "grad_norm": 24.239877700805664,
      "learning_rate": 0.000533,
      "loss": 2.8596,
      "step": 533
    },
    {
      "epoch": 3.5291201982651796,
      "grad_norm": 35.604434967041016,
      "learning_rate": 0.0005340000000000001,
      "loss": 4.4744,
      "step": 534
    },
    {
      "epoch": 3.5357290375877737,
      "grad_norm": 95.72991180419922,
      "learning_rate": 0.000535,
      "loss": 4.5319,
      "step": 535
    },
    {
      "epoch": 3.5423378769103677,
      "grad_norm": 26.519975662231445,
      "learning_rate": 0.000536,
      "loss": 3.0179,
      "step": 536
    },
    {
      "epoch": 3.5489467162329618,
      "grad_norm": 144.38414001464844,
      "learning_rate": 0.000537,
      "loss": 5.01,
      "step": 537
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 177.46299743652344,
      "learning_rate": 0.0005380000000000001,
      "loss": 7.4226,
      "step": 538
    },
    {
      "epoch": 3.5621643948781494,
      "grad_norm": 163.498779296875,
      "learning_rate": 0.0005390000000000001,
      "loss": 5.6734,
      "step": 539
    },
    {
      "epoch": 3.5687732342007434,
      "grad_norm": 183.6916046142578,
      "learning_rate": 0.00054,
      "loss": 9.1102,
      "step": 540
    },
    {
      "epoch": 3.5753820735233375,
      "grad_norm": 95.22909545898438,
      "learning_rate": 0.000541,
      "loss": 3.5452,
      "step": 541
    },
    {
      "epoch": 3.5819909128459315,
      "grad_norm": 6.664873123168945,
      "learning_rate": 0.0005420000000000001,
      "loss": 2.1301,
      "step": 542
    },
    {
      "epoch": 3.5885997521685256,
      "grad_norm": 157.7886962890625,
      "learning_rate": 0.0005430000000000001,
      "loss": 6.4384,
      "step": 543
    },
    {
      "epoch": 3.5952085914911196,
      "grad_norm": 266.3159484863281,
      "learning_rate": 0.0005440000000000001,
      "loss": 12.8315,
      "step": 544
    },
    {
      "epoch": 3.601817430813713,
      "grad_norm": 72.17463684082031,
      "learning_rate": 0.000545,
      "loss": 2.9675,
      "step": 545
    },
    {
      "epoch": 3.6084262701363072,
      "grad_norm": 417.6143798828125,
      "learning_rate": 0.000546,
      "loss": 16.0915,
      "step": 546
    },
    {
      "epoch": 3.6150351094589013,
      "grad_norm": 139.8954620361328,
      "learning_rate": 0.0005470000000000001,
      "loss": 5.4966,
      "step": 547
    },
    {
      "epoch": 3.6216439487814953,
      "grad_norm": 70.69461822509766,
      "learning_rate": 0.0005480000000000001,
      "loss": 2.1586,
      "step": 548
    },
    {
      "epoch": 3.6282527881040894,
      "grad_norm": 51.97860336303711,
      "learning_rate": 0.000549,
      "loss": 2.0358,
      "step": 549
    },
    {
      "epoch": 3.634861627426683,
      "grad_norm": 19.365468978881836,
      "learning_rate": 0.00055,
      "loss": 4.9833,
      "step": 550
    },
    {
      "epoch": 3.6414704667492774,
      "grad_norm": 70.98758697509766,
      "learning_rate": 0.0005510000000000001,
      "loss": 4.2943,
      "step": 551
    },
    {
      "epoch": 3.648079306071871,
      "grad_norm": 65.9694595336914,
      "learning_rate": 0.0005520000000000001,
      "loss": 6.8942,
      "step": 552
    },
    {
      "epoch": 3.654688145394465,
      "grad_norm": 80.6600570678711,
      "learning_rate": 0.0005530000000000001,
      "loss": 2.7871,
      "step": 553
    },
    {
      "epoch": 3.661296984717059,
      "grad_norm": 23.076154708862305,
      "learning_rate": 0.000554,
      "loss": 4.2353,
      "step": 554
    },
    {
      "epoch": 3.667905824039653,
      "grad_norm": 17.900928497314453,
      "learning_rate": 0.000555,
      "loss": 2.7952,
      "step": 555
    },
    {
      "epoch": 3.674514663362247,
      "grad_norm": 24.76361846923828,
      "learning_rate": 0.0005560000000000001,
      "loss": 1.8341,
      "step": 556
    },
    {
      "epoch": 3.681123502684841,
      "grad_norm": 30.768625259399414,
      "learning_rate": 0.0005570000000000001,
      "loss": 5.3905,
      "step": 557
    },
    {
      "epoch": 3.687732342007435,
      "grad_norm": 156.09156799316406,
      "learning_rate": 0.000558,
      "loss": 5.2897,
      "step": 558
    },
    {
      "epoch": 3.694341181330029,
      "grad_norm": 242.924072265625,
      "learning_rate": 0.000559,
      "loss": 9.3778,
      "step": 559
    },
    {
      "epoch": 3.700950020652623,
      "grad_norm": 292.10546875,
      "learning_rate": 0.0005600000000000001,
      "loss": 12.9271,
      "step": 560
    },
    {
      "epoch": 3.707558859975217,
      "grad_norm": 217.34278869628906,
      "learning_rate": 0.0005610000000000001,
      "loss": 7.7444,
      "step": 561
    },
    {
      "epoch": 3.714167699297811,
      "grad_norm": 138.3389434814453,
      "learning_rate": 0.0005620000000000001,
      "loss": 6.3613,
      "step": 562
    },
    {
      "epoch": 3.720776538620405,
      "grad_norm": 76.14706420898438,
      "learning_rate": 0.0005629999999999999,
      "loss": 7.915,
      "step": 563
    },
    {
      "epoch": 3.7273853779429986,
      "grad_norm": 70.85309600830078,
      "learning_rate": 0.0005639999999999999,
      "loss": 4.4502,
      "step": 564
    },
    {
      "epoch": 3.7339942172655927,
      "grad_norm": 118.46989440917969,
      "learning_rate": 0.000565,
      "loss": 8.0463,
      "step": 565
    },
    {
      "epoch": 3.7406030565881867,
      "grad_norm": 17.792001724243164,
      "learning_rate": 0.000566,
      "loss": 2.8387,
      "step": 566
    },
    {
      "epoch": 3.7472118959107807,
      "grad_norm": 58.43312454223633,
      "learning_rate": 0.000567,
      "loss": 3.7704,
      "step": 567
    },
    {
      "epoch": 3.753820735233375,
      "grad_norm": 94.77266693115234,
      "learning_rate": 0.0005679999999999999,
      "loss": 5.7005,
      "step": 568
    },
    {
      "epoch": 3.7604295745559684,
      "grad_norm": 144.4183349609375,
      "learning_rate": 0.000569,
      "loss": 4.8747,
      "step": 569
    },
    {
      "epoch": 3.7670384138785624,
      "grad_norm": 156.2067108154297,
      "learning_rate": 0.00057,
      "loss": 7.7842,
      "step": 570
    },
    {
      "epoch": 3.7736472532011565,
      "grad_norm": 264.778564453125,
      "learning_rate": 0.000571,
      "loss": 14.9263,
      "step": 571
    },
    {
      "epoch": 3.7802560925237505,
      "grad_norm": 59.54915237426758,
      "learning_rate": 0.0005719999999999999,
      "loss": 2.7601,
      "step": 572
    },
    {
      "epoch": 3.7868649318463445,
      "grad_norm": 105.49241638183594,
      "learning_rate": 0.0005729999999999999,
      "loss": 2.2744,
      "step": 573
    },
    {
      "epoch": 3.7934737711689386,
      "grad_norm": 94.48561096191406,
      "learning_rate": 0.000574,
      "loss": 4.104,
      "step": 574
    },
    {
      "epoch": 3.8000826104915326,
      "grad_norm": 138.53880310058594,
      "learning_rate": 0.000575,
      "loss": 4.4683,
      "step": 575
    },
    {
      "epoch": 3.806691449814126,
      "grad_norm": 36.241641998291016,
      "learning_rate": 0.000576,
      "loss": 4.9388,
      "step": 576
    },
    {
      "epoch": 3.8133002891367203,
      "grad_norm": 43.94838333129883,
      "learning_rate": 0.0005769999999999999,
      "loss": 2.8988,
      "step": 577
    },
    {
      "epoch": 3.8199091284593143,
      "grad_norm": 119.23477935791016,
      "learning_rate": 0.000578,
      "loss": 3.546,
      "step": 578
    },
    {
      "epoch": 3.8265179677819083,
      "grad_norm": 81.46861267089844,
      "learning_rate": 0.000579,
      "loss": 6.5516,
      "step": 579
    },
    {
      "epoch": 3.8331268071045024,
      "grad_norm": 143.0000457763672,
      "learning_rate": 0.00058,
      "loss": 6.1571,
      "step": 580
    },
    {
      "epoch": 3.839735646427096,
      "grad_norm": 69.97425079345703,
      "learning_rate": 0.0005809999999999999,
      "loss": 3.3476,
      "step": 581
    },
    {
      "epoch": 3.8463444857496905,
      "grad_norm": 22.972858428955078,
      "learning_rate": 0.0005819999999999999,
      "loss": 4.9063,
      "step": 582
    },
    {
      "epoch": 3.852953325072284,
      "grad_norm": 70.56771850585938,
      "learning_rate": 0.000583,
      "loss": 4.8632,
      "step": 583
    },
    {
      "epoch": 3.859562164394878,
      "grad_norm": 201.64248657226562,
      "learning_rate": 0.000584,
      "loss": 6.9225,
      "step": 584
    },
    {
      "epoch": 3.866171003717472,
      "grad_norm": 159.19593811035156,
      "learning_rate": 0.000585,
      "loss": 7.119,
      "step": 585
    },
    {
      "epoch": 3.872779843040066,
      "grad_norm": 49.43452835083008,
      "learning_rate": 0.0005859999999999999,
      "loss": 3.314,
      "step": 586
    },
    {
      "epoch": 3.87938868236266,
      "grad_norm": 10.92116928100586,
      "learning_rate": 0.000587,
      "loss": 4.427,
      "step": 587
    },
    {
      "epoch": 3.885997521685254,
      "grad_norm": 128.23756408691406,
      "learning_rate": 0.000588,
      "loss": 7.7469,
      "step": 588
    },
    {
      "epoch": 3.892606361007848,
      "grad_norm": 176.22511291503906,
      "learning_rate": 0.000589,
      "loss": 8.7941,
      "step": 589
    },
    {
      "epoch": 3.899215200330442,
      "grad_norm": 94.97054290771484,
      "learning_rate": 0.00059,
      "loss": 5.6818,
      "step": 590
    },
    {
      "epoch": 3.905824039653036,
      "grad_norm": 25.773460388183594,
      "learning_rate": 0.0005909999999999999,
      "loss": 2.9647,
      "step": 591
    },
    {
      "epoch": 3.91243287897563,
      "grad_norm": 30.056182861328125,
      "learning_rate": 0.000592,
      "loss": 2.4772,
      "step": 592
    },
    {
      "epoch": 3.919041718298224,
      "grad_norm": 145.05128479003906,
      "learning_rate": 0.000593,
      "loss": 4.5438,
      "step": 593
    },
    {
      "epoch": 3.925650557620818,
      "grad_norm": 201.22227478027344,
      "learning_rate": 0.000594,
      "loss": 8.3557,
      "step": 594
    },
    {
      "epoch": 3.9322593969434116,
      "grad_norm": 165.4879150390625,
      "learning_rate": 0.0005949999999999999,
      "loss": 8.6348,
      "step": 595
    },
    {
      "epoch": 3.9388682362660057,
      "grad_norm": 176.82998657226562,
      "learning_rate": 0.000596,
      "loss": 7.6403,
      "step": 596
    },
    {
      "epoch": 3.9454770755885997,
      "grad_norm": 159.39622497558594,
      "learning_rate": 0.000597,
      "loss": 6.7296,
      "step": 597
    },
    {
      "epoch": 3.9520859149111938,
      "grad_norm": 33.55745315551758,
      "learning_rate": 0.000598,
      "loss": 7.6431,
      "step": 598
    },
    {
      "epoch": 3.958694754233788,
      "grad_norm": 118.51714324951172,
      "learning_rate": 0.000599,
      "loss": 5.3946,
      "step": 599
    },
    {
      "epoch": 3.9653035935563814,
      "grad_norm": 228.8177947998047,
      "learning_rate": 0.0006,
      "loss": 12.6294,
      "step": 600
    },
    {
      "epoch": 3.971912432878976,
      "grad_norm": 238.1396942138672,
      "learning_rate": 0.000601,
      "loss": 13.2203,
      "step": 601
    },
    {
      "epoch": 3.9785212722015695,
      "grad_norm": 215.7576904296875,
      "learning_rate": 0.000602,
      "loss": 8.4773,
      "step": 602
    },
    {
      "epoch": 3.9851301115241635,
      "grad_norm": 162.0133056640625,
      "learning_rate": 0.000603,
      "loss": 8.3297,
      "step": 603
    },
    {
      "epoch": 3.9917389508467576,
      "grad_norm": 242.44830322265625,
      "learning_rate": 0.000604,
      "loss": 9.5079,
      "step": 604
    },
    {
      "epoch": 3.9983477901693516,
      "grad_norm": 348.6841125488281,
      "learning_rate": 0.000605,
      "loss": 17.6866,
      "step": 605
    },
    {
      "epoch": 3.9983477901693516,
      "eval_validation_error_bar": 0.06251586921764736,
      "eval_validation_loss": 11.145903587341309,
      "eval_validation_pearsonr": 0.4154640579759321,
      "eval_validation_rmse": 3.338548183441162,
      "eval_validation_runtime": 33.7083,
      "eval_validation_samples_per_second": 6.022,
      "eval_validation_spearman": 0.3829091952389881,
      "eval_validation_steps_per_second": 6.022,
      "step": 605
    },
    {
      "epoch": 3.9983477901693516,
      "eval_test_error_bar": 0.052279182333382114,
      "eval_test_loss": 12.19424057006836,
      "eval_test_pearsonr": 0.19042312246018508,
      "eval_test_rmse": 3.492025375366211,
      "eval_test_runtime": 40.0206,
      "eval_test_samples_per_second": 8.146,
      "eval_test_spearman": 0.2798148462853257,
      "eval_test_steps_per_second": 8.146,
      "step": 605
    },
    {
      "epoch": 4.004956629491946,
      "grad_norm": 169.8550567626953,
      "learning_rate": 0.000606,
      "loss": 12.688,
      "step": 606
    },
    {
      "epoch": 4.011565468814539,
      "grad_norm": 132.45779418945312,
      "learning_rate": 0.000607,
      "loss": 12.3416,
      "step": 607
    },
    {
      "epoch": 4.018174308137134,
      "grad_norm": 136.83680725097656,
      "learning_rate": 0.000608,
      "loss": 6.7671,
      "step": 608
    },
    {
      "epoch": 4.024783147459727,
      "grad_norm": 148.13436889648438,
      "learning_rate": 0.000609,
      "loss": 6.7643,
      "step": 609
    },
    {
      "epoch": 4.031391986782321,
      "grad_norm": 121.1900405883789,
      "learning_rate": 0.00061,
      "loss": 5.8016,
      "step": 610
    },
    {
      "epoch": 4.038000826104915,
      "grad_norm": 105.5165023803711,
      "learning_rate": 0.000611,
      "loss": 4.7946,
      "step": 611
    },
    {
      "epoch": 4.044609665427509,
      "grad_norm": 44.050514221191406,
      "learning_rate": 0.000612,
      "loss": 3.0875,
      "step": 612
    },
    {
      "epoch": 4.0512185047501035,
      "grad_norm": 52.7273063659668,
      "learning_rate": 0.000613,
      "loss": 3.9011,
      "step": 613
    },
    {
      "epoch": 4.057827344072697,
      "grad_norm": 39.813812255859375,
      "learning_rate": 0.000614,
      "loss": 2.5594,
      "step": 614
    },
    {
      "epoch": 4.064436183395292,
      "grad_norm": 35.53188705444336,
      "learning_rate": 0.000615,
      "loss": 3.7143,
      "step": 615
    },
    {
      "epoch": 4.071045022717885,
      "grad_norm": 45.77827835083008,
      "learning_rate": 0.000616,
      "loss": 3.7366,
      "step": 616
    },
    {
      "epoch": 4.077653862040479,
      "grad_norm": 111.19463348388672,
      "learning_rate": 0.000617,
      "loss": 5.1657,
      "step": 617
    },
    {
      "epoch": 4.084262701363073,
      "grad_norm": 70.19451904296875,
      "learning_rate": 0.0006180000000000001,
      "loss": 4.9516,
      "step": 618
    },
    {
      "epoch": 4.090871540685667,
      "grad_norm": 5.352163791656494,
      "learning_rate": 0.000619,
      "loss": 2.2718,
      "step": 619
    },
    {
      "epoch": 4.097480380008261,
      "grad_norm": 36.2163200378418,
      "learning_rate": 0.00062,
      "loss": 2.5812,
      "step": 620
    },
    {
      "epoch": 4.104089219330855,
      "grad_norm": 67.21466827392578,
      "learning_rate": 0.000621,
      "loss": 3.9123,
      "step": 621
    },
    {
      "epoch": 4.110698058653449,
      "grad_norm": 91.57096862792969,
      "learning_rate": 0.000622,
      "loss": 4.1928,
      "step": 622
    },
    {
      "epoch": 4.117306897976043,
      "grad_norm": 86.12800598144531,
      "learning_rate": 0.000623,
      "loss": 6.3305,
      "step": 623
    },
    {
      "epoch": 4.123915737298637,
      "grad_norm": 22.868572235107422,
      "learning_rate": 0.000624,
      "loss": 1.6054,
      "step": 624
    },
    {
      "epoch": 4.130524576621231,
      "grad_norm": 74.37866973876953,
      "learning_rate": 0.000625,
      "loss": 3.991,
      "step": 625
    },
    {
      "epoch": 4.137133415943825,
      "grad_norm": 37.17798614501953,
      "learning_rate": 0.000626,
      "loss": 3.7672,
      "step": 626
    },
    {
      "epoch": 4.143742255266419,
      "grad_norm": 66.60847473144531,
      "learning_rate": 0.0006270000000000001,
      "loss": 6.5454,
      "step": 627
    },
    {
      "epoch": 4.150351094589013,
      "grad_norm": 94.64199829101562,
      "learning_rate": 0.000628,
      "loss": 3.1904,
      "step": 628
    },
    {
      "epoch": 4.156959933911606,
      "grad_norm": 4.800579071044922,
      "learning_rate": 0.000629,
      "loss": 2.6836,
      "step": 629
    },
    {
      "epoch": 4.163568773234201,
      "grad_norm": 9.594704627990723,
      "learning_rate": 0.00063,
      "loss": 1.3205,
      "step": 630
    },
    {
      "epoch": 4.170177612556794,
      "grad_norm": 85.77218627929688,
      "learning_rate": 0.000631,
      "loss": 4.3845,
      "step": 631
    },
    {
      "epoch": 4.176786451879389,
      "grad_norm": 25.891111373901367,
      "learning_rate": 0.000632,
      "loss": 3.697,
      "step": 632
    },
    {
      "epoch": 4.1833952912019825,
      "grad_norm": 68.28105163574219,
      "learning_rate": 0.000633,
      "loss": 3.1493,
      "step": 633
    },
    {
      "epoch": 4.190004130524577,
      "grad_norm": 19.938135147094727,
      "learning_rate": 0.000634,
      "loss": 7.4321,
      "step": 634
    },
    {
      "epoch": 4.196612969847171,
      "grad_norm": 66.98948669433594,
      "learning_rate": 0.000635,
      "loss": 4.8747,
      "step": 635
    },
    {
      "epoch": 4.203221809169764,
      "grad_norm": 27.174196243286133,
      "learning_rate": 0.0006360000000000001,
      "loss": 4.4332,
      "step": 636
    },
    {
      "epoch": 4.209830648492359,
      "grad_norm": 35.35761260986328,
      "learning_rate": 0.000637,
      "loss": 3.5509,
      "step": 637
    },
    {
      "epoch": 4.216439487814952,
      "grad_norm": 7.135176181793213,
      "learning_rate": 0.000638,
      "loss": 3.4551,
      "step": 638
    },
    {
      "epoch": 4.223048327137547,
      "grad_norm": 76.90605163574219,
      "learning_rate": 0.000639,
      "loss": 4.0529,
      "step": 639
    },
    {
      "epoch": 4.22965716646014,
      "grad_norm": 107.00155639648438,
      "learning_rate": 0.00064,
      "loss": 6.4477,
      "step": 640
    },
    {
      "epoch": 4.236266005782735,
      "grad_norm": 108.70310974121094,
      "learning_rate": 0.0006410000000000001,
      "loss": 5.3895,
      "step": 641
    },
    {
      "epoch": 4.242874845105328,
      "grad_norm": 64.60545349121094,
      "learning_rate": 0.000642,
      "loss": 5.2547,
      "step": 642
    },
    {
      "epoch": 4.249483684427922,
      "grad_norm": 42.110450744628906,
      "learning_rate": 0.000643,
      "loss": 2.6227,
      "step": 643
    },
    {
      "epoch": 4.2560925237505165,
      "grad_norm": 152.13629150390625,
      "learning_rate": 0.000644,
      "loss": 4.1498,
      "step": 644
    },
    {
      "epoch": 4.26270136307311,
      "grad_norm": 206.80328369140625,
      "learning_rate": 0.0006450000000000001,
      "loss": 9.0792,
      "step": 645
    },
    {
      "epoch": 4.269310202395705,
      "grad_norm": 154.04771423339844,
      "learning_rate": 0.000646,
      "loss": 5.1883,
      "step": 646
    },
    {
      "epoch": 4.275919041718298,
      "grad_norm": 141.4829864501953,
      "learning_rate": 0.000647,
      "loss": 4.0477,
      "step": 647
    },
    {
      "epoch": 4.282527881040892,
      "grad_norm": 129.86634826660156,
      "learning_rate": 0.000648,
      "loss": 6.6322,
      "step": 648
    },
    {
      "epoch": 4.289136720363486,
      "grad_norm": 119.79125213623047,
      "learning_rate": 0.0006490000000000001,
      "loss": 5.2087,
      "step": 649
    },
    {
      "epoch": 4.29574555968608,
      "grad_norm": 230.42213439941406,
      "learning_rate": 0.0006500000000000001,
      "loss": 10.2645,
      "step": 650
    },
    {
      "epoch": 4.302354399008674,
      "grad_norm": 62.98639678955078,
      "learning_rate": 0.000651,
      "loss": 3.8658,
      "step": 651
    },
    {
      "epoch": 4.308963238331268,
      "grad_norm": 10.57579231262207,
      "learning_rate": 0.000652,
      "loss": 6.5985,
      "step": 652
    },
    {
      "epoch": 4.315572077653862,
      "grad_norm": 22.011993408203125,
      "learning_rate": 0.000653,
      "loss": 2.1449,
      "step": 653
    },
    {
      "epoch": 4.322180916976456,
      "grad_norm": 99.31817626953125,
      "learning_rate": 0.0006540000000000001,
      "loss": 6.9,
      "step": 654
    },
    {
      "epoch": 4.32878975629905,
      "grad_norm": 16.928613662719727,
      "learning_rate": 0.0006550000000000001,
      "loss": 2.1196,
      "step": 655
    },
    {
      "epoch": 4.335398595621644,
      "grad_norm": 76.56813049316406,
      "learning_rate": 0.000656,
      "loss": 4.9979,
      "step": 656
    },
    {
      "epoch": 4.342007434944238,
      "grad_norm": 131.5786590576172,
      "learning_rate": 0.000657,
      "loss": 5.1514,
      "step": 657
    },
    {
      "epoch": 4.348616274266832,
      "grad_norm": 32.636104583740234,
      "learning_rate": 0.0006580000000000001,
      "loss": 2.2024,
      "step": 658
    },
    {
      "epoch": 4.355225113589426,
      "grad_norm": 105.40153503417969,
      "learning_rate": 0.0006590000000000001,
      "loss": 5.3507,
      "step": 659
    },
    {
      "epoch": 4.36183395291202,
      "grad_norm": 26.88259506225586,
      "learning_rate": 0.00066,
      "loss": 1.7862,
      "step": 660
    },
    {
      "epoch": 4.368442792234614,
      "grad_norm": 38.70640182495117,
      "learning_rate": 0.000661,
      "loss": 5.5862,
      "step": 661
    },
    {
      "epoch": 4.375051631557207,
      "grad_norm": 77.35487365722656,
      "learning_rate": 0.000662,
      "loss": 4.1527,
      "step": 662
    },
    {
      "epoch": 4.381660470879802,
      "grad_norm": 67.5771255493164,
      "learning_rate": 0.0006630000000000001,
      "loss": 7.9705,
      "step": 663
    },
    {
      "epoch": 4.3882693102023955,
      "grad_norm": 10.011418342590332,
      "learning_rate": 0.0006640000000000001,
      "loss": 2.8537,
      "step": 664
    },
    {
      "epoch": 4.39487814952499,
      "grad_norm": 81.74862670898438,
      "learning_rate": 0.000665,
      "loss": 3.8593,
      "step": 665
    },
    {
      "epoch": 4.401486988847584,
      "grad_norm": 138.3758544921875,
      "learning_rate": 0.000666,
      "loss": 3.3285,
      "step": 666
    },
    {
      "epoch": 4.408095828170177,
      "grad_norm": 73.56365966796875,
      "learning_rate": 0.0006670000000000001,
      "loss": 6.9967,
      "step": 667
    },
    {
      "epoch": 4.414704667492772,
      "grad_norm": 3.1704583168029785,
      "learning_rate": 0.0006680000000000001,
      "loss": 3.6759,
      "step": 668
    },
    {
      "epoch": 4.421313506815365,
      "grad_norm": 64.39422607421875,
      "learning_rate": 0.0006690000000000001,
      "loss": 2.6068,
      "step": 669
    },
    {
      "epoch": 4.42792234613796,
      "grad_norm": 38.11708450317383,
      "learning_rate": 0.00067,
      "loss": 3.9887,
      "step": 670
    },
    {
      "epoch": 4.434531185460553,
      "grad_norm": 59.208953857421875,
      "learning_rate": 0.000671,
      "loss": 9.6339,
      "step": 671
    },
    {
      "epoch": 4.441140024783148,
      "grad_norm": 14.121850967407227,
      "learning_rate": 0.0006720000000000001,
      "loss": 2.4253,
      "step": 672
    },
    {
      "epoch": 4.447748864105741,
      "grad_norm": 14.258886337280273,
      "learning_rate": 0.0006730000000000001,
      "loss": 5.019,
      "step": 673
    },
    {
      "epoch": 4.454357703428335,
      "grad_norm": 47.38711929321289,
      "learning_rate": 0.000674,
      "loss": 2.4215,
      "step": 674
    },
    {
      "epoch": 4.4609665427509295,
      "grad_norm": 42.85325622558594,
      "learning_rate": 0.000675,
      "loss": 5.2201,
      "step": 675
    },
    {
      "epoch": 4.467575382073523,
      "grad_norm": 45.959110260009766,
      "learning_rate": 0.0006760000000000001,
      "loss": 1.2105,
      "step": 676
    },
    {
      "epoch": 4.474184221396118,
      "grad_norm": 32.79924774169922,
      "learning_rate": 0.0006770000000000001,
      "loss": 3.3228,
      "step": 677
    },
    {
      "epoch": 4.480793060718711,
      "grad_norm": 26.52910614013672,
      "learning_rate": 0.0006780000000000001,
      "loss": 3.3551,
      "step": 678
    },
    {
      "epoch": 4.487401900041306,
      "grad_norm": 50.762977600097656,
      "learning_rate": 0.000679,
      "loss": 2.4706,
      "step": 679
    },
    {
      "epoch": 4.494010739363899,
      "grad_norm": 34.016109466552734,
      "learning_rate": 0.00068,
      "loss": 3.1583,
      "step": 680
    },
    {
      "epoch": 4.500619578686493,
      "grad_norm": 50.580589294433594,
      "learning_rate": 0.0006810000000000001,
      "loss": 4.3308,
      "step": 681
    },
    {
      "epoch": 4.507228418009087,
      "grad_norm": 34.70700454711914,
      "learning_rate": 0.0006820000000000001,
      "loss": 5.4627,
      "step": 682
    },
    {
      "epoch": 4.513837257331681,
      "grad_norm": 235.51531982421875,
      "learning_rate": 0.000683,
      "loss": 8.7994,
      "step": 683
    },
    {
      "epoch": 4.520446096654275,
      "grad_norm": 127.13734436035156,
      "learning_rate": 0.000684,
      "loss": 4.4541,
      "step": 684
    },
    {
      "epoch": 4.527054935976869,
      "grad_norm": 118.96380615234375,
      "learning_rate": 0.0006850000000000001,
      "loss": 3.8868,
      "step": 685
    },
    {
      "epoch": 4.533663775299463,
      "grad_norm": 27.02745819091797,
      "learning_rate": 0.0006860000000000001,
      "loss": 2.317,
      "step": 686
    },
    {
      "epoch": 4.540272614622057,
      "grad_norm": 28.021438598632812,
      "learning_rate": 0.0006870000000000001,
      "loss": 6.8822,
      "step": 687
    },
    {
      "epoch": 4.546881453944651,
      "grad_norm": 52.27419662475586,
      "learning_rate": 0.0006879999999999999,
      "loss": 5.3352,
      "step": 688
    },
    {
      "epoch": 4.553490293267245,
      "grad_norm": 15.782506942749023,
      "learning_rate": 0.0006889999999999999,
      "loss": 3.3356,
      "step": 689
    },
    {
      "epoch": 4.560099132589839,
      "grad_norm": 12.874760627746582,
      "learning_rate": 0.00069,
      "loss": 5.4751,
      "step": 690
    },
    {
      "epoch": 4.566707971912432,
      "grad_norm": 44.8868293762207,
      "learning_rate": 0.000691,
      "loss": 3.0414,
      "step": 691
    },
    {
      "epoch": 4.573316811235027,
      "grad_norm": 57.30372619628906,
      "learning_rate": 0.000692,
      "loss": 5.7561,
      "step": 692
    },
    {
      "epoch": 4.5799256505576205,
      "grad_norm": 132.5127410888672,
      "learning_rate": 0.0006929999999999999,
      "loss": 11.3328,
      "step": 693
    },
    {
      "epoch": 4.586534489880215,
      "grad_norm": 99.17439270019531,
      "learning_rate": 0.000694,
      "loss": 4.236,
      "step": 694
    },
    {
      "epoch": 4.5931433292028085,
      "grad_norm": 18.36679458618164,
      "learning_rate": 0.000695,
      "loss": 3.1611,
      "step": 695
    },
    {
      "epoch": 4.599752168525403,
      "grad_norm": 4.366281986236572,
      "learning_rate": 0.000696,
      "loss": 4.618,
      "step": 696
    },
    {
      "epoch": 4.606361007847997,
      "grad_norm": 100.95568084716797,
      "learning_rate": 0.0006969999999999999,
      "loss": 4.7722,
      "step": 697
    },
    {
      "epoch": 4.612969847170591,
      "grad_norm": 58.957035064697266,
      "learning_rate": 0.0006979999999999999,
      "loss": 4.4071,
      "step": 698
    },
    {
      "epoch": 4.619578686493185,
      "grad_norm": 21.44277572631836,
      "learning_rate": 0.000699,
      "loss": 5.0169,
      "step": 699
    },
    {
      "epoch": 4.626187525815778,
      "grad_norm": 67.74970245361328,
      "learning_rate": 0.0007,
      "loss": 2.9362,
      "step": 700
    },
    {
      "epoch": 4.632796365138373,
      "grad_norm": 179.6505126953125,
      "learning_rate": 0.000701,
      "loss": 8.3156,
      "step": 701
    },
    {
      "epoch": 4.639405204460966,
      "grad_norm": 84.5019302368164,
      "learning_rate": 0.0007019999999999999,
      "loss": 2.7302,
      "step": 702
    },
    {
      "epoch": 4.646014043783561,
      "grad_norm": 82.19628143310547,
      "learning_rate": 0.000703,
      "loss": 4.7201,
      "step": 703
    },
    {
      "epoch": 4.6526228831061545,
      "grad_norm": 133.8188018798828,
      "learning_rate": 0.000704,
      "loss": 4.7535,
      "step": 704
    },
    {
      "epoch": 4.659231722428748,
      "grad_norm": 4.6874566078186035,
      "learning_rate": 0.000705,
      "loss": 4.0548,
      "step": 705
    },
    {
      "epoch": 4.6658405617513425,
      "grad_norm": 56.797149658203125,
      "learning_rate": 0.0007059999999999999,
      "loss": 2.1626,
      "step": 706
    },
    {
      "epoch": 4.672449401073936,
      "grad_norm": 111.69780731201172,
      "learning_rate": 0.000707,
      "loss": 4.406,
      "step": 707
    },
    {
      "epoch": 4.679058240396531,
      "grad_norm": 78.45220184326172,
      "learning_rate": 0.000708,
      "loss": 5.8738,
      "step": 708
    },
    {
      "epoch": 4.685667079719124,
      "grad_norm": 5.4850544929504395,
      "learning_rate": 0.000709,
      "loss": 2.4479,
      "step": 709
    },
    {
      "epoch": 4.692275919041718,
      "grad_norm": 87.23101806640625,
      "learning_rate": 0.00071,
      "loss": 4.0007,
      "step": 710
    },
    {
      "epoch": 4.698884758364312,
      "grad_norm": 118.24637603759766,
      "learning_rate": 0.0007109999999999999,
      "loss": 5.8697,
      "step": 711
    },
    {
      "epoch": 4.705493597686906,
      "grad_norm": 70.20880889892578,
      "learning_rate": 0.000712,
      "loss": 2.491,
      "step": 712
    },
    {
      "epoch": 4.7121024370095,
      "grad_norm": 122.7666015625,
      "learning_rate": 0.000713,
      "loss": 7.2354,
      "step": 713
    },
    {
      "epoch": 4.718711276332094,
      "grad_norm": 120.55083465576172,
      "learning_rate": 0.000714,
      "loss": 7.0599,
      "step": 714
    },
    {
      "epoch": 4.7253201156546885,
      "grad_norm": 75.74674224853516,
      "learning_rate": 0.000715,
      "loss": 9.9723,
      "step": 715
    },
    {
      "epoch": 4.731928954977282,
      "grad_norm": 39.32014846801758,
      "learning_rate": 0.000716,
      "loss": 9.6791,
      "step": 716
    },
    {
      "epoch": 4.7385377942998765,
      "grad_norm": 66.1412124633789,
      "learning_rate": 0.000717,
      "loss": 4.3606,
      "step": 717
    },
    {
      "epoch": 4.74514663362247,
      "grad_norm": 17.643098831176758,
      "learning_rate": 0.000718,
      "loss": 4.8395,
      "step": 718
    },
    {
      "epoch": 4.751755472945064,
      "grad_norm": 61.705894470214844,
      "learning_rate": 0.000719,
      "loss": 7.3376,
      "step": 719
    },
    {
      "epoch": 4.758364312267658,
      "grad_norm": 48.09379196166992,
      "learning_rate": 0.0007199999999999999,
      "loss": 5.036,
      "step": 720
    },
    {
      "epoch": 4.764973151590252,
      "grad_norm": 8.22140884399414,
      "learning_rate": 0.000721,
      "loss": 7.6393,
      "step": 721
    },
    {
      "epoch": 4.771581990912846,
      "grad_norm": 60.72739791870117,
      "learning_rate": 0.000722,
      "loss": 4.2674,
      "step": 722
    },
    {
      "epoch": 4.77819083023544,
      "grad_norm": 16.509294509887695,
      "learning_rate": 0.000723,
      "loss": 4.8831,
      "step": 723
    },
    {
      "epoch": 4.7847996695580335,
      "grad_norm": 14.066762924194336,
      "learning_rate": 0.000724,
      "loss": 3.8101,
      "step": 724
    },
    {
      "epoch": 4.791408508880628,
      "grad_norm": 23.470691680908203,
      "learning_rate": 0.000725,
      "loss": 6.1457,
      "step": 725
    },
    {
      "epoch": 4.798017348203222,
      "grad_norm": 62.42998504638672,
      "learning_rate": 0.000726,
      "loss": 9.3841,
      "step": 726
    },
    {
      "epoch": 4.804626187525816,
      "grad_norm": 102.61141967773438,
      "learning_rate": 0.000727,
      "loss": 3.263,
      "step": 727
    },
    {
      "epoch": 4.81123502684841,
      "grad_norm": 45.45171356201172,
      "learning_rate": 0.000728,
      "loss": 8.2113,
      "step": 728
    },
    {
      "epoch": 4.817843866171003,
      "grad_norm": 110.03903198242188,
      "learning_rate": 0.000729,
      "loss": 3.936,
      "step": 729
    },
    {
      "epoch": 4.824452705493598,
      "grad_norm": 66.77473449707031,
      "learning_rate": 0.00073,
      "loss": 3.2108,
      "step": 730
    },
    {
      "epoch": 4.831061544816191,
      "grad_norm": 7.905325889587402,
      "learning_rate": 0.000731,
      "loss": 5.3718,
      "step": 731
    },
    {
      "epoch": 4.837670384138786,
      "grad_norm": 31.26406478881836,
      "learning_rate": 0.000732,
      "loss": 5.2251,
      "step": 732
    },
    {
      "epoch": 4.844279223461379,
      "grad_norm": 222.5835723876953,
      "learning_rate": 0.000733,
      "loss": 9.4959,
      "step": 733
    },
    {
      "epoch": 4.850888062783974,
      "grad_norm": 158.5899200439453,
      "learning_rate": 0.000734,
      "loss": 7.4591,
      "step": 734
    },
    {
      "epoch": 4.8574969021065675,
      "grad_norm": 180.31564331054688,
      "learning_rate": 0.000735,
      "loss": 8.0055,
      "step": 735
    },
    {
      "epoch": 4.864105741429162,
      "grad_norm": 175.7733154296875,
      "learning_rate": 0.000736,
      "loss": 7.4309,
      "step": 736
    },
    {
      "epoch": 4.870714580751756,
      "grad_norm": 60.93629837036133,
      "learning_rate": 0.000737,
      "loss": 4.5354,
      "step": 737
    },
    {
      "epoch": 4.877323420074349,
      "grad_norm": 27.154781341552734,
      "learning_rate": 0.000738,
      "loss": 5.8431,
      "step": 738
    },
    {
      "epoch": 4.883932259396944,
      "grad_norm": 141.7852325439453,
      "learning_rate": 0.000739,
      "loss": 4.9476,
      "step": 739
    },
    {
      "epoch": 4.890541098719537,
      "grad_norm": 167.98963928222656,
      "learning_rate": 0.00074,
      "loss": 5.747,
      "step": 740
    },
    {
      "epoch": 4.897149938042132,
      "grad_norm": 151.14793395996094,
      "learning_rate": 0.000741,
      "loss": 6.3488,
      "step": 741
    },
    {
      "epoch": 4.903758777364725,
      "grad_norm": 103.57836151123047,
      "learning_rate": 0.000742,
      "loss": 3.6751,
      "step": 742
    },
    {
      "epoch": 4.910367616687319,
      "grad_norm": 9.898557662963867,
      "learning_rate": 0.0007430000000000001,
      "loss": 2.8307,
      "step": 743
    },
    {
      "epoch": 4.916976456009913,
      "grad_norm": 144.61029052734375,
      "learning_rate": 0.000744,
      "loss": 5.5814,
      "step": 744
    },
    {
      "epoch": 4.923585295332507,
      "grad_norm": 14.347565650939941,
      "learning_rate": 0.000745,
      "loss": 3.1214,
      "step": 745
    },
    {
      "epoch": 4.9301941346551015,
      "grad_norm": 72.88663482666016,
      "learning_rate": 0.000746,
      "loss": 2.8719,
      "step": 746
    },
    {
      "epoch": 4.936802973977695,
      "grad_norm": 90.24765014648438,
      "learning_rate": 0.000747,
      "loss": 2.8248,
      "step": 747
    },
    {
      "epoch": 4.943411813300289,
      "grad_norm": 7.002901554107666,
      "learning_rate": 0.000748,
      "loss": 1.5543,
      "step": 748
    },
    {
      "epoch": 4.950020652622883,
      "grad_norm": 7.172297477722168,
      "learning_rate": 0.000749,
      "loss": 3.8748,
      "step": 749
    },
    {
      "epoch": 4.956629491945477,
      "grad_norm": 9.542627334594727,
      "learning_rate": 0.00075,
      "loss": 4.7548,
      "step": 750
    },
    {
      "epoch": 4.963238331268071,
      "grad_norm": 30.06089973449707,
      "learning_rate": 0.000751,
      "loss": 7.9113,
      "step": 751
    },
    {
      "epoch": 4.969847170590665,
      "grad_norm": 88.9833755493164,
      "learning_rate": 0.0007520000000000001,
      "loss": 6.0482,
      "step": 752
    },
    {
      "epoch": 4.976456009913259,
      "grad_norm": 90.21887969970703,
      "learning_rate": 0.000753,
      "loss": 5.6518,
      "step": 753
    },
    {
      "epoch": 4.983064849235853,
      "grad_norm": 30.940303802490234,
      "learning_rate": 0.000754,
      "loss": 3.5168,
      "step": 754
    },
    {
      "epoch": 4.989673688558447,
      "grad_norm": 55.54535675048828,
      "learning_rate": 0.000755,
      "loss": 3.794,
      "step": 755
    },
    {
      "epoch": 4.996282527881041,
      "grad_norm": 75.97532653808594,
      "learning_rate": 0.000756,
      "loss": 4.5244,
      "step": 756
    },
    {
      "epoch": 4.996282527881041,
      "eval_validation_error_bar": 0.05863034782691053,
      "eval_validation_loss": 8.679930686950684,
      "eval_validation_pearsonr": 0.4845754664820027,
      "eval_validation_rmse": 2.9461722373962402,
      "eval_validation_runtime": 33.4632,
      "eval_validation_samples_per_second": 6.066,
      "eval_validation_spearman": 0.4598737627332082,
      "eval_validation_steps_per_second": 6.066,
      "step": 756
    },
    {
      "epoch": 4.996282527881041,
      "eval_test_error_bar": 0.0545314811048857,
      "eval_test_loss": 13.785557746887207,
      "eval_test_pearsonr": 0.0953901205580283,
      "eval_test_rmse": 3.712890863418579,
      "eval_test_runtime": 41.2346,
      "eval_test_samples_per_second": 7.906,
      "eval_test_spearman": 0.16229543373318533,
      "eval_test_steps_per_second": 7.906,
      "step": 756
    },
    {
      "epoch": 5.002891367203635,
      "grad_norm": 58.37813949584961,
      "learning_rate": 0.000757,
      "loss": 4.3287,
      "step": 757
    },
    {
      "epoch": 5.009500206526229,
      "grad_norm": 22.76296043395996,
      "learning_rate": 0.000758,
      "loss": 4.3656,
      "step": 758
    },
    {
      "epoch": 5.016109045848823,
      "grad_norm": 19.58133888244629,
      "learning_rate": 0.000759,
      "loss": 3.2177,
      "step": 759
    },
    {
      "epoch": 5.022717885171417,
      "grad_norm": 31.014081954956055,
      "learning_rate": 0.00076,
      "loss": 2.7079,
      "step": 760
    },
    {
      "epoch": 5.029326724494011,
      "grad_norm": 28.958879470825195,
      "learning_rate": 0.0007610000000000001,
      "loss": 4.2875,
      "step": 761
    },
    {
      "epoch": 5.035935563816604,
      "grad_norm": 6.136753082275391,
      "learning_rate": 0.000762,
      "loss": 2.7304,
      "step": 762
    },
    {
      "epoch": 5.042544403139199,
      "grad_norm": 16.592817306518555,
      "learning_rate": 0.000763,
      "loss": 3.1736,
      "step": 763
    },
    {
      "epoch": 5.049153242461792,
      "grad_norm": 92.55804443359375,
      "learning_rate": 0.000764,
      "loss": 3.3387,
      "step": 764
    },
    {
      "epoch": 5.055762081784387,
      "grad_norm": 31.49854278564453,
      "learning_rate": 0.0007650000000000001,
      "loss": 3.4668,
      "step": 765
    },
    {
      "epoch": 5.0623709211069805,
      "grad_norm": 48.59599304199219,
      "learning_rate": 0.0007660000000000001,
      "loss": 4.6122,
      "step": 766
    },
    {
      "epoch": 5.068979760429575,
      "grad_norm": 43.629276275634766,
      "learning_rate": 0.000767,
      "loss": 5.1339,
      "step": 767
    },
    {
      "epoch": 5.075588599752169,
      "grad_norm": 91.24675750732422,
      "learning_rate": 0.000768,
      "loss": 4.4173,
      "step": 768
    },
    {
      "epoch": 5.082197439074762,
      "grad_norm": 8.51315975189209,
      "learning_rate": 0.000769,
      "loss": 3.2196,
      "step": 769
    },
    {
      "epoch": 5.088806278397357,
      "grad_norm": 8.099005699157715,
      "learning_rate": 0.0007700000000000001,
      "loss": 3.2273,
      "step": 770
    },
    {
      "epoch": 5.09541511771995,
      "grad_norm": 61.04855728149414,
      "learning_rate": 0.000771,
      "loss": 3.2086,
      "step": 771
    },
    {
      "epoch": 5.102023957042545,
      "grad_norm": 10.354486465454102,
      "learning_rate": 0.000772,
      "loss": 1.7274,
      "step": 772
    },
    {
      "epoch": 5.108632796365138,
      "grad_norm": 180.79930114746094,
      "learning_rate": 0.000773,
      "loss": 7.859,
      "step": 773
    },
    {
      "epoch": 5.115241635687732,
      "grad_norm": 189.5688934326172,
      "learning_rate": 0.0007740000000000001,
      "loss": 9.4287,
      "step": 774
    },
    {
      "epoch": 5.121850475010326,
      "grad_norm": 104.1963119506836,
      "learning_rate": 0.0007750000000000001,
      "loss": 8.1769,
      "step": 775
    },
    {
      "epoch": 5.12845931433292,
      "grad_norm": 139.47901916503906,
      "learning_rate": 0.000776,
      "loss": 8.4313,
      "step": 776
    },
    {
      "epoch": 5.1350681536555145,
      "grad_norm": 23.9520263671875,
      "learning_rate": 0.000777,
      "loss": 2.6133,
      "step": 777
    },
    {
      "epoch": 5.141676992978108,
      "grad_norm": 55.045372009277344,
      "learning_rate": 0.000778,
      "loss": 3.9742,
      "step": 778
    },
    {
      "epoch": 5.148285832300703,
      "grad_norm": 157.6604766845703,
      "learning_rate": 0.0007790000000000001,
      "loss": 5.7401,
      "step": 779
    },
    {
      "epoch": 5.154894671623296,
      "grad_norm": 148.02182006835938,
      "learning_rate": 0.0007800000000000001,
      "loss": 6.4821,
      "step": 780
    },
    {
      "epoch": 5.16150351094589,
      "grad_norm": 72.6662368774414,
      "learning_rate": 0.000781,
      "loss": 2.9165,
      "step": 781
    },
    {
      "epoch": 5.168112350268484,
      "grad_norm": 27.109825134277344,
      "learning_rate": 0.000782,
      "loss": 2.4704,
      "step": 782
    },
    {
      "epoch": 5.174721189591078,
      "grad_norm": 5.786407470703125,
      "learning_rate": 0.0007830000000000001,
      "loss": 2.8799,
      "step": 783
    },
    {
      "epoch": 5.181330028913672,
      "grad_norm": 8.622081756591797,
      "learning_rate": 0.0007840000000000001,
      "loss": 3.786,
      "step": 784
    },
    {
      "epoch": 5.187938868236266,
      "grad_norm": 145.38153076171875,
      "learning_rate": 0.000785,
      "loss": 4.7834,
      "step": 785
    },
    {
      "epoch": 5.1945477075588595,
      "grad_norm": 28.272457122802734,
      "learning_rate": 0.000786,
      "loss": 4.3778,
      "step": 786
    },
    {
      "epoch": 5.201156546881454,
      "grad_norm": 25.84263038635254,
      "learning_rate": 0.000787,
      "loss": 9.5873,
      "step": 787
    },
    {
      "epoch": 5.207765386204048,
      "grad_norm": 196.46231079101562,
      "learning_rate": 0.0007880000000000001,
      "loss": 8.7127,
      "step": 788
    },
    {
      "epoch": 5.214374225526642,
      "grad_norm": 189.74945068359375,
      "learning_rate": 0.0007890000000000001,
      "loss": 8.7172,
      "step": 789
    },
    {
      "epoch": 5.220983064849236,
      "grad_norm": 161.2541961669922,
      "learning_rate": 0.00079,
      "loss": 6.3755,
      "step": 790
    },
    {
      "epoch": 5.22759190417183,
      "grad_norm": 95.37240600585938,
      "learning_rate": 0.000791,
      "loss": 3.6104,
      "step": 791
    },
    {
      "epoch": 5.234200743494424,
      "grad_norm": 4.292642593383789,
      "learning_rate": 0.0007920000000000001,
      "loss": 4.5604,
      "step": 792
    },
    {
      "epoch": 5.240809582817017,
      "grad_norm": 9.559467315673828,
      "learning_rate": 0.0007930000000000001,
      "loss": 10.3764,
      "step": 793
    },
    {
      "epoch": 5.247418422139612,
      "grad_norm": 43.979190826416016,
      "learning_rate": 0.0007940000000000001,
      "loss": 5.0134,
      "step": 794
    },
    {
      "epoch": 5.2540272614622054,
      "grad_norm": 177.3466339111328,
      "learning_rate": 0.000795,
      "loss": 10.9341,
      "step": 795
    },
    {
      "epoch": 5.2606361007848,
      "grad_norm": 185.54458618164062,
      "learning_rate": 0.000796,
      "loss": 11.3778,
      "step": 796
    },
    {
      "epoch": 5.2672449401073935,
      "grad_norm": 77.58665466308594,
      "learning_rate": 0.0007970000000000001,
      "loss": 3.0558,
      "step": 797
    },
    {
      "epoch": 5.273853779429988,
      "grad_norm": 41.30608367919922,
      "learning_rate": 0.0007980000000000001,
      "loss": 5.3157,
      "step": 798
    },
    {
      "epoch": 5.280462618752582,
      "grad_norm": 24.608720779418945,
      "learning_rate": 0.000799,
      "loss": 6.2427,
      "step": 799
    },
    {
      "epoch": 5.287071458075175,
      "grad_norm": 87.73158264160156,
      "learning_rate": 0.0008,
      "loss": 4.2136,
      "step": 800
    },
    {
      "epoch": 5.29368029739777,
      "grad_norm": 91.83043670654297,
      "learning_rate": 0.0008010000000000001,
      "loss": 2.967,
      "step": 801
    },
    {
      "epoch": 5.300289136720363,
      "grad_norm": 17.98523712158203,
      "learning_rate": 0.0008020000000000001,
      "loss": 5.5549,
      "step": 802
    },
    {
      "epoch": 5.306897976042958,
      "grad_norm": 335.5492248535156,
      "learning_rate": 0.0008030000000000001,
      "loss": 12.1525,
      "step": 803
    },
    {
      "epoch": 5.313506815365551,
      "grad_norm": 226.88941955566406,
      "learning_rate": 0.000804,
      "loss": 9.6538,
      "step": 804
    },
    {
      "epoch": 5.320115654688145,
      "grad_norm": 50.850830078125,
      "learning_rate": 0.000805,
      "loss": 3.9539,
      "step": 805
    },
    {
      "epoch": 5.326724494010739,
      "grad_norm": 28.16789436340332,
      "learning_rate": 0.0008060000000000001,
      "loss": 4.8988,
      "step": 806
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 114.34037780761719,
      "learning_rate": 0.0008070000000000001,
      "loss": 5.4526,
      "step": 807
    },
    {
      "epoch": 5.3399421726559275,
      "grad_norm": 11.555724143981934,
      "learning_rate": 0.000808,
      "loss": 1.7974,
      "step": 808
    },
    {
      "epoch": 5.346551011978521,
      "grad_norm": 59.38849639892578,
      "learning_rate": 0.000809,
      "loss": 5.4265,
      "step": 809
    },
    {
      "epoch": 5.353159851301116,
      "grad_norm": 78.20355224609375,
      "learning_rate": 0.0008100000000000001,
      "loss": 3.8696,
      "step": 810
    },
    {
      "epoch": 5.359768690623709,
      "grad_norm": 62.53700637817383,
      "learning_rate": 0.0008110000000000001,
      "loss": 6.4061,
      "step": 811
    },
    {
      "epoch": 5.366377529946303,
      "grad_norm": 65.27337646484375,
      "learning_rate": 0.0008120000000000001,
      "loss": 4.2381,
      "step": 812
    },
    {
      "epoch": 5.372986369268897,
      "grad_norm": 19.09345245361328,
      "learning_rate": 0.0008129999999999999,
      "loss": 5.342,
      "step": 813
    },
    {
      "epoch": 5.379595208591491,
      "grad_norm": 39.63429641723633,
      "learning_rate": 0.0008139999999999999,
      "loss": 2.626,
      "step": 814
    },
    {
      "epoch": 5.386204047914085,
      "grad_norm": 3.4708220958709717,
      "learning_rate": 0.000815,
      "loss": 3.0926,
      "step": 815
    },
    {
      "epoch": 5.392812887236679,
      "grad_norm": 81.95539093017578,
      "learning_rate": 0.000816,
      "loss": 5.4319,
      "step": 816
    },
    {
      "epoch": 5.399421726559273,
      "grad_norm": 54.38606262207031,
      "learning_rate": 0.000817,
      "loss": 2.3419,
      "step": 817
    },
    {
      "epoch": 5.406030565881867,
      "grad_norm": 18.116228103637695,
      "learning_rate": 0.0008179999999999999,
      "loss": 3.0654,
      "step": 818
    },
    {
      "epoch": 5.412639405204461,
      "grad_norm": 19.418289184570312,
      "learning_rate": 0.000819,
      "loss": 5.8067,
      "step": 819
    },
    {
      "epoch": 5.419248244527055,
      "grad_norm": 19.477563858032227,
      "learning_rate": 0.00082,
      "loss": 3.523,
      "step": 820
    },
    {
      "epoch": 5.425857083849649,
      "grad_norm": 31.48193359375,
      "learning_rate": 0.000821,
      "loss": 2.1979,
      "step": 821
    },
    {
      "epoch": 5.432465923172243,
      "grad_norm": 116.5487060546875,
      "learning_rate": 0.0008219999999999999,
      "loss": 6.3062,
      "step": 822
    },
    {
      "epoch": 5.439074762494837,
      "grad_norm": 2.8941569328308105,
      "learning_rate": 0.000823,
      "loss": 4.7145,
      "step": 823
    },
    {
      "epoch": 5.44568360181743,
      "grad_norm": 115.67693328857422,
      "learning_rate": 0.000824,
      "loss": 6.5116,
      "step": 824
    },
    {
      "epoch": 5.452292441140025,
      "grad_norm": 5.631402015686035,
      "learning_rate": 0.000825,
      "loss": 4.3937,
      "step": 825
    },
    {
      "epoch": 5.4589012804626185,
      "grad_norm": 45.23522186279297,
      "learning_rate": 0.000826,
      "loss": 6.4828,
      "step": 826
    },
    {
      "epoch": 5.465510119785213,
      "grad_norm": 32.24765396118164,
      "learning_rate": 0.0008269999999999999,
      "loss": 2.5774,
      "step": 827
    },
    {
      "epoch": 5.4721189591078065,
      "grad_norm": 52.51401901245117,
      "learning_rate": 0.000828,
      "loss": 2.8592,
      "step": 828
    },
    {
      "epoch": 5.478727798430401,
      "grad_norm": 10.662103652954102,
      "learning_rate": 0.000829,
      "loss": 3.7225,
      "step": 829
    },
    {
      "epoch": 5.485336637752995,
      "grad_norm": 19.206411361694336,
      "learning_rate": 0.00083,
      "loss": 1.6488,
      "step": 830
    },
    {
      "epoch": 5.491945477075588,
      "grad_norm": 56.76573944091797,
      "learning_rate": 0.0008309999999999999,
      "loss": 4.7047,
      "step": 831
    },
    {
      "epoch": 5.498554316398183,
      "grad_norm": 52.17630386352539,
      "learning_rate": 0.000832,
      "loss": 6.8126,
      "step": 832
    },
    {
      "epoch": 5.505163155720776,
      "grad_norm": 31.564678192138672,
      "learning_rate": 0.000833,
      "loss": 4.0283,
      "step": 833
    },
    {
      "epoch": 5.511771995043371,
      "grad_norm": 13.889437675476074,
      "learning_rate": 0.000834,
      "loss": 2.6238,
      "step": 834
    },
    {
      "epoch": 5.518380834365964,
      "grad_norm": 94.99429321289062,
      "learning_rate": 0.000835,
      "loss": 4.9963,
      "step": 835
    },
    {
      "epoch": 5.524989673688559,
      "grad_norm": 25.9215145111084,
      "learning_rate": 0.0008359999999999999,
      "loss": 5.7756,
      "step": 836
    },
    {
      "epoch": 5.5315985130111525,
      "grad_norm": 11.932005882263184,
      "learning_rate": 0.000837,
      "loss": 2.0817,
      "step": 837
    },
    {
      "epoch": 5.538207352333746,
      "grad_norm": 96.27550506591797,
      "learning_rate": 0.000838,
      "loss": 6.2291,
      "step": 838
    },
    {
      "epoch": 5.5448161916563405,
      "grad_norm": 46.79521942138672,
      "learning_rate": 0.000839,
      "loss": 3.1477,
      "step": 839
    },
    {
      "epoch": 5.551425030978934,
      "grad_norm": 29.30012321472168,
      "learning_rate": 0.00084,
      "loss": 2.8545,
      "step": 840
    },
    {
      "epoch": 5.558033870301529,
      "grad_norm": 46.32674789428711,
      "learning_rate": 0.000841,
      "loss": 3.5872,
      "step": 841
    },
    {
      "epoch": 5.564642709624122,
      "grad_norm": 21.167699813842773,
      "learning_rate": 0.000842,
      "loss": 8.8163,
      "step": 842
    },
    {
      "epoch": 5.571251548946716,
      "grad_norm": 107.45317077636719,
      "learning_rate": 0.000843,
      "loss": 3.8742,
      "step": 843
    },
    {
      "epoch": 5.57786038826931,
      "grad_norm": 10.489757537841797,
      "learning_rate": 0.000844,
      "loss": 5.0598,
      "step": 844
    },
    {
      "epoch": 5.584469227591904,
      "grad_norm": 138.8271484375,
      "learning_rate": 0.0008449999999999999,
      "loss": 8.6507,
      "step": 845
    },
    {
      "epoch": 5.591078066914498,
      "grad_norm": 44.43925476074219,
      "learning_rate": 0.000846,
      "loss": 2.5288,
      "step": 846
    },
    {
      "epoch": 5.597686906237092,
      "grad_norm": 39.60756301879883,
      "learning_rate": 0.000847,
      "loss": 2.6812,
      "step": 847
    },
    {
      "epoch": 5.6042957455596865,
      "grad_norm": 48.74962615966797,
      "learning_rate": 0.000848,
      "loss": 4.546,
      "step": 848
    },
    {
      "epoch": 5.61090458488228,
      "grad_norm": 136.56419372558594,
      "learning_rate": 0.000849,
      "loss": 7.6871,
      "step": 849
    },
    {
      "epoch": 5.617513424204874,
      "grad_norm": 82.9117202758789,
      "learning_rate": 0.00085,
      "loss": 2.788,
      "step": 850
    },
    {
      "epoch": 5.624122263527468,
      "grad_norm": 79.68645477294922,
      "learning_rate": 0.000851,
      "loss": 3.9317,
      "step": 851
    },
    {
      "epoch": 5.630731102850062,
      "grad_norm": 71.22555541992188,
      "learning_rate": 0.000852,
      "loss": 3.4856,
      "step": 852
    },
    {
      "epoch": 5.637339942172656,
      "grad_norm": 71.41301727294922,
      "learning_rate": 0.000853,
      "loss": 2.375,
      "step": 853
    },
    {
      "epoch": 5.64394878149525,
      "grad_norm": 42.376312255859375,
      "learning_rate": 0.000854,
      "loss": 5.5401,
      "step": 854
    },
    {
      "epoch": 5.650557620817844,
      "grad_norm": 66.2809066772461,
      "learning_rate": 0.000855,
      "loss": 3.3427,
      "step": 855
    },
    {
      "epoch": 5.657166460140438,
      "grad_norm": 26.161827087402344,
      "learning_rate": 0.000856,
      "loss": 1.7486,
      "step": 856
    },
    {
      "epoch": 5.6637752994630315,
      "grad_norm": 79.3062973022461,
      "learning_rate": 0.000857,
      "loss": 4.6899,
      "step": 857
    },
    {
      "epoch": 5.670384138785626,
      "grad_norm": 119.12382507324219,
      "learning_rate": 0.000858,
      "loss": 4.785,
      "step": 858
    },
    {
      "epoch": 5.67699297810822,
      "grad_norm": 4.81470251083374,
      "learning_rate": 0.000859,
      "loss": 5.9254,
      "step": 859
    },
    {
      "epoch": 5.683601817430814,
      "grad_norm": 114.13710021972656,
      "learning_rate": 0.00086,
      "loss": 5.8718,
      "step": 860
    },
    {
      "epoch": 5.690210656753408,
      "grad_norm": 123.02344512939453,
      "learning_rate": 0.000861,
      "loss": 6.0188,
      "step": 861
    },
    {
      "epoch": 5.696819496076001,
      "grad_norm": 149.25706481933594,
      "learning_rate": 0.000862,
      "loss": 8.3536,
      "step": 862
    },
    {
      "epoch": 5.703428335398596,
      "grad_norm": 12.897260665893555,
      "learning_rate": 0.000863,
      "loss": 3.8849,
      "step": 863
    },
    {
      "epoch": 5.710037174721189,
      "grad_norm": 31.382587432861328,
      "learning_rate": 0.000864,
      "loss": 5.1939,
      "step": 864
    },
    {
      "epoch": 5.716646014043784,
      "grad_norm": 88.06259155273438,
      "learning_rate": 0.000865,
      "loss": 9.5581,
      "step": 865
    },
    {
      "epoch": 5.723254853366377,
      "grad_norm": 72.83074951171875,
      "learning_rate": 0.000866,
      "loss": 2.5758,
      "step": 866
    },
    {
      "epoch": 5.729863692688972,
      "grad_norm": 98.12167358398438,
      "learning_rate": 0.000867,
      "loss": 6.5907,
      "step": 867
    },
    {
      "epoch": 5.7364725320115655,
      "grad_norm": 88.77214050292969,
      "learning_rate": 0.0008680000000000001,
      "loss": 2.8523,
      "step": 868
    },
    {
      "epoch": 5.743081371334159,
      "grad_norm": 7.744528770446777,
      "learning_rate": 0.000869,
      "loss": 3.0259,
      "step": 869
    },
    {
      "epoch": 5.749690210656754,
      "grad_norm": 114.6942367553711,
      "learning_rate": 0.00087,
      "loss": 3.2685,
      "step": 870
    },
    {
      "epoch": 5.756299049979347,
      "grad_norm": 87.4588851928711,
      "learning_rate": 0.000871,
      "loss": 4.9988,
      "step": 871
    },
    {
      "epoch": 5.762907889301942,
      "grad_norm": 146.50624084472656,
      "learning_rate": 0.000872,
      "loss": 6.7776,
      "step": 872
    },
    {
      "epoch": 5.769516728624535,
      "grad_norm": 79.11908721923828,
      "learning_rate": 0.000873,
      "loss": 6.4033,
      "step": 873
    },
    {
      "epoch": 5.77612556794713,
      "grad_norm": 192.62255859375,
      "learning_rate": 0.000874,
      "loss": 10.6712,
      "step": 874
    },
    {
      "epoch": 5.782734407269723,
      "grad_norm": 8.980929374694824,
      "learning_rate": 0.000875,
      "loss": 3.9749,
      "step": 875
    },
    {
      "epoch": 5.789343246592317,
      "grad_norm": 54.722103118896484,
      "learning_rate": 0.000876,
      "loss": 4.0146,
      "step": 876
    },
    {
      "epoch": 5.795952085914911,
      "grad_norm": 89.00492095947266,
      "learning_rate": 0.0008770000000000001,
      "loss": 4.571,
      "step": 877
    },
    {
      "epoch": 5.802560925237505,
      "grad_norm": 23.891429901123047,
      "learning_rate": 0.000878,
      "loss": 3.7309,
      "step": 878
    },
    {
      "epoch": 5.8091697645600995,
      "grad_norm": 55.02206039428711,
      "learning_rate": 0.000879,
      "loss": 2.8425,
      "step": 879
    },
    {
      "epoch": 5.815778603882693,
      "grad_norm": 82.04492950439453,
      "learning_rate": 0.00088,
      "loss": 4.5776,
      "step": 880
    },
    {
      "epoch": 5.822387443205287,
      "grad_norm": 43.34416198730469,
      "learning_rate": 0.0008810000000000001,
      "loss": 5.1093,
      "step": 881
    },
    {
      "epoch": 5.828996282527881,
      "grad_norm": 9.802139282226562,
      "learning_rate": 0.000882,
      "loss": 3.7178,
      "step": 882
    },
    {
      "epoch": 5.835605121850475,
      "grad_norm": 113.818115234375,
      "learning_rate": 0.000883,
      "loss": 4.5792,
      "step": 883
    },
    {
      "epoch": 5.842213961173069,
      "grad_norm": 29.821699142456055,
      "learning_rate": 0.000884,
      "loss": 4.9997,
      "step": 884
    },
    {
      "epoch": 5.848822800495663,
      "grad_norm": 43.62467956542969,
      "learning_rate": 0.000885,
      "loss": 1.5069,
      "step": 885
    },
    {
      "epoch": 5.855431639818256,
      "grad_norm": 104.39703369140625,
      "learning_rate": 0.0008860000000000001,
      "loss": 4.7067,
      "step": 886
    },
    {
      "epoch": 5.862040479140851,
      "grad_norm": 33.211570739746094,
      "learning_rate": 0.000887,
      "loss": 3.5678,
      "step": 887
    },
    {
      "epoch": 5.8686493184634445,
      "grad_norm": 53.251277923583984,
      "learning_rate": 0.000888,
      "loss": 2.1842,
      "step": 888
    },
    {
      "epoch": 5.875258157786039,
      "grad_norm": 75.14928436279297,
      "learning_rate": 0.000889,
      "loss": 4.4672,
      "step": 889
    },
    {
      "epoch": 5.881866997108633,
      "grad_norm": 30.934467315673828,
      "learning_rate": 0.0008900000000000001,
      "loss": 1.6975,
      "step": 890
    },
    {
      "epoch": 5.888475836431227,
      "grad_norm": 52.72368240356445,
      "learning_rate": 0.0008910000000000001,
      "loss": 2.1375,
      "step": 891
    },
    {
      "epoch": 5.895084675753821,
      "grad_norm": 77.77156829833984,
      "learning_rate": 0.000892,
      "loss": 6.3267,
      "step": 892
    },
    {
      "epoch": 5.901693515076415,
      "grad_norm": 124.22557830810547,
      "learning_rate": 0.000893,
      "loss": 6.3184,
      "step": 893
    },
    {
      "epoch": 5.908302354399009,
      "grad_norm": 26.211753845214844,
      "learning_rate": 0.000894,
      "loss": 4.0097,
      "step": 894
    },
    {
      "epoch": 5.914911193721602,
      "grad_norm": 9.87497329711914,
      "learning_rate": 0.0008950000000000001,
      "loss": 3.2107,
      "step": 895
    },
    {
      "epoch": 5.921520033044197,
      "grad_norm": 134.2920684814453,
      "learning_rate": 0.000896,
      "loss": 6.756,
      "step": 896
    },
    {
      "epoch": 5.92812887236679,
      "grad_norm": 21.44779396057129,
      "learning_rate": 0.000897,
      "loss": 2.9696,
      "step": 897
    },
    {
      "epoch": 5.934737711689385,
      "grad_norm": 15.405512809753418,
      "learning_rate": 0.000898,
      "loss": 9.0296,
      "step": 898
    },
    {
      "epoch": 5.9413465510119785,
      "grad_norm": 22.57497215270996,
      "learning_rate": 0.0008990000000000001,
      "loss": 1.3777,
      "step": 899
    },
    {
      "epoch": 5.947955390334572,
      "grad_norm": 16.823862075805664,
      "learning_rate": 0.0009000000000000001,
      "loss": 5.2892,
      "step": 900
    },
    {
      "epoch": 5.954564229657167,
      "grad_norm": 184.43328857421875,
      "learning_rate": 0.000901,
      "loss": 9.3026,
      "step": 901
    },
    {
      "epoch": 5.96117306897976,
      "grad_norm": 236.55870056152344,
      "learning_rate": 0.000902,
      "loss": 13.6044,
      "step": 902
    },
    {
      "epoch": 5.967781908302355,
      "grad_norm": 226.83465576171875,
      "learning_rate": 0.000903,
      "loss": 13.9184,
      "step": 903
    },
    {
      "epoch": 5.974390747624948,
      "grad_norm": 168.6199188232422,
      "learning_rate": 0.0009040000000000001,
      "loss": 7.1206,
      "step": 904
    },
    {
      "epoch": 5.980999586947542,
      "grad_norm": 172.69149780273438,
      "learning_rate": 0.0009050000000000001,
      "loss": 6.3402,
      "step": 905
    },
    {
      "epoch": 5.987608426270136,
      "grad_norm": 132.9891357421875,
      "learning_rate": 0.000906,
      "loss": 5.9785,
      "step": 906
    },
    {
      "epoch": 5.99421726559273,
      "grad_norm": 68.323974609375,
      "learning_rate": 0.000907,
      "loss": 5.0666,
      "step": 907
    },
    {
      "epoch": 5.99421726559273,
      "eval_validation_error_bar": 0.057017882981291984,
      "eval_validation_loss": 6.688638687133789,
      "eval_validation_pearsonr": 0.5018933968025938,
      "eval_validation_rmse": 2.586240291595459,
      "eval_validation_runtime": 33.3978,
      "eval_validation_samples_per_second": 6.078,
      "eval_validation_spearman": 0.48751398651757094,
      "eval_validation_steps_per_second": 6.078,
      "step": 907
    },
    {
      "epoch": 5.99421726559273,
      "eval_test_error_bar": 0.05211211561844549,
      "eval_test_loss": 10.990020751953125,
      "eval_test_pearsonr": 0.1844391789855782,
      "eval_test_rmse": 3.315119981765747,
      "eval_test_runtime": 41.4656,
      "eval_test_samples_per_second": 7.862,
      "eval_test_spearman": 0.2864893304461068,
      "eval_test_steps_per_second": 7.862,
      "step": 907
    },
    {
      "epoch": 6.000826104915324,
      "grad_norm": 9.836653709411621,
      "learning_rate": 0.0009080000000000001,
      "loss": 4.7721,
      "step": 908
    },
    {
      "epoch": 6.007434944237918,
      "grad_norm": 110.83981323242188,
      "learning_rate": 0.0009090000000000001,
      "loss": 3.5593,
      "step": 909
    },
    {
      "epoch": 6.0140437835605125,
      "grad_norm": 101.9655990600586,
      "learning_rate": 0.00091,
      "loss": 4.2367,
      "step": 910
    },
    {
      "epoch": 6.020652622883106,
      "grad_norm": 257.8682861328125,
      "learning_rate": 0.000911,
      "loss": 12.2492,
      "step": 911
    },
    {
      "epoch": 6.0272614622057,
      "grad_norm": 240.42625427246094,
      "learning_rate": 0.000912,
      "loss": 12.4748,
      "step": 912
    },
    {
      "epoch": 6.033870301528294,
      "grad_norm": 232.57192993164062,
      "learning_rate": 0.0009130000000000001,
      "loss": 12.7881,
      "step": 913
    },
    {
      "epoch": 6.040479140850888,
      "grad_norm": 110.74342346191406,
      "learning_rate": 0.0009140000000000001,
      "loss": 7.7772,
      "step": 914
    },
    {
      "epoch": 6.047087980173482,
      "grad_norm": 174.33326721191406,
      "learning_rate": 0.000915,
      "loss": 6.2486,
      "step": 915
    },
    {
      "epoch": 6.053696819496076,
      "grad_norm": 27.373592376708984,
      "learning_rate": 0.000916,
      "loss": 5.6932,
      "step": 916
    },
    {
      "epoch": 6.06030565881867,
      "grad_norm": 32.8973503112793,
      "learning_rate": 0.0009170000000000001,
      "loss": 3.182,
      "step": 917
    },
    {
      "epoch": 6.066914498141264,
      "grad_norm": 123.2594985961914,
      "learning_rate": 0.0009180000000000001,
      "loss": 4.4461,
      "step": 918
    },
    {
      "epoch": 6.0735233374638575,
      "grad_norm": 150.4246826171875,
      "learning_rate": 0.0009190000000000001,
      "loss": 5.7597,
      "step": 919
    },
    {
      "epoch": 6.080132176786452,
      "grad_norm": 227.50888061523438,
      "learning_rate": 0.00092,
      "loss": 12.9463,
      "step": 920
    },
    {
      "epoch": 6.086741016109046,
      "grad_norm": 169.48069763183594,
      "learning_rate": 0.000921,
      "loss": 7.0861,
      "step": 921
    },
    {
      "epoch": 6.09334985543164,
      "grad_norm": 33.66697692871094,
      "learning_rate": 0.0009220000000000001,
      "loss": 2.3035,
      "step": 922
    },
    {
      "epoch": 6.099958694754234,
      "grad_norm": 7.461622714996338,
      "learning_rate": 0.0009230000000000001,
      "loss": 4.0562,
      "step": 923
    },
    {
      "epoch": 6.106567534076828,
      "grad_norm": 12.063194274902344,
      "learning_rate": 0.000924,
      "loss": 4.5016,
      "step": 924
    },
    {
      "epoch": 6.113176373399422,
      "grad_norm": 84.87882995605469,
      "learning_rate": 0.000925,
      "loss": 3.7088,
      "step": 925
    },
    {
      "epoch": 6.119785212722015,
      "grad_norm": 126.1572265625,
      "learning_rate": 0.0009260000000000001,
      "loss": 4.6878,
      "step": 926
    },
    {
      "epoch": 6.12639405204461,
      "grad_norm": 186.6483917236328,
      "learning_rate": 0.0009270000000000001,
      "loss": 7.0989,
      "step": 927
    },
    {
      "epoch": 6.1330028913672034,
      "grad_norm": 28.154518127441406,
      "learning_rate": 0.0009280000000000001,
      "loss": 3.3429,
      "step": 928
    },
    {
      "epoch": 6.139611730689798,
      "grad_norm": 14.644132614135742,
      "learning_rate": 0.000929,
      "loss": 4.8082,
      "step": 929
    },
    {
      "epoch": 6.1462205700123915,
      "grad_norm": 61.87894058227539,
      "learning_rate": 0.00093,
      "loss": 3.9281,
      "step": 930
    },
    {
      "epoch": 6.152829409334985,
      "grad_norm": 45.123748779296875,
      "learning_rate": 0.0009310000000000001,
      "loss": 2.186,
      "step": 931
    },
    {
      "epoch": 6.15943824865758,
      "grad_norm": 150.8759002685547,
      "learning_rate": 0.0009320000000000001,
      "loss": 5.3616,
      "step": 932
    },
    {
      "epoch": 6.166047087980173,
      "grad_norm": 127.84278106689453,
      "learning_rate": 0.000933,
      "loss": 5.1717,
      "step": 933
    },
    {
      "epoch": 6.172655927302768,
      "grad_norm": 138.087158203125,
      "learning_rate": 0.000934,
      "loss": 9.1674,
      "step": 934
    },
    {
      "epoch": 6.179264766625361,
      "grad_norm": 89.03680419921875,
      "learning_rate": 0.0009350000000000001,
      "loss": 2.232,
      "step": 935
    },
    {
      "epoch": 6.185873605947956,
      "grad_norm": 40.80050277709961,
      "learning_rate": 0.0009360000000000001,
      "loss": 3.9982,
      "step": 936
    },
    {
      "epoch": 6.192482445270549,
      "grad_norm": 13.611346244812012,
      "learning_rate": 0.0009370000000000001,
      "loss": 3.4806,
      "step": 937
    },
    {
      "epoch": 6.199091284593143,
      "grad_norm": 15.323686599731445,
      "learning_rate": 0.0009379999999999999,
      "loss": 4.5773,
      "step": 938
    },
    {
      "epoch": 6.205700123915737,
      "grad_norm": 50.593017578125,
      "learning_rate": 0.000939,
      "loss": 2.6935,
      "step": 939
    },
    {
      "epoch": 6.212308963238331,
      "grad_norm": 109.7395248413086,
      "learning_rate": 0.00094,
      "loss": 4.7397,
      "step": 940
    },
    {
      "epoch": 6.2189178025609255,
      "grad_norm": 162.60223388671875,
      "learning_rate": 0.000941,
      "loss": 6.0446,
      "step": 941
    },
    {
      "epoch": 6.225526641883519,
      "grad_norm": 89.74327850341797,
      "learning_rate": 0.000942,
      "loss": 7.7399,
      "step": 942
    },
    {
      "epoch": 6.232135481206114,
      "grad_norm": 11.804095268249512,
      "learning_rate": 0.0009429999999999999,
      "loss": 3.5007,
      "step": 943
    },
    {
      "epoch": 6.238744320528707,
      "grad_norm": 11.14539623260498,
      "learning_rate": 0.000944,
      "loss": 4.0414,
      "step": 944
    },
    {
      "epoch": 6.245353159851301,
      "grad_norm": 55.238525390625,
      "learning_rate": 0.000945,
      "loss": 4.8969,
      "step": 945
    },
    {
      "epoch": 6.251961999173895,
      "grad_norm": 17.806594848632812,
      "learning_rate": 0.000946,
      "loss": 4.9962,
      "step": 946
    },
    {
      "epoch": 6.258570838496489,
      "grad_norm": 16.68880844116211,
      "learning_rate": 0.0009469999999999999,
      "loss": 4.1165,
      "step": 947
    },
    {
      "epoch": 6.265179677819083,
      "grad_norm": 60.156280517578125,
      "learning_rate": 0.000948,
      "loss": 2.1053,
      "step": 948
    },
    {
      "epoch": 6.271788517141677,
      "grad_norm": 72.84313201904297,
      "learning_rate": 0.000949,
      "loss": 5.1736,
      "step": 949
    },
    {
      "epoch": 6.2783973564642706,
      "grad_norm": 63.591590881347656,
      "learning_rate": 0.00095,
      "loss": 5.224,
      "step": 950
    },
    {
      "epoch": 6.285006195786865,
      "grad_norm": 31.602935791015625,
      "learning_rate": 0.000951,
      "loss": 5.9671,
      "step": 951
    },
    {
      "epoch": 6.291615035109459,
      "grad_norm": 60.798423767089844,
      "learning_rate": 0.0009519999999999999,
      "loss": 2.9118,
      "step": 952
    },
    {
      "epoch": 6.298223874432053,
      "grad_norm": 29.94178009033203,
      "learning_rate": 0.000953,
      "loss": 2.9761,
      "step": 953
    },
    {
      "epoch": 6.304832713754647,
      "grad_norm": 9.320218086242676,
      "learning_rate": 0.000954,
      "loss": 4.7506,
      "step": 954
    },
    {
      "epoch": 6.311441553077241,
      "grad_norm": 52.73628616333008,
      "learning_rate": 0.000955,
      "loss": 2.2424,
      "step": 955
    },
    {
      "epoch": 6.318050392399835,
      "grad_norm": 36.55826187133789,
      "learning_rate": 0.0009559999999999999,
      "loss": 3.997,
      "step": 956
    },
    {
      "epoch": 6.324659231722428,
      "grad_norm": 31.238040924072266,
      "learning_rate": 0.000957,
      "loss": 3.8713,
      "step": 957
    },
    {
      "epoch": 6.331268071045023,
      "grad_norm": 80.58206176757812,
      "learning_rate": 0.000958,
      "loss": 3.6353,
      "step": 958
    },
    {
      "epoch": 6.3378769103676165,
      "grad_norm": 5.625772476196289,
      "learning_rate": 0.000959,
      "loss": 2.2702,
      "step": 959
    },
    {
      "epoch": 6.344485749690211,
      "grad_norm": 26.715124130249023,
      "learning_rate": 0.00096,
      "loss": 3.4328,
      "step": 960
    },
    {
      "epoch": 6.3510945890128045,
      "grad_norm": 23.665969848632812,
      "learning_rate": 0.0009609999999999999,
      "loss": 2.9365,
      "step": 961
    },
    {
      "epoch": 6.357703428335398,
      "grad_norm": 9.267570495605469,
      "learning_rate": 0.000962,
      "loss": 2.9043,
      "step": 962
    },
    {
      "epoch": 6.364312267657993,
      "grad_norm": 90.599609375,
      "learning_rate": 0.000963,
      "loss": 5.73,
      "step": 963
    },
    {
      "epoch": 6.370921106980586,
      "grad_norm": 60.75407791137695,
      "learning_rate": 0.000964,
      "loss": 3.5562,
      "step": 964
    },
    {
      "epoch": 6.377529946303181,
      "grad_norm": 110.24954986572266,
      "learning_rate": 0.000965,
      "loss": 6.444,
      "step": 965
    },
    {
      "epoch": 6.384138785625774,
      "grad_norm": 38.11735534667969,
      "learning_rate": 0.000966,
      "loss": 8.408,
      "step": 966
    },
    {
      "epoch": 6.390747624948369,
      "grad_norm": 18.969335556030273,
      "learning_rate": 0.000967,
      "loss": 1.9784,
      "step": 967
    },
    {
      "epoch": 6.397356464270962,
      "grad_norm": 12.27520751953125,
      "learning_rate": 0.000968,
      "loss": 2.7231,
      "step": 968
    },
    {
      "epoch": 6.403965303593556,
      "grad_norm": 25.33905029296875,
      "learning_rate": 0.000969,
      "loss": 4.8079,
      "step": 969
    },
    {
      "epoch": 6.4105741429161505,
      "grad_norm": 16.230159759521484,
      "learning_rate": 0.0009699999999999999,
      "loss": 1.4874,
      "step": 970
    },
    {
      "epoch": 6.417182982238744,
      "grad_norm": 11.25037670135498,
      "learning_rate": 0.000971,
      "loss": 2.4617,
      "step": 971
    },
    {
      "epoch": 6.4237918215613385,
      "grad_norm": 44.57924270629883,
      "learning_rate": 0.000972,
      "loss": 2.4003,
      "step": 972
    },
    {
      "epoch": 6.430400660883932,
      "grad_norm": 69.47674560546875,
      "learning_rate": 0.000973,
      "loss": 4.8298,
      "step": 973
    },
    {
      "epoch": 6.437009500206527,
      "grad_norm": 50.10020065307617,
      "learning_rate": 0.000974,
      "loss": 2.8642,
      "step": 974
    },
    {
      "epoch": 6.44361833952912,
      "grad_norm": 117.39408111572266,
      "learning_rate": 0.000975,
      "loss": 11.8496,
      "step": 975
    },
    {
      "epoch": 6.450227178851714,
      "grad_norm": 42.721893310546875,
      "learning_rate": 0.000976,
      "loss": 3.3102,
      "step": 976
    },
    {
      "epoch": 6.456836018174308,
      "grad_norm": 6.587103366851807,
      "learning_rate": 0.000977,
      "loss": 3.3168,
      "step": 977
    },
    {
      "epoch": 6.463444857496902,
      "grad_norm": 32.91880416870117,
      "learning_rate": 0.000978,
      "loss": 3.7916,
      "step": 978
    },
    {
      "epoch": 6.470053696819496,
      "grad_norm": 19.02326011657715,
      "learning_rate": 0.000979,
      "loss": 1.0631,
      "step": 979
    },
    {
      "epoch": 6.47666253614209,
      "grad_norm": 10.796072006225586,
      "learning_rate": 0.00098,
      "loss": 2.2847,
      "step": 980
    },
    {
      "epoch": 6.483271375464684,
      "grad_norm": 5.676680088043213,
      "learning_rate": 0.000981,
      "loss": 6.5634,
      "step": 981
    },
    {
      "epoch": 6.489880214787278,
      "grad_norm": 15.532458305358887,
      "learning_rate": 0.000982,
      "loss": 5.4731,
      "step": 982
    },
    {
      "epoch": 6.496489054109872,
      "grad_norm": 80.37825012207031,
      "learning_rate": 0.000983,
      "loss": 2.6793,
      "step": 983
    },
    {
      "epoch": 6.503097893432466,
      "grad_norm": 188.5556182861328,
      "learning_rate": 0.000984,
      "loss": 10.8216,
      "step": 984
    },
    {
      "epoch": 6.50970673275506,
      "grad_norm": 190.90130615234375,
      "learning_rate": 0.000985,
      "loss": 10.5604,
      "step": 985
    },
    {
      "epoch": 6.516315572077654,
      "grad_norm": 120.73905944824219,
      "learning_rate": 0.0009860000000000001,
      "loss": 5.0816,
      "step": 986
    },
    {
      "epoch": 6.522924411400248,
      "grad_norm": 91.98590087890625,
      "learning_rate": 0.000987,
      "loss": 7.3036,
      "step": 987
    },
    {
      "epoch": 6.529533250722842,
      "grad_norm": 10.548829078674316,
      "learning_rate": 0.000988,
      "loss": 2.2707,
      "step": 988
    },
    {
      "epoch": 6.536142090045436,
      "grad_norm": 73.49504089355469,
      "learning_rate": 0.000989,
      "loss": 5.2912,
      "step": 989
    },
    {
      "epoch": 6.5427509293680295,
      "grad_norm": 21.697595596313477,
      "learning_rate": 0.00099,
      "loss": 1.9963,
      "step": 990
    },
    {
      "epoch": 6.549359768690624,
      "grad_norm": 97.96782684326172,
      "learning_rate": 0.000991,
      "loss": 3.4803,
      "step": 991
    },
    {
      "epoch": 6.555968608013218,
      "grad_norm": 67.39688110351562,
      "learning_rate": 0.000992,
      "loss": 4.6018,
      "step": 992
    },
    {
      "epoch": 6.562577447335812,
      "grad_norm": 16.845369338989258,
      "learning_rate": 0.000993,
      "loss": 3.1497,
      "step": 993
    },
    {
      "epoch": 6.569186286658406,
      "grad_norm": 11.446920394897461,
      "learning_rate": 0.000994,
      "loss": 2.5951,
      "step": 994
    },
    {
      "epoch": 6.575795125980999,
      "grad_norm": 18.34082794189453,
      "learning_rate": 0.000995,
      "loss": 2.7874,
      "step": 995
    },
    {
      "epoch": 6.582403965303594,
      "grad_norm": 151.97044372558594,
      "learning_rate": 0.000996,
      "loss": 7.5839,
      "step": 996
    },
    {
      "epoch": 6.589012804626187,
      "grad_norm": 9.642086029052734,
      "learning_rate": 0.000997,
      "loss": 5.7158,
      "step": 997
    },
    {
      "epoch": 6.595621643948782,
      "grad_norm": 39.52112579345703,
      "learning_rate": 0.000998,
      "loss": 1.198,
      "step": 998
    },
    {
      "epoch": 6.602230483271375,
      "grad_norm": 11.832232475280762,
      "learning_rate": 0.000999,
      "loss": 4.8365,
      "step": 999
    },
    {
      "epoch": 6.608839322593969,
      "grad_norm": 139.60018920898438,
      "learning_rate": 0.001,
      "loss": 6.8349,
      "step": 1000
    },
    {
      "epoch": 6.6154481619165635,
      "grad_norm": 6.128602981567383,
      "learning_rate": 0.0009995049504950494,
      "loss": 4.8642,
      "step": 1001
    },
    {
      "epoch": 6.622057001239157,
      "grad_norm": 75.95767211914062,
      "learning_rate": 0.000999009900990099,
      "loss": 3.4173,
      "step": 1002
    },
    {
      "epoch": 6.628665840561752,
      "grad_norm": 8.841500282287598,
      "learning_rate": 0.0009985148514851485,
      "loss": 3.5886,
      "step": 1003
    },
    {
      "epoch": 6.635274679884345,
      "grad_norm": 3.5249929428100586,
      "learning_rate": 0.0009980198019801981,
      "loss": 5.2735,
      "step": 1004
    },
    {
      "epoch": 6.64188351920694,
      "grad_norm": 100.41464233398438,
      "learning_rate": 0.0009975247524752475,
      "loss": 5.5168,
      "step": 1005
    },
    {
      "epoch": 6.648492358529533,
      "grad_norm": 59.74799728393555,
      "learning_rate": 0.000997029702970297,
      "loss": 4.1151,
      "step": 1006
    },
    {
      "epoch": 6.655101197852127,
      "grad_norm": 55.95287322998047,
      "learning_rate": 0.0009965346534653466,
      "loss": 3.6435,
      "step": 1007
    },
    {
      "epoch": 6.661710037174721,
      "grad_norm": 82.55490112304688,
      "learning_rate": 0.000996039603960396,
      "loss": 3.6809,
      "step": 1008
    },
    {
      "epoch": 6.668318876497315,
      "grad_norm": 8.738617897033691,
      "learning_rate": 0.0009955445544554456,
      "loss": 5.8854,
      "step": 1009
    },
    {
      "epoch": 6.674927715819909,
      "grad_norm": 7.894087314605713,
      "learning_rate": 0.000995049504950495,
      "loss": 10.58,
      "step": 1010
    },
    {
      "epoch": 6.681536555142503,
      "grad_norm": 49.646793365478516,
      "learning_rate": 0.0009945544554455445,
      "loss": 4.3505,
      "step": 1011
    },
    {
      "epoch": 6.6881453944650975,
      "grad_norm": 105.6515884399414,
      "learning_rate": 0.0009940594059405941,
      "loss": 5.1305,
      "step": 1012
    },
    {
      "epoch": 6.694754233787691,
      "grad_norm": 19.806106567382812,
      "learning_rate": 0.0009935643564356435,
      "loss": 6.0116,
      "step": 1013
    },
    {
      "epoch": 6.701363073110285,
      "grad_norm": 3.6924052238464355,
      "learning_rate": 0.0009930693069306932,
      "loss": 2.0028,
      "step": 1014
    },
    {
      "epoch": 6.707971912432879,
      "grad_norm": 85.56317138671875,
      "learning_rate": 0.0009925742574257426,
      "loss": 5.1186,
      "step": 1015
    },
    {
      "epoch": 6.714580751755473,
      "grad_norm": 49.84730529785156,
      "learning_rate": 0.000992079207920792,
      "loss": 3.6985,
      "step": 1016
    },
    {
      "epoch": 6.721189591078067,
      "grad_norm": 9.931159019470215,
      "learning_rate": 0.0009915841584158416,
      "loss": 2.9682,
      "step": 1017
    },
    {
      "epoch": 6.727798430400661,
      "grad_norm": 24.07836151123047,
      "learning_rate": 0.000991089108910891,
      "loss": 4.6974,
      "step": 1018
    },
    {
      "epoch": 6.734407269723254,
      "grad_norm": 31.562055587768555,
      "learning_rate": 0.0009905940594059407,
      "loss": 1.3231,
      "step": 1019
    },
    {
      "epoch": 6.741016109045849,
      "grad_norm": 83.2640380859375,
      "learning_rate": 0.0009900990099009901,
      "loss": 5.1526,
      "step": 1020
    },
    {
      "epoch": 6.7476249483684425,
      "grad_norm": 64.40272521972656,
      "learning_rate": 0.0009896039603960395,
      "loss": 2.9678,
      "step": 1021
    },
    {
      "epoch": 6.754233787691037,
      "grad_norm": 107.2246322631836,
      "learning_rate": 0.0009891089108910892,
      "loss": 5.6357,
      "step": 1022
    },
    {
      "epoch": 6.760842627013631,
      "grad_norm": 68.54190826416016,
      "learning_rate": 0.0009886138613861386,
      "loss": 3.5182,
      "step": 1023
    },
    {
      "epoch": 6.767451466336225,
      "grad_norm": 37.85031509399414,
      "learning_rate": 0.0009881188118811882,
      "loss": 7.3207,
      "step": 1024
    },
    {
      "epoch": 6.774060305658819,
      "grad_norm": 16.33344841003418,
      "learning_rate": 0.0009876237623762376,
      "loss": 2.5862,
      "step": 1025
    },
    {
      "epoch": 6.780669144981412,
      "grad_norm": 65.14350128173828,
      "learning_rate": 0.000987128712871287,
      "loss": 3.0408,
      "step": 1026
    },
    {
      "epoch": 6.787277984304007,
      "grad_norm": 27.95182228088379,
      "learning_rate": 0.0009866336633663367,
      "loss": 2.254,
      "step": 1027
    },
    {
      "epoch": 6.7938868236266,
      "grad_norm": 25.007699966430664,
      "learning_rate": 0.000986138613861386,
      "loss": 2.8658,
      "step": 1028
    },
    {
      "epoch": 6.800495662949195,
      "grad_norm": 68.3518295288086,
      "learning_rate": 0.0009856435643564357,
      "loss": 5.2645,
      "step": 1029
    },
    {
      "epoch": 6.807104502271788,
      "grad_norm": 23.555644989013672,
      "learning_rate": 0.0009851485148514852,
      "loss": 2.5457,
      "step": 1030
    },
    {
      "epoch": 6.813713341594383,
      "grad_norm": 72.27516174316406,
      "learning_rate": 0.0009846534653465346,
      "loss": 3.1081,
      "step": 1031
    },
    {
      "epoch": 6.8203221809169765,
      "grad_norm": 55.32929229736328,
      "learning_rate": 0.0009841584158415842,
      "loss": 4.1324,
      "step": 1032
    },
    {
      "epoch": 6.82693102023957,
      "grad_norm": 71.79061126708984,
      "learning_rate": 0.0009836633663366336,
      "loss": 3.8112,
      "step": 1033
    },
    {
      "epoch": 6.833539859562165,
      "grad_norm": 24.94426727294922,
      "learning_rate": 0.0009831683168316833,
      "loss": 3.2544,
      "step": 1034
    },
    {
      "epoch": 6.840148698884758,
      "grad_norm": 83.6445083618164,
      "learning_rate": 0.0009826732673267327,
      "loss": 2.1804,
      "step": 1035
    },
    {
      "epoch": 6.846757538207353,
      "grad_norm": 8.995238304138184,
      "learning_rate": 0.000982178217821782,
      "loss": 2.1273,
      "step": 1036
    },
    {
      "epoch": 6.853366377529946,
      "grad_norm": 70.74458312988281,
      "learning_rate": 0.0009816831683168317,
      "loss": 3.4961,
      "step": 1037
    },
    {
      "epoch": 6.85997521685254,
      "grad_norm": 59.480674743652344,
      "learning_rate": 0.0009811881188118811,
      "loss": 3.8585,
      "step": 1038
    },
    {
      "epoch": 6.866584056175134,
      "grad_norm": 3.388923406600952,
      "learning_rate": 0.0009806930693069308,
      "loss": 1.2332,
      "step": 1039
    },
    {
      "epoch": 6.873192895497728,
      "grad_norm": 67.11172485351562,
      "learning_rate": 0.0009801980198019802,
      "loss": 11.7415,
      "step": 1040
    },
    {
      "epoch": 6.879801734820322,
      "grad_norm": 58.38042449951172,
      "learning_rate": 0.0009797029702970296,
      "loss": 4.1109,
      "step": 1041
    },
    {
      "epoch": 6.886410574142916,
      "grad_norm": 8.019716262817383,
      "learning_rate": 0.0009792079207920793,
      "loss": 2.1102,
      "step": 1042
    },
    {
      "epoch": 6.8930194134655105,
      "grad_norm": 10.01161003112793,
      "learning_rate": 0.0009787128712871287,
      "loss": 3.741,
      "step": 1043
    },
    {
      "epoch": 6.899628252788104,
      "grad_norm": 63.015438079833984,
      "learning_rate": 0.0009782178217821783,
      "loss": 3.923,
      "step": 1044
    },
    {
      "epoch": 6.906237092110698,
      "grad_norm": 82.65654754638672,
      "learning_rate": 0.0009777227722772277,
      "loss": 14.0239,
      "step": 1045
    },
    {
      "epoch": 6.912845931433292,
      "grad_norm": 36.726219177246094,
      "learning_rate": 0.0009772277227722771,
      "loss": 3.2601,
      "step": 1046
    },
    {
      "epoch": 6.919454770755886,
      "grad_norm": 12.952642440795898,
      "learning_rate": 0.0009767326732673268,
      "loss": 3.0398,
      "step": 1047
    },
    {
      "epoch": 6.92606361007848,
      "grad_norm": 28.7177791595459,
      "learning_rate": 0.0009762376237623762,
      "loss": 2.858,
      "step": 1048
    },
    {
      "epoch": 6.932672449401074,
      "grad_norm": 35.92677307128906,
      "learning_rate": 0.0009757425742574257,
      "loss": 3.9272,
      "step": 1049
    },
    {
      "epoch": 6.939281288723668,
      "grad_norm": 19.18927764892578,
      "learning_rate": 0.0009752475247524752,
      "loss": 7.7125,
      "step": 1050
    },
    {
      "epoch": 6.945890128046262,
      "grad_norm": 50.3621940612793,
      "learning_rate": 0.0009747524752475248,
      "loss": 3.0095,
      "step": 1051
    },
    {
      "epoch": 6.9524989673688555,
      "grad_norm": 6.532358646392822,
      "learning_rate": 0.0009742574257425743,
      "loss": 6.899,
      "step": 1052
    },
    {
      "epoch": 6.95910780669145,
      "grad_norm": 34.274654388427734,
      "learning_rate": 0.0009737623762376237,
      "loss": 2.9348,
      "step": 1053
    },
    {
      "epoch": 6.965716646014044,
      "grad_norm": 92.36742401123047,
      "learning_rate": 0.0009732673267326732,
      "loss": 2.8757,
      "step": 1054
    },
    {
      "epoch": 6.972325485336638,
      "grad_norm": 88.24072265625,
      "learning_rate": 0.0009727722772277228,
      "loss": 4.0165,
      "step": 1055
    },
    {
      "epoch": 6.978934324659232,
      "grad_norm": 6.382627010345459,
      "learning_rate": 0.0009722772277227723,
      "loss": 2.5144,
      "step": 1056
    },
    {
      "epoch": 6.985543163981825,
      "grad_norm": 80.3464584350586,
      "learning_rate": 0.0009717821782178218,
      "loss": 2.4026,
      "step": 1057
    },
    {
      "epoch": 6.99215200330442,
      "grad_norm": 5.576284885406494,
      "learning_rate": 0.0009712871287128712,
      "loss": 2.5323,
      "step": 1058
    },
    {
      "epoch": 6.998760842627013,
      "grad_norm": 75.30821228027344,
      "learning_rate": 0.0009707920792079208,
      "loss": 6.7086,
      "step": 1059
    },
    {
      "epoch": 6.998760842627013,
      "eval_validation_error_bar": 0.05599045535366452,
      "eval_validation_loss": 7.896940231323242,
      "eval_validation_pearsonr": 0.5185978201245848,
      "eval_validation_rmse": 2.8101494312286377,
      "eval_validation_runtime": 33.4672,
      "eval_validation_samples_per_second": 6.066,
      "eval_validation_spearman": 0.5041306842654517,
      "eval_validation_steps_per_second": 6.066,
      "step": 1059
    },
    {
      "epoch": 6.998760842627013,
      "eval_test_error_bar": 0.05063736398650178,
      "eval_test_loss": 13.858506202697754,
      "eval_test_pearsonr": 0.15696700892937834,
      "eval_test_rmse": 3.7227015495300293,
      "eval_test_runtime": 41.4977,
      "eval_test_samples_per_second": 7.856,
      "eval_test_spearman": 0.33915869981293495,
      "eval_test_steps_per_second": 7.856,
      "step": 1059
    },
    {
      "epoch": 7.005369681949608,
      "grad_norm": 13.969694137573242,
      "learning_rate": 0.0009702970297029703,
      "loss": 2.9437,
      "step": 1060
    },
    {
      "epoch": 7.0119785212722014,
      "grad_norm": 7.728158950805664,
      "learning_rate": 0.0009698019801980198,
      "loss": 3.5099,
      "step": 1061
    },
    {
      "epoch": 7.018587360594796,
      "grad_norm": 108.31745910644531,
      "learning_rate": 0.0009693069306930693,
      "loss": 5.9926,
      "step": 1062
    },
    {
      "epoch": 7.0251961999173895,
      "grad_norm": 48.937034606933594,
      "learning_rate": 0.0009688118811881188,
      "loss": 5.2359,
      "step": 1063
    },
    {
      "epoch": 7.031805039239983,
      "grad_norm": 12.554818153381348,
      "learning_rate": 0.0009683168316831683,
      "loss": 2.0583,
      "step": 1064
    },
    {
      "epoch": 7.038413878562578,
      "grad_norm": 11.892806053161621,
      "learning_rate": 0.0009678217821782178,
      "loss": 1.877,
      "step": 1065
    },
    {
      "epoch": 7.045022717885171,
      "grad_norm": 38.087589263916016,
      "learning_rate": 0.0009673267326732673,
      "loss": 2.7507,
      "step": 1066
    },
    {
      "epoch": 7.051631557207766,
      "grad_norm": 74.69864654541016,
      "learning_rate": 0.0009668316831683169,
      "loss": 2.9249,
      "step": 1067
    },
    {
      "epoch": 7.058240396530359,
      "grad_norm": 141.75221252441406,
      "learning_rate": 0.0009663366336633663,
      "loss": 8.4299,
      "step": 1068
    },
    {
      "epoch": 7.064849235852954,
      "grad_norm": 58.68815231323242,
      "learning_rate": 0.0009658415841584158,
      "loss": 2.1091,
      "step": 1069
    },
    {
      "epoch": 7.071458075175547,
      "grad_norm": 36.070953369140625,
      "learning_rate": 0.0009653465346534653,
      "loss": 2.2547,
      "step": 1070
    },
    {
      "epoch": 7.078066914498141,
      "grad_norm": 24.773006439208984,
      "learning_rate": 0.0009648514851485149,
      "loss": 2.0046,
      "step": 1071
    },
    {
      "epoch": 7.0846757538207354,
      "grad_norm": 17.25505828857422,
      "learning_rate": 0.0009643564356435644,
      "loss": 2.085,
      "step": 1072
    },
    {
      "epoch": 7.091284593143329,
      "grad_norm": 46.60443115234375,
      "learning_rate": 0.0009638613861386138,
      "loss": 3.563,
      "step": 1073
    },
    {
      "epoch": 7.0978934324659235,
      "grad_norm": 6.494596004486084,
      "learning_rate": 0.0009633663366336633,
      "loss": 3.1646,
      "step": 1074
    },
    {
      "epoch": 7.104502271788517,
      "grad_norm": 19.134849548339844,
      "learning_rate": 0.0009628712871287129,
      "loss": 1.934,
      "step": 1075
    },
    {
      "epoch": 7.111111111111111,
      "grad_norm": 28.832469940185547,
      "learning_rate": 0.0009623762376237624,
      "loss": 5.2478,
      "step": 1076
    },
    {
      "epoch": 7.117719950433705,
      "grad_norm": 30.26678466796875,
      "learning_rate": 0.0009618811881188119,
      "loss": 0.8997,
      "step": 1077
    },
    {
      "epoch": 7.124328789756299,
      "grad_norm": 11.774782180786133,
      "learning_rate": 0.0009613861386138613,
      "loss": 3.697,
      "step": 1078
    },
    {
      "epoch": 7.130937629078893,
      "grad_norm": 18.118196487426758,
      "learning_rate": 0.0009608910891089109,
      "loss": 6.8884,
      "step": 1079
    },
    {
      "epoch": 7.137546468401487,
      "grad_norm": 63.03857421875,
      "learning_rate": 0.0009603960396039604,
      "loss": 3.8147,
      "step": 1080
    },
    {
      "epoch": 7.144155307724081,
      "grad_norm": 2.163149118423462,
      "learning_rate": 0.0009599009900990099,
      "loss": 1.3886,
      "step": 1081
    },
    {
      "epoch": 7.150764147046675,
      "grad_norm": 11.149518013000488,
      "learning_rate": 0.0009594059405940594,
      "loss": 4.245,
      "step": 1082
    },
    {
      "epoch": 7.1573729863692686,
      "grad_norm": 56.37872314453125,
      "learning_rate": 0.0009589108910891089,
      "loss": 1.6826,
      "step": 1083
    },
    {
      "epoch": 7.163981825691863,
      "grad_norm": 91.66059875488281,
      "learning_rate": 0.0009584158415841584,
      "loss": 9.8597,
      "step": 1084
    },
    {
      "epoch": 7.170590665014457,
      "grad_norm": 18.097164154052734,
      "learning_rate": 0.0009579207920792079,
      "loss": 3.0798,
      "step": 1085
    },
    {
      "epoch": 7.177199504337051,
      "grad_norm": 26.83009910583496,
      "learning_rate": 0.0009574257425742574,
      "loss": 3.2267,
      "step": 1086
    },
    {
      "epoch": 7.183808343659645,
      "grad_norm": 33.178260803222656,
      "learning_rate": 0.000956930693069307,
      "loss": 6.7617,
      "step": 1087
    },
    {
      "epoch": 7.190417182982239,
      "grad_norm": 8.440204620361328,
      "learning_rate": 0.0009564356435643564,
      "loss": 3.3703,
      "step": 1088
    },
    {
      "epoch": 7.197026022304833,
      "grad_norm": 107.61639404296875,
      "learning_rate": 0.0009559405940594059,
      "loss": 4.9876,
      "step": 1089
    },
    {
      "epoch": 7.203634861627426,
      "grad_norm": 143.61029052734375,
      "learning_rate": 0.0009554455445544554,
      "loss": 7.3556,
      "step": 1090
    },
    {
      "epoch": 7.210243700950021,
      "grad_norm": 69.26902770996094,
      "learning_rate": 0.000954950495049505,
      "loss": 5.1623,
      "step": 1091
    },
    {
      "epoch": 7.2168525402726145,
      "grad_norm": 29.58247184753418,
      "learning_rate": 0.0009544554455445545,
      "loss": 2.8401,
      "step": 1092
    },
    {
      "epoch": 7.223461379595209,
      "grad_norm": 121.89722442626953,
      "learning_rate": 0.0009539603960396039,
      "loss": 4.9706,
      "step": 1093
    },
    {
      "epoch": 7.2300702189178025,
      "grad_norm": 47.17170715332031,
      "learning_rate": 0.0009534653465346534,
      "loss": 4.1413,
      "step": 1094
    },
    {
      "epoch": 7.236679058240396,
      "grad_norm": 36.786380767822266,
      "learning_rate": 0.000952970297029703,
      "loss": 1.776,
      "step": 1095
    },
    {
      "epoch": 7.243287897562991,
      "grad_norm": 48.2944450378418,
      "learning_rate": 0.0009524752475247525,
      "loss": 4.9923,
      "step": 1096
    },
    {
      "epoch": 7.249896736885584,
      "grad_norm": 19.855083465576172,
      "learning_rate": 0.000951980198019802,
      "loss": 5.0811,
      "step": 1097
    },
    {
      "epoch": 7.256505576208179,
      "grad_norm": 16.483638763427734,
      "learning_rate": 0.0009514851485148514,
      "loss": 2.3696,
      "step": 1098
    },
    {
      "epoch": 7.263114415530772,
      "grad_norm": 103.28192901611328,
      "learning_rate": 0.0009509900990099009,
      "loss": 5.2638,
      "step": 1099
    },
    {
      "epoch": 7.269723254853367,
      "grad_norm": 61.060848236083984,
      "learning_rate": 0.0009504950495049505,
      "loss": 4.4409,
      "step": 1100
    },
    {
      "epoch": 7.27633209417596,
      "grad_norm": 132.78504943847656,
      "learning_rate": 0.00095,
      "loss": 10.9545,
      "step": 1101
    },
    {
      "epoch": 7.282940933498554,
      "grad_norm": 107.5558853149414,
      "learning_rate": 0.0009495049504950495,
      "loss": 3.3293,
      "step": 1102
    },
    {
      "epoch": 7.2895497728211485,
      "grad_norm": 84.58040618896484,
      "learning_rate": 0.0009490099009900989,
      "loss": 6.994,
      "step": 1103
    },
    {
      "epoch": 7.296158612143742,
      "grad_norm": 63.67609786987305,
      "learning_rate": 0.0009485148514851485,
      "loss": 6.2319,
      "step": 1104
    },
    {
      "epoch": 7.3027674514663365,
      "grad_norm": 22.0672607421875,
      "learning_rate": 0.000948019801980198,
      "loss": 3.4186,
      "step": 1105
    },
    {
      "epoch": 7.30937629078893,
      "grad_norm": 13.594515800476074,
      "learning_rate": 0.0009475247524752475,
      "loss": 3.4161,
      "step": 1106
    },
    {
      "epoch": 7.315985130111525,
      "grad_norm": 191.32786560058594,
      "learning_rate": 0.000947029702970297,
      "loss": 9.8081,
      "step": 1107
    },
    {
      "epoch": 7.322593969434118,
      "grad_norm": 192.15936279296875,
      "learning_rate": 0.0009465346534653465,
      "loss": 13.3071,
      "step": 1108
    },
    {
      "epoch": 7.329202808756712,
      "grad_norm": 139.6182861328125,
      "learning_rate": 0.000946039603960396,
      "loss": 5.4644,
      "step": 1109
    },
    {
      "epoch": 7.335811648079306,
      "grad_norm": 115.19478607177734,
      "learning_rate": 0.0009455445544554455,
      "loss": 10.2048,
      "step": 1110
    },
    {
      "epoch": 7.3424204874019,
      "grad_norm": 133.14590454101562,
      "learning_rate": 0.000945049504950495,
      "loss": 10.4191,
      "step": 1111
    },
    {
      "epoch": 7.349029326724494,
      "grad_norm": 11.471386909484863,
      "learning_rate": 0.0009445544554455446,
      "loss": 2.6095,
      "step": 1112
    },
    {
      "epoch": 7.355638166047088,
      "grad_norm": 2.9177589416503906,
      "learning_rate": 0.000944059405940594,
      "loss": 2.2803,
      "step": 1113
    },
    {
      "epoch": 7.362247005369682,
      "grad_norm": 17.181838989257812,
      "learning_rate": 0.0009435643564356435,
      "loss": 1.64,
      "step": 1114
    },
    {
      "epoch": 7.368855844692276,
      "grad_norm": 72.33171081542969,
      "learning_rate": 0.000943069306930693,
      "loss": 5.3159,
      "step": 1115
    },
    {
      "epoch": 7.37546468401487,
      "grad_norm": 122.1622543334961,
      "learning_rate": 0.0009425742574257426,
      "loss": 5.249,
      "step": 1116
    },
    {
      "epoch": 7.382073523337464,
      "grad_norm": 156.31561279296875,
      "learning_rate": 0.0009420792079207921,
      "loss": 6.4789,
      "step": 1117
    },
    {
      "epoch": 7.388682362660058,
      "grad_norm": 71.87828826904297,
      "learning_rate": 0.0009415841584158415,
      "loss": 4.9584,
      "step": 1118
    },
    {
      "epoch": 7.395291201982651,
      "grad_norm": 27.5500545501709,
      "learning_rate": 0.000941089108910891,
      "loss": 1.0382,
      "step": 1119
    },
    {
      "epoch": 7.401900041305246,
      "grad_norm": 45.709503173828125,
      "learning_rate": 0.0009405940594059406,
      "loss": 1.7208,
      "step": 1120
    },
    {
      "epoch": 7.408508880627839,
      "grad_norm": 59.99596405029297,
      "learning_rate": 0.0009400990099009901,
      "loss": 4.0142,
      "step": 1121
    },
    {
      "epoch": 7.415117719950434,
      "grad_norm": 55.763946533203125,
      "learning_rate": 0.0009396039603960396,
      "loss": 1.9937,
      "step": 1122
    },
    {
      "epoch": 7.4217265592730275,
      "grad_norm": 76.61412811279297,
      "learning_rate": 0.000939108910891089,
      "loss": 1.9043,
      "step": 1123
    },
    {
      "epoch": 7.428335398595622,
      "grad_norm": 54.55863571166992,
      "learning_rate": 0.0009386138613861386,
      "loss": 1.7563,
      "step": 1124
    },
    {
      "epoch": 7.434944237918216,
      "grad_norm": 30.87622833251953,
      "learning_rate": 0.0009381188118811881,
      "loss": 2.9425,
      "step": 1125
    },
    {
      "epoch": 7.44155307724081,
      "grad_norm": 54.77566909790039,
      "learning_rate": 0.0009376237623762376,
      "loss": 3.4935,
      "step": 1126
    },
    {
      "epoch": 7.448161916563404,
      "grad_norm": 77.55738830566406,
      "learning_rate": 0.0009371287128712872,
      "loss": 3.5452,
      "step": 1127
    },
    {
      "epoch": 7.454770755885997,
      "grad_norm": 63.51996994018555,
      "learning_rate": 0.0009366336633663367,
      "loss": 5.0834,
      "step": 1128
    },
    {
      "epoch": 7.461379595208592,
      "grad_norm": 86.63987731933594,
      "learning_rate": 0.0009361386138613862,
      "loss": 2.7544,
      "step": 1129
    },
    {
      "epoch": 7.467988434531185,
      "grad_norm": 75.5976333618164,
      "learning_rate": 0.0009356435643564357,
      "loss": 5.0897,
      "step": 1130
    },
    {
      "epoch": 7.47459727385378,
      "grad_norm": 43.93575668334961,
      "learning_rate": 0.0009351485148514852,
      "loss": 3.246,
      "step": 1131
    },
    {
      "epoch": 7.481206113176373,
      "grad_norm": 64.34656524658203,
      "learning_rate": 0.0009346534653465348,
      "loss": 3.5483,
      "step": 1132
    },
    {
      "epoch": 7.487814952498967,
      "grad_norm": 14.006722450256348,
      "learning_rate": 0.0009341584158415842,
      "loss": 2.2221,
      "step": 1133
    },
    {
      "epoch": 7.4944237918215615,
      "grad_norm": 156.0224609375,
      "learning_rate": 0.0009336633663366337,
      "loss": 6.597,
      "step": 1134
    },
    {
      "epoch": 7.501032631144155,
      "grad_norm": 86.53233337402344,
      "learning_rate": 0.0009331683168316832,
      "loss": 3.0721,
      "step": 1135
    },
    {
      "epoch": 7.50764147046675,
      "grad_norm": 86.52477264404297,
      "learning_rate": 0.0009326732673267328,
      "loss": 4.5771,
      "step": 1136
    },
    {
      "epoch": 7.514250309789343,
      "grad_norm": 24.71758460998535,
      "learning_rate": 0.0009321782178217823,
      "loss": 2.3399,
      "step": 1137
    },
    {
      "epoch": 7.520859149111937,
      "grad_norm": 30.642353057861328,
      "learning_rate": 0.0009316831683168317,
      "loss": 4.9227,
      "step": 1138
    },
    {
      "epoch": 7.527467988434531,
      "grad_norm": 28.038021087646484,
      "learning_rate": 0.0009311881188118812,
      "loss": 2.4708,
      "step": 1139
    },
    {
      "epoch": 7.534076827757125,
      "grad_norm": 91.57538604736328,
      "learning_rate": 0.0009306930693069308,
      "loss": 5.4109,
      "step": 1140
    },
    {
      "epoch": 7.540685667079719,
      "grad_norm": 18.968839645385742,
      "learning_rate": 0.0009301980198019803,
      "loss": 3.857,
      "step": 1141
    },
    {
      "epoch": 7.547294506402313,
      "grad_norm": 133.1857452392578,
      "learning_rate": 0.0009297029702970298,
      "loss": 3.9081,
      "step": 1142
    },
    {
      "epoch": 7.553903345724907,
      "grad_norm": 83.5099105834961,
      "learning_rate": 0.0009292079207920792,
      "loss": 3.3657,
      "step": 1143
    },
    {
      "epoch": 7.560512185047501,
      "grad_norm": 153.43157958984375,
      "learning_rate": 0.0009287128712871288,
      "loss": 6.7526,
      "step": 1144
    },
    {
      "epoch": 7.5671210243700955,
      "grad_norm": 16.861164093017578,
      "learning_rate": 0.0009282178217821783,
      "loss": 1.0168,
      "step": 1145
    },
    {
      "epoch": 7.573729863692689,
      "grad_norm": 10.34142017364502,
      "learning_rate": 0.0009277227722772278,
      "loss": 5.7765,
      "step": 1146
    },
    {
      "epoch": 7.580338703015283,
      "grad_norm": 46.49338150024414,
      "learning_rate": 0.0009272277227722773,
      "loss": 2.4331,
      "step": 1147
    },
    {
      "epoch": 7.586947542337877,
      "grad_norm": 8.72353458404541,
      "learning_rate": 0.0009267326732673268,
      "loss": 2.685,
      "step": 1148
    },
    {
      "epoch": 7.593556381660471,
      "grad_norm": 48.50528335571289,
      "learning_rate": 0.0009262376237623763,
      "loss": 3.9968,
      "step": 1149
    },
    {
      "epoch": 7.600165220983065,
      "grad_norm": 28.352872848510742,
      "learning_rate": 0.0009257425742574258,
      "loss": 4.6826,
      "step": 1150
    },
    {
      "epoch": 7.606774060305659,
      "grad_norm": 108.88326263427734,
      "learning_rate": 0.0009252475247524753,
      "loss": 7.4105,
      "step": 1151
    },
    {
      "epoch": 7.613382899628252,
      "grad_norm": 88.14212036132812,
      "learning_rate": 0.0009247524752475249,
      "loss": 4.0288,
      "step": 1152
    },
    {
      "epoch": 7.619991738950847,
      "grad_norm": 124.12454223632812,
      "learning_rate": 0.0009242574257425743,
      "loss": 7.3439,
      "step": 1153
    },
    {
      "epoch": 7.6266005782734405,
      "grad_norm": 20.03642463684082,
      "learning_rate": 0.0009237623762376238,
      "loss": 3.8735,
      "step": 1154
    },
    {
      "epoch": 7.633209417596035,
      "grad_norm": 7.944656848907471,
      "learning_rate": 0.0009232673267326733,
      "loss": 2.9843,
      "step": 1155
    },
    {
      "epoch": 7.639818256918629,
      "grad_norm": 33.09815979003906,
      "learning_rate": 0.0009227722772277229,
      "loss": 4.6175,
      "step": 1156
    },
    {
      "epoch": 7.646427096241222,
      "grad_norm": 183.6685791015625,
      "learning_rate": 0.0009222772277227724,
      "loss": 11.0804,
      "step": 1157
    },
    {
      "epoch": 7.653035935563817,
      "grad_norm": 159.70643615722656,
      "learning_rate": 0.0009217821782178218,
      "loss": 8.3878,
      "step": 1158
    },
    {
      "epoch": 7.65964477488641,
      "grad_norm": 75.91881561279297,
      "learning_rate": 0.0009212871287128713,
      "loss": 5.2953,
      "step": 1159
    },
    {
      "epoch": 7.666253614209005,
      "grad_norm": 30.70538330078125,
      "learning_rate": 0.0009207920792079209,
      "loss": 4.2329,
      "step": 1160
    },
    {
      "epoch": 7.672862453531598,
      "grad_norm": 66.59880828857422,
      "learning_rate": 0.0009202970297029704,
      "loss": 9.2447,
      "step": 1161
    },
    {
      "epoch": 7.679471292854193,
      "grad_norm": 40.077083587646484,
      "learning_rate": 0.0009198019801980199,
      "loss": 1.4848,
      "step": 1162
    },
    {
      "epoch": 7.686080132176786,
      "grad_norm": 102.05989074707031,
      "learning_rate": 0.0009193069306930693,
      "loss": 8.254,
      "step": 1163
    },
    {
      "epoch": 7.692688971499381,
      "grad_norm": 5.848964214324951,
      "learning_rate": 0.0009188118811881188,
      "loss": 7.0885,
      "step": 1164
    },
    {
      "epoch": 7.6992978108219745,
      "grad_norm": 105.85005187988281,
      "learning_rate": 0.0009183168316831684,
      "loss": 5.1001,
      "step": 1165
    },
    {
      "epoch": 7.705906650144568,
      "grad_norm": 26.608049392700195,
      "learning_rate": 0.0009178217821782179,
      "loss": 3.51,
      "step": 1166
    },
    {
      "epoch": 7.712515489467163,
      "grad_norm": 92.90756225585938,
      "learning_rate": 0.0009173267326732674,
      "loss": 2.5016,
      "step": 1167
    },
    {
      "epoch": 7.719124328789756,
      "grad_norm": 43.31144714355469,
      "learning_rate": 0.0009168316831683168,
      "loss": 2.33,
      "step": 1168
    },
    {
      "epoch": 7.725733168112351,
      "grad_norm": 14.286528587341309,
      "learning_rate": 0.0009163366336633664,
      "loss": 2.2475,
      "step": 1169
    },
    {
      "epoch": 7.732342007434944,
      "grad_norm": 13.747246742248535,
      "learning_rate": 0.0009158415841584159,
      "loss": 3.7212,
      "step": 1170
    },
    {
      "epoch": 7.738950846757538,
      "grad_norm": 8.960413932800293,
      "learning_rate": 0.0009153465346534654,
      "loss": 2.9134,
      "step": 1171
    },
    {
      "epoch": 7.745559686080132,
      "grad_norm": 89.36597442626953,
      "learning_rate": 0.000914851485148515,
      "loss": 3.3079,
      "step": 1172
    },
    {
      "epoch": 7.752168525402726,
      "grad_norm": 95.03166198730469,
      "learning_rate": 0.0009143564356435644,
      "loss": 2.6828,
      "step": 1173
    },
    {
      "epoch": 7.75877736472532,
      "grad_norm": 32.11909866333008,
      "learning_rate": 0.0009138613861386139,
      "loss": 5.9826,
      "step": 1174
    },
    {
      "epoch": 7.765386204047914,
      "grad_norm": 37.19668197631836,
      "learning_rate": 0.0009133663366336634,
      "loss": 3.4992,
      "step": 1175
    },
    {
      "epoch": 7.771995043370508,
      "grad_norm": 66.48888397216797,
      "learning_rate": 0.0009128712871287129,
      "loss": 2.6855,
      "step": 1176
    },
    {
      "epoch": 7.778603882693102,
      "grad_norm": 75.7616958618164,
      "learning_rate": 0.0009123762376237625,
      "loss": 9.3839,
      "step": 1177
    },
    {
      "epoch": 7.785212722015696,
      "grad_norm": 93.72834777832031,
      "learning_rate": 0.0009118811881188119,
      "loss": 3.1204,
      "step": 1178
    },
    {
      "epoch": 7.79182156133829,
      "grad_norm": 110.02208709716797,
      "learning_rate": 0.0009113861386138614,
      "loss": 5.4866,
      "step": 1179
    },
    {
      "epoch": 7.798430400660884,
      "grad_norm": 11.489439964294434,
      "learning_rate": 0.0009108910891089109,
      "loss": 1.5602,
      "step": 1180
    },
    {
      "epoch": 7.805039239983478,
      "grad_norm": 54.12522506713867,
      "learning_rate": 0.0009103960396039605,
      "loss": 2.8667,
      "step": 1181
    },
    {
      "epoch": 7.811648079306072,
      "grad_norm": 10.926434516906738,
      "learning_rate": 0.00090990099009901,
      "loss": 3.3647,
      "step": 1182
    },
    {
      "epoch": 7.8182569186286655,
      "grad_norm": 54.36850357055664,
      "learning_rate": 0.0009094059405940594,
      "loss": 2.4164,
      "step": 1183
    },
    {
      "epoch": 7.82486575795126,
      "grad_norm": 66.96129608154297,
      "learning_rate": 0.0009089108910891089,
      "loss": 3.0421,
      "step": 1184
    },
    {
      "epoch": 7.8314745972738535,
      "grad_norm": 80.09681701660156,
      "learning_rate": 0.0009084158415841585,
      "loss": 4.2823,
      "step": 1185
    },
    {
      "epoch": 7.838083436596448,
      "grad_norm": 71.24142456054688,
      "learning_rate": 0.000907920792079208,
      "loss": 6.0878,
      "step": 1186
    },
    {
      "epoch": 7.844692275919042,
      "grad_norm": 64.96395111083984,
      "learning_rate": 0.0009074257425742575,
      "loss": 2.6352,
      "step": 1187
    },
    {
      "epoch": 7.851301115241636,
      "grad_norm": 18.14093589782715,
      "learning_rate": 0.0009069306930693069,
      "loss": 2.6974,
      "step": 1188
    },
    {
      "epoch": 7.85790995456423,
      "grad_norm": 65.47784423828125,
      "learning_rate": 0.0009064356435643565,
      "loss": 1.943,
      "step": 1189
    },
    {
      "epoch": 7.864518793886823,
      "grad_norm": 43.60243225097656,
      "learning_rate": 0.000905940594059406,
      "loss": 2.6581,
      "step": 1190
    },
    {
      "epoch": 7.871127633209418,
      "grad_norm": 16.527700424194336,
      "learning_rate": 0.0009054455445544555,
      "loss": 3.0936,
      "step": 1191
    },
    {
      "epoch": 7.877736472532011,
      "grad_norm": 34.50596237182617,
      "learning_rate": 0.000904950495049505,
      "loss": 2.5111,
      "step": 1192
    },
    {
      "epoch": 7.884345311854606,
      "grad_norm": 29.673601150512695,
      "learning_rate": 0.0009044554455445545,
      "loss": 2.9028,
      "step": 1193
    },
    {
      "epoch": 7.8909541511771994,
      "grad_norm": 40.72050094604492,
      "learning_rate": 0.000903960396039604,
      "loss": 1.379,
      "step": 1194
    },
    {
      "epoch": 7.897562990499793,
      "grad_norm": 78.28426361083984,
      "learning_rate": 0.0009034653465346535,
      "loss": 1.4795,
      "step": 1195
    },
    {
      "epoch": 7.9041718298223875,
      "grad_norm": 14.07768726348877,
      "learning_rate": 0.000902970297029703,
      "loss": 2.5736,
      "step": 1196
    },
    {
      "epoch": 7.910780669144981,
      "grad_norm": 2.285792827606201,
      "learning_rate": 0.0009024752475247526,
      "loss": 1.2699,
      "step": 1197
    },
    {
      "epoch": 7.917389508467576,
      "grad_norm": 30.092966079711914,
      "learning_rate": 0.000901980198019802,
      "loss": 3.0378,
      "step": 1198
    },
    {
      "epoch": 7.923998347790169,
      "grad_norm": 15.298738479614258,
      "learning_rate": 0.0009014851485148515,
      "loss": 6.1125,
      "step": 1199
    },
    {
      "epoch": 7.930607187112764,
      "grad_norm": 23.172277450561523,
      "learning_rate": 0.000900990099009901,
      "loss": 3.8756,
      "step": 1200
    },
    {
      "epoch": 7.937216026435357,
      "grad_norm": 112.81214141845703,
      "learning_rate": 0.0009004950495049506,
      "loss": 4.0804,
      "step": 1201
    },
    {
      "epoch": 7.943824865757951,
      "grad_norm": 73.21605682373047,
      "learning_rate": 0.0009000000000000001,
      "loss": 3.0052,
      "step": 1202
    },
    {
      "epoch": 7.950433705080545,
      "grad_norm": 21.181224822998047,
      "learning_rate": 0.0008995049504950495,
      "loss": 1.9324,
      "step": 1203
    },
    {
      "epoch": 7.957042544403139,
      "grad_norm": 16.376340866088867,
      "learning_rate": 0.000899009900990099,
      "loss": 2.0211,
      "step": 1204
    },
    {
      "epoch": 7.9636513837257334,
      "grad_norm": 28.247753143310547,
      "learning_rate": 0.0008985148514851486,
      "loss": 1.4898,
      "step": 1205
    },
    {
      "epoch": 7.970260223048327,
      "grad_norm": 10.634356498718262,
      "learning_rate": 0.0008980198019801981,
      "loss": 4.0855,
      "step": 1206
    },
    {
      "epoch": 7.9768690623709215,
      "grad_norm": 4.4315505027771,
      "learning_rate": 0.0008975247524752476,
      "loss": 1.5426,
      "step": 1207
    },
    {
      "epoch": 7.983477901693515,
      "grad_norm": 43.75382995605469,
      "learning_rate": 0.000897029702970297,
      "loss": 3.3161,
      "step": 1208
    },
    {
      "epoch": 7.990086741016109,
      "grad_norm": 49.527584075927734,
      "learning_rate": 0.0008965346534653466,
      "loss": 2.4301,
      "step": 1209
    },
    {
      "epoch": 7.996695580338703,
      "grad_norm": 98.41478729248047,
      "learning_rate": 0.0008960396039603961,
      "loss": 4.9758,
      "step": 1210
    },
    {
      "epoch": 7.996695580338703,
      "eval_validation_error_bar": 0.05990148869301598,
      "eval_validation_loss": 7.222747802734375,
      "eval_validation_pearsonr": 0.4733834776856938,
      "eval_validation_rmse": 2.687516927719116,
      "eval_validation_runtime": 33.6603,
      "eval_validation_samples_per_second": 6.031,
      "eval_validation_spearman": 0.4365177162548449,
      "eval_validation_steps_per_second": 6.031,
      "step": 1210
    },
    {
      "epoch": 7.996695580338703,
      "eval_test_error_bar": 0.04659364870481985,
      "eval_test_loss": 9.269906044006348,
      "eval_test_pearsonr": 0.3978602794692412,
      "eval_test_rmse": 3.044651985168457,
      "eval_test_runtime": 39.1212,
      "eval_test_samples_per_second": 8.333,
      "eval_test_spearman": 0.4493656209482403,
      "eval_test_steps_per_second": 8.333,
      "step": 1210
    },
    {
      "epoch": 8.003304419661298,
      "grad_norm": 42.29468536376953,
      "learning_rate": 0.0008955445544554456,
      "loss": 2.448,
      "step": 1211
    },
    {
      "epoch": 8.009913258983891,
      "grad_norm": 55.81739807128906,
      "learning_rate": 0.0008950495049504951,
      "loss": 2.5187,
      "step": 1212
    },
    {
      "epoch": 8.016522098306485,
      "grad_norm": 26.114545822143555,
      "learning_rate": 0.0008945544554455445,
      "loss": 1.7727,
      "step": 1213
    },
    {
      "epoch": 8.023130937629078,
      "grad_norm": 28.305850982666016,
      "learning_rate": 0.0008940594059405941,
      "loss": 1.0417,
      "step": 1214
    },
    {
      "epoch": 8.029739776951672,
      "grad_norm": 9.439226150512695,
      "learning_rate": 0.0008935643564356436,
      "loss": 6.1822,
      "step": 1215
    },
    {
      "epoch": 8.036348616274267,
      "grad_norm": 59.898189544677734,
      "learning_rate": 0.0008930693069306931,
      "loss": 3.453,
      "step": 1216
    },
    {
      "epoch": 8.042957455596861,
      "grad_norm": 27.91861915588379,
      "learning_rate": 0.0008925742574257427,
      "loss": 0.4305,
      "step": 1217
    },
    {
      "epoch": 8.049566294919455,
      "grad_norm": 41.08014678955078,
      "learning_rate": 0.0008920792079207921,
      "loss": 3.9794,
      "step": 1218
    },
    {
      "epoch": 8.056175134242048,
      "grad_norm": 29.911060333251953,
      "learning_rate": 0.0008915841584158416,
      "loss": 2.1374,
      "step": 1219
    },
    {
      "epoch": 8.062783973564642,
      "grad_norm": 36.654972076416016,
      "learning_rate": 0.0008910891089108911,
      "loss": 1.9214,
      "step": 1220
    },
    {
      "epoch": 8.069392812887237,
      "grad_norm": 24.453754425048828,
      "learning_rate": 0.0008905940594059406,
      "loss": 2.6742,
      "step": 1221
    },
    {
      "epoch": 8.07600165220983,
      "grad_norm": 33.65746307373047,
      "learning_rate": 0.0008900990099009902,
      "loss": 6.8244,
      "step": 1222
    },
    {
      "epoch": 8.082610491532424,
      "grad_norm": 20.056468963623047,
      "learning_rate": 0.0008896039603960396,
      "loss": 3.6541,
      "step": 1223
    },
    {
      "epoch": 8.089219330855018,
      "grad_norm": 7.35847806930542,
      "learning_rate": 0.0008891089108910891,
      "loss": 2.18,
      "step": 1224
    },
    {
      "epoch": 8.095828170177613,
      "grad_norm": 31.52103614807129,
      "learning_rate": 0.0008886138613861386,
      "loss": 2.1291,
      "step": 1225
    },
    {
      "epoch": 8.102437009500207,
      "grad_norm": 58.36952209472656,
      "learning_rate": 0.0008881188118811882,
      "loss": 7.2896,
      "step": 1226
    },
    {
      "epoch": 8.1090458488228,
      "grad_norm": 25.185007095336914,
      "learning_rate": 0.0008876237623762377,
      "loss": 4.0608,
      "step": 1227
    },
    {
      "epoch": 8.115654688145394,
      "grad_norm": 21.333520889282227,
      "learning_rate": 0.0008871287128712871,
      "loss": 1.8344,
      "step": 1228
    },
    {
      "epoch": 8.122263527467988,
      "grad_norm": 14.04853343963623,
      "learning_rate": 0.0008866336633663366,
      "loss": 2.7333,
      "step": 1229
    },
    {
      "epoch": 8.128872366790583,
      "grad_norm": 66.83182525634766,
      "learning_rate": 0.0008861386138613862,
      "loss": 3.3524,
      "step": 1230
    },
    {
      "epoch": 8.135481206113177,
      "grad_norm": 27.716339111328125,
      "learning_rate": 0.0008856435643564357,
      "loss": 2.5787,
      "step": 1231
    },
    {
      "epoch": 8.14209004543577,
      "grad_norm": 19.890657424926758,
      "learning_rate": 0.0008851485148514852,
      "loss": 2.3883,
      "step": 1232
    },
    {
      "epoch": 8.148698884758364,
      "grad_norm": 39.684173583984375,
      "learning_rate": 0.0008846534653465346,
      "loss": 2.301,
      "step": 1233
    },
    {
      "epoch": 8.155307724080957,
      "grad_norm": 10.90774917602539,
      "learning_rate": 0.0008841584158415842,
      "loss": 0.974,
      "step": 1234
    },
    {
      "epoch": 8.161916563403553,
      "grad_norm": 71.74696350097656,
      "learning_rate": 0.0008836633663366337,
      "loss": 4.3313,
      "step": 1235
    },
    {
      "epoch": 8.168525402726146,
      "grad_norm": 30.734485626220703,
      "learning_rate": 0.0008831683168316832,
      "loss": 3.2299,
      "step": 1236
    },
    {
      "epoch": 8.17513424204874,
      "grad_norm": 24.99168586730957,
      "learning_rate": 0.0008826732673267327,
      "loss": 3.0785,
      "step": 1237
    },
    {
      "epoch": 8.181743081371334,
      "grad_norm": 17.97743034362793,
      "learning_rate": 0.0008821782178217822,
      "loss": 2.3478,
      "step": 1238
    },
    {
      "epoch": 8.188351920693927,
      "grad_norm": 12.224428176879883,
      "learning_rate": 0.0008816831683168317,
      "loss": 1.7965,
      "step": 1239
    },
    {
      "epoch": 8.194960760016523,
      "grad_norm": 17.579858779907227,
      "learning_rate": 0.0008811881188118812,
      "loss": 6.1336,
      "step": 1240
    },
    {
      "epoch": 8.201569599339116,
      "grad_norm": 20.863706588745117,
      "learning_rate": 0.0008806930693069307,
      "loss": 1.0691,
      "step": 1241
    },
    {
      "epoch": 8.20817843866171,
      "grad_norm": 37.546791076660156,
      "learning_rate": 0.0008801980198019803,
      "loss": 1.4596,
      "step": 1242
    },
    {
      "epoch": 8.214787277984303,
      "grad_norm": 67.4808120727539,
      "learning_rate": 0.0008797029702970297,
      "loss": 2.3157,
      "step": 1243
    },
    {
      "epoch": 8.221396117306899,
      "grad_norm": 67.88327026367188,
      "learning_rate": 0.0008792079207920792,
      "loss": 5.7454,
      "step": 1244
    },
    {
      "epoch": 8.228004956629492,
      "grad_norm": 30.898483276367188,
      "learning_rate": 0.0008787128712871287,
      "loss": 1.761,
      "step": 1245
    },
    {
      "epoch": 8.234613795952086,
      "grad_norm": 3.661832094192505,
      "learning_rate": 0.0008782178217821783,
      "loss": 5.5662,
      "step": 1246
    },
    {
      "epoch": 8.24122263527468,
      "grad_norm": 41.044525146484375,
      "learning_rate": 0.0008777227722772278,
      "loss": 1.2703,
      "step": 1247
    },
    {
      "epoch": 8.247831474597273,
      "grad_norm": 50.87030029296875,
      "learning_rate": 0.0008772277227722772,
      "loss": 1.1753,
      "step": 1248
    },
    {
      "epoch": 8.254440313919869,
      "grad_norm": 20.98345184326172,
      "learning_rate": 0.0008767326732673267,
      "loss": 1.4823,
      "step": 1249
    },
    {
      "epoch": 8.261049153242462,
      "grad_norm": 37.997657775878906,
      "learning_rate": 0.0008762376237623763,
      "loss": 4.1078,
      "step": 1250
    },
    {
      "epoch": 8.267657992565056,
      "grad_norm": 12.749436378479004,
      "learning_rate": 0.0008757425742574258,
      "loss": 4.4689,
      "step": 1251
    },
    {
      "epoch": 8.27426683188765,
      "grad_norm": 21.946269989013672,
      "learning_rate": 0.0008752475247524753,
      "loss": 4.2745,
      "step": 1252
    },
    {
      "epoch": 8.280875671210243,
      "grad_norm": 35.8050651550293,
      "learning_rate": 0.0008747524752475247,
      "loss": 5.1499,
      "step": 1253
    },
    {
      "epoch": 8.287484510532838,
      "grad_norm": 57.934471130371094,
      "learning_rate": 0.0008742574257425743,
      "loss": 4.2884,
      "step": 1254
    },
    {
      "epoch": 8.294093349855432,
      "grad_norm": 56.76875686645508,
      "learning_rate": 0.0008737623762376238,
      "loss": 3.9966,
      "step": 1255
    },
    {
      "epoch": 8.300702189178025,
      "grad_norm": 3.3378822803497314,
      "learning_rate": 0.0008732673267326733,
      "loss": 3.7583,
      "step": 1256
    },
    {
      "epoch": 8.307311028500619,
      "grad_norm": 23.299034118652344,
      "learning_rate": 0.0008727722772277228,
      "loss": 3.5335,
      "step": 1257
    },
    {
      "epoch": 8.313919867823213,
      "grad_norm": 44.0013427734375,
      "learning_rate": 0.0008722772277227722,
      "loss": 2.1825,
      "step": 1258
    },
    {
      "epoch": 8.320528707145808,
      "grad_norm": 62.930992126464844,
      "learning_rate": 0.0008717821782178218,
      "loss": 2.3497,
      "step": 1259
    },
    {
      "epoch": 8.327137546468402,
      "grad_norm": 29.71891975402832,
      "learning_rate": 0.0008712871287128713,
      "loss": 1.8682,
      "step": 1260
    },
    {
      "epoch": 8.333746385790995,
      "grad_norm": 52.87635803222656,
      "learning_rate": 0.0008707920792079208,
      "loss": 3.9251,
      "step": 1261
    },
    {
      "epoch": 8.340355225113589,
      "grad_norm": 74.2187271118164,
      "learning_rate": 0.0008702970297029704,
      "loss": 5.8404,
      "step": 1262
    },
    {
      "epoch": 8.346964064436184,
      "grad_norm": 58.508602142333984,
      "learning_rate": 0.0008698019801980198,
      "loss": 2.9355,
      "step": 1263
    },
    {
      "epoch": 8.353572903758778,
      "grad_norm": 42.707862854003906,
      "learning_rate": 0.0008693069306930693,
      "loss": 6.7021,
      "step": 1264
    },
    {
      "epoch": 8.360181743081371,
      "grad_norm": 38.311912536621094,
      "learning_rate": 0.0008688118811881188,
      "loss": 2.5312,
      "step": 1265
    },
    {
      "epoch": 8.366790582403965,
      "grad_norm": 36.2779426574707,
      "learning_rate": 0.0008683168316831684,
      "loss": 3.9895,
      "step": 1266
    },
    {
      "epoch": 8.373399421726559,
      "grad_norm": 100.53809356689453,
      "learning_rate": 0.0008678217821782179,
      "loss": 3.2978,
      "step": 1267
    },
    {
      "epoch": 8.380008261049154,
      "grad_norm": 10.777597427368164,
      "learning_rate": 0.0008673267326732673,
      "loss": 7.9112,
      "step": 1268
    },
    {
      "epoch": 8.386617100371748,
      "grad_norm": 37.17796325683594,
      "learning_rate": 0.0008668316831683168,
      "loss": 1.4928,
      "step": 1269
    },
    {
      "epoch": 8.393225939694341,
      "grad_norm": 30.646448135375977,
      "learning_rate": 0.0008663366336633663,
      "loss": 3.3476,
      "step": 1270
    },
    {
      "epoch": 8.399834779016935,
      "grad_norm": 23.37578010559082,
      "learning_rate": 0.0008658415841584159,
      "loss": 4.4683,
      "step": 1271
    },
    {
      "epoch": 8.406443618339528,
      "grad_norm": 11.554094314575195,
      "learning_rate": 0.0008653465346534654,
      "loss": 2.6224,
      "step": 1272
    },
    {
      "epoch": 8.413052457662124,
      "grad_norm": 51.00938415527344,
      "learning_rate": 0.0008648514851485148,
      "loss": 4.7532,
      "step": 1273
    },
    {
      "epoch": 8.419661296984717,
      "grad_norm": 84.3720474243164,
      "learning_rate": 0.0008643564356435643,
      "loss": 2.7348,
      "step": 1274
    },
    {
      "epoch": 8.426270136307311,
      "grad_norm": 49.15500259399414,
      "learning_rate": 0.0008638613861386139,
      "loss": 4.1152,
      "step": 1275
    },
    {
      "epoch": 8.432878975629905,
      "grad_norm": 28.06789779663086,
      "learning_rate": 0.0008633663366336634,
      "loss": 1.8583,
      "step": 1276
    },
    {
      "epoch": 8.439487814952498,
      "grad_norm": 6.269429683685303,
      "learning_rate": 0.0008628712871287129,
      "loss": 2.4918,
      "step": 1277
    },
    {
      "epoch": 8.446096654275093,
      "grad_norm": 26.603363037109375,
      "learning_rate": 0.0008623762376237623,
      "loss": 1.95,
      "step": 1278
    },
    {
      "epoch": 8.452705493597687,
      "grad_norm": 37.86461639404297,
      "learning_rate": 0.0008618811881188119,
      "loss": 2.9005,
      "step": 1279
    },
    {
      "epoch": 8.45931433292028,
      "grad_norm": 8.822689056396484,
      "learning_rate": 0.0008613861386138614,
      "loss": 1.3904,
      "step": 1280
    },
    {
      "epoch": 8.465923172242874,
      "grad_norm": 56.60968017578125,
      "learning_rate": 0.0008608910891089109,
      "loss": 3.0605,
      "step": 1281
    },
    {
      "epoch": 8.47253201156547,
      "grad_norm": 56.97211456298828,
      "learning_rate": 0.0008603960396039604,
      "loss": 2.8889,
      "step": 1282
    },
    {
      "epoch": 8.479140850888063,
      "grad_norm": 53.980979919433594,
      "learning_rate": 0.0008599009900990099,
      "loss": 2.8931,
      "step": 1283
    },
    {
      "epoch": 8.485749690210657,
      "grad_norm": 47.439308166503906,
      "learning_rate": 0.0008594059405940594,
      "loss": 2.6038,
      "step": 1284
    },
    {
      "epoch": 8.49235852953325,
      "grad_norm": 14.191790580749512,
      "learning_rate": 0.0008589108910891089,
      "loss": 2.0285,
      "step": 1285
    },
    {
      "epoch": 8.498967368855844,
      "grad_norm": 44.72404479980469,
      "learning_rate": 0.0008584158415841584,
      "loss": 3.1982,
      "step": 1286
    },
    {
      "epoch": 8.50557620817844,
      "grad_norm": 11.319068908691406,
      "learning_rate": 0.000857920792079208,
      "loss": 3.0999,
      "step": 1287
    },
    {
      "epoch": 8.512185047501033,
      "grad_norm": 69.51909637451172,
      "learning_rate": 0.0008574257425742574,
      "loss": 4.8597,
      "step": 1288
    },
    {
      "epoch": 8.518793886823627,
      "grad_norm": 71.04581451416016,
      "learning_rate": 0.0008569306930693069,
      "loss": 3.9597,
      "step": 1289
    },
    {
      "epoch": 8.52540272614622,
      "grad_norm": 4.334019660949707,
      "learning_rate": 0.0008564356435643564,
      "loss": 5.9676,
      "step": 1290
    },
    {
      "epoch": 8.532011565468814,
      "grad_norm": 25.10134506225586,
      "learning_rate": 0.000855940594059406,
      "loss": 1.6866,
      "step": 1291
    },
    {
      "epoch": 8.53862040479141,
      "grad_norm": 32.62464904785156,
      "learning_rate": 0.0008554455445544555,
      "loss": 6.1199,
      "step": 1292
    },
    {
      "epoch": 8.545229244114003,
      "grad_norm": 68.6829833984375,
      "learning_rate": 0.0008549504950495049,
      "loss": 4.3806,
      "step": 1293
    },
    {
      "epoch": 8.551838083436596,
      "grad_norm": 61.35932540893555,
      "learning_rate": 0.0008544554455445544,
      "loss": 4.5568,
      "step": 1294
    },
    {
      "epoch": 8.55844692275919,
      "grad_norm": 44.04335403442383,
      "learning_rate": 0.000853960396039604,
      "loss": 3.2808,
      "step": 1295
    },
    {
      "epoch": 8.565055762081784,
      "grad_norm": 5.837870121002197,
      "learning_rate": 0.0008534653465346535,
      "loss": 2.0889,
      "step": 1296
    },
    {
      "epoch": 8.571664601404379,
      "grad_norm": 13.58509635925293,
      "learning_rate": 0.000852970297029703,
      "loss": 1.7707,
      "step": 1297
    },
    {
      "epoch": 8.578273440726973,
      "grad_norm": 53.34494400024414,
      "learning_rate": 0.0008524752475247524,
      "loss": 3.5894,
      "step": 1298
    },
    {
      "epoch": 8.584882280049566,
      "grad_norm": 60.40747833251953,
      "learning_rate": 0.000851980198019802,
      "loss": 2.0489,
      "step": 1299
    },
    {
      "epoch": 8.59149111937216,
      "grad_norm": 8.301854133605957,
      "learning_rate": 0.0008514851485148515,
      "loss": 5.266,
      "step": 1300
    },
    {
      "epoch": 8.598099958694753,
      "grad_norm": 26.7348575592041,
      "learning_rate": 0.000850990099009901,
      "loss": 2.1603,
      "step": 1301
    },
    {
      "epoch": 8.604708798017349,
      "grad_norm": 38.05387878417969,
      "learning_rate": 0.0008504950495049505,
      "loss": 1.9913,
      "step": 1302
    },
    {
      "epoch": 8.611317637339942,
      "grad_norm": 48.8467903137207,
      "learning_rate": 0.00085,
      "loss": 1.7491,
      "step": 1303
    },
    {
      "epoch": 8.617926476662536,
      "grad_norm": 37.366065979003906,
      "learning_rate": 0.0008495049504950495,
      "loss": 2.3936,
      "step": 1304
    },
    {
      "epoch": 8.62453531598513,
      "grad_norm": 25.477191925048828,
      "learning_rate": 0.000849009900990099,
      "loss": 2.8512,
      "step": 1305
    },
    {
      "epoch": 8.631144155307725,
      "grad_norm": 58.225650787353516,
      "learning_rate": 0.0008485148514851485,
      "loss": 5.2101,
      "step": 1306
    },
    {
      "epoch": 8.637752994630318,
      "grad_norm": 40.35709762573242,
      "learning_rate": 0.0008480198019801981,
      "loss": 5.1723,
      "step": 1307
    },
    {
      "epoch": 8.644361833952912,
      "grad_norm": 97.68840789794922,
      "learning_rate": 0.0008475247524752475,
      "loss": 4.107,
      "step": 1308
    },
    {
      "epoch": 8.650970673275506,
      "grad_norm": 73.26724243164062,
      "learning_rate": 0.000847029702970297,
      "loss": 3.9382,
      "step": 1309
    },
    {
      "epoch": 8.6575795125981,
      "grad_norm": 53.345245361328125,
      "learning_rate": 0.0008465346534653465,
      "loss": 7.1053,
      "step": 1310
    },
    {
      "epoch": 8.664188351920695,
      "grad_norm": 85.36642456054688,
      "learning_rate": 0.000846039603960396,
      "loss": 3.561,
      "step": 1311
    },
    {
      "epoch": 8.670797191243288,
      "grad_norm": 69.83601379394531,
      "learning_rate": 0.0008455445544554456,
      "loss": 4.5687,
      "step": 1312
    },
    {
      "epoch": 8.677406030565882,
      "grad_norm": 97.19466400146484,
      "learning_rate": 0.000845049504950495,
      "loss": 3.219,
      "step": 1313
    },
    {
      "epoch": 8.684014869888475,
      "grad_norm": 38.4555549621582,
      "learning_rate": 0.0008445544554455445,
      "loss": 3.3736,
      "step": 1314
    },
    {
      "epoch": 8.69062370921107,
      "grad_norm": 24.920578002929688,
      "learning_rate": 0.000844059405940594,
      "loss": 0.9116,
      "step": 1315
    },
    {
      "epoch": 8.697232548533664,
      "grad_norm": 6.378538608551025,
      "learning_rate": 0.0008435643564356436,
      "loss": 1.9897,
      "step": 1316
    },
    {
      "epoch": 8.703841387856258,
      "grad_norm": 40.712642669677734,
      "learning_rate": 0.0008430693069306931,
      "loss": 4.0221,
      "step": 1317
    },
    {
      "epoch": 8.710450227178852,
      "grad_norm": 60.9165153503418,
      "learning_rate": 0.0008425742574257425,
      "loss": 2.1942,
      "step": 1318
    },
    {
      "epoch": 8.717059066501445,
      "grad_norm": 24.532880783081055,
      "learning_rate": 0.000842079207920792,
      "loss": 2.214,
      "step": 1319
    },
    {
      "epoch": 8.72366790582404,
      "grad_norm": 109.56123352050781,
      "learning_rate": 0.0008415841584158416,
      "loss": 3.5928,
      "step": 1320
    },
    {
      "epoch": 8.730276745146634,
      "grad_norm": 49.47329330444336,
      "learning_rate": 0.0008410891089108911,
      "loss": 3.6488,
      "step": 1321
    },
    {
      "epoch": 8.736885584469228,
      "grad_norm": 51.755828857421875,
      "learning_rate": 0.0008405940594059406,
      "loss": 4.6249,
      "step": 1322
    },
    {
      "epoch": 8.743494423791821,
      "grad_norm": 34.7579460144043,
      "learning_rate": 0.00084009900990099,
      "loss": 2.5576,
      "step": 1323
    },
    {
      "epoch": 8.750103263114415,
      "grad_norm": 21.67481803894043,
      "learning_rate": 0.0008396039603960396,
      "loss": 1.3331,
      "step": 1324
    },
    {
      "epoch": 8.75671210243701,
      "grad_norm": 32.06697082519531,
      "learning_rate": 0.0008391089108910891,
      "loss": 1.439,
      "step": 1325
    },
    {
      "epoch": 8.763320941759604,
      "grad_norm": 19.621549606323242,
      "learning_rate": 0.0008386138613861386,
      "loss": 4.6945,
      "step": 1326
    },
    {
      "epoch": 8.769929781082197,
      "grad_norm": 3.9051573276519775,
      "learning_rate": 0.0008381188118811881,
      "loss": 3.9734,
      "step": 1327
    },
    {
      "epoch": 8.776538620404791,
      "grad_norm": 45.29432678222656,
      "learning_rate": 0.0008376237623762376,
      "loss": 3.3574,
      "step": 1328
    },
    {
      "epoch": 8.783147459727385,
      "grad_norm": 57.46285629272461,
      "learning_rate": 0.0008371287128712871,
      "loss": 2.3638,
      "step": 1329
    },
    {
      "epoch": 8.78975629904998,
      "grad_norm": 3.9887866973876953,
      "learning_rate": 0.0008366336633663366,
      "loss": 3.6264,
      "step": 1330
    },
    {
      "epoch": 8.796365138372574,
      "grad_norm": 39.457088470458984,
      "learning_rate": 0.0008361386138613861,
      "loss": 2.0137,
      "step": 1331
    },
    {
      "epoch": 8.802973977695167,
      "grad_norm": 16.976388931274414,
      "learning_rate": 0.0008356435643564357,
      "loss": 2.0791,
      "step": 1332
    },
    {
      "epoch": 8.80958281701776,
      "grad_norm": 12.172575950622559,
      "learning_rate": 0.0008351485148514851,
      "loss": 1.6552,
      "step": 1333
    },
    {
      "epoch": 8.816191656340354,
      "grad_norm": 48.076725006103516,
      "learning_rate": 0.0008346534653465346,
      "loss": 5.1543,
      "step": 1334
    },
    {
      "epoch": 8.82280049566295,
      "grad_norm": 54.74911117553711,
      "learning_rate": 0.0008341584158415841,
      "loss": 2.7653,
      "step": 1335
    },
    {
      "epoch": 8.829409334985543,
      "grad_norm": 26.12548065185547,
      "learning_rate": 0.0008336633663366337,
      "loss": 4.6826,
      "step": 1336
    },
    {
      "epoch": 8.836018174308137,
      "grad_norm": 74.13733673095703,
      "learning_rate": 0.0008331683168316832,
      "loss": 3.2442,
      "step": 1337
    },
    {
      "epoch": 8.84262701363073,
      "grad_norm": 58.4826545715332,
      "learning_rate": 0.0008326732673267326,
      "loss": 2.8118,
      "step": 1338
    },
    {
      "epoch": 8.849235852953324,
      "grad_norm": 75.88359832763672,
      "learning_rate": 0.0008321782178217821,
      "loss": 2.745,
      "step": 1339
    },
    {
      "epoch": 8.85584469227592,
      "grad_norm": 97.335205078125,
      "learning_rate": 0.0008316831683168317,
      "loss": 4.5398,
      "step": 1340
    },
    {
      "epoch": 8.862453531598513,
      "grad_norm": 15.408233642578125,
      "learning_rate": 0.0008311881188118812,
      "loss": 1.7636,
      "step": 1341
    },
    {
      "epoch": 8.869062370921107,
      "grad_norm": 28.742916107177734,
      "learning_rate": 0.0008306930693069307,
      "loss": 1.7463,
      "step": 1342
    },
    {
      "epoch": 8.8756712102437,
      "grad_norm": 74.04745483398438,
      "learning_rate": 0.0008301980198019801,
      "loss": 4.3787,
      "step": 1343
    },
    {
      "epoch": 8.882280049566296,
      "grad_norm": 32.634918212890625,
      "learning_rate": 0.0008297029702970297,
      "loss": 3.8612,
      "step": 1344
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 68.16336822509766,
      "learning_rate": 0.0008292079207920792,
      "loss": 3.6513,
      "step": 1345
    },
    {
      "epoch": 8.895497728211483,
      "grad_norm": 125.36934661865234,
      "learning_rate": 0.0008287128712871287,
      "loss": 5.0833,
      "step": 1346
    },
    {
      "epoch": 8.902106567534076,
      "grad_norm": 129.06097412109375,
      "learning_rate": 0.0008282178217821782,
      "loss": 7.3033,
      "step": 1347
    },
    {
      "epoch": 8.90871540685667,
      "grad_norm": 39.009605407714844,
      "learning_rate": 0.0008277227722772277,
      "loss": 6.3404,
      "step": 1348
    },
    {
      "epoch": 8.915324246179265,
      "grad_norm": 42.61911392211914,
      "learning_rate": 0.0008272277227722772,
      "loss": 2.501,
      "step": 1349
    },
    {
      "epoch": 8.921933085501859,
      "grad_norm": 20.73809051513672,
      "learning_rate": 0.0008267326732673267,
      "loss": 2.2951,
      "step": 1350
    },
    {
      "epoch": 8.928541924824453,
      "grad_norm": 85.08067321777344,
      "learning_rate": 0.0008262376237623762,
      "loss": 2.8774,
      "step": 1351
    },
    {
      "epoch": 8.935150764147046,
      "grad_norm": 102.00881958007812,
      "learning_rate": 0.0008257425742574258,
      "loss": 3.6173,
      "step": 1352
    },
    {
      "epoch": 8.94175960346964,
      "grad_norm": 133.618896484375,
      "learning_rate": 0.0008252475247524752,
      "loss": 7.1445,
      "step": 1353
    },
    {
      "epoch": 8.948368442792235,
      "grad_norm": 99.1639404296875,
      "learning_rate": 0.0008247524752475247,
      "loss": 2.6286,
      "step": 1354
    },
    {
      "epoch": 8.954977282114829,
      "grad_norm": 105.95664978027344,
      "learning_rate": 0.0008242574257425742,
      "loss": 7.2493,
      "step": 1355
    },
    {
      "epoch": 8.961586121437422,
      "grad_norm": 84.00997924804688,
      "learning_rate": 0.0008237623762376238,
      "loss": 3.8824,
      "step": 1356
    },
    {
      "epoch": 8.968194960760016,
      "grad_norm": 56.773616790771484,
      "learning_rate": 0.0008232673267326733,
      "loss": 4.6239,
      "step": 1357
    },
    {
      "epoch": 8.974803800082611,
      "grad_norm": 32.76252365112305,
      "learning_rate": 0.0008227722772277227,
      "loss": 1.2236,
      "step": 1358
    },
    {
      "epoch": 8.981412639405205,
      "grad_norm": 30.38979148864746,
      "learning_rate": 0.0008222772277227722,
      "loss": 4.1214,
      "step": 1359
    },
    {
      "epoch": 8.988021478727799,
      "grad_norm": 92.94005584716797,
      "learning_rate": 0.0008217821782178218,
      "loss": 5.3754,
      "step": 1360
    },
    {
      "epoch": 8.994630318050392,
      "grad_norm": 47.85139083862305,
      "learning_rate": 0.0008212871287128713,
      "loss": 2.1359,
      "step": 1361
    },
    {
      "epoch": 8.994630318050392,
      "eval_validation_error_bar": 0.05216575670311573,
      "eval_validation_loss": 10.303545951843262,
      "eval_validation_pearsonr": 0.5623523369809863,
      "eval_validation_rmse": 3.209913730621338,
      "eval_validation_runtime": 33.587,
      "eval_validation_samples_per_second": 6.044,
      "eval_validation_spearman": 0.560506383591462,
      "eval_validation_steps_per_second": 6.044,
      "step": 1361
    },
    {
      "epoch": 8.994630318050392,
      "eval_test_error_bar": 0.04533814573821629,
      "eval_test_loss": 10.671319007873535,
      "eval_test_pearsonr": 0.4814198188594433,
      "eval_test_rmse": 3.2666985988616943,
      "eval_test_runtime": 41.4892,
      "eval_test_samples_per_second": 7.857,
      "eval_test_spearman": 0.47748192877135426,
      "eval_test_steps_per_second": 7.857,
      "step": 1361
    },
    {
      "epoch": 9.001239157372986,
      "grad_norm": 54.59892654418945,
      "learning_rate": 0.0008207920792079208,
      "loss": 1.8847,
      "step": 1362
    },
    {
      "epoch": 9.007847996695581,
      "grad_norm": 21.994794845581055,
      "learning_rate": 0.0008202970297029702,
      "loss": 1.3777,
      "step": 1363
    },
    {
      "epoch": 9.014456836018175,
      "grad_norm": 37.04120635986328,
      "learning_rate": 0.0008198019801980197,
      "loss": 4.6012,
      "step": 1364
    },
    {
      "epoch": 9.021065675340768,
      "grad_norm": 22.69769287109375,
      "learning_rate": 0.0008193069306930693,
      "loss": 2.3072,
      "step": 1365
    },
    {
      "epoch": 9.027674514663362,
      "grad_norm": 4.294836521148682,
      "learning_rate": 0.0008188118811881188,
      "loss": 1.7642,
      "step": 1366
    },
    {
      "epoch": 9.034283353985956,
      "grad_norm": 62.16071319580078,
      "learning_rate": 0.0008183168316831683,
      "loss": 3.5605,
      "step": 1367
    },
    {
      "epoch": 9.04089219330855,
      "grad_norm": 30.826982498168945,
      "learning_rate": 0.0008178217821782177,
      "loss": 6.9765,
      "step": 1368
    },
    {
      "epoch": 9.047501032631144,
      "grad_norm": 82.51143646240234,
      "learning_rate": 0.0008173267326732673,
      "loss": 3.4801,
      "step": 1369
    },
    {
      "epoch": 9.054109871953738,
      "grad_norm": 7.049687385559082,
      "learning_rate": 0.0008168316831683168,
      "loss": 2.5192,
      "step": 1370
    },
    {
      "epoch": 9.060718711276332,
      "grad_norm": 14.85507583618164,
      "learning_rate": 0.0008163366336633663,
      "loss": 1.1405,
      "step": 1371
    },
    {
      "epoch": 9.067327550598925,
      "grad_norm": 19.393333435058594,
      "learning_rate": 0.0008158415841584159,
      "loss": 1.8597,
      "step": 1372
    },
    {
      "epoch": 9.07393638992152,
      "grad_norm": 7.1723527908325195,
      "learning_rate": 0.0008153465346534653,
      "loss": 2.3728,
      "step": 1373
    },
    {
      "epoch": 9.080545229244114,
      "grad_norm": 8.370259284973145,
      "learning_rate": 0.0008148514851485148,
      "loss": 3.4194,
      "step": 1374
    },
    {
      "epoch": 9.087154068566708,
      "grad_norm": 17.0804386138916,
      "learning_rate": 0.0008143564356435643,
      "loss": 2.7058,
      "step": 1375
    },
    {
      "epoch": 9.093762907889301,
      "grad_norm": 16.294055938720703,
      "learning_rate": 0.0008138613861386138,
      "loss": 0.6329,
      "step": 1376
    },
    {
      "epoch": 9.100371747211897,
      "grad_norm": 18.802953720092773,
      "learning_rate": 0.0008133663366336634,
      "loss": 5.1174,
      "step": 1377
    },
    {
      "epoch": 9.10698058653449,
      "grad_norm": 16.05490493774414,
      "learning_rate": 0.0008128712871287128,
      "loss": 3.5884,
      "step": 1378
    },
    {
      "epoch": 9.113589425857084,
      "grad_norm": 52.28898620605469,
      "learning_rate": 0.0008123762376237624,
      "loss": 1.3548,
      "step": 1379
    },
    {
      "epoch": 9.120198265179678,
      "grad_norm": 33.351654052734375,
      "learning_rate": 0.000811881188118812,
      "loss": 2.4893,
      "step": 1380
    },
    {
      "epoch": 9.126807104502271,
      "grad_norm": 8.112040519714355,
      "learning_rate": 0.0008113861386138615,
      "loss": 5.9757,
      "step": 1381
    },
    {
      "epoch": 9.133415943824867,
      "grad_norm": 67.82093048095703,
      "learning_rate": 0.000810891089108911,
      "loss": 7.3288,
      "step": 1382
    },
    {
      "epoch": 9.14002478314746,
      "grad_norm": 18.64212989807129,
      "learning_rate": 0.0008103960396039604,
      "loss": 3.6743,
      "step": 1383
    },
    {
      "epoch": 9.146633622470054,
      "grad_norm": 90.78270721435547,
      "learning_rate": 0.00080990099009901,
      "loss": 2.5394,
      "step": 1384
    },
    {
      "epoch": 9.153242461792647,
      "grad_norm": 90.01923370361328,
      "learning_rate": 0.0008094059405940595,
      "loss": 5.7887,
      "step": 1385
    },
    {
      "epoch": 9.159851301115241,
      "grad_norm": 12.725202560424805,
      "learning_rate": 0.000808910891089109,
      "loss": 3.3169,
      "step": 1386
    },
    {
      "epoch": 9.166460140437836,
      "grad_norm": 35.86615753173828,
      "learning_rate": 0.0008084158415841585,
      "loss": 2.6156,
      "step": 1387
    },
    {
      "epoch": 9.17306897976043,
      "grad_norm": 31.55839729309082,
      "learning_rate": 0.0008079207920792079,
      "loss": 3.948,
      "step": 1388
    },
    {
      "epoch": 9.179677819083023,
      "grad_norm": 23.350597381591797,
      "learning_rate": 0.0008074257425742575,
      "loss": 4.7896,
      "step": 1389
    },
    {
      "epoch": 9.186286658405617,
      "grad_norm": 65.99113464355469,
      "learning_rate": 0.000806930693069307,
      "loss": 2.4349,
      "step": 1390
    },
    {
      "epoch": 9.19289549772821,
      "grad_norm": 115.29206848144531,
      "learning_rate": 0.0008064356435643565,
      "loss": 6.3735,
      "step": 1391
    },
    {
      "epoch": 9.199504337050806,
      "grad_norm": 41.831581115722656,
      "learning_rate": 0.000805940594059406,
      "loss": 3.6448,
      "step": 1392
    },
    {
      "epoch": 9.2061131763734,
      "grad_norm": 27.684890747070312,
      "learning_rate": 0.0008054455445544555,
      "loss": 2.3198,
      "step": 1393
    },
    {
      "epoch": 9.212722015695993,
      "grad_norm": 47.305023193359375,
      "learning_rate": 0.000804950495049505,
      "loss": 3.7531,
      "step": 1394
    },
    {
      "epoch": 9.219330855018587,
      "grad_norm": 45.548797607421875,
      "learning_rate": 0.0008044554455445545,
      "loss": 4.5995,
      "step": 1395
    },
    {
      "epoch": 9.225939694341182,
      "grad_norm": 40.060001373291016,
      "learning_rate": 0.000803960396039604,
      "loss": 2.2993,
      "step": 1396
    },
    {
      "epoch": 9.232548533663776,
      "grad_norm": 24.113229751586914,
      "learning_rate": 0.0008034653465346536,
      "loss": 2.438,
      "step": 1397
    },
    {
      "epoch": 9.23915737298637,
      "grad_norm": 22.290124893188477,
      "learning_rate": 0.000802970297029703,
      "loss": 3.1382,
      "step": 1398
    },
    {
      "epoch": 9.245766212308963,
      "grad_norm": 9.852357864379883,
      "learning_rate": 0.0008024752475247525,
      "loss": 1.163,
      "step": 1399
    },
    {
      "epoch": 9.252375051631557,
      "grad_norm": 37.51157760620117,
      "learning_rate": 0.000801980198019802,
      "loss": 2.3305,
      "step": 1400
    },
    {
      "epoch": 9.258983890954152,
      "grad_norm": 66.46269989013672,
      "learning_rate": 0.0008014851485148516,
      "loss": 2.9602,
      "step": 1401
    },
    {
      "epoch": 9.265592730276746,
      "grad_norm": 11.30797004699707,
      "learning_rate": 0.0008009900990099011,
      "loss": 3.2404,
      "step": 1402
    },
    {
      "epoch": 9.27220156959934,
      "grad_norm": 65.89195251464844,
      "learning_rate": 0.0008004950495049505,
      "loss": 4.7343,
      "step": 1403
    },
    {
      "epoch": 9.278810408921933,
      "grad_norm": 99.3857650756836,
      "learning_rate": 0.0008,
      "loss": 6.0554,
      "step": 1404
    },
    {
      "epoch": 9.285419248244526,
      "grad_norm": 10.706009864807129,
      "learning_rate": 0.0007995049504950496,
      "loss": 1.0199,
      "step": 1405
    },
    {
      "epoch": 9.292028087567122,
      "grad_norm": 28.11290740966797,
      "learning_rate": 0.0007990099009900991,
      "loss": 2.0216,
      "step": 1406
    },
    {
      "epoch": 9.298636926889715,
      "grad_norm": 21.24781608581543,
      "learning_rate": 0.0007985148514851486,
      "loss": 0.4458,
      "step": 1407
    },
    {
      "epoch": 9.305245766212309,
      "grad_norm": 67.20814514160156,
      "learning_rate": 0.000798019801980198,
      "loss": 6.06,
      "step": 1408
    },
    {
      "epoch": 9.311854605534903,
      "grad_norm": 44.653263092041016,
      "learning_rate": 0.0007975247524752476,
      "loss": 3.0074,
      "step": 1409
    },
    {
      "epoch": 9.318463444857496,
      "grad_norm": 42.47145080566406,
      "learning_rate": 0.0007970297029702971,
      "loss": 2.7649,
      "step": 1410
    },
    {
      "epoch": 9.325072284180091,
      "grad_norm": 9.784658432006836,
      "learning_rate": 0.0007965346534653466,
      "loss": 1.1697,
      "step": 1411
    },
    {
      "epoch": 9.331681123502685,
      "grad_norm": 40.12320327758789,
      "learning_rate": 0.0007960396039603961,
      "loss": 2.2036,
      "step": 1412
    },
    {
      "epoch": 9.338289962825279,
      "grad_norm": 73.02687072753906,
      "learning_rate": 0.0007955445544554456,
      "loss": 2.7085,
      "step": 1413
    },
    {
      "epoch": 9.344898802147872,
      "grad_norm": 72.30899810791016,
      "learning_rate": 0.0007950495049504951,
      "loss": 3.4483,
      "step": 1414
    },
    {
      "epoch": 9.351507641470466,
      "grad_norm": 1.5322725772857666,
      "learning_rate": 0.0007945544554455446,
      "loss": 3.0608,
      "step": 1415
    },
    {
      "epoch": 9.358116480793061,
      "grad_norm": 46.812255859375,
      "learning_rate": 0.0007940594059405941,
      "loss": 1.221,
      "step": 1416
    },
    {
      "epoch": 9.364725320115655,
      "grad_norm": 14.129435539245605,
      "learning_rate": 0.0007935643564356437,
      "loss": 2.3968,
      "step": 1417
    },
    {
      "epoch": 9.371334159438248,
      "grad_norm": 42.238037109375,
      "learning_rate": 0.0007930693069306931,
      "loss": 3.016,
      "step": 1418
    },
    {
      "epoch": 9.377942998760842,
      "grad_norm": 74.18523406982422,
      "learning_rate": 0.0007925742574257426,
      "loss": 3.2051,
      "step": 1419
    },
    {
      "epoch": 9.384551838083437,
      "grad_norm": 39.617366790771484,
      "learning_rate": 0.0007920792079207921,
      "loss": 1.7447,
      "step": 1420
    },
    {
      "epoch": 9.391160677406031,
      "grad_norm": 54.60411834716797,
      "learning_rate": 0.0007915841584158417,
      "loss": 1.9005,
      "step": 1421
    },
    {
      "epoch": 9.397769516728625,
      "grad_norm": 19.548213958740234,
      "learning_rate": 0.0007910891089108912,
      "loss": 1.5232,
      "step": 1422
    },
    {
      "epoch": 9.404378356051218,
      "grad_norm": 9.524247169494629,
      "learning_rate": 0.0007905940594059406,
      "loss": 0.9167,
      "step": 1423
    },
    {
      "epoch": 9.410987195373812,
      "grad_norm": 71.38125610351562,
      "learning_rate": 0.0007900990099009901,
      "loss": 5.9795,
      "step": 1424
    },
    {
      "epoch": 9.417596034696407,
      "grad_norm": 20.486095428466797,
      "learning_rate": 0.0007896039603960397,
      "loss": 2.545,
      "step": 1425
    },
    {
      "epoch": 9.424204874019,
      "grad_norm": 32.11564636230469,
      "learning_rate": 0.0007891089108910892,
      "loss": 2.5388,
      "step": 1426
    },
    {
      "epoch": 9.430813713341594,
      "grad_norm": 61.22936248779297,
      "learning_rate": 0.0007886138613861387,
      "loss": 4.2185,
      "step": 1427
    },
    {
      "epoch": 9.437422552664188,
      "grad_norm": 51.8521842956543,
      "learning_rate": 0.0007881188118811881,
      "loss": 3.0656,
      "step": 1428
    },
    {
      "epoch": 9.444031391986782,
      "grad_norm": 42.7036018371582,
      "learning_rate": 0.0007876237623762377,
      "loss": 2.9666,
      "step": 1429
    },
    {
      "epoch": 9.450640231309377,
      "grad_norm": 23.940326690673828,
      "learning_rate": 0.0007871287128712872,
      "loss": 1.6758,
      "step": 1430
    },
    {
      "epoch": 9.45724907063197,
      "grad_norm": 44.754493713378906,
      "learning_rate": 0.0007866336633663367,
      "loss": 2.6415,
      "step": 1431
    },
    {
      "epoch": 9.463857909954564,
      "grad_norm": 17.01629066467285,
      "learning_rate": 0.0007861386138613862,
      "loss": 2.1809,
      "step": 1432
    },
    {
      "epoch": 9.470466749277158,
      "grad_norm": 23.9902286529541,
      "learning_rate": 0.0007856435643564356,
      "loss": 1.4,
      "step": 1433
    },
    {
      "epoch": 9.477075588599753,
      "grad_norm": 10.07022762298584,
      "learning_rate": 0.0007851485148514852,
      "loss": 1.3977,
      "step": 1434
    },
    {
      "epoch": 9.483684427922347,
      "grad_norm": 21.143346786499023,
      "learning_rate": 0.0007846534653465347,
      "loss": 1.234,
      "step": 1435
    },
    {
      "epoch": 9.49029326724494,
      "grad_norm": 41.8581428527832,
      "learning_rate": 0.0007841584158415842,
      "loss": 3.422,
      "step": 1436
    },
    {
      "epoch": 9.496902106567534,
      "grad_norm": 17.687515258789062,
      "learning_rate": 0.0007836633663366338,
      "loss": 0.5181,
      "step": 1437
    },
    {
      "epoch": 9.503510945890127,
      "grad_norm": 7.576839923858643,
      "learning_rate": 0.0007831683168316832,
      "loss": 4.5658,
      "step": 1438
    },
    {
      "epoch": 9.510119785212723,
      "grad_norm": 13.74926471710205,
      "learning_rate": 0.0007826732673267327,
      "loss": 1.5375,
      "step": 1439
    },
    {
      "epoch": 9.516728624535316,
      "grad_norm": 55.613826751708984,
      "learning_rate": 0.0007821782178217822,
      "loss": 5.054,
      "step": 1440
    },
    {
      "epoch": 9.52333746385791,
      "grad_norm": 3.4284253120422363,
      "learning_rate": 0.0007816831683168317,
      "loss": 3.1213,
      "step": 1441
    },
    {
      "epoch": 9.529946303180504,
      "grad_norm": 16.754634857177734,
      "learning_rate": 0.0007811881188118813,
      "loss": 1.7598,
      "step": 1442
    },
    {
      "epoch": 9.536555142503097,
      "grad_norm": 32.762264251708984,
      "learning_rate": 0.0007806930693069307,
      "loss": 2.9988,
      "step": 1443
    },
    {
      "epoch": 9.543163981825693,
      "grad_norm": 5.883528232574463,
      "learning_rate": 0.0007801980198019802,
      "loss": 1.6991,
      "step": 1444
    },
    {
      "epoch": 9.549772821148286,
      "grad_norm": 89.78239440917969,
      "learning_rate": 0.0007797029702970297,
      "loss": 4.2383,
      "step": 1445
    },
    {
      "epoch": 9.55638166047088,
      "grad_norm": 28.969524383544922,
      "learning_rate": 0.0007792079207920793,
      "loss": 2.1952,
      "step": 1446
    },
    {
      "epoch": 9.562990499793473,
      "grad_norm": 106.23456573486328,
      "learning_rate": 0.0007787128712871288,
      "loss": 4.066,
      "step": 1447
    },
    {
      "epoch": 9.569599339116067,
      "grad_norm": 47.00510025024414,
      "learning_rate": 0.0007782178217821782,
      "loss": 1.8834,
      "step": 1448
    },
    {
      "epoch": 9.576208178438662,
      "grad_norm": 58.01096725463867,
      "learning_rate": 0.0007777227722772277,
      "loss": 3.8234,
      "step": 1449
    },
    {
      "epoch": 9.582817017761256,
      "grad_norm": 39.23684310913086,
      "learning_rate": 0.0007772277227722773,
      "loss": 3.4341,
      "step": 1450
    },
    {
      "epoch": 9.58942585708385,
      "grad_norm": 25.60099220275879,
      "learning_rate": 0.0007767326732673268,
      "loss": 3.237,
      "step": 1451
    },
    {
      "epoch": 9.596034696406443,
      "grad_norm": 4.237191677093506,
      "learning_rate": 0.0007762376237623763,
      "loss": 0.8251,
      "step": 1452
    },
    {
      "epoch": 9.602643535729037,
      "grad_norm": 50.6492805480957,
      "learning_rate": 0.0007757425742574257,
      "loss": 5.1494,
      "step": 1453
    },
    {
      "epoch": 9.609252375051632,
      "grad_norm": 5.227016925811768,
      "learning_rate": 0.0007752475247524753,
      "loss": 2.793,
      "step": 1454
    },
    {
      "epoch": 9.615861214374226,
      "grad_norm": 3.697200059890747,
      "learning_rate": 0.0007747524752475248,
      "loss": 1.1482,
      "step": 1455
    },
    {
      "epoch": 9.62247005369682,
      "grad_norm": 4.690084457397461,
      "learning_rate": 0.0007742574257425743,
      "loss": 2.0939,
      "step": 1456
    },
    {
      "epoch": 9.629078893019413,
      "grad_norm": 25.690383911132812,
      "learning_rate": 0.0007737623762376238,
      "loss": 1.3545,
      "step": 1457
    },
    {
      "epoch": 9.635687732342008,
      "grad_norm": 19.55171012878418,
      "learning_rate": 0.0007732673267326733,
      "loss": 2.8869,
      "step": 1458
    },
    {
      "epoch": 9.642296571664602,
      "grad_norm": 55.068328857421875,
      "learning_rate": 0.0007727722772277228,
      "loss": 2.2288,
      "step": 1459
    },
    {
      "epoch": 9.648905410987195,
      "grad_norm": 68.82179260253906,
      "learning_rate": 0.0007722772277227723,
      "loss": 2.6758,
      "step": 1460
    },
    {
      "epoch": 9.655514250309789,
      "grad_norm": 25.847692489624023,
      "learning_rate": 0.0007717821782178218,
      "loss": 2.4083,
      "step": 1461
    },
    {
      "epoch": 9.662123089632383,
      "grad_norm": 31.387489318847656,
      "learning_rate": 0.0007712871287128714,
      "loss": 2.4304,
      "step": 1462
    },
    {
      "epoch": 9.668731928954978,
      "grad_norm": 42.84869384765625,
      "learning_rate": 0.0007707920792079208,
      "loss": 2.7677,
      "step": 1463
    },
    {
      "epoch": 9.675340768277572,
      "grad_norm": 94.31855010986328,
      "learning_rate": 0.0007702970297029703,
      "loss": 5.7886,
      "step": 1464
    },
    {
      "epoch": 9.681949607600165,
      "grad_norm": 21.42600440979004,
      "learning_rate": 0.0007698019801980198,
      "loss": 1.8669,
      "step": 1465
    },
    {
      "epoch": 9.688558446922759,
      "grad_norm": 60.206748962402344,
      "learning_rate": 0.0007693069306930694,
      "loss": 1.9826,
      "step": 1466
    },
    {
      "epoch": 9.695167286245352,
      "grad_norm": 57.4105224609375,
      "learning_rate": 0.0007688118811881189,
      "loss": 1.4771,
      "step": 1467
    },
    {
      "epoch": 9.701776125567948,
      "grad_norm": 64.18180847167969,
      "learning_rate": 0.0007683168316831683,
      "loss": 2.8431,
      "step": 1468
    },
    {
      "epoch": 9.708384964890541,
      "grad_norm": 26.864967346191406,
      "learning_rate": 0.0007678217821782178,
      "loss": 3.026,
      "step": 1469
    },
    {
      "epoch": 9.714993804213135,
      "grad_norm": 59.39569091796875,
      "learning_rate": 0.0007673267326732674,
      "loss": 4.287,
      "step": 1470
    },
    {
      "epoch": 9.721602643535729,
      "grad_norm": 9.32503604888916,
      "learning_rate": 0.0007668316831683169,
      "loss": 2.326,
      "step": 1471
    },
    {
      "epoch": 9.728211482858324,
      "grad_norm": 43.334964752197266,
      "learning_rate": 0.0007663366336633664,
      "loss": 2.5896,
      "step": 1472
    },
    {
      "epoch": 9.734820322180918,
      "grad_norm": 54.20448684692383,
      "learning_rate": 0.0007658415841584158,
      "loss": 3.4354,
      "step": 1473
    },
    {
      "epoch": 9.741429161503511,
      "grad_norm": 46.74931716918945,
      "learning_rate": 0.0007653465346534654,
      "loss": 1.7847,
      "step": 1474
    },
    {
      "epoch": 9.748038000826105,
      "grad_norm": 30.7366886138916,
      "learning_rate": 0.0007648514851485149,
      "loss": 1.3471,
      "step": 1475
    },
    {
      "epoch": 9.754646840148698,
      "grad_norm": 7.002117156982422,
      "learning_rate": 0.0007643564356435644,
      "loss": 1.6548,
      "step": 1476
    },
    {
      "epoch": 9.761255679471294,
      "grad_norm": 12.60658073425293,
      "learning_rate": 0.0007638613861386139,
      "loss": 2.6683,
      "step": 1477
    },
    {
      "epoch": 9.767864518793887,
      "grad_norm": 19.65204429626465,
      "learning_rate": 0.0007633663366336634,
      "loss": 1.3058,
      "step": 1478
    },
    {
      "epoch": 9.77447335811648,
      "grad_norm": 54.989376068115234,
      "learning_rate": 0.0007628712871287129,
      "loss": 3.2578,
      "step": 1479
    },
    {
      "epoch": 9.781082197439074,
      "grad_norm": 107.45320129394531,
      "learning_rate": 0.0007623762376237624,
      "loss": 3.9358,
      "step": 1480
    },
    {
      "epoch": 9.787691036761668,
      "grad_norm": 25.533740997314453,
      "learning_rate": 0.0007618811881188119,
      "loss": 2.7247,
      "step": 1481
    },
    {
      "epoch": 9.794299876084263,
      "grad_norm": 27.41006851196289,
      "learning_rate": 0.0007613861386138615,
      "loss": 3.805,
      "step": 1482
    },
    {
      "epoch": 9.800908715406857,
      "grad_norm": 39.61787414550781,
      "learning_rate": 0.0007608910891089109,
      "loss": 1.8471,
      "step": 1483
    },
    {
      "epoch": 9.80751755472945,
      "grad_norm": 32.32651138305664,
      "learning_rate": 0.0007603960396039604,
      "loss": 1.9244,
      "step": 1484
    },
    {
      "epoch": 9.814126394052044,
      "grad_norm": 42.84823226928711,
      "learning_rate": 0.0007599009900990099,
      "loss": 3.0126,
      "step": 1485
    },
    {
      "epoch": 9.820735233374638,
      "grad_norm": 19.979787826538086,
      "learning_rate": 0.0007594059405940595,
      "loss": 2.5048,
      "step": 1486
    },
    {
      "epoch": 9.827344072697233,
      "grad_norm": 37.16937255859375,
      "learning_rate": 0.000758910891089109,
      "loss": 0.9283,
      "step": 1487
    },
    {
      "epoch": 9.833952912019827,
      "grad_norm": 36.69809341430664,
      "learning_rate": 0.0007584158415841584,
      "loss": 2.0546,
      "step": 1488
    },
    {
      "epoch": 9.84056175134242,
      "grad_norm": 21.216100692749023,
      "learning_rate": 0.0007579207920792079,
      "loss": 1.4721,
      "step": 1489
    },
    {
      "epoch": 9.847170590665014,
      "grad_norm": 9.381563186645508,
      "learning_rate": 0.0007574257425742574,
      "loss": 1.4527,
      "step": 1490
    },
    {
      "epoch": 9.853779429987608,
      "grad_norm": 44.205543518066406,
      "learning_rate": 0.000756930693069307,
      "loss": 1.6977,
      "step": 1491
    },
    {
      "epoch": 9.860388269310203,
      "grad_norm": 1.8338521718978882,
      "learning_rate": 0.0007564356435643565,
      "loss": 5.516,
      "step": 1492
    },
    {
      "epoch": 9.866997108632797,
      "grad_norm": 20.253633499145508,
      "learning_rate": 0.0007559405940594059,
      "loss": 1.2206,
      "step": 1493
    },
    {
      "epoch": 9.87360594795539,
      "grad_norm": 13.058185577392578,
      "learning_rate": 0.0007554455445544554,
      "loss": 2.9262,
      "step": 1494
    },
    {
      "epoch": 9.880214787277984,
      "grad_norm": 15.926457405090332,
      "learning_rate": 0.000754950495049505,
      "loss": 5.6399,
      "step": 1495
    },
    {
      "epoch": 9.886823626600577,
      "grad_norm": 84.45011138916016,
      "learning_rate": 0.0007544554455445545,
      "loss": 4.5027,
      "step": 1496
    },
    {
      "epoch": 9.893432465923173,
      "grad_norm": 13.111159324645996,
      "learning_rate": 0.000753960396039604,
      "loss": 1.4409,
      "step": 1497
    },
    {
      "epoch": 9.900041305245766,
      "grad_norm": 23.228849411010742,
      "learning_rate": 0.0007534653465346534,
      "loss": 1.418,
      "step": 1498
    },
    {
      "epoch": 9.90665014456836,
      "grad_norm": 28.91863250732422,
      "learning_rate": 0.000752970297029703,
      "loss": 2.194,
      "step": 1499
    },
    {
      "epoch": 9.913258983890954,
      "grad_norm": 4.484068393707275,
      "learning_rate": 0.0007524752475247525,
      "loss": 2.3326,
      "step": 1500
    },
    {
      "epoch": 9.919867823213549,
      "grad_norm": 28.563722610473633,
      "learning_rate": 0.000751980198019802,
      "loss": 2.174,
      "step": 1501
    },
    {
      "epoch": 9.926476662536142,
      "grad_norm": 43.18531799316406,
      "learning_rate": 0.0007514851485148515,
      "loss": 2.0907,
      "step": 1502
    },
    {
      "epoch": 9.933085501858736,
      "grad_norm": 81.908447265625,
      "learning_rate": 0.000750990099009901,
      "loss": 4.766,
      "step": 1503
    },
    {
      "epoch": 9.93969434118133,
      "grad_norm": 108.5950698852539,
      "learning_rate": 0.0007504950495049505,
      "loss": 6.4636,
      "step": 1504
    },
    {
      "epoch": 9.946303180503923,
      "grad_norm": 23.744159698486328,
      "learning_rate": 0.00075,
      "loss": 2.9157,
      "step": 1505
    },
    {
      "epoch": 9.952912019826519,
      "grad_norm": 15.654166221618652,
      "learning_rate": 0.0007495049504950495,
      "loss": 5.4214,
      "step": 1506
    },
    {
      "epoch": 9.959520859149112,
      "grad_norm": 35.32447052001953,
      "learning_rate": 0.0007490099009900991,
      "loss": 1.6976,
      "step": 1507
    },
    {
      "epoch": 9.966129698471706,
      "grad_norm": 26.11514663696289,
      "learning_rate": 0.0007485148514851485,
      "loss": 1.3908,
      "step": 1508
    },
    {
      "epoch": 9.9727385377943,
      "grad_norm": 2.7931015491485596,
      "learning_rate": 0.000748019801980198,
      "loss": 2.0885,
      "step": 1509
    },
    {
      "epoch": 9.979347377116893,
      "grad_norm": 12.954058647155762,
      "learning_rate": 0.0007475247524752475,
      "loss": 3.5765,
      "step": 1510
    },
    {
      "epoch": 9.985956216439488,
      "grad_norm": 9.89164924621582,
      "learning_rate": 0.0007470297029702971,
      "loss": 1.3215,
      "step": 1511
    },
    {
      "epoch": 9.992565055762082,
      "grad_norm": 70.59182739257812,
      "learning_rate": 0.0007465346534653466,
      "loss": 2.7976,
      "step": 1512
    },
    {
      "epoch": 9.999173895084676,
      "grad_norm": 11.459671020507812,
      "learning_rate": 0.000746039603960396,
      "loss": 3.5198,
      "step": 1513
    },
    {
      "epoch": 9.999173895084676,
      "eval_validation_error_bar": 0.05695717767330248,
      "eval_validation_loss": 8.33390998840332,
      "eval_validation_pearsonr": 0.5186518853937607,
      "eval_validation_rmse": 2.8868513107299805,
      "eval_validation_runtime": 33.4983,
      "eval_validation_samples_per_second": 6.06,
      "eval_validation_spearman": 0.48851599483775865,
      "eval_validation_steps_per_second": 6.06,
      "step": 1513
    },
    {
      "epoch": 9.999173895084676,
      "eval_test_error_bar": 0.04543684879682438,
      "eval_test_loss": 10.404711723327637,
      "eval_test_pearsonr": 0.4680941166796071,
      "eval_test_rmse": 3.2256336212158203,
      "eval_test_runtime": 41.5477,
      "eval_test_samples_per_second": 7.846,
      "eval_test_spearman": 0.47534709830301103,
      "eval_test_steps_per_second": 7.846,
      "step": 1513
    },
    {
      "epoch": 10.00578273440727,
      "grad_norm": 18.67454719543457,
      "learning_rate": 0.0007455445544554455,
      "loss": 4.0865,
      "step": 1514
    },
    {
      "epoch": 10.012391573729865,
      "grad_norm": 15.630317687988281,
      "learning_rate": 0.0007450495049504951,
      "loss": 1.9664,
      "step": 1515
    },
    {
      "epoch": 10.019000413052458,
      "grad_norm": 30.999801635742188,
      "learning_rate": 0.0007445544554455446,
      "loss": 1.203,
      "step": 1516
    },
    {
      "epoch": 10.025609252375052,
      "grad_norm": 41.887481689453125,
      "learning_rate": 0.0007440594059405941,
      "loss": 3.7806,
      "step": 1517
    },
    {
      "epoch": 10.032218091697645,
      "grad_norm": 84.97608184814453,
      "learning_rate": 0.0007435643564356435,
      "loss": 4.8636,
      "step": 1518
    },
    {
      "epoch": 10.038826931020239,
      "grad_norm": 62.6044921875,
      "learning_rate": 0.0007430693069306931,
      "loss": 4.0412,
      "step": 1519
    },
    {
      "epoch": 10.045435770342834,
      "grad_norm": 81.303466796875,
      "learning_rate": 0.0007425742574257426,
      "loss": 3.7427,
      "step": 1520
    },
    {
      "epoch": 10.052044609665428,
      "grad_norm": 106.70072937011719,
      "learning_rate": 0.0007420792079207921,
      "loss": 4.5143,
      "step": 1521
    },
    {
      "epoch": 10.058653448988021,
      "grad_norm": 71.15091705322266,
      "learning_rate": 0.0007415841584158416,
      "loss": 2.2929,
      "step": 1522
    },
    {
      "epoch": 10.065262288310615,
      "grad_norm": 35.11711120605469,
      "learning_rate": 0.000741089108910891,
      "loss": 2.0079,
      "step": 1523
    },
    {
      "epoch": 10.071871127633209,
      "grad_norm": 15.786232948303223,
      "learning_rate": 0.0007405940594059406,
      "loss": 1.9487,
      "step": 1524
    },
    {
      "epoch": 10.078479966955804,
      "grad_norm": 5.959544658660889,
      "learning_rate": 0.0007400990099009901,
      "loss": 2.7026,
      "step": 1525
    },
    {
      "epoch": 10.085088806278398,
      "grad_norm": 21.116483688354492,
      "learning_rate": 0.0007396039603960396,
      "loss": 2.574,
      "step": 1526
    },
    {
      "epoch": 10.091697645600991,
      "grad_norm": 74.37638092041016,
      "learning_rate": 0.0007391089108910892,
      "loss": 2.4051,
      "step": 1527
    },
    {
      "epoch": 10.098306484923585,
      "grad_norm": 100.43646240234375,
      "learning_rate": 0.0007386138613861386,
      "loss": 5.1076,
      "step": 1528
    },
    {
      "epoch": 10.104915324246178,
      "grad_norm": 130.47984313964844,
      "learning_rate": 0.0007381188118811881,
      "loss": 5.2819,
      "step": 1529
    },
    {
      "epoch": 10.111524163568774,
      "grad_norm": 115.58052825927734,
      "learning_rate": 0.0007376237623762376,
      "loss": 3.9159,
      "step": 1530
    },
    {
      "epoch": 10.118133002891367,
      "grad_norm": 127.74234008789062,
      "learning_rate": 0.0007371287128712872,
      "loss": 6.4391,
      "step": 1531
    },
    {
      "epoch": 10.124741842213961,
      "grad_norm": 86.08834838867188,
      "learning_rate": 0.0007366336633663367,
      "loss": 3.5896,
      "step": 1532
    },
    {
      "epoch": 10.131350681536555,
      "grad_norm": 70.98362731933594,
      "learning_rate": 0.0007361386138613861,
      "loss": 4.4677,
      "step": 1533
    },
    {
      "epoch": 10.13795952085915,
      "grad_norm": 17.390657424926758,
      "learning_rate": 0.0007356435643564356,
      "loss": 3.6891,
      "step": 1534
    },
    {
      "epoch": 10.144568360181744,
      "grad_norm": 9.49012565612793,
      "learning_rate": 0.0007351485148514852,
      "loss": 2.0148,
      "step": 1535
    },
    {
      "epoch": 10.151177199504337,
      "grad_norm": 106.4757080078125,
      "learning_rate": 0.0007346534653465347,
      "loss": 4.0518,
      "step": 1536
    },
    {
      "epoch": 10.15778603882693,
      "grad_norm": 62.562347412109375,
      "learning_rate": 0.0007341584158415842,
      "loss": 3.0662,
      "step": 1537
    },
    {
      "epoch": 10.164394878149524,
      "grad_norm": 35.527976989746094,
      "learning_rate": 0.0007336633663366336,
      "loss": 2.0024,
      "step": 1538
    },
    {
      "epoch": 10.17100371747212,
      "grad_norm": 92.31183624267578,
      "learning_rate": 0.0007331683168316831,
      "loss": 3.9613,
      "step": 1539
    },
    {
      "epoch": 10.177612556794713,
      "grad_norm": 77.65153503417969,
      "learning_rate": 0.0007326732673267327,
      "loss": 2.2549,
      "step": 1540
    },
    {
      "epoch": 10.184221396117307,
      "grad_norm": 35.302528381347656,
      "learning_rate": 0.0007321782178217822,
      "loss": 1.1998,
      "step": 1541
    },
    {
      "epoch": 10.1908302354399,
      "grad_norm": 57.23801040649414,
      "learning_rate": 0.0007316831683168317,
      "loss": 1.7374,
      "step": 1542
    },
    {
      "epoch": 10.197439074762494,
      "grad_norm": 40.245880126953125,
      "learning_rate": 0.0007311881188118811,
      "loss": 6.9672,
      "step": 1543
    },
    {
      "epoch": 10.20404791408509,
      "grad_norm": 59.39970397949219,
      "learning_rate": 0.0007306930693069307,
      "loss": 2.5958,
      "step": 1544
    },
    {
      "epoch": 10.210656753407683,
      "grad_norm": 19.46383285522461,
      "learning_rate": 0.0007301980198019802,
      "loss": 0.8366,
      "step": 1545
    },
    {
      "epoch": 10.217265592730277,
      "grad_norm": 69.85926818847656,
      "learning_rate": 0.0007297029702970297,
      "loss": 3.3752,
      "step": 1546
    },
    {
      "epoch": 10.22387443205287,
      "grad_norm": 64.59925079345703,
      "learning_rate": 0.0007292079207920792,
      "loss": 4.483,
      "step": 1547
    },
    {
      "epoch": 10.230483271375464,
      "grad_norm": 8.491512298583984,
      "learning_rate": 0.0007287128712871287,
      "loss": 1.9046,
      "step": 1548
    },
    {
      "epoch": 10.23709211069806,
      "grad_norm": 17.131711959838867,
      "learning_rate": 0.0007282178217821782,
      "loss": 0.9255,
      "step": 1549
    },
    {
      "epoch": 10.243700950020653,
      "grad_norm": 50.700931549072266,
      "learning_rate": 0.0007277227722772277,
      "loss": 4.6051,
      "step": 1550
    },
    {
      "epoch": 10.250309789343246,
      "grad_norm": 2.7429263591766357,
      "learning_rate": 0.0007272277227722772,
      "loss": 2.4038,
      "step": 1551
    },
    {
      "epoch": 10.25691862866584,
      "grad_norm": 6.790215015411377,
      "learning_rate": 0.0007267326732673268,
      "loss": 0.5752,
      "step": 1552
    },
    {
      "epoch": 10.263527467988435,
      "grad_norm": 58.91506576538086,
      "learning_rate": 0.0007262376237623762,
      "loss": 2.1,
      "step": 1553
    },
    {
      "epoch": 10.270136307311029,
      "grad_norm": 10.444648742675781,
      "learning_rate": 0.0007257425742574257,
      "loss": 4.8864,
      "step": 1554
    },
    {
      "epoch": 10.276745146633623,
      "grad_norm": 23.250478744506836,
      "learning_rate": 0.0007252475247524752,
      "loss": 1.2763,
      "step": 1555
    },
    {
      "epoch": 10.283353985956216,
      "grad_norm": 65.85025024414062,
      "learning_rate": 0.0007247524752475248,
      "loss": 3.371,
      "step": 1556
    },
    {
      "epoch": 10.28996282527881,
      "grad_norm": 26.15419578552246,
      "learning_rate": 0.0007242574257425743,
      "loss": 1.899,
      "step": 1557
    },
    {
      "epoch": 10.296571664601405,
      "grad_norm": 15.393244743347168,
      "learning_rate": 0.0007237623762376237,
      "loss": 1.216,
      "step": 1558
    },
    {
      "epoch": 10.303180503923999,
      "grad_norm": 13.220633506774902,
      "learning_rate": 0.0007232673267326732,
      "loss": 2.4681,
      "step": 1559
    },
    {
      "epoch": 10.309789343246592,
      "grad_norm": 39.228431701660156,
      "learning_rate": 0.0007227722772277228,
      "loss": 2.4538,
      "step": 1560
    },
    {
      "epoch": 10.316398182569186,
      "grad_norm": 71.33902740478516,
      "learning_rate": 0.0007222772277227723,
      "loss": 2.363,
      "step": 1561
    },
    {
      "epoch": 10.32300702189178,
      "grad_norm": 65.32069396972656,
      "learning_rate": 0.0007217821782178218,
      "loss": 2.4376,
      "step": 1562
    },
    {
      "epoch": 10.329615861214375,
      "grad_norm": 37.53573226928711,
      "learning_rate": 0.0007212871287128712,
      "loss": 1.7984,
      "step": 1563
    },
    {
      "epoch": 10.336224700536969,
      "grad_norm": 27.611740112304688,
      "learning_rate": 0.0007207920792079208,
      "loss": 1.9529,
      "step": 1564
    },
    {
      "epoch": 10.342833539859562,
      "grad_norm": 21.92400550842285,
      "learning_rate": 0.0007202970297029703,
      "loss": 3.6833,
      "step": 1565
    },
    {
      "epoch": 10.349442379182156,
      "grad_norm": 1.6858155727386475,
      "learning_rate": 0.0007198019801980198,
      "loss": 2.9262,
      "step": 1566
    },
    {
      "epoch": 10.35605121850475,
      "grad_norm": 13.222084045410156,
      "learning_rate": 0.0007193069306930693,
      "loss": 1.9509,
      "step": 1567
    },
    {
      "epoch": 10.362660057827345,
      "grad_norm": 36.45280838012695,
      "learning_rate": 0.0007188118811881188,
      "loss": 3.453,
      "step": 1568
    },
    {
      "epoch": 10.369268897149938,
      "grad_norm": 57.519832611083984,
      "learning_rate": 0.0007183168316831683,
      "loss": 2.219,
      "step": 1569
    },
    {
      "epoch": 10.375877736472532,
      "grad_norm": 58.033935546875,
      "learning_rate": 0.0007178217821782178,
      "loss": 2.2752,
      "step": 1570
    },
    {
      "epoch": 10.382486575795125,
      "grad_norm": 44.86048889160156,
      "learning_rate": 0.0007173267326732673,
      "loss": 2.9676,
      "step": 1571
    },
    {
      "epoch": 10.389095415117719,
      "grad_norm": 5.545311450958252,
      "learning_rate": 0.0007168316831683169,
      "loss": 1.7076,
      "step": 1572
    },
    {
      "epoch": 10.395704254440314,
      "grad_norm": 7.003310203552246,
      "learning_rate": 0.0007163366336633663,
      "loss": 7.7205,
      "step": 1573
    },
    {
      "epoch": 10.402313093762908,
      "grad_norm": 32.057777404785156,
      "learning_rate": 0.0007158415841584158,
      "loss": 4.1152,
      "step": 1574
    },
    {
      "epoch": 10.408921933085502,
      "grad_norm": 10.20093822479248,
      "learning_rate": 0.0007153465346534653,
      "loss": 1.3922,
      "step": 1575
    },
    {
      "epoch": 10.415530772408095,
      "grad_norm": 19.673187255859375,
      "learning_rate": 0.0007148514851485149,
      "loss": 1.8789,
      "step": 1576
    },
    {
      "epoch": 10.42213961173069,
      "grad_norm": 64.54242706298828,
      "learning_rate": 0.0007143564356435644,
      "loss": 3.4639,
      "step": 1577
    },
    {
      "epoch": 10.428748451053284,
      "grad_norm": 31.054176330566406,
      "learning_rate": 0.0007138613861386138,
      "loss": 1.6506,
      "step": 1578
    },
    {
      "epoch": 10.435357290375878,
      "grad_norm": 17.961570739746094,
      "learning_rate": 0.0007133663366336633,
      "loss": 2.9831,
      "step": 1579
    },
    {
      "epoch": 10.441966129698471,
      "grad_norm": 64.52120971679688,
      "learning_rate": 0.0007128712871287129,
      "loss": 2.191,
      "step": 1580
    },
    {
      "epoch": 10.448574969021065,
      "grad_norm": 54.75177764892578,
      "learning_rate": 0.0007123762376237624,
      "loss": 1.4467,
      "step": 1581
    },
    {
      "epoch": 10.45518380834366,
      "grad_norm": 62.96556854248047,
      "learning_rate": 0.0007118811881188119,
      "loss": 3.6273,
      "step": 1582
    },
    {
      "epoch": 10.461792647666254,
      "grad_norm": 142.4689483642578,
      "learning_rate": 0.0007113861386138613,
      "loss": 7.327,
      "step": 1583
    },
    {
      "epoch": 10.468401486988848,
      "grad_norm": 11.200250625610352,
      "learning_rate": 0.0007108910891089109,
      "loss": 1.5717,
      "step": 1584
    },
    {
      "epoch": 10.475010326311441,
      "grad_norm": 11.014381408691406,
      "learning_rate": 0.0007103960396039604,
      "loss": 3.8993,
      "step": 1585
    },
    {
      "epoch": 10.481619165634035,
      "grad_norm": 19.2813777923584,
      "learning_rate": 0.0007099009900990099,
      "loss": 1.3799,
      "step": 1586
    },
    {
      "epoch": 10.48822800495663,
      "grad_norm": 55.946197509765625,
      "learning_rate": 0.0007094059405940594,
      "loss": 2.9152,
      "step": 1587
    },
    {
      "epoch": 10.494836844279224,
      "grad_norm": 25.933996200561523,
      "learning_rate": 0.0007089108910891088,
      "loss": 1.716,
      "step": 1588
    },
    {
      "epoch": 10.501445683601817,
      "grad_norm": 25.71131134033203,
      "learning_rate": 0.0007084158415841584,
      "loss": 1.1724,
      "step": 1589
    },
    {
      "epoch": 10.508054522924411,
      "grad_norm": 32.54851150512695,
      "learning_rate": 0.0007079207920792079,
      "loss": 2.2834,
      "step": 1590
    },
    {
      "epoch": 10.514663362247006,
      "grad_norm": 11.023811340332031,
      "learning_rate": 0.0007074257425742574,
      "loss": 0.8253,
      "step": 1591
    },
    {
      "epoch": 10.5212722015696,
      "grad_norm": 63.52500534057617,
      "learning_rate": 0.000706930693069307,
      "loss": 3.7849,
      "step": 1592
    },
    {
      "epoch": 10.527881040892193,
      "grad_norm": 24.810625076293945,
      "learning_rate": 0.0007064356435643564,
      "loss": 0.4983,
      "step": 1593
    },
    {
      "epoch": 10.534489880214787,
      "grad_norm": 32.3546257019043,
      "learning_rate": 0.0007059405940594059,
      "loss": 2.0708,
      "step": 1594
    },
    {
      "epoch": 10.54109871953738,
      "grad_norm": 17.41428565979004,
      "learning_rate": 0.0007054455445544554,
      "loss": 2.7432,
      "step": 1595
    },
    {
      "epoch": 10.547707558859976,
      "grad_norm": 46.39140701293945,
      "learning_rate": 0.000704950495049505,
      "loss": 2.2614,
      "step": 1596
    },
    {
      "epoch": 10.55431639818257,
      "grad_norm": 21.682703018188477,
      "learning_rate": 0.0007044554455445545,
      "loss": 2.6608,
      "step": 1597
    },
    {
      "epoch": 10.560925237505163,
      "grad_norm": 18.653432846069336,
      "learning_rate": 0.0007039603960396039,
      "loss": 1.3798,
      "step": 1598
    },
    {
      "epoch": 10.567534076827757,
      "grad_norm": 19.360937118530273,
      "learning_rate": 0.0007034653465346534,
      "loss": 2.7159,
      "step": 1599
    },
    {
      "epoch": 10.57414291615035,
      "grad_norm": 46.08726119995117,
      "learning_rate": 0.0007029702970297029,
      "loss": 3.3451,
      "step": 1600
    },
    {
      "epoch": 10.580751755472946,
      "grad_norm": 49.2598876953125,
      "learning_rate": 0.0007024752475247525,
      "loss": 2.8508,
      "step": 1601
    },
    {
      "epoch": 10.58736059479554,
      "grad_norm": 40.099937438964844,
      "learning_rate": 0.000701980198019802,
      "loss": 3.0175,
      "step": 1602
    },
    {
      "epoch": 10.593969434118133,
      "grad_norm": 23.93809700012207,
      "learning_rate": 0.0007014851485148514,
      "loss": 5.3115,
      "step": 1603
    },
    {
      "epoch": 10.600578273440727,
      "grad_norm": 21.539966583251953,
      "learning_rate": 0.0007009900990099009,
      "loss": 3.3678,
      "step": 1604
    },
    {
      "epoch": 10.60718711276332,
      "grad_norm": 5.89319372177124,
      "learning_rate": 0.0007004950495049505,
      "loss": 1.3751,
      "step": 1605
    },
    {
      "epoch": 10.613795952085916,
      "grad_norm": 5.469673156738281,
      "learning_rate": 0.0007,
      "loss": 4.683,
      "step": 1606
    },
    {
      "epoch": 10.62040479140851,
      "grad_norm": 54.64615249633789,
      "learning_rate": 0.0006995049504950495,
      "loss": 1.8987,
      "step": 1607
    },
    {
      "epoch": 10.627013630731103,
      "grad_norm": 11.061318397521973,
      "learning_rate": 0.0006990099009900989,
      "loss": 0.5842,
      "step": 1608
    },
    {
      "epoch": 10.633622470053696,
      "grad_norm": 16.001298904418945,
      "learning_rate": 0.0006985148514851485,
      "loss": 1.5736,
      "step": 1609
    },
    {
      "epoch": 10.64023130937629,
      "grad_norm": 42.85798645019531,
      "learning_rate": 0.000698019801980198,
      "loss": 2.2965,
      "step": 1610
    },
    {
      "epoch": 10.646840148698885,
      "grad_norm": 4.477202415466309,
      "learning_rate": 0.0006975247524752475,
      "loss": 0.9339,
      "step": 1611
    },
    {
      "epoch": 10.653448988021479,
      "grad_norm": 27.46164321899414,
      "learning_rate": 0.000697029702970297,
      "loss": 2.9231,
      "step": 1612
    },
    {
      "epoch": 10.660057827344072,
      "grad_norm": 52.40860366821289,
      "learning_rate": 0.0006965346534653465,
      "loss": 3.8635,
      "step": 1613
    },
    {
      "epoch": 10.666666666666666,
      "grad_norm": 91.68099975585938,
      "learning_rate": 0.000696039603960396,
      "loss": 3.4727,
      "step": 1614
    },
    {
      "epoch": 10.673275505989261,
      "grad_norm": 89.07930755615234,
      "learning_rate": 0.0006955445544554455,
      "loss": 2.8311,
      "step": 1615
    },
    {
      "epoch": 10.679884345311855,
      "grad_norm": 114.6540298461914,
      "learning_rate": 0.000695049504950495,
      "loss": 3.036,
      "step": 1616
    },
    {
      "epoch": 10.686493184634449,
      "grad_norm": 55.861019134521484,
      "learning_rate": 0.0006945544554455446,
      "loss": 3.6242,
      "step": 1617
    },
    {
      "epoch": 10.693102023957042,
      "grad_norm": 62.274200439453125,
      "learning_rate": 0.000694059405940594,
      "loss": 1.9173,
      "step": 1618
    },
    {
      "epoch": 10.699710863279636,
      "grad_norm": 95.60338592529297,
      "learning_rate": 0.0006935643564356435,
      "loss": 4.1056,
      "step": 1619
    },
    {
      "epoch": 10.706319702602231,
      "grad_norm": 66.6445541381836,
      "learning_rate": 0.000693069306930693,
      "loss": 2.7673,
      "step": 1620
    },
    {
      "epoch": 10.712928541924825,
      "grad_norm": 21.130022048950195,
      "learning_rate": 0.0006925742574257426,
      "loss": 2.5315,
      "step": 1621
    },
    {
      "epoch": 10.719537381247418,
      "grad_norm": 27.836145401000977,
      "learning_rate": 0.0006920792079207921,
      "loss": 3.1811,
      "step": 1622
    },
    {
      "epoch": 10.726146220570012,
      "grad_norm": 39.21490478515625,
      "learning_rate": 0.0006915841584158415,
      "loss": 2.3637,
      "step": 1623
    },
    {
      "epoch": 10.732755059892606,
      "grad_norm": 15.983800888061523,
      "learning_rate": 0.000691089108910891,
      "loss": 0.677,
      "step": 1624
    },
    {
      "epoch": 10.739363899215201,
      "grad_norm": 8.12338638305664,
      "learning_rate": 0.0006905940594059406,
      "loss": 1.9829,
      "step": 1625
    },
    {
      "epoch": 10.745972738537795,
      "grad_norm": 7.838983535766602,
      "learning_rate": 0.0006900990099009901,
      "loss": 1.4894,
      "step": 1626
    },
    {
      "epoch": 10.752581577860388,
      "grad_norm": 25.916948318481445,
      "learning_rate": 0.0006896039603960396,
      "loss": 1.2696,
      "step": 1627
    },
    {
      "epoch": 10.759190417182982,
      "grad_norm": 58.6929817199707,
      "learning_rate": 0.000689108910891089,
      "loss": 3.5705,
      "step": 1628
    },
    {
      "epoch": 10.765799256505577,
      "grad_norm": 59.77568054199219,
      "learning_rate": 0.0006886138613861386,
      "loss": 2.4713,
      "step": 1629
    },
    {
      "epoch": 10.77240809582817,
      "grad_norm": 5.270514488220215,
      "learning_rate": 0.0006881188118811881,
      "loss": 1.6719,
      "step": 1630
    },
    {
      "epoch": 10.779016935150764,
      "grad_norm": 23.871326446533203,
      "learning_rate": 0.0006876237623762376,
      "loss": 1.7705,
      "step": 1631
    },
    {
      "epoch": 10.785625774473358,
      "grad_norm": 4.7406325340271,
      "learning_rate": 0.0006871287128712872,
      "loss": 2.9797,
      "step": 1632
    },
    {
      "epoch": 10.792234613795952,
      "grad_norm": 16.822402954101562,
      "learning_rate": 0.0006866336633663367,
      "loss": 1.2875,
      "step": 1633
    },
    {
      "epoch": 10.798843453118547,
      "grad_norm": 39.19648361206055,
      "learning_rate": 0.0006861386138613862,
      "loss": 2.4201,
      "step": 1634
    },
    {
      "epoch": 10.80545229244114,
      "grad_norm": 30.444259643554688,
      "learning_rate": 0.0006856435643564357,
      "loss": 2.5518,
      "step": 1635
    },
    {
      "epoch": 10.812061131763734,
      "grad_norm": 2.0031449794769287,
      "learning_rate": 0.0006851485148514852,
      "loss": 1.8337,
      "step": 1636
    },
    {
      "epoch": 10.818669971086328,
      "grad_norm": 25.499786376953125,
      "learning_rate": 0.0006846534653465348,
      "loss": 1.1461,
      "step": 1637
    },
    {
      "epoch": 10.825278810408921,
      "grad_norm": 5.354098796844482,
      "learning_rate": 0.0006841584158415842,
      "loss": 2.0946,
      "step": 1638
    },
    {
      "epoch": 10.831887649731517,
      "grad_norm": 42.53362274169922,
      "learning_rate": 0.0006836633663366337,
      "loss": 1.5871,
      "step": 1639
    },
    {
      "epoch": 10.83849648905411,
      "grad_norm": 26.31174659729004,
      "learning_rate": 0.0006831683168316832,
      "loss": 3.4406,
      "step": 1640
    },
    {
      "epoch": 10.845105328376704,
      "grad_norm": 4.7506585121154785,
      "learning_rate": 0.0006826732673267328,
      "loss": 0.7346,
      "step": 1641
    },
    {
      "epoch": 10.851714167699297,
      "grad_norm": 23.221059799194336,
      "learning_rate": 0.0006821782178217823,
      "loss": 2.1744,
      "step": 1642
    },
    {
      "epoch": 10.858323007021891,
      "grad_norm": 77.903564453125,
      "learning_rate": 0.0006816831683168317,
      "loss": 3.4342,
      "step": 1643
    },
    {
      "epoch": 10.864931846344486,
      "grad_norm": 5.060241222381592,
      "learning_rate": 0.0006811881188118812,
      "loss": 1.8136,
      "step": 1644
    },
    {
      "epoch": 10.87154068566708,
      "grad_norm": 30.128162384033203,
      "learning_rate": 0.0006806930693069308,
      "loss": 3.2326,
      "step": 1645
    },
    {
      "epoch": 10.878149524989674,
      "grad_norm": 33.931549072265625,
      "learning_rate": 0.0006801980198019803,
      "loss": 2.1687,
      "step": 1646
    },
    {
      "epoch": 10.884758364312267,
      "grad_norm": 17.61072540283203,
      "learning_rate": 0.0006797029702970298,
      "loss": 4.4684,
      "step": 1647
    },
    {
      "epoch": 10.89136720363486,
      "grad_norm": 27.492172241210938,
      "learning_rate": 0.0006792079207920792,
      "loss": 3.0881,
      "step": 1648
    },
    {
      "epoch": 10.897976042957456,
      "grad_norm": 29.82512855529785,
      "learning_rate": 0.0006787128712871288,
      "loss": 3.0995,
      "step": 1649
    },
    {
      "epoch": 10.90458488228005,
      "grad_norm": 22.027374267578125,
      "learning_rate": 0.0006782178217821783,
      "loss": 1.2734,
      "step": 1650
    },
    {
      "epoch": 10.911193721602643,
      "grad_norm": 19.478092193603516,
      "learning_rate": 0.0006777227722772278,
      "loss": 5.0723,
      "step": 1651
    },
    {
      "epoch": 10.917802560925237,
      "grad_norm": 49.05753707885742,
      "learning_rate": 0.0006772277227722773,
      "loss": 0.9966,
      "step": 1652
    },
    {
      "epoch": 10.924411400247832,
      "grad_norm": 34.18728256225586,
      "learning_rate": 0.0006767326732673267,
      "loss": 3.4653,
      "step": 1653
    },
    {
      "epoch": 10.931020239570426,
      "grad_norm": 29.048906326293945,
      "learning_rate": 0.0006762376237623763,
      "loss": 1.5226,
      "step": 1654
    },
    {
      "epoch": 10.93762907889302,
      "grad_norm": 15.907516479492188,
      "learning_rate": 0.0006757425742574258,
      "loss": 1.3939,
      "step": 1655
    },
    {
      "epoch": 10.944237918215613,
      "grad_norm": 45.793365478515625,
      "learning_rate": 0.0006752475247524753,
      "loss": 3.5802,
      "step": 1656
    },
    {
      "epoch": 10.950846757538207,
      "grad_norm": 29.130874633789062,
      "learning_rate": 0.0006747524752475249,
      "loss": 1.4448,
      "step": 1657
    },
    {
      "epoch": 10.957455596860802,
      "grad_norm": 45.36323928833008,
      "learning_rate": 0.0006742574257425743,
      "loss": 2.8264,
      "step": 1658
    },
    {
      "epoch": 10.964064436183396,
      "grad_norm": 10.818854331970215,
      "learning_rate": 0.0006737623762376238,
      "loss": 3.8078,
      "step": 1659
    },
    {
      "epoch": 10.97067327550599,
      "grad_norm": 35.3408088684082,
      "learning_rate": 0.0006732673267326733,
      "loss": 2.0617,
      "step": 1660
    },
    {
      "epoch": 10.977282114828583,
      "grad_norm": 1.9516839981079102,
      "learning_rate": 0.0006727722772277228,
      "loss": 3.536,
      "step": 1661
    },
    {
      "epoch": 10.983890954151176,
      "grad_norm": 3.229539632797241,
      "learning_rate": 0.0006722772277227724,
      "loss": 1.8014,
      "step": 1662
    },
    {
      "epoch": 10.990499793473772,
      "grad_norm": 25.05298614501953,
      "learning_rate": 0.0006717821782178218,
      "loss": 6.0881,
      "step": 1663
    },
    {
      "epoch": 10.997108632796365,
      "grad_norm": 27.05402946472168,
      "learning_rate": 0.0006712871287128713,
      "loss": 0.915,
      "step": 1664
    },
    {
      "epoch": 10.997108632796365,
      "eval_validation_error_bar": 0.05052720786287903,
      "eval_validation_loss": 7.369189739227295,
      "eval_validation_pearsonr": 0.5723441342738375,
      "eval_validation_rmse": 2.714625120162964,
      "eval_validation_runtime": 33.3199,
      "eval_validation_samples_per_second": 6.092,
      "eval_validation_spearman": 0.5824824272007906,
      "eval_validation_steps_per_second": 6.092,
      "step": 1664
    },
    {
      "epoch": 10.997108632796365,
      "eval_test_error_bar": 0.04201625300314551,
      "eval_test_loss": 8.254347801208496,
      "eval_test_pearsonr": 0.5299765328636954,
      "eval_test_rmse": 2.8730380535125732,
      "eval_test_runtime": 41.3658,
      "eval_test_samples_per_second": 7.881,
      "eval_test_spearman": 0.5432217877225746,
      "eval_test_steps_per_second": 7.881,
      "step": 1664
    },
    {
      "epoch": 11.003717472118959,
      "grad_norm": 8.002259254455566,
      "learning_rate": 0.0006707920792079208,
      "loss": 1.222,
      "step": 1665
    },
    {
      "epoch": 11.010326311441553,
      "grad_norm": 6.958491325378418,
      "learning_rate": 0.0006702970297029704,
      "loss": 3.2926,
      "step": 1666
    },
    {
      "epoch": 11.016935150764146,
      "grad_norm": 31.512577056884766,
      "learning_rate": 0.0006698019801980199,
      "loss": 0.7045,
      "step": 1667
    },
    {
      "epoch": 11.023543990086742,
      "grad_norm": 7.586275577545166,
      "learning_rate": 0.0006693069306930693,
      "loss": 1.9525,
      "step": 1668
    },
    {
      "epoch": 11.030152829409335,
      "grad_norm": 18.29734992980957,
      "learning_rate": 0.0006688118811881188,
      "loss": 0.8404,
      "step": 1669
    },
    {
      "epoch": 11.036761668731929,
      "grad_norm": 12.092473983764648,
      "learning_rate": 0.0006683168316831684,
      "loss": 4.0366,
      "step": 1670
    },
    {
      "epoch": 11.043370508054522,
      "grad_norm": 10.483429908752441,
      "learning_rate": 0.0006678217821782179,
      "loss": 4.5197,
      "step": 1671
    },
    {
      "epoch": 11.049979347377118,
      "grad_norm": 64.79544067382812,
      "learning_rate": 0.0006673267326732674,
      "loss": 3.1223,
      "step": 1672
    },
    {
      "epoch": 11.056588186699711,
      "grad_norm": 47.064876556396484,
      "learning_rate": 0.0006668316831683168,
      "loss": 1.179,
      "step": 1673
    },
    {
      "epoch": 11.063197026022305,
      "grad_norm": 57.84483337402344,
      "learning_rate": 0.0006663366336633664,
      "loss": 2.282,
      "step": 1674
    },
    {
      "epoch": 11.069805865344899,
      "grad_norm": 49.359256744384766,
      "learning_rate": 0.0006658415841584159,
      "loss": 2.8126,
      "step": 1675
    },
    {
      "epoch": 11.076414704667492,
      "grad_norm": 26.37553596496582,
      "learning_rate": 0.0006653465346534654,
      "loss": 1.0872,
      "step": 1676
    },
    {
      "epoch": 11.083023543990087,
      "grad_norm": 48.16448211669922,
      "learning_rate": 0.0006648514851485149,
      "loss": 2.5208,
      "step": 1677
    },
    {
      "epoch": 11.089632383312681,
      "grad_norm": 78.48553466796875,
      "learning_rate": 0.0006643564356435644,
      "loss": 2.1894,
      "step": 1678
    },
    {
      "epoch": 11.096241222635275,
      "grad_norm": 21.47753143310547,
      "learning_rate": 0.0006638613861386139,
      "loss": 1.5008,
      "step": 1679
    },
    {
      "epoch": 11.102850061957868,
      "grad_norm": 32.10436248779297,
      "learning_rate": 0.0006633663366336634,
      "loss": 6.812,
      "step": 1680
    },
    {
      "epoch": 11.109458901280462,
      "grad_norm": 35.8696403503418,
      "learning_rate": 0.0006628712871287129,
      "loss": 1.335,
      "step": 1681
    },
    {
      "epoch": 11.116067740603057,
      "grad_norm": 2.6079859733581543,
      "learning_rate": 0.0006623762376237625,
      "loss": 0.5833,
      "step": 1682
    },
    {
      "epoch": 11.12267657992565,
      "grad_norm": 34.28611373901367,
      "learning_rate": 0.0006618811881188119,
      "loss": 1.2987,
      "step": 1683
    },
    {
      "epoch": 11.129285419248244,
      "grad_norm": 31.527286529541016,
      "learning_rate": 0.0006613861386138614,
      "loss": 1.1172,
      "step": 1684
    },
    {
      "epoch": 11.135894258570838,
      "grad_norm": 39.42784118652344,
      "learning_rate": 0.0006608910891089109,
      "loss": 3.4287,
      "step": 1685
    },
    {
      "epoch": 11.142503097893432,
      "grad_norm": 78.66917419433594,
      "learning_rate": 0.0006603960396039605,
      "loss": 4.4974,
      "step": 1686
    },
    {
      "epoch": 11.149111937216027,
      "grad_norm": 46.463775634765625,
      "learning_rate": 0.00065990099009901,
      "loss": 1.73,
      "step": 1687
    },
    {
      "epoch": 11.15572077653862,
      "grad_norm": 41.925785064697266,
      "learning_rate": 0.0006594059405940594,
      "loss": 2.5055,
      "step": 1688
    },
    {
      "epoch": 11.162329615861214,
      "grad_norm": 3.9000282287597656,
      "learning_rate": 0.0006589108910891089,
      "loss": 2.2674,
      "step": 1689
    },
    {
      "epoch": 11.168938455183808,
      "grad_norm": 13.222474098205566,
      "learning_rate": 0.0006584158415841585,
      "loss": 3.2341,
      "step": 1690
    },
    {
      "epoch": 11.175547294506403,
      "grad_norm": 57.52634811401367,
      "learning_rate": 0.000657920792079208,
      "loss": 3.3541,
      "step": 1691
    },
    {
      "epoch": 11.182156133828997,
      "grad_norm": 7.054818153381348,
      "learning_rate": 0.0006574257425742575,
      "loss": 5.3493,
      "step": 1692
    },
    {
      "epoch": 11.18876497315159,
      "grad_norm": 29.85091209411621,
      "learning_rate": 0.0006569306930693069,
      "loss": 1.2346,
      "step": 1693
    },
    {
      "epoch": 11.195373812474184,
      "grad_norm": 10.14582633972168,
      "learning_rate": 0.0006564356435643565,
      "loss": 2.2824,
      "step": 1694
    },
    {
      "epoch": 11.201982651796778,
      "grad_norm": 26.83354949951172,
      "learning_rate": 0.000655940594059406,
      "loss": 1.1144,
      "step": 1695
    },
    {
      "epoch": 11.208591491119373,
      "grad_norm": 12.03781795501709,
      "learning_rate": 0.0006554455445544555,
      "loss": 2.468,
      "step": 1696
    },
    {
      "epoch": 11.215200330441967,
      "grad_norm": 35.21531295776367,
      "learning_rate": 0.000654950495049505,
      "loss": 2.6139,
      "step": 1697
    },
    {
      "epoch": 11.22180916976456,
      "grad_norm": 95.59779357910156,
      "learning_rate": 0.0006544554455445545,
      "loss": 2.4754,
      "step": 1698
    },
    {
      "epoch": 11.228418009087154,
      "grad_norm": 53.414676666259766,
      "learning_rate": 0.000653960396039604,
      "loss": 3.1631,
      "step": 1699
    },
    {
      "epoch": 11.235026848409747,
      "grad_norm": 103.48184204101562,
      "learning_rate": 0.0006534653465346535,
      "loss": 3.0601,
      "step": 1700
    },
    {
      "epoch": 11.241635687732343,
      "grad_norm": 77.1518783569336,
      "learning_rate": 0.000652970297029703,
      "loss": 4.1841,
      "step": 1701
    },
    {
      "epoch": 11.248244527054936,
      "grad_norm": 12.931929588317871,
      "learning_rate": 0.0006524752475247526,
      "loss": 1.4029,
      "step": 1702
    },
    {
      "epoch": 11.25485336637753,
      "grad_norm": 81.73975372314453,
      "learning_rate": 0.000651980198019802,
      "loss": 3.1377,
      "step": 1703
    },
    {
      "epoch": 11.261462205700123,
      "grad_norm": 50.644927978515625,
      "learning_rate": 0.0006514851485148515,
      "loss": 2.9194,
      "step": 1704
    },
    {
      "epoch": 11.268071045022717,
      "grad_norm": 42.210933685302734,
      "learning_rate": 0.000650990099009901,
      "loss": 2.8042,
      "step": 1705
    },
    {
      "epoch": 11.274679884345312,
      "grad_norm": 17.494455337524414,
      "learning_rate": 0.0006504950495049506,
      "loss": 2.0984,
      "step": 1706
    },
    {
      "epoch": 11.281288723667906,
      "grad_norm": 64.46641540527344,
      "learning_rate": 0.0006500000000000001,
      "loss": 1.6841,
      "step": 1707
    },
    {
      "epoch": 11.2878975629905,
      "grad_norm": 70.83894348144531,
      "learning_rate": 0.0006495049504950495,
      "loss": 3.4199,
      "step": 1708
    },
    {
      "epoch": 11.294506402313093,
      "grad_norm": 83.19216918945312,
      "learning_rate": 0.000649009900990099,
      "loss": 4.2718,
      "step": 1709
    },
    {
      "epoch": 11.301115241635689,
      "grad_norm": 59.44252014160156,
      "learning_rate": 0.0006485148514851485,
      "loss": 3.4308,
      "step": 1710
    },
    {
      "epoch": 11.307724080958282,
      "grad_norm": 43.86209487915039,
      "learning_rate": 0.0006480198019801981,
      "loss": 4.1947,
      "step": 1711
    },
    {
      "epoch": 11.314332920280876,
      "grad_norm": 9.806853294372559,
      "learning_rate": 0.0006475247524752476,
      "loss": 3.8502,
      "step": 1712
    },
    {
      "epoch": 11.32094175960347,
      "grad_norm": 25.388336181640625,
      "learning_rate": 0.000647029702970297,
      "loss": 1.4018,
      "step": 1713
    },
    {
      "epoch": 11.327550598926063,
      "grad_norm": 24.563735961914062,
      "learning_rate": 0.0006465346534653465,
      "loss": 1.1711,
      "step": 1714
    },
    {
      "epoch": 11.334159438248658,
      "grad_norm": 39.467308044433594,
      "learning_rate": 0.0006460396039603961,
      "loss": 0.9184,
      "step": 1715
    },
    {
      "epoch": 11.340768277571252,
      "grad_norm": 30.685951232910156,
      "learning_rate": 0.0006455445544554456,
      "loss": 1.2328,
      "step": 1716
    },
    {
      "epoch": 11.347377116893846,
      "grad_norm": 48.703487396240234,
      "learning_rate": 0.0006450495049504951,
      "loss": 3.4101,
      "step": 1717
    },
    {
      "epoch": 11.35398595621644,
      "grad_norm": 31.605113983154297,
      "learning_rate": 0.0006445544554455445,
      "loss": 1.5174,
      "step": 1718
    },
    {
      "epoch": 11.360594795539033,
      "grad_norm": 33.392093658447266,
      "learning_rate": 0.0006440594059405941,
      "loss": 2.6429,
      "step": 1719
    },
    {
      "epoch": 11.367203634861628,
      "grad_norm": 37.099708557128906,
      "learning_rate": 0.0006435643564356436,
      "loss": 2.9497,
      "step": 1720
    },
    {
      "epoch": 11.373812474184222,
      "grad_norm": 28.22068977355957,
      "learning_rate": 0.0006430693069306931,
      "loss": 1.7342,
      "step": 1721
    },
    {
      "epoch": 11.380421313506815,
      "grad_norm": 66.28018951416016,
      "learning_rate": 0.0006425742574257426,
      "loss": 2.1258,
      "step": 1722
    },
    {
      "epoch": 11.387030152829409,
      "grad_norm": 52.309844970703125,
      "learning_rate": 0.0006420792079207921,
      "loss": 1.9214,
      "step": 1723
    },
    {
      "epoch": 11.393638992152002,
      "grad_norm": 17.926870346069336,
      "learning_rate": 0.0006415841584158416,
      "loss": 3.4303,
      "step": 1724
    },
    {
      "epoch": 11.400247831474598,
      "grad_norm": 16.863513946533203,
      "learning_rate": 0.0006410891089108911,
      "loss": 3.4415,
      "step": 1725
    },
    {
      "epoch": 11.406856670797191,
      "grad_norm": 71.94983673095703,
      "learning_rate": 0.0006405940594059406,
      "loss": 1.7782,
      "step": 1726
    },
    {
      "epoch": 11.413465510119785,
      "grad_norm": 40.044471740722656,
      "learning_rate": 0.0006400990099009902,
      "loss": 1.2625,
      "step": 1727
    },
    {
      "epoch": 11.420074349442379,
      "grad_norm": 12.85054874420166,
      "learning_rate": 0.0006396039603960396,
      "loss": 2.1658,
      "step": 1728
    },
    {
      "epoch": 11.426683188764972,
      "grad_norm": 45.054901123046875,
      "learning_rate": 0.0006391089108910891,
      "loss": 2.3814,
      "step": 1729
    },
    {
      "epoch": 11.433292028087568,
      "grad_norm": 11.663820266723633,
      "learning_rate": 0.0006386138613861386,
      "loss": 2.2894,
      "step": 1730
    },
    {
      "epoch": 11.439900867410161,
      "grad_norm": 23.673057556152344,
      "learning_rate": 0.0006381188118811882,
      "loss": 2.927,
      "step": 1731
    },
    {
      "epoch": 11.446509706732755,
      "grad_norm": 17.62347984313965,
      "learning_rate": 0.0006376237623762377,
      "loss": 1.0167,
      "step": 1732
    },
    {
      "epoch": 11.453118546055348,
      "grad_norm": 32.43389892578125,
      "learning_rate": 0.0006371287128712871,
      "loss": 1.3423,
      "step": 1733
    },
    {
      "epoch": 11.459727385377944,
      "grad_norm": 50.96030807495117,
      "learning_rate": 0.0006366336633663366,
      "loss": 1.8207,
      "step": 1734
    },
    {
      "epoch": 11.466336224700537,
      "grad_norm": 42.49726104736328,
      "learning_rate": 0.0006361386138613862,
      "loss": 2.8437,
      "step": 1735
    },
    {
      "epoch": 11.472945064023131,
      "grad_norm": 4.588711738586426,
      "learning_rate": 0.0006356435643564357,
      "loss": 1.0029,
      "step": 1736
    },
    {
      "epoch": 11.479553903345725,
      "grad_norm": 19.577730178833008,
      "learning_rate": 0.0006351485148514852,
      "loss": 0.7975,
      "step": 1737
    },
    {
      "epoch": 11.486162742668318,
      "grad_norm": 15.620845794677734,
      "learning_rate": 0.0006346534653465346,
      "loss": 0.8965,
      "step": 1738
    },
    {
      "epoch": 11.492771581990914,
      "grad_norm": 37.52016067504883,
      "learning_rate": 0.0006341584158415842,
      "loss": 1.6572,
      "step": 1739
    },
    {
      "epoch": 11.499380421313507,
      "grad_norm": 25.017574310302734,
      "learning_rate": 0.0006336633663366337,
      "loss": 1.4797,
      "step": 1740
    },
    {
      "epoch": 11.5059892606361,
      "grad_norm": 87.87091064453125,
      "learning_rate": 0.0006331683168316832,
      "loss": 3.276,
      "step": 1741
    },
    {
      "epoch": 11.512598099958694,
      "grad_norm": 53.59896469116211,
      "learning_rate": 0.0006326732673267327,
      "loss": 2.5746,
      "step": 1742
    },
    {
      "epoch": 11.519206939281288,
      "grad_norm": 30.1254825592041,
      "learning_rate": 0.0006321782178217822,
      "loss": 2.7506,
      "step": 1743
    },
    {
      "epoch": 11.525815778603883,
      "grad_norm": 9.208407402038574,
      "learning_rate": 0.0006316831683168317,
      "loss": 2.4171,
      "step": 1744
    },
    {
      "epoch": 11.532424617926477,
      "grad_norm": 3.985079288482666,
      "learning_rate": 0.0006311881188118812,
      "loss": 1.8436,
      "step": 1745
    },
    {
      "epoch": 11.53903345724907,
      "grad_norm": 3.077781915664673,
      "learning_rate": 0.0006306930693069307,
      "loss": 1.0488,
      "step": 1746
    },
    {
      "epoch": 11.545642296571664,
      "grad_norm": 15.265460968017578,
      "learning_rate": 0.0006301980198019803,
      "loss": 2.7895,
      "step": 1747
    },
    {
      "epoch": 11.55225113589426,
      "grad_norm": 15.8674898147583,
      "learning_rate": 0.0006297029702970297,
      "loss": 3.753,
      "step": 1748
    },
    {
      "epoch": 11.558859975216853,
      "grad_norm": 30.970073699951172,
      "learning_rate": 0.0006292079207920792,
      "loss": 1.6927,
      "step": 1749
    },
    {
      "epoch": 11.565468814539447,
      "grad_norm": 45.098052978515625,
      "learning_rate": 0.0006287128712871287,
      "loss": 2.3744,
      "step": 1750
    },
    {
      "epoch": 11.57207765386204,
      "grad_norm": 42.480247497558594,
      "learning_rate": 0.0006282178217821783,
      "loss": 1.719,
      "step": 1751
    },
    {
      "epoch": 11.578686493184634,
      "grad_norm": 15.460658073425293,
      "learning_rate": 0.0006277227722772278,
      "loss": 2.6761,
      "step": 1752
    },
    {
      "epoch": 11.58529533250723,
      "grad_norm": 2.3577418327331543,
      "learning_rate": 0.0006272277227722772,
      "loss": 1.2928,
      "step": 1753
    },
    {
      "epoch": 11.591904171829823,
      "grad_norm": 49.99907302856445,
      "learning_rate": 0.0006267326732673267,
      "loss": 3.1221,
      "step": 1754
    },
    {
      "epoch": 11.598513011152416,
      "grad_norm": 8.390487670898438,
      "learning_rate": 0.0006262376237623763,
      "loss": 2.312,
      "step": 1755
    },
    {
      "epoch": 11.60512185047501,
      "grad_norm": 61.42626190185547,
      "learning_rate": 0.0006257425742574258,
      "loss": 2.0947,
      "step": 1756
    },
    {
      "epoch": 11.611730689797604,
      "grad_norm": 14.58639907836914,
      "learning_rate": 0.0006252475247524753,
      "loss": 4.2608,
      "step": 1757
    },
    {
      "epoch": 11.618339529120199,
      "grad_norm": 38.34088134765625,
      "learning_rate": 0.0006247524752475247,
      "loss": 1.5963,
      "step": 1758
    },
    {
      "epoch": 11.624948368442793,
      "grad_norm": 59.7463493347168,
      "learning_rate": 0.0006242574257425742,
      "loss": 1.778,
      "step": 1759
    },
    {
      "epoch": 11.631557207765386,
      "grad_norm": 23.354598999023438,
      "learning_rate": 0.0006237623762376238,
      "loss": 1.8574,
      "step": 1760
    },
    {
      "epoch": 11.63816604708798,
      "grad_norm": 6.818528175354004,
      "learning_rate": 0.0006232673267326733,
      "loss": 1.5972,
      "step": 1761
    },
    {
      "epoch": 11.644774886410573,
      "grad_norm": 34.21983337402344,
      "learning_rate": 0.0006227722772277228,
      "loss": 1.4439,
      "step": 1762
    },
    {
      "epoch": 11.651383725733169,
      "grad_norm": 3.013076066970825,
      "learning_rate": 0.0006222772277227722,
      "loss": 1.0953,
      "step": 1763
    },
    {
      "epoch": 11.657992565055762,
      "grad_norm": 15.462626457214355,
      "learning_rate": 0.0006217821782178218,
      "loss": 4.0937,
      "step": 1764
    },
    {
      "epoch": 11.664601404378356,
      "grad_norm": 42.05521011352539,
      "learning_rate": 0.0006212871287128713,
      "loss": 1.0651,
      "step": 1765
    },
    {
      "epoch": 11.67121024370095,
      "grad_norm": 47.57916259765625,
      "learning_rate": 0.0006207920792079208,
      "loss": 4.5009,
      "step": 1766
    },
    {
      "epoch": 11.677819083023543,
      "grad_norm": 8.628520965576172,
      "learning_rate": 0.0006202970297029703,
      "loss": 4.702,
      "step": 1767
    },
    {
      "epoch": 11.684427922346138,
      "grad_norm": 18.276992797851562,
      "learning_rate": 0.0006198019801980198,
      "loss": 2.3751,
      "step": 1768
    },
    {
      "epoch": 11.691036761668732,
      "grad_norm": 71.0797348022461,
      "learning_rate": 0.0006193069306930693,
      "loss": 4.6453,
      "step": 1769
    },
    {
      "epoch": 11.697645600991326,
      "grad_norm": 3.7856945991516113,
      "learning_rate": 0.0006188118811881188,
      "loss": 0.5305,
      "step": 1770
    },
    {
      "epoch": 11.70425444031392,
      "grad_norm": 16.819808959960938,
      "learning_rate": 0.0006183168316831683,
      "loss": 1.749,
      "step": 1771
    },
    {
      "epoch": 11.710863279636515,
      "grad_norm": 75.8904037475586,
      "learning_rate": 0.0006178217821782179,
      "loss": 2.5411,
      "step": 1772
    },
    {
      "epoch": 11.717472118959108,
      "grad_norm": 66.19648742675781,
      "learning_rate": 0.0006173267326732673,
      "loss": 2.5984,
      "step": 1773
    },
    {
      "epoch": 11.724080958281702,
      "grad_norm": 9.907790184020996,
      "learning_rate": 0.0006168316831683168,
      "loss": 2.8147,
      "step": 1774
    },
    {
      "epoch": 11.730689797604295,
      "grad_norm": 57.8854866027832,
      "learning_rate": 0.0006163366336633663,
      "loss": 3.3717,
      "step": 1775
    },
    {
      "epoch": 11.737298636926889,
      "grad_norm": 59.59059524536133,
      "learning_rate": 0.0006158415841584159,
      "loss": 2.1598,
      "step": 1776
    },
    {
      "epoch": 11.743907476249484,
      "grad_norm": 25.6090087890625,
      "learning_rate": 0.0006153465346534654,
      "loss": 1.1737,
      "step": 1777
    },
    {
      "epoch": 11.750516315572078,
      "grad_norm": 63.52229309082031,
      "learning_rate": 0.0006148514851485148,
      "loss": 3.0845,
      "step": 1778
    },
    {
      "epoch": 11.757125154894672,
      "grad_norm": 10.487502098083496,
      "learning_rate": 0.0006143564356435643,
      "loss": 1.0726,
      "step": 1779
    },
    {
      "epoch": 11.763733994217265,
      "grad_norm": 23.764585494995117,
      "learning_rate": 0.0006138613861386139,
      "loss": 1.9183,
      "step": 1780
    },
    {
      "epoch": 11.770342833539859,
      "grad_norm": 65.02439880371094,
      "learning_rate": 0.0006133663366336634,
      "loss": 1.7308,
      "step": 1781
    },
    {
      "epoch": 11.776951672862454,
      "grad_norm": 60.41595458984375,
      "learning_rate": 0.0006128712871287129,
      "loss": 1.7324,
      "step": 1782
    },
    {
      "epoch": 11.783560512185048,
      "grad_norm": 84.3721923828125,
      "learning_rate": 0.0006123762376237623,
      "loss": 2.5897,
      "step": 1783
    },
    {
      "epoch": 11.790169351507641,
      "grad_norm": 9.138956069946289,
      "learning_rate": 0.0006118811881188119,
      "loss": 2.135,
      "step": 1784
    },
    {
      "epoch": 11.796778190830235,
      "grad_norm": 49.243831634521484,
      "learning_rate": 0.0006113861386138614,
      "loss": 2.7737,
      "step": 1785
    },
    {
      "epoch": 11.80338703015283,
      "grad_norm": 23.330900192260742,
      "learning_rate": 0.0006108910891089109,
      "loss": 1.4116,
      "step": 1786
    },
    {
      "epoch": 11.809995869475424,
      "grad_norm": 80.380126953125,
      "learning_rate": 0.0006103960396039604,
      "loss": 2.3504,
      "step": 1787
    },
    {
      "epoch": 11.816604708798017,
      "grad_norm": 38.65131378173828,
      "learning_rate": 0.0006099009900990099,
      "loss": 0.9754,
      "step": 1788
    },
    {
      "epoch": 11.823213548120611,
      "grad_norm": 9.70030689239502,
      "learning_rate": 0.0006094059405940594,
      "loss": 4.3138,
      "step": 1789
    },
    {
      "epoch": 11.829822387443205,
      "grad_norm": 21.615278244018555,
      "learning_rate": 0.0006089108910891089,
      "loss": 2.5642,
      "step": 1790
    },
    {
      "epoch": 11.8364312267658,
      "grad_norm": 17.817089080810547,
      "learning_rate": 0.0006084158415841584,
      "loss": 0.9026,
      "step": 1791
    },
    {
      "epoch": 11.843040066088394,
      "grad_norm": 7.215211391448975,
      "learning_rate": 0.000607920792079208,
      "loss": 2.6064,
      "step": 1792
    },
    {
      "epoch": 11.849648905410987,
      "grad_norm": 7.419558048248291,
      "learning_rate": 0.0006074257425742574,
      "loss": 1.1173,
      "step": 1793
    },
    {
      "epoch": 11.85625774473358,
      "grad_norm": 15.30114459991455,
      "learning_rate": 0.0006069306930693069,
      "loss": 3.9103,
      "step": 1794
    },
    {
      "epoch": 11.862866584056174,
      "grad_norm": 1.2338372468948364,
      "learning_rate": 0.0006064356435643564,
      "loss": 2.3578,
      "step": 1795
    },
    {
      "epoch": 11.86947542337877,
      "grad_norm": 24.36606216430664,
      "learning_rate": 0.000605940594059406,
      "loss": 1.01,
      "step": 1796
    },
    {
      "epoch": 11.876084262701363,
      "grad_norm": 24.44964599609375,
      "learning_rate": 0.0006054455445544555,
      "loss": 2.0205,
      "step": 1797
    },
    {
      "epoch": 11.882693102023957,
      "grad_norm": 17.2473201751709,
      "learning_rate": 0.0006049504950495049,
      "loss": 1.4467,
      "step": 1798
    },
    {
      "epoch": 11.88930194134655,
      "grad_norm": 59.05057907104492,
      "learning_rate": 0.0006044554455445544,
      "loss": 4.1325,
      "step": 1799
    },
    {
      "epoch": 11.895910780669144,
      "grad_norm": 10.829244613647461,
      "learning_rate": 0.000603960396039604,
      "loss": 2.8501,
      "step": 1800
    },
    {
      "epoch": 11.90251961999174,
      "grad_norm": 134.60060119628906,
      "learning_rate": 0.0006034653465346535,
      "loss": 7.6215,
      "step": 1801
    },
    {
      "epoch": 11.909128459314333,
      "grad_norm": 50.85784912109375,
      "learning_rate": 0.000602970297029703,
      "loss": 4.6365,
      "step": 1802
    },
    {
      "epoch": 11.915737298636927,
      "grad_norm": 39.0904541015625,
      "learning_rate": 0.0006024752475247524,
      "loss": 2.9148,
      "step": 1803
    },
    {
      "epoch": 11.92234613795952,
      "grad_norm": 3.5915753841400146,
      "learning_rate": 0.000601980198019802,
      "loss": 2.2028,
      "step": 1804
    },
    {
      "epoch": 11.928954977282114,
      "grad_norm": 23.450725555419922,
      "learning_rate": 0.0006014851485148515,
      "loss": 4.5351,
      "step": 1805
    },
    {
      "epoch": 11.93556381660471,
      "grad_norm": 46.99969482421875,
      "learning_rate": 0.000600990099009901,
      "loss": 4.8854,
      "step": 1806
    },
    {
      "epoch": 11.942172655927303,
      "grad_norm": 30.363056182861328,
      "learning_rate": 0.0006004950495049505,
      "loss": 1.352,
      "step": 1807
    },
    {
      "epoch": 11.948781495249897,
      "grad_norm": 45.13411331176758,
      "learning_rate": 0.0006,
      "loss": 1.2147,
      "step": 1808
    },
    {
      "epoch": 11.95539033457249,
      "grad_norm": 6.006586074829102,
      "learning_rate": 0.0005995049504950495,
      "loss": 2.1971,
      "step": 1809
    },
    {
      "epoch": 11.961999173895085,
      "grad_norm": 59.621402740478516,
      "learning_rate": 0.000599009900990099,
      "loss": 2.1085,
      "step": 1810
    },
    {
      "epoch": 11.968608013217679,
      "grad_norm": 30.576486587524414,
      "learning_rate": 0.0005985148514851485,
      "loss": 5.1186,
      "step": 1811
    },
    {
      "epoch": 11.975216852540273,
      "grad_norm": 33.01768493652344,
      "learning_rate": 0.000598019801980198,
      "loss": 2.5443,
      "step": 1812
    },
    {
      "epoch": 11.981825691862866,
      "grad_norm": 77.25086975097656,
      "learning_rate": 0.0005975247524752475,
      "loss": 4.8037,
      "step": 1813
    },
    {
      "epoch": 11.98843453118546,
      "grad_norm": 8.293791770935059,
      "learning_rate": 0.000597029702970297,
      "loss": 3.7925,
      "step": 1814
    },
    {
      "epoch": 11.995043370508055,
      "grad_norm": 39.73959732055664,
      "learning_rate": 0.0005965346534653465,
      "loss": 2.5796,
      "step": 1815
    },
    {
      "epoch": 11.995043370508055,
      "eval_validation_error_bar": 0.048842267884948436,
      "eval_validation_loss": 7.6173248291015625,
      "eval_validation_pearsonr": 0.5822455457823603,
      "eval_validation_rmse": 2.7599501609802246,
      "eval_validation_runtime": 33.4862,
      "eval_validation_samples_per_second": 6.062,
      "eval_validation_spearman": 0.603961411564672,
      "eval_validation_steps_per_second": 6.062,
      "step": 1815
    },
    {
      "epoch": 11.995043370508055,
      "eval_test_error_bar": 0.041801486769451085,
      "eval_test_loss": 7.929616928100586,
      "eval_test_pearsonr": 0.5506602578271885,
      "eval_test_rmse": 2.815957546234131,
      "eval_test_runtime": 38.9955,
      "eval_test_samples_per_second": 8.36,
      "eval_test_spearman": 0.5471213559594273,
      "eval_test_steps_per_second": 8.36,
      "step": 1815
    },
    {
      "epoch": 12.001652209830649,
      "grad_norm": 5.739715099334717,
      "learning_rate": 0.000596039603960396,
      "loss": 3.2068,
      "step": 1816
    },
    {
      "epoch": 12.008261049153242,
      "grad_norm": 18.632522583007812,
      "learning_rate": 0.0005955445544554456,
      "loss": 1.0313,
      "step": 1817
    },
    {
      "epoch": 12.014869888475836,
      "grad_norm": 3.1285972595214844,
      "learning_rate": 0.000595049504950495,
      "loss": 2.0278,
      "step": 1818
    },
    {
      "epoch": 12.02147872779843,
      "grad_norm": 31.147911071777344,
      "learning_rate": 0.0005945544554455445,
      "loss": 1.3984,
      "step": 1819
    },
    {
      "epoch": 12.028087567121025,
      "grad_norm": 48.70476531982422,
      "learning_rate": 0.000594059405940594,
      "loss": 1.3449,
      "step": 1820
    },
    {
      "epoch": 12.034696406443619,
      "grad_norm": 22.26484489440918,
      "learning_rate": 0.0005935643564356436,
      "loss": 2.1468,
      "step": 1821
    },
    {
      "epoch": 12.041305245766212,
      "grad_norm": 11.073841094970703,
      "learning_rate": 0.0005930693069306931,
      "loss": 2.9158,
      "step": 1822
    },
    {
      "epoch": 12.047914085088806,
      "grad_norm": 3.765252113342285,
      "learning_rate": 0.0005925742574257425,
      "loss": 3.432,
      "step": 1823
    },
    {
      "epoch": 12.0545229244114,
      "grad_norm": 42.13814926147461,
      "learning_rate": 0.000592079207920792,
      "loss": 1.6103,
      "step": 1824
    },
    {
      "epoch": 12.061131763733995,
      "grad_norm": 4.800465106964111,
      "learning_rate": 0.0005915841584158416,
      "loss": 3.5581,
      "step": 1825
    },
    {
      "epoch": 12.067740603056588,
      "grad_norm": 15.210591316223145,
      "learning_rate": 0.0005910891089108911,
      "loss": 1.794,
      "step": 1826
    },
    {
      "epoch": 12.074349442379182,
      "grad_norm": 23.82373046875,
      "learning_rate": 0.0005905940594059406,
      "loss": 1.5259,
      "step": 1827
    },
    {
      "epoch": 12.080958281701776,
      "grad_norm": 24.494359970092773,
      "learning_rate": 0.00059009900990099,
      "loss": 0.7441,
      "step": 1828
    },
    {
      "epoch": 12.087567121024371,
      "grad_norm": 38.45529556274414,
      "learning_rate": 0.0005896039603960396,
      "loss": 2.3823,
      "step": 1829
    },
    {
      "epoch": 12.094175960346965,
      "grad_norm": 71.9893569946289,
      "learning_rate": 0.0005891089108910891,
      "loss": 2.3021,
      "step": 1830
    },
    {
      "epoch": 12.100784799669558,
      "grad_norm": 54.326290130615234,
      "learning_rate": 0.0005886138613861386,
      "loss": 1.4209,
      "step": 1831
    },
    {
      "epoch": 12.107393638992152,
      "grad_norm": 27.193912506103516,
      "learning_rate": 0.0005881188118811881,
      "loss": 2.1965,
      "step": 1832
    },
    {
      "epoch": 12.114002478314745,
      "grad_norm": 15.484054565429688,
      "learning_rate": 0.0005876237623762376,
      "loss": 1.0442,
      "step": 1833
    },
    {
      "epoch": 12.12061131763734,
      "grad_norm": 35.98983383178711,
      "learning_rate": 0.0005871287128712871,
      "loss": 1.5848,
      "step": 1834
    },
    {
      "epoch": 12.127220156959934,
      "grad_norm": 56.16700744628906,
      "learning_rate": 0.0005866336633663366,
      "loss": 2.0863,
      "step": 1835
    },
    {
      "epoch": 12.133828996282528,
      "grad_norm": 50.27846145629883,
      "learning_rate": 0.0005861386138613861,
      "loss": 2.2915,
      "step": 1836
    },
    {
      "epoch": 12.140437835605121,
      "grad_norm": 34.995018005371094,
      "learning_rate": 0.0005856435643564357,
      "loss": 2.4804,
      "step": 1837
    },
    {
      "epoch": 12.147046674927715,
      "grad_norm": 21.696439743041992,
      "learning_rate": 0.0005851485148514851,
      "loss": 3.5537,
      "step": 1838
    },
    {
      "epoch": 12.15365551425031,
      "grad_norm": 7.307315826416016,
      "learning_rate": 0.0005846534653465346,
      "loss": 2.7831,
      "step": 1839
    },
    {
      "epoch": 12.160264353572904,
      "grad_norm": 30.72752571105957,
      "learning_rate": 0.0005841584158415841,
      "loss": 1.5616,
      "step": 1840
    },
    {
      "epoch": 12.166873192895498,
      "grad_norm": 6.361548900604248,
      "learning_rate": 0.0005836633663366337,
      "loss": 2.2583,
      "step": 1841
    },
    {
      "epoch": 12.173482032218091,
      "grad_norm": 33.15582275390625,
      "learning_rate": 0.0005831683168316832,
      "loss": 3.2287,
      "step": 1842
    },
    {
      "epoch": 12.180090871540685,
      "grad_norm": 43.94182586669922,
      "learning_rate": 0.0005826732673267326,
      "loss": 4.2312,
      "step": 1843
    },
    {
      "epoch": 12.18669971086328,
      "grad_norm": 49.02111053466797,
      "learning_rate": 0.0005821782178217821,
      "loss": 1.5988,
      "step": 1844
    },
    {
      "epoch": 12.193308550185874,
      "grad_norm": 12.412351608276367,
      "learning_rate": 0.0005816831683168317,
      "loss": 1.479,
      "step": 1845
    },
    {
      "epoch": 12.199917389508467,
      "grad_norm": 10.04604721069336,
      "learning_rate": 0.0005811881188118812,
      "loss": 3.7047,
      "step": 1846
    },
    {
      "epoch": 12.206526228831061,
      "grad_norm": 15.220686912536621,
      "learning_rate": 0.0005806930693069307,
      "loss": 2.2127,
      "step": 1847
    },
    {
      "epoch": 12.213135068153656,
      "grad_norm": 13.976868629455566,
      "learning_rate": 0.0005801980198019801,
      "loss": 1.6947,
      "step": 1848
    },
    {
      "epoch": 12.21974390747625,
      "grad_norm": 3.5585010051727295,
      "learning_rate": 0.0005797029702970297,
      "loss": 2.0916,
      "step": 1849
    },
    {
      "epoch": 12.226352746798844,
      "grad_norm": 6.855222702026367,
      "learning_rate": 0.0005792079207920792,
      "loss": 1.0324,
      "step": 1850
    },
    {
      "epoch": 12.232961586121437,
      "grad_norm": 24.28536033630371,
      "learning_rate": 0.0005787128712871287,
      "loss": 0.718,
      "step": 1851
    },
    {
      "epoch": 12.23957042544403,
      "grad_norm": 12.276209831237793,
      "learning_rate": 0.0005782178217821782,
      "loss": 1.0988,
      "step": 1852
    },
    {
      "epoch": 12.246179264766626,
      "grad_norm": 27.073562622070312,
      "learning_rate": 0.0005777227722772277,
      "loss": 2.5287,
      "step": 1853
    },
    {
      "epoch": 12.25278810408922,
      "grad_norm": 36.71006774902344,
      "learning_rate": 0.0005772277227722772,
      "loss": 5.6359,
      "step": 1854
    },
    {
      "epoch": 12.259396943411813,
      "grad_norm": 1.6456966400146484,
      "learning_rate": 0.0005767326732673267,
      "loss": 2.7283,
      "step": 1855
    },
    {
      "epoch": 12.266005782734407,
      "grad_norm": 7.885274410247803,
      "learning_rate": 0.0005762376237623762,
      "loss": 0.6757,
      "step": 1856
    },
    {
      "epoch": 12.272614622057,
      "grad_norm": 20.30952262878418,
      "learning_rate": 0.0005757425742574258,
      "loss": 0.8744,
      "step": 1857
    },
    {
      "epoch": 12.279223461379596,
      "grad_norm": 21.255895614624023,
      "learning_rate": 0.0005752475247524752,
      "loss": 1.2597,
      "step": 1858
    },
    {
      "epoch": 12.28583230070219,
      "grad_norm": 40.21072006225586,
      "learning_rate": 0.0005747524752475247,
      "loss": 2.5304,
      "step": 1859
    },
    {
      "epoch": 12.292441140024783,
      "grad_norm": 40.795108795166016,
      "learning_rate": 0.0005742574257425742,
      "loss": 1.5837,
      "step": 1860
    },
    {
      "epoch": 12.299049979347377,
      "grad_norm": 25.991575241088867,
      "learning_rate": 0.0005737623762376238,
      "loss": 2.537,
      "step": 1861
    },
    {
      "epoch": 12.30565881866997,
      "grad_norm": 38.609092712402344,
      "learning_rate": 0.0005732673267326733,
      "loss": 2.526,
      "step": 1862
    },
    {
      "epoch": 12.312267657992566,
      "grad_norm": 1.8147058486938477,
      "learning_rate": 0.0005727722772277227,
      "loss": 3.6318,
      "step": 1863
    },
    {
      "epoch": 12.31887649731516,
      "grad_norm": 29.649322509765625,
      "learning_rate": 0.0005722772277227722,
      "loss": 3.2852,
      "step": 1864
    },
    {
      "epoch": 12.325485336637753,
      "grad_norm": 27.890140533447266,
      "learning_rate": 0.0005717821782178217,
      "loss": 2.6337,
      "step": 1865
    },
    {
      "epoch": 12.332094175960346,
      "grad_norm": 69.83243560791016,
      "learning_rate": 0.0005712871287128713,
      "loss": 3.1591,
      "step": 1866
    },
    {
      "epoch": 12.338703015282942,
      "grad_norm": 24.059255599975586,
      "learning_rate": 0.0005707920792079208,
      "loss": 2.7939,
      "step": 1867
    },
    {
      "epoch": 12.345311854605535,
      "grad_norm": 84.82294464111328,
      "learning_rate": 0.0005702970297029702,
      "loss": 3.1086,
      "step": 1868
    },
    {
      "epoch": 12.351920693928129,
      "grad_norm": 61.21704864501953,
      "learning_rate": 0.0005698019801980197,
      "loss": 1.8069,
      "step": 1869
    },
    {
      "epoch": 12.358529533250723,
      "grad_norm": 83.34674835205078,
      "learning_rate": 0.0005693069306930693,
      "loss": 2.1311,
      "step": 1870
    },
    {
      "epoch": 12.365138372573316,
      "grad_norm": 58.11820983886719,
      "learning_rate": 0.0005688118811881188,
      "loss": 3.6659,
      "step": 1871
    },
    {
      "epoch": 12.371747211895912,
      "grad_norm": 5.122825622558594,
      "learning_rate": 0.0005683168316831683,
      "loss": 2.4836,
      "step": 1872
    },
    {
      "epoch": 12.378356051218505,
      "grad_norm": 27.12978744506836,
      "learning_rate": 0.0005678217821782177,
      "loss": 1.2776,
      "step": 1873
    },
    {
      "epoch": 12.384964890541099,
      "grad_norm": 36.11037826538086,
      "learning_rate": 0.0005673267326732673,
      "loss": 2.7261,
      "step": 1874
    },
    {
      "epoch": 12.391573729863692,
      "grad_norm": 15.669361114501953,
      "learning_rate": 0.0005668316831683168,
      "loss": 2.7895,
      "step": 1875
    },
    {
      "epoch": 12.398182569186286,
      "grad_norm": 14.313199996948242,
      "learning_rate": 0.0005663366336633663,
      "loss": 2.2504,
      "step": 1876
    },
    {
      "epoch": 12.404791408508881,
      "grad_norm": 2.3410115242004395,
      "learning_rate": 0.0005658415841584158,
      "loss": 3.3334,
      "step": 1877
    },
    {
      "epoch": 12.411400247831475,
      "grad_norm": 25.041112899780273,
      "learning_rate": 0.0005653465346534653,
      "loss": 2.6145,
      "step": 1878
    },
    {
      "epoch": 12.418009087154068,
      "grad_norm": 24.01076889038086,
      "learning_rate": 0.0005648514851485148,
      "loss": 3.4724,
      "step": 1879
    },
    {
      "epoch": 12.424617926476662,
      "grad_norm": 55.57948303222656,
      "learning_rate": 0.0005643564356435643,
      "loss": 2.2243,
      "step": 1880
    },
    {
      "epoch": 12.431226765799256,
      "grad_norm": 3.519763231277466,
      "learning_rate": 0.0005638613861386138,
      "loss": 0.8076,
      "step": 1881
    },
    {
      "epoch": 12.437835605121851,
      "grad_norm": 27.229969024658203,
      "learning_rate": 0.0005633663366336634,
      "loss": 8.6165,
      "step": 1882
    },
    {
      "epoch": 12.444444444444445,
      "grad_norm": 56.15483093261719,
      "learning_rate": 0.0005628712871287128,
      "loss": 2.9166,
      "step": 1883
    },
    {
      "epoch": 12.451053283767038,
      "grad_norm": 20.76680564880371,
      "learning_rate": 0.0005623762376237624,
      "loss": 0.631,
      "step": 1884
    },
    {
      "epoch": 12.457662123089632,
      "grad_norm": 5.286612033843994,
      "learning_rate": 0.000561881188118812,
      "loss": 1.165,
      "step": 1885
    },
    {
      "epoch": 12.464270962412227,
      "grad_norm": 16.30975914001465,
      "learning_rate": 0.0005613861386138615,
      "loss": 1.0317,
      "step": 1886
    },
    {
      "epoch": 12.47087980173482,
      "grad_norm": 30.390634536743164,
      "learning_rate": 0.000560891089108911,
      "loss": 0.8432,
      "step": 1887
    },
    {
      "epoch": 12.477488641057414,
      "grad_norm": 9.159732818603516,
      "learning_rate": 0.0005603960396039604,
      "loss": 1.4816,
      "step": 1888
    },
    {
      "epoch": 12.484097480380008,
      "grad_norm": 7.548007011413574,
      "learning_rate": 0.0005599009900990099,
      "loss": 2.0033,
      "step": 1889
    },
    {
      "epoch": 12.490706319702602,
      "grad_norm": 4.549122333526611,
      "learning_rate": 0.0005594059405940595,
      "loss": 0.8064,
      "step": 1890
    },
    {
      "epoch": 12.497315159025197,
      "grad_norm": 1.9219876527786255,
      "learning_rate": 0.000558910891089109,
      "loss": 0.8499,
      "step": 1891
    },
    {
      "epoch": 12.50392399834779,
      "grad_norm": 4.039571285247803,
      "learning_rate": 0.0005584158415841585,
      "loss": 1.4057,
      "step": 1892
    },
    {
      "epoch": 12.510532837670384,
      "grad_norm": 11.60981559753418,
      "learning_rate": 0.0005579207920792079,
      "loss": 1.8658,
      "step": 1893
    },
    {
      "epoch": 12.517141676992978,
      "grad_norm": 15.383515357971191,
      "learning_rate": 0.0005574257425742575,
      "loss": 2.7424,
      "step": 1894
    },
    {
      "epoch": 12.523750516315571,
      "grad_norm": 2.442725419998169,
      "learning_rate": 0.000556930693069307,
      "loss": 1.3451,
      "step": 1895
    },
    {
      "epoch": 12.530359355638167,
      "grad_norm": 38.12295150756836,
      "learning_rate": 0.0005564356435643565,
      "loss": 2.4748,
      "step": 1896
    },
    {
      "epoch": 12.53696819496076,
      "grad_norm": 33.58564376831055,
      "learning_rate": 0.000555940594059406,
      "loss": 3.6892,
      "step": 1897
    },
    {
      "epoch": 12.543577034283354,
      "grad_norm": 37.46861267089844,
      "learning_rate": 0.0005554455445544555,
      "loss": 1.6209,
      "step": 1898
    },
    {
      "epoch": 12.550185873605948,
      "grad_norm": 7.68718957901001,
      "learning_rate": 0.000554950495049505,
      "loss": 4.3022,
      "step": 1899
    },
    {
      "epoch": 12.556794712928541,
      "grad_norm": 16.050615310668945,
      "learning_rate": 0.0005544554455445545,
      "loss": 1.5711,
      "step": 1900
    },
    {
      "epoch": 12.563403552251136,
      "grad_norm": 27.83277702331543,
      "learning_rate": 0.000553960396039604,
      "loss": 3.3128,
      "step": 1901
    },
    {
      "epoch": 12.57001239157373,
      "grad_norm": 11.566618919372559,
      "learning_rate": 0.0005534653465346536,
      "loss": 4.1965,
      "step": 1902
    },
    {
      "epoch": 12.576621230896324,
      "grad_norm": 53.41416549682617,
      "learning_rate": 0.000552970297029703,
      "loss": 3.378,
      "step": 1903
    },
    {
      "epoch": 12.583230070218917,
      "grad_norm": 34.51812744140625,
      "learning_rate": 0.0005524752475247525,
      "loss": 1.0159,
      "step": 1904
    },
    {
      "epoch": 12.589838909541513,
      "grad_norm": 16.7205810546875,
      "learning_rate": 0.000551980198019802,
      "loss": 1.0083,
      "step": 1905
    },
    {
      "epoch": 12.596447748864106,
      "grad_norm": 14.208585739135742,
      "learning_rate": 0.0005514851485148516,
      "loss": 2.1343,
      "step": 1906
    },
    {
      "epoch": 12.6030565881867,
      "grad_norm": 4.824237823486328,
      "learning_rate": 0.0005509900990099011,
      "loss": 2.178,
      "step": 1907
    },
    {
      "epoch": 12.609665427509293,
      "grad_norm": 14.400321960449219,
      "learning_rate": 0.0005504950495049505,
      "loss": 2.9942,
      "step": 1908
    },
    {
      "epoch": 12.616274266831887,
      "grad_norm": 19.239788055419922,
      "learning_rate": 0.00055,
      "loss": 2.8897,
      "step": 1909
    },
    {
      "epoch": 12.622883106154482,
      "grad_norm": 28.71565818786621,
      "learning_rate": 0.0005495049504950496,
      "loss": 1.4351,
      "step": 1910
    },
    {
      "epoch": 12.629491945477076,
      "grad_norm": 63.04395294189453,
      "learning_rate": 0.0005490099009900991,
      "loss": 2.3668,
      "step": 1911
    },
    {
      "epoch": 12.63610078479967,
      "grad_norm": 18.04156494140625,
      "learning_rate": 0.0005485148514851486,
      "loss": 3.8402,
      "step": 1912
    },
    {
      "epoch": 12.642709624122263,
      "grad_norm": 11.931111335754395,
      "learning_rate": 0.000548019801980198,
      "loss": 2.2605,
      "step": 1913
    },
    {
      "epoch": 12.649318463444857,
      "grad_norm": 48.46494674682617,
      "learning_rate": 0.0005475247524752476,
      "loss": 1.681,
      "step": 1914
    },
    {
      "epoch": 12.655927302767452,
      "grad_norm": 24.54639434814453,
      "learning_rate": 0.0005470297029702971,
      "loss": 3.5171,
      "step": 1915
    },
    {
      "epoch": 12.662536142090046,
      "grad_norm": 46.385292053222656,
      "learning_rate": 0.0005465346534653466,
      "loss": 4.8464,
      "step": 1916
    },
    {
      "epoch": 12.66914498141264,
      "grad_norm": 58.13218688964844,
      "learning_rate": 0.0005460396039603961,
      "loss": 2.3634,
      "step": 1917
    },
    {
      "epoch": 12.675753820735233,
      "grad_norm": 48.1768684387207,
      "learning_rate": 0.0005455445544554456,
      "loss": 2.4182,
      "step": 1918
    },
    {
      "epoch": 12.682362660057827,
      "grad_norm": 7.843333721160889,
      "learning_rate": 0.0005450495049504951,
      "loss": 1.126,
      "step": 1919
    },
    {
      "epoch": 12.688971499380422,
      "grad_norm": 38.61696243286133,
      "learning_rate": 0.0005445544554455446,
      "loss": 2.1342,
      "step": 1920
    },
    {
      "epoch": 12.695580338703015,
      "grad_norm": 12.935929298400879,
      "learning_rate": 0.0005440594059405941,
      "loss": 0.6634,
      "step": 1921
    },
    {
      "epoch": 12.702189178025609,
      "grad_norm": 73.9249267578125,
      "learning_rate": 0.0005435643564356437,
      "loss": 1.5815,
      "step": 1922
    },
    {
      "epoch": 12.708798017348203,
      "grad_norm": 23.913198471069336,
      "learning_rate": 0.0005430693069306931,
      "loss": 2.6508,
      "step": 1923
    },
    {
      "epoch": 12.715406856670796,
      "grad_norm": 20.2437686920166,
      "learning_rate": 0.0005425742574257426,
      "loss": 1.7097,
      "step": 1924
    },
    {
      "epoch": 12.722015695993392,
      "grad_norm": 22.2022647857666,
      "learning_rate": 0.0005420792079207921,
      "loss": 1.0148,
      "step": 1925
    },
    {
      "epoch": 12.728624535315985,
      "grad_norm": 10.761656761169434,
      "learning_rate": 0.0005415841584158417,
      "loss": 1.1198,
      "step": 1926
    },
    {
      "epoch": 12.735233374638579,
      "grad_norm": 27.742534637451172,
      "learning_rate": 0.0005410891089108912,
      "loss": 2.1428,
      "step": 1927
    },
    {
      "epoch": 12.741842213961172,
      "grad_norm": 56.750511169433594,
      "learning_rate": 0.0005405940594059406,
      "loss": 2.7197,
      "step": 1928
    },
    {
      "epoch": 12.748451053283768,
      "grad_norm": 52.213134765625,
      "learning_rate": 0.0005400990099009901,
      "loss": 5.7598,
      "step": 1929
    },
    {
      "epoch": 12.755059892606361,
      "grad_norm": 40.17658233642578,
      "learning_rate": 0.0005396039603960396,
      "loss": 2.7408,
      "step": 1930
    },
    {
      "epoch": 12.761668731928955,
      "grad_norm": 6.131472587585449,
      "learning_rate": 0.0005391089108910892,
      "loss": 1.398,
      "step": 1931
    },
    {
      "epoch": 12.768277571251549,
      "grad_norm": 55.1296272277832,
      "learning_rate": 0.0005386138613861387,
      "loss": 3.4041,
      "step": 1932
    },
    {
      "epoch": 12.774886410574142,
      "grad_norm": 104.98790740966797,
      "learning_rate": 0.0005381188118811881,
      "loss": 3.5957,
      "step": 1933
    },
    {
      "epoch": 12.781495249896738,
      "grad_norm": 41.223968505859375,
      "learning_rate": 0.0005376237623762376,
      "loss": 1.4904,
      "step": 1934
    },
    {
      "epoch": 12.788104089219331,
      "grad_norm": 6.945297718048096,
      "learning_rate": 0.0005371287128712872,
      "loss": 1.6382,
      "step": 1935
    },
    {
      "epoch": 12.794712928541925,
      "grad_norm": 6.47659158706665,
      "learning_rate": 0.0005366336633663367,
      "loss": 2.6199,
      "step": 1936
    },
    {
      "epoch": 12.801321767864518,
      "grad_norm": 52.134498596191406,
      "learning_rate": 0.0005361386138613862,
      "loss": 2.1057,
      "step": 1937
    },
    {
      "epoch": 12.807930607187112,
      "grad_norm": 2.7829747200012207,
      "learning_rate": 0.0005356435643564356,
      "loss": 1.7015,
      "step": 1938
    },
    {
      "epoch": 12.814539446509707,
      "grad_norm": 25.56222915649414,
      "learning_rate": 0.0005351485148514852,
      "loss": 1.3709,
      "step": 1939
    },
    {
      "epoch": 12.821148285832301,
      "grad_norm": 26.445354461669922,
      "learning_rate": 0.0005346534653465347,
      "loss": 1.9449,
      "step": 1940
    },
    {
      "epoch": 12.827757125154895,
      "grad_norm": 10.255355834960938,
      "learning_rate": 0.0005341584158415842,
      "loss": 2.9541,
      "step": 1941
    },
    {
      "epoch": 12.834365964477488,
      "grad_norm": 3.51704740524292,
      "learning_rate": 0.0005336633663366337,
      "loss": 1.0338,
      "step": 1942
    },
    {
      "epoch": 12.840974803800083,
      "grad_norm": 28.80780792236328,
      "learning_rate": 0.0005331683168316832,
      "loss": 1.0399,
      "step": 1943
    },
    {
      "epoch": 12.847583643122677,
      "grad_norm": 13.475639343261719,
      "learning_rate": 0.0005326732673267327,
      "loss": 1.0958,
      "step": 1944
    },
    {
      "epoch": 12.85419248244527,
      "grad_norm": 20.28940773010254,
      "learning_rate": 0.0005321782178217822,
      "loss": 2.1172,
      "step": 1945
    },
    {
      "epoch": 12.860801321767864,
      "grad_norm": 2.2552742958068848,
      "learning_rate": 0.0005316831683168317,
      "loss": 1.6858,
      "step": 1946
    },
    {
      "epoch": 12.867410161090458,
      "grad_norm": 8.54118824005127,
      "learning_rate": 0.0005311881188118813,
      "loss": 5.9029,
      "step": 1947
    },
    {
      "epoch": 12.874019000413053,
      "grad_norm": 73.61677551269531,
      "learning_rate": 0.0005306930693069307,
      "loss": 3.2702,
      "step": 1948
    },
    {
      "epoch": 12.880627839735647,
      "grad_norm": 89.26966094970703,
      "learning_rate": 0.0005301980198019802,
      "loss": 5.538,
      "step": 1949
    },
    {
      "epoch": 12.88723667905824,
      "grad_norm": 6.6988348960876465,
      "learning_rate": 0.0005297029702970297,
      "loss": 1.4804,
      "step": 1950
    },
    {
      "epoch": 12.893845518380834,
      "grad_norm": 8.506999969482422,
      "learning_rate": 0.0005292079207920793,
      "loss": 1.7702,
      "step": 1951
    },
    {
      "epoch": 12.900454357703428,
      "grad_norm": 24.494905471801758,
      "learning_rate": 0.0005287128712871288,
      "loss": 3.1698,
      "step": 1952
    },
    {
      "epoch": 12.907063197026023,
      "grad_norm": 11.9649658203125,
      "learning_rate": 0.0005282178217821782,
      "loss": 1.9737,
      "step": 1953
    },
    {
      "epoch": 12.913672036348617,
      "grad_norm": 32.07781219482422,
      "learning_rate": 0.0005277227722772277,
      "loss": 1.6237,
      "step": 1954
    },
    {
      "epoch": 12.92028087567121,
      "grad_norm": 50.90495681762695,
      "learning_rate": 0.0005272277227722773,
      "loss": 1.4275,
      "step": 1955
    },
    {
      "epoch": 12.926889714993804,
      "grad_norm": 11.480277061462402,
      "learning_rate": 0.0005267326732673268,
      "loss": 2.8775,
      "step": 1956
    },
    {
      "epoch": 12.933498554316397,
      "grad_norm": 5.445415496826172,
      "learning_rate": 0.0005262376237623763,
      "loss": 2.76,
      "step": 1957
    },
    {
      "epoch": 12.940107393638993,
      "grad_norm": 49.94186782836914,
      "learning_rate": 0.0005257425742574257,
      "loss": 2.7239,
      "step": 1958
    },
    {
      "epoch": 12.946716232961586,
      "grad_norm": 23.137042999267578,
      "learning_rate": 0.0005252475247524753,
      "loss": 1.1321,
      "step": 1959
    },
    {
      "epoch": 12.95332507228418,
      "grad_norm": 12.13595199584961,
      "learning_rate": 0.0005247524752475248,
      "loss": 1.9055,
      "step": 1960
    },
    {
      "epoch": 12.959933911606774,
      "grad_norm": 13.964301109313965,
      "learning_rate": 0.0005242574257425743,
      "loss": 1.1592,
      "step": 1961
    },
    {
      "epoch": 12.966542750929367,
      "grad_norm": 41.71305847167969,
      "learning_rate": 0.0005237623762376238,
      "loss": 1.1299,
      "step": 1962
    },
    {
      "epoch": 12.973151590251963,
      "grad_norm": 13.672584533691406,
      "learning_rate": 0.0005232673267326733,
      "loss": 3.4536,
      "step": 1963
    },
    {
      "epoch": 12.979760429574556,
      "grad_norm": 25.063570022583008,
      "learning_rate": 0.0005227722772277228,
      "loss": 5.6956,
      "step": 1964
    },
    {
      "epoch": 12.98636926889715,
      "grad_norm": 4.350609302520752,
      "learning_rate": 0.0005222772277227723,
      "loss": 2.7488,
      "step": 1965
    },
    {
      "epoch": 12.992978108219743,
      "grad_norm": 8.497121810913086,
      "learning_rate": 0.0005217821782178218,
      "loss": 5.4549,
      "step": 1966
    },
    {
      "epoch": 12.999586947542339,
      "grad_norm": 32.57640075683594,
      "learning_rate": 0.0005212871287128714,
      "loss": 1.2471,
      "step": 1967
    },
    {
      "epoch": 12.999586947542339,
      "eval_validation_error_bar": 0.048219314273356316,
      "eval_validation_loss": 6.90424108505249,
      "eval_validation_pearsonr": 0.6073421169497218,
      "eval_validation_rmse": 2.6275923252105713,
      "eval_validation_runtime": 33.5172,
      "eval_validation_samples_per_second": 6.057,
      "eval_validation_spearman": 0.611641801752625,
      "eval_validation_steps_per_second": 6.057,
      "step": 1967
    },
    {
      "epoch": 12.999586947542339,
      "eval_test_error_bar": 0.03953882485476564,
      "eval_test_loss": 7.776801586151123,
      "eval_test_pearsonr": 0.5758200549541103,
      "eval_test_rmse": 2.788691759109497,
      "eval_test_runtime": 41.7311,
      "eval_test_samples_per_second": 7.812,
      "eval_test_spearman": 0.5861295075251424,
      "eval_test_steps_per_second": 7.812,
      "step": 1967
    },
    {
      "epoch": 13.006195786864932,
      "grad_norm": 26.252656936645508,
      "learning_rate": 0.0005207920792079208,
      "loss": 5.8008,
      "step": 1968
    },
    {
      "epoch": 13.012804626187526,
      "grad_norm": 6.440273761749268,
      "learning_rate": 0.0005202970297029703,
      "loss": 1.7005,
      "step": 1969
    },
    {
      "epoch": 13.01941346551012,
      "grad_norm": 23.91971206665039,
      "learning_rate": 0.0005198019801980198,
      "loss": 1.8128,
      "step": 1970
    },
    {
      "epoch": 13.026022304832713,
      "grad_norm": 31.06356430053711,
      "learning_rate": 0.0005193069306930694,
      "loss": 1.3187,
      "step": 1971
    },
    {
      "epoch": 13.032631144155308,
      "grad_norm": 26.087160110473633,
      "learning_rate": 0.0005188118811881189,
      "loss": 1.2273,
      "step": 1972
    },
    {
      "epoch": 13.039239983477902,
      "grad_norm": 40.445186614990234,
      "learning_rate": 0.0005183168316831683,
      "loss": 5.1434,
      "step": 1973
    },
    {
      "epoch": 13.045848822800496,
      "grad_norm": 57.85099792480469,
      "learning_rate": 0.0005178217821782178,
      "loss": 5.267,
      "step": 1974
    },
    {
      "epoch": 13.05245766212309,
      "grad_norm": 20.816999435424805,
      "learning_rate": 0.0005173267326732674,
      "loss": 0.8537,
      "step": 1975
    },
    {
      "epoch": 13.059066501445683,
      "grad_norm": 28.105602264404297,
      "learning_rate": 0.0005168316831683169,
      "loss": 4.4636,
      "step": 1976
    },
    {
      "epoch": 13.065675340768278,
      "grad_norm": 9.816234588623047,
      "learning_rate": 0.0005163366336633664,
      "loss": 2.7776,
      "step": 1977
    },
    {
      "epoch": 13.072284180090872,
      "grad_norm": 48.226993560791016,
      "learning_rate": 0.0005158415841584158,
      "loss": 1.4435,
      "step": 1978
    },
    {
      "epoch": 13.078893019413465,
      "grad_norm": 24.046710968017578,
      "learning_rate": 0.0005153465346534653,
      "loss": 1.4861,
      "step": 1979
    },
    {
      "epoch": 13.085501858736059,
      "grad_norm": 29.54195785522461,
      "learning_rate": 0.0005148514851485149,
      "loss": 1.9767,
      "step": 1980
    },
    {
      "epoch": 13.092110698058653,
      "grad_norm": 70.0890884399414,
      "learning_rate": 0.0005143564356435644,
      "loss": 2.8687,
      "step": 1981
    },
    {
      "epoch": 13.098719537381248,
      "grad_norm": 57.79975509643555,
      "learning_rate": 0.0005138613861386139,
      "loss": 2.7722,
      "step": 1982
    },
    {
      "epoch": 13.105328376703842,
      "grad_norm": 32.85224151611328,
      "learning_rate": 0.0005133663366336633,
      "loss": 2.836,
      "step": 1983
    },
    {
      "epoch": 13.111937216026435,
      "grad_norm": 44.73877716064453,
      "learning_rate": 0.0005128712871287129,
      "loss": 1.54,
      "step": 1984
    },
    {
      "epoch": 13.118546055349029,
      "grad_norm": 18.586688995361328,
      "learning_rate": 0.0005123762376237624,
      "loss": 5.3028,
      "step": 1985
    },
    {
      "epoch": 13.125154894671624,
      "grad_norm": 34.350852966308594,
      "learning_rate": 0.0005118811881188119,
      "loss": 3.4541,
      "step": 1986
    },
    {
      "epoch": 13.131763733994218,
      "grad_norm": 14.050934791564941,
      "learning_rate": 0.0005113861386138615,
      "loss": 1.7258,
      "step": 1987
    },
    {
      "epoch": 13.138372573316811,
      "grad_norm": 16.130264282226562,
      "learning_rate": 0.0005108910891089109,
      "loss": 1.6875,
      "step": 1988
    },
    {
      "epoch": 13.144981412639405,
      "grad_norm": 9.618779182434082,
      "learning_rate": 0.0005103960396039604,
      "loss": 0.3429,
      "step": 1989
    },
    {
      "epoch": 13.151590251961998,
      "grad_norm": 5.843326091766357,
      "learning_rate": 0.0005099009900990099,
      "loss": 2.2083,
      "step": 1990
    },
    {
      "epoch": 13.158199091284594,
      "grad_norm": 5.606666088104248,
      "learning_rate": 0.0005094059405940594,
      "loss": 2.3067,
      "step": 1991
    },
    {
      "epoch": 13.164807930607187,
      "grad_norm": 7.57645320892334,
      "learning_rate": 0.000508910891089109,
      "loss": 0.2901,
      "step": 1992
    },
    {
      "epoch": 13.171416769929781,
      "grad_norm": 26.22303009033203,
      "learning_rate": 0.0005084158415841584,
      "loss": 0.9709,
      "step": 1993
    },
    {
      "epoch": 13.178025609252375,
      "grad_norm": 24.311067581176758,
      "learning_rate": 0.0005079207920792079,
      "loss": 0.9781,
      "step": 1994
    },
    {
      "epoch": 13.184634448574968,
      "grad_norm": 5.409374237060547,
      "learning_rate": 0.0005074257425742574,
      "loss": 1.0485,
      "step": 1995
    },
    {
      "epoch": 13.191243287897564,
      "grad_norm": 11.866065979003906,
      "learning_rate": 0.000506930693069307,
      "loss": 1.8922,
      "step": 1996
    },
    {
      "epoch": 13.197852127220157,
      "grad_norm": 43.807891845703125,
      "learning_rate": 0.0005064356435643565,
      "loss": 0.6385,
      "step": 1997
    },
    {
      "epoch": 13.20446096654275,
      "grad_norm": 12.380105972290039,
      "learning_rate": 0.0005059405940594059,
      "loss": 1.2449,
      "step": 1998
    },
    {
      "epoch": 13.211069805865344,
      "grad_norm": 112.44891357421875,
      "learning_rate": 0.0005054455445544554,
      "loss": 6.1781,
      "step": 1999
    },
    {
      "epoch": 13.217678645187938,
      "grad_norm": 10.380480766296387,
      "learning_rate": 0.000504950495049505,
      "loss": 1.08,
      "step": 2000
    },
    {
      "epoch": 13.224287484510533,
      "grad_norm": 35.761070251464844,
      "learning_rate": 0.0005044554455445545,
      "loss": 1.4946,
      "step": 2001
    },
    {
      "epoch": 13.230896323833127,
      "grad_norm": 8.884869575500488,
      "learning_rate": 0.000503960396039604,
      "loss": 4.1158,
      "step": 2002
    },
    {
      "epoch": 13.23750516315572,
      "grad_norm": 43.22886657714844,
      "learning_rate": 0.0005034653465346534,
      "loss": 1.2603,
      "step": 2003
    },
    {
      "epoch": 13.244114002478314,
      "grad_norm": 21.716930389404297,
      "learning_rate": 0.000502970297029703,
      "loss": 1.9342,
      "step": 2004
    },
    {
      "epoch": 13.25072284180091,
      "grad_norm": 41.67658996582031,
      "learning_rate": 0.0005024752475247525,
      "loss": 2.8453,
      "step": 2005
    },
    {
      "epoch": 13.257331681123503,
      "grad_norm": 53.876285552978516,
      "learning_rate": 0.000501980198019802,
      "loss": 1.7253,
      "step": 2006
    },
    {
      "epoch": 13.263940520446097,
      "grad_norm": 1.8263670206069946,
      "learning_rate": 0.0005014851485148515,
      "loss": 3.3042,
      "step": 2007
    },
    {
      "epoch": 13.27054935976869,
      "grad_norm": 78.81852722167969,
      "learning_rate": 0.000500990099009901,
      "loss": 2.6418,
      "step": 2008
    },
    {
      "epoch": 13.277158199091284,
      "grad_norm": 35.41996383666992,
      "learning_rate": 0.0005004950495049505,
      "loss": 4.4374,
      "step": 2009
    },
    {
      "epoch": 13.28376703841388,
      "grad_norm": 0.7282382845878601,
      "learning_rate": 0.0005,
      "loss": 1.0622,
      "step": 2010
    },
    {
      "epoch": 13.290375877736473,
      "grad_norm": 18.665376663208008,
      "learning_rate": 0.0004995049504950495,
      "loss": 1.2937,
      "step": 2011
    },
    {
      "epoch": 13.296984717059066,
      "grad_norm": 25.125181198120117,
      "learning_rate": 0.0004990099009900991,
      "loss": 1.6425,
      "step": 2012
    },
    {
      "epoch": 13.30359355638166,
      "grad_norm": 12.721062660217285,
      "learning_rate": 0.0004985148514851485,
      "loss": 0.7709,
      "step": 2013
    },
    {
      "epoch": 13.310202395704254,
      "grad_norm": 2.624721050262451,
      "learning_rate": 0.000498019801980198,
      "loss": 2.2264,
      "step": 2014
    },
    {
      "epoch": 13.316811235026849,
      "grad_norm": 51.2957763671875,
      "learning_rate": 0.0004975247524752475,
      "loss": 2.9532,
      "step": 2015
    },
    {
      "epoch": 13.323420074349443,
      "grad_norm": 4.252074241638184,
      "learning_rate": 0.0004970297029702971,
      "loss": 2.0223,
      "step": 2016
    },
    {
      "epoch": 13.330028913672036,
      "grad_norm": 23.317758560180664,
      "learning_rate": 0.0004965346534653466,
      "loss": 2.3221,
      "step": 2017
    },
    {
      "epoch": 13.33663775299463,
      "grad_norm": 12.325739860534668,
      "learning_rate": 0.000496039603960396,
      "loss": 1.9395,
      "step": 2018
    },
    {
      "epoch": 13.343246592317223,
      "grad_norm": 26.65448570251465,
      "learning_rate": 0.0004955445544554455,
      "loss": 1.2601,
      "step": 2019
    },
    {
      "epoch": 13.349855431639819,
      "grad_norm": 6.771697044372559,
      "learning_rate": 0.0004950495049504951,
      "loss": 2.2047,
      "step": 2020
    },
    {
      "epoch": 13.356464270962412,
      "grad_norm": 62.922454833984375,
      "learning_rate": 0.0004945544554455446,
      "loss": 4.4809,
      "step": 2021
    },
    {
      "epoch": 13.363073110285006,
      "grad_norm": 15.365762710571289,
      "learning_rate": 0.0004940594059405941,
      "loss": 2.1946,
      "step": 2022
    },
    {
      "epoch": 13.3696819496076,
      "grad_norm": 12.134713172912598,
      "learning_rate": 0.0004935643564356435,
      "loss": 1.6447,
      "step": 2023
    },
    {
      "epoch": 13.376290788930195,
      "grad_norm": 103.46717071533203,
      "learning_rate": 0.000493069306930693,
      "loss": 4.2411,
      "step": 2024
    },
    {
      "epoch": 13.382899628252789,
      "grad_norm": 66.60265350341797,
      "learning_rate": 0.0004925742574257426,
      "loss": 1.8898,
      "step": 2025
    },
    {
      "epoch": 13.389508467575382,
      "grad_norm": 37.113826751708984,
      "learning_rate": 0.0004920792079207921,
      "loss": 2.5606,
      "step": 2026
    },
    {
      "epoch": 13.396117306897976,
      "grad_norm": 27.22715950012207,
      "learning_rate": 0.0004915841584158416,
      "loss": 2.7004,
      "step": 2027
    },
    {
      "epoch": 13.40272614622057,
      "grad_norm": 6.3097734451293945,
      "learning_rate": 0.000491089108910891,
      "loss": 0.9995,
      "step": 2028
    },
    {
      "epoch": 13.409334985543165,
      "grad_norm": 11.796113014221191,
      "learning_rate": 0.0004905940594059406,
      "loss": 2.6342,
      "step": 2029
    },
    {
      "epoch": 13.415943824865758,
      "grad_norm": 18.647846221923828,
      "learning_rate": 0.0004900990099009901,
      "loss": 1.4513,
      "step": 2030
    },
    {
      "epoch": 13.422552664188352,
      "grad_norm": 7.8629021644592285,
      "learning_rate": 0.0004896039603960396,
      "loss": 2.1022,
      "step": 2031
    },
    {
      "epoch": 13.429161503510946,
      "grad_norm": 42.38986587524414,
      "learning_rate": 0.0004891089108910892,
      "loss": 2.6776,
      "step": 2032
    },
    {
      "epoch": 13.435770342833539,
      "grad_norm": 7.31282377243042,
      "learning_rate": 0.0004886138613861386,
      "loss": 3.767,
      "step": 2033
    },
    {
      "epoch": 13.442379182156134,
      "grad_norm": 40.58951187133789,
      "learning_rate": 0.0004881188118811881,
      "loss": 3.1095,
      "step": 2034
    },
    {
      "epoch": 13.448988021478728,
      "grad_norm": 10.330265998840332,
      "learning_rate": 0.0004876237623762376,
      "loss": 1.622,
      "step": 2035
    },
    {
      "epoch": 13.455596860801322,
      "grad_norm": 15.741917610168457,
      "learning_rate": 0.00048712871287128715,
      "loss": 1.9718,
      "step": 2036
    },
    {
      "epoch": 13.462205700123915,
      "grad_norm": 8.709709167480469,
      "learning_rate": 0.0004866336633663366,
      "loss": 1.9686,
      "step": 2037
    },
    {
      "epoch": 13.468814539446509,
      "grad_norm": 79.98920440673828,
      "learning_rate": 0.00048613861386138615,
      "loss": 2.7459,
      "step": 2038
    },
    {
      "epoch": 13.475423378769104,
      "grad_norm": 61.25081253051758,
      "learning_rate": 0.0004856435643564356,
      "loss": 2.4428,
      "step": 2039
    },
    {
      "epoch": 13.482032218091698,
      "grad_norm": 63.84176254272461,
      "learning_rate": 0.00048514851485148515,
      "loss": 3.567,
      "step": 2040
    },
    {
      "epoch": 13.488641057414291,
      "grad_norm": 25.82693862915039,
      "learning_rate": 0.00048465346534653467,
      "loss": 1.356,
      "step": 2041
    },
    {
      "epoch": 13.495249896736885,
      "grad_norm": 12.607298851013184,
      "learning_rate": 0.00048415841584158414,
      "loss": 1.2127,
      "step": 2042
    },
    {
      "epoch": 13.501858736059479,
      "grad_norm": 31.096752166748047,
      "learning_rate": 0.00048366336633663367,
      "loss": 1.6095,
      "step": 2043
    },
    {
      "epoch": 13.508467575382074,
      "grad_norm": 16.920883178710938,
      "learning_rate": 0.00048316831683168314,
      "loss": 4.1095,
      "step": 2044
    },
    {
      "epoch": 13.515076414704668,
      "grad_norm": 35.34759521484375,
      "learning_rate": 0.00048267326732673267,
      "loss": 2.6876,
      "step": 2045
    },
    {
      "epoch": 13.521685254027261,
      "grad_norm": 18.547361373901367,
      "learning_rate": 0.0004821782178217822,
      "loss": 2.7576,
      "step": 2046
    },
    {
      "epoch": 13.528294093349855,
      "grad_norm": 15.8602876663208,
      "learning_rate": 0.00048168316831683167,
      "loss": 3.6616,
      "step": 2047
    },
    {
      "epoch": 13.53490293267245,
      "grad_norm": 12.749826431274414,
      "learning_rate": 0.0004811881188118812,
      "loss": 1.9225,
      "step": 2048
    },
    {
      "epoch": 13.541511771995044,
      "grad_norm": 22.94283103942871,
      "learning_rate": 0.00048069306930693066,
      "loss": 2.044,
      "step": 2049
    },
    {
      "epoch": 13.548120611317637,
      "grad_norm": 13.947975158691406,
      "learning_rate": 0.0004801980198019802,
      "loss": 4.3262,
      "step": 2050
    },
    {
      "epoch": 13.554729450640231,
      "grad_norm": 36.212974548339844,
      "learning_rate": 0.0004797029702970297,
      "loss": 2.0922,
      "step": 2051
    },
    {
      "epoch": 13.561338289962825,
      "grad_norm": 43.09233093261719,
      "learning_rate": 0.0004792079207920792,
      "loss": 1.2002,
      "step": 2052
    },
    {
      "epoch": 13.56794712928542,
      "grad_norm": 54.19009017944336,
      "learning_rate": 0.0004787128712871287,
      "loss": 2.5006,
      "step": 2053
    },
    {
      "epoch": 13.574555968608014,
      "grad_norm": 5.552460193634033,
      "learning_rate": 0.0004782178217821782,
      "loss": 1.7766,
      "step": 2054
    },
    {
      "epoch": 13.581164807930607,
      "grad_norm": 48.75627899169922,
      "learning_rate": 0.0004777227722772277,
      "loss": 1.9521,
      "step": 2055
    },
    {
      "epoch": 13.5877736472532,
      "grad_norm": 90.89608001708984,
      "learning_rate": 0.00047722772277227724,
      "loss": 5.5551,
      "step": 2056
    },
    {
      "epoch": 13.594382486575796,
      "grad_norm": 49.05234146118164,
      "learning_rate": 0.0004767326732673267,
      "loss": 1.7221,
      "step": 2057
    },
    {
      "epoch": 13.60099132589839,
      "grad_norm": 51.59455490112305,
      "learning_rate": 0.00047623762376237624,
      "loss": 2.9222,
      "step": 2058
    },
    {
      "epoch": 13.607600165220983,
      "grad_norm": 50.22018814086914,
      "learning_rate": 0.0004757425742574257,
      "loss": 1.7922,
      "step": 2059
    },
    {
      "epoch": 13.614209004543577,
      "grad_norm": 41.45778274536133,
      "learning_rate": 0.00047524752475247524,
      "loss": 1.0823,
      "step": 2060
    },
    {
      "epoch": 13.62081784386617,
      "grad_norm": 50.787757873535156,
      "learning_rate": 0.00047475247524752476,
      "loss": 3.5937,
      "step": 2061
    },
    {
      "epoch": 13.627426683188766,
      "grad_norm": 43.59532165527344,
      "learning_rate": 0.00047425742574257423,
      "loss": 0.9716,
      "step": 2062
    },
    {
      "epoch": 13.63403552251136,
      "grad_norm": 52.74211883544922,
      "learning_rate": 0.00047376237623762376,
      "loss": 2.9693,
      "step": 2063
    },
    {
      "epoch": 13.640644361833953,
      "grad_norm": 22.410140991210938,
      "learning_rate": 0.00047326732673267323,
      "loss": 2.3339,
      "step": 2064
    },
    {
      "epoch": 13.647253201156547,
      "grad_norm": 8.049286842346191,
      "learning_rate": 0.00047277227722772276,
      "loss": 6.3133,
      "step": 2065
    },
    {
      "epoch": 13.65386204047914,
      "grad_norm": 3.140220880508423,
      "learning_rate": 0.0004722772277227723,
      "loss": 1.1166,
      "step": 2066
    },
    {
      "epoch": 13.660470879801736,
      "grad_norm": 11.015413284301758,
      "learning_rate": 0.00047178217821782176,
      "loss": 1.5068,
      "step": 2067
    },
    {
      "epoch": 13.66707971912433,
      "grad_norm": 3.172689437866211,
      "learning_rate": 0.0004712871287128713,
      "loss": 0.9253,
      "step": 2068
    },
    {
      "epoch": 13.673688558446923,
      "grad_norm": 54.024696350097656,
      "learning_rate": 0.00047079207920792075,
      "loss": 2.1496,
      "step": 2069
    },
    {
      "epoch": 13.680297397769516,
      "grad_norm": 13.413777351379395,
      "learning_rate": 0.0004702970297029703,
      "loss": 3.0021,
      "step": 2070
    },
    {
      "epoch": 13.68690623709211,
      "grad_norm": 9.360325813293457,
      "learning_rate": 0.0004698019801980198,
      "loss": 2.2152,
      "step": 2071
    },
    {
      "epoch": 13.693515076414705,
      "grad_norm": 4.275818347930908,
      "learning_rate": 0.0004693069306930693,
      "loss": 1.0095,
      "step": 2072
    },
    {
      "epoch": 13.700123915737299,
      "grad_norm": 9.237832069396973,
      "learning_rate": 0.0004688118811881188,
      "loss": 1.2898,
      "step": 2073
    },
    {
      "epoch": 13.706732755059893,
      "grad_norm": 10.889135360717773,
      "learning_rate": 0.00046831683168316833,
      "loss": 3.261,
      "step": 2074
    },
    {
      "epoch": 13.713341594382486,
      "grad_norm": 27.4136905670166,
      "learning_rate": 0.00046782178217821786,
      "loss": 1.4952,
      "step": 2075
    },
    {
      "epoch": 13.71995043370508,
      "grad_norm": 7.271935939788818,
      "learning_rate": 0.0004673267326732674,
      "loss": 1.5634,
      "step": 2076
    },
    {
      "epoch": 13.726559273027675,
      "grad_norm": 25.000329971313477,
      "learning_rate": 0.00046683168316831686,
      "loss": 1.6215,
      "step": 2077
    },
    {
      "epoch": 13.733168112350269,
      "grad_norm": 35.66639709472656,
      "learning_rate": 0.0004663366336633664,
      "loss": 4.4189,
      "step": 2078
    },
    {
      "epoch": 13.739776951672862,
      "grad_norm": 33.751678466796875,
      "learning_rate": 0.00046584158415841585,
      "loss": 1.8788,
      "step": 2079
    },
    {
      "epoch": 13.746385790995456,
      "grad_norm": 35.38987350463867,
      "learning_rate": 0.0004653465346534654,
      "loss": 2.0249,
      "step": 2080
    },
    {
      "epoch": 13.75299463031805,
      "grad_norm": 14.940454483032227,
      "learning_rate": 0.0004648514851485149,
      "loss": 0.8072,
      "step": 2081
    },
    {
      "epoch": 13.759603469640645,
      "grad_norm": 35.10153579711914,
      "learning_rate": 0.0004643564356435644,
      "loss": 3.3284,
      "step": 2082
    },
    {
      "epoch": 13.766212308963238,
      "grad_norm": 18.875497817993164,
      "learning_rate": 0.0004638613861386139,
      "loss": 1.3933,
      "step": 2083
    },
    {
      "epoch": 13.772821148285832,
      "grad_norm": 71.5887451171875,
      "learning_rate": 0.0004633663366336634,
      "loss": 2.882,
      "step": 2084
    },
    {
      "epoch": 13.779429987608426,
      "grad_norm": 18.002851486206055,
      "learning_rate": 0.0004628712871287129,
      "loss": 0.4736,
      "step": 2085
    },
    {
      "epoch": 13.786038826931021,
      "grad_norm": 20.180206298828125,
      "learning_rate": 0.00046237623762376243,
      "loss": 4.3618,
      "step": 2086
    },
    {
      "epoch": 13.792647666253615,
      "grad_norm": 8.097412109375,
      "learning_rate": 0.0004618811881188119,
      "loss": 1.1073,
      "step": 2087
    },
    {
      "epoch": 13.799256505576208,
      "grad_norm": 34.99201583862305,
      "learning_rate": 0.00046138613861386143,
      "loss": 1.637,
      "step": 2088
    },
    {
      "epoch": 13.805865344898802,
      "grad_norm": 17.308422088623047,
      "learning_rate": 0.0004608910891089109,
      "loss": 3.2767,
      "step": 2089
    },
    {
      "epoch": 13.812474184221395,
      "grad_norm": 8.381561279296875,
      "learning_rate": 0.0004603960396039604,
      "loss": 1.8458,
      "step": 2090
    },
    {
      "epoch": 13.81908302354399,
      "grad_norm": 9.253844261169434,
      "learning_rate": 0.00045990099009900995,
      "loss": 2.1163,
      "step": 2091
    },
    {
      "epoch": 13.825691862866584,
      "grad_norm": 11.772425651550293,
      "learning_rate": 0.0004594059405940594,
      "loss": 2.026,
      "step": 2092
    },
    {
      "epoch": 13.832300702189178,
      "grad_norm": 1.6844420433044434,
      "learning_rate": 0.00045891089108910895,
      "loss": 1.2083,
      "step": 2093
    },
    {
      "epoch": 13.838909541511772,
      "grad_norm": 27.617938995361328,
      "learning_rate": 0.0004584158415841584,
      "loss": 0.9416,
      "step": 2094
    },
    {
      "epoch": 13.845518380834365,
      "grad_norm": 38.31634521484375,
      "learning_rate": 0.00045792079207920795,
      "loss": 2.1062,
      "step": 2095
    },
    {
      "epoch": 13.85212722015696,
      "grad_norm": 11.915045738220215,
      "learning_rate": 0.0004574257425742575,
      "loss": 2.1222,
      "step": 2096
    },
    {
      "epoch": 13.858736059479554,
      "grad_norm": 43.65061950683594,
      "learning_rate": 0.00045693069306930695,
      "loss": 2.3353,
      "step": 2097
    },
    {
      "epoch": 13.865344898802148,
      "grad_norm": 9.656750679016113,
      "learning_rate": 0.00045643564356435647,
      "loss": 0.7962,
      "step": 2098
    },
    {
      "epoch": 13.871953738124741,
      "grad_norm": 29.263235092163086,
      "learning_rate": 0.00045594059405940594,
      "loss": 2.6353,
      "step": 2099
    },
    {
      "epoch": 13.878562577447337,
      "grad_norm": 11.453495025634766,
      "learning_rate": 0.00045544554455445547,
      "loss": 0.8337,
      "step": 2100
    },
    {
      "epoch": 13.88517141676993,
      "grad_norm": 4.479966163635254,
      "learning_rate": 0.000454950495049505,
      "loss": 1.8638,
      "step": 2101
    },
    {
      "epoch": 13.891780256092524,
      "grad_norm": 29.834949493408203,
      "learning_rate": 0.00045445544554455447,
      "loss": 0.6318,
      "step": 2102
    },
    {
      "epoch": 13.898389095415117,
      "grad_norm": 10.915285110473633,
      "learning_rate": 0.000453960396039604,
      "loss": 2.506,
      "step": 2103
    },
    {
      "epoch": 13.904997934737711,
      "grad_norm": 11.1167631149292,
      "learning_rate": 0.00045346534653465347,
      "loss": 2.3917,
      "step": 2104
    },
    {
      "epoch": 13.911606774060306,
      "grad_norm": 27.39034652709961,
      "learning_rate": 0.000452970297029703,
      "loss": 1.3659,
      "step": 2105
    },
    {
      "epoch": 13.9182156133829,
      "grad_norm": 5.2766852378845215,
      "learning_rate": 0.0004524752475247525,
      "loss": 0.9773,
      "step": 2106
    },
    {
      "epoch": 13.924824452705494,
      "grad_norm": 9.006142616271973,
      "learning_rate": 0.000451980198019802,
      "loss": 1.1784,
      "step": 2107
    },
    {
      "epoch": 13.931433292028087,
      "grad_norm": 16.201889038085938,
      "learning_rate": 0.0004514851485148515,
      "loss": 2.6486,
      "step": 2108
    },
    {
      "epoch": 13.93804213135068,
      "grad_norm": 9.197786331176758,
      "learning_rate": 0.000450990099009901,
      "loss": 0.6799,
      "step": 2109
    },
    {
      "epoch": 13.944650970673276,
      "grad_norm": 47.02708053588867,
      "learning_rate": 0.0004504950495049505,
      "loss": 1.356,
      "step": 2110
    },
    {
      "epoch": 13.95125980999587,
      "grad_norm": 33.44550704956055,
      "learning_rate": 0.00045000000000000004,
      "loss": 1.086,
      "step": 2111
    },
    {
      "epoch": 13.957868649318463,
      "grad_norm": 27.995716094970703,
      "learning_rate": 0.0004495049504950495,
      "loss": 2.1015,
      "step": 2112
    },
    {
      "epoch": 13.964477488641057,
      "grad_norm": 11.61143970489502,
      "learning_rate": 0.00044900990099009904,
      "loss": 1.9143,
      "step": 2113
    },
    {
      "epoch": 13.97108632796365,
      "grad_norm": 18.283641815185547,
      "learning_rate": 0.0004485148514851485,
      "loss": 1.1176,
      "step": 2114
    },
    {
      "epoch": 13.977695167286246,
      "grad_norm": 30.907102584838867,
      "learning_rate": 0.00044801980198019804,
      "loss": 1.7127,
      "step": 2115
    },
    {
      "epoch": 13.98430400660884,
      "grad_norm": 74.03524780273438,
      "learning_rate": 0.00044752475247524756,
      "loss": 3.4861,
      "step": 2116
    },
    {
      "epoch": 13.990912845931433,
      "grad_norm": 17.423494338989258,
      "learning_rate": 0.00044702970297029704,
      "loss": 3.5836,
      "step": 2117
    },
    {
      "epoch": 13.997521685254027,
      "grad_norm": 64.04613494873047,
      "learning_rate": 0.00044653465346534656,
      "loss": 1.8896,
      "step": 2118
    },
    {
      "epoch": 13.997521685254027,
      "eval_validation_error_bar": 0.049025004764235276,
      "eval_validation_loss": 5.955613136291504,
      "eval_validation_pearsonr": 0.5825859965344136,
      "eval_validation_rmse": 2.4404125213623047,
      "eval_validation_runtime": 33.3391,
      "eval_validation_samples_per_second": 6.089,
      "eval_validation_spearman": 0.6016826854134647,
      "eval_validation_steps_per_second": 6.089,
      "step": 2118
    },
    {
      "epoch": 13.997521685254027,
      "eval_test_error_bar": 0.03911206514374925,
      "eval_test_loss": 6.362438201904297,
      "eval_test_pearsonr": 0.5827410836026823,
      "eval_test_rmse": 2.5223875045776367,
      "eval_test_runtime": 41.4294,
      "eval_test_samples_per_second": 7.869,
      "eval_test_spearman": 0.593101347402168,
      "eval_test_steps_per_second": 7.869,
      "step": 2118
    },
    {
      "epoch": 14.004130524576622,
      "grad_norm": 44.199031829833984,
      "learning_rate": 0.00044603960396039603,
      "loss": 2.496,
      "step": 2119
    },
    {
      "epoch": 14.010739363899216,
      "grad_norm": 66.57209777832031,
      "learning_rate": 0.00044554455445544556,
      "loss": 2.2624,
      "step": 2120
    },
    {
      "epoch": 14.01734820322181,
      "grad_norm": 20.402070999145508,
      "learning_rate": 0.0004450495049504951,
      "loss": 1.5968,
      "step": 2121
    },
    {
      "epoch": 14.023957042544403,
      "grad_norm": 22.175424575805664,
      "learning_rate": 0.00044455445544554456,
      "loss": 0.8187,
      "step": 2122
    },
    {
      "epoch": 14.030565881866996,
      "grad_norm": 20.508485794067383,
      "learning_rate": 0.0004440594059405941,
      "loss": 0.6452,
      "step": 2123
    },
    {
      "epoch": 14.037174721189592,
      "grad_norm": 4.395239353179932,
      "learning_rate": 0.00044356435643564356,
      "loss": 1.3459,
      "step": 2124
    },
    {
      "epoch": 14.043783560512185,
      "grad_norm": 33.80575180053711,
      "learning_rate": 0.0004430693069306931,
      "loss": 1.0283,
      "step": 2125
    },
    {
      "epoch": 14.050392399834779,
      "grad_norm": 35.269126892089844,
      "learning_rate": 0.0004425742574257426,
      "loss": 2.1806,
      "step": 2126
    },
    {
      "epoch": 14.057001239157373,
      "grad_norm": 33.11464309692383,
      "learning_rate": 0.0004420792079207921,
      "loss": 1.4311,
      "step": 2127
    },
    {
      "epoch": 14.063610078479966,
      "grad_norm": 20.712322235107422,
      "learning_rate": 0.0004415841584158416,
      "loss": 0.8799,
      "step": 2128
    },
    {
      "epoch": 14.070218917802562,
      "grad_norm": 63.579769134521484,
      "learning_rate": 0.0004410891089108911,
      "loss": 5.5282,
      "step": 2129
    },
    {
      "epoch": 14.076827757125155,
      "grad_norm": 65.39974212646484,
      "learning_rate": 0.0004405940594059406,
      "loss": 1.603,
      "step": 2130
    },
    {
      "epoch": 14.083436596447749,
      "grad_norm": 5.387505054473877,
      "learning_rate": 0.00044009900990099013,
      "loss": 2.9675,
      "step": 2131
    },
    {
      "epoch": 14.090045435770342,
      "grad_norm": 16.69095230102539,
      "learning_rate": 0.0004396039603960396,
      "loss": 2.1857,
      "step": 2132
    },
    {
      "epoch": 14.096654275092936,
      "grad_norm": 14.126182556152344,
      "learning_rate": 0.00043910891089108913,
      "loss": 1.375,
      "step": 2133
    },
    {
      "epoch": 14.103263114415531,
      "grad_norm": 14.204379081726074,
      "learning_rate": 0.0004386138613861386,
      "loss": 0.8941,
      "step": 2134
    },
    {
      "epoch": 14.109871953738125,
      "grad_norm": 16.881938934326172,
      "learning_rate": 0.00043811881188118813,
      "loss": 3.4822,
      "step": 2135
    },
    {
      "epoch": 14.116480793060719,
      "grad_norm": 31.575716018676758,
      "learning_rate": 0.00043762376237623765,
      "loss": 2.8017,
      "step": 2136
    },
    {
      "epoch": 14.123089632383312,
      "grad_norm": 17.121353149414062,
      "learning_rate": 0.0004371287128712871,
      "loss": 2.6851,
      "step": 2137
    },
    {
      "epoch": 14.129698471705908,
      "grad_norm": 56.87480163574219,
      "learning_rate": 0.00043663366336633665,
      "loss": 1.4278,
      "step": 2138
    },
    {
      "epoch": 14.136307311028501,
      "grad_norm": 36.79244613647461,
      "learning_rate": 0.0004361386138613861,
      "loss": 1.651,
      "step": 2139
    },
    {
      "epoch": 14.142916150351095,
      "grad_norm": 10.168498992919922,
      "learning_rate": 0.00043564356435643565,
      "loss": 0.7455,
      "step": 2140
    },
    {
      "epoch": 14.149524989673688,
      "grad_norm": 16.08884620666504,
      "learning_rate": 0.0004351485148514852,
      "loss": 0.9915,
      "step": 2141
    },
    {
      "epoch": 14.156133828996282,
      "grad_norm": 31.59925651550293,
      "learning_rate": 0.00043465346534653465,
      "loss": 1.8452,
      "step": 2142
    },
    {
      "epoch": 14.162742668318877,
      "grad_norm": 47.67173767089844,
      "learning_rate": 0.0004341584158415842,
      "loss": 7.5809,
      "step": 2143
    },
    {
      "epoch": 14.169351507641471,
      "grad_norm": 19.994556427001953,
      "learning_rate": 0.00043366336633663365,
      "loss": 3.1806,
      "step": 2144
    },
    {
      "epoch": 14.175960346964064,
      "grad_norm": 1.9810826778411865,
      "learning_rate": 0.0004331683168316832,
      "loss": 0.4525,
      "step": 2145
    },
    {
      "epoch": 14.182569186286658,
      "grad_norm": 21.229307174682617,
      "learning_rate": 0.0004326732673267327,
      "loss": 1.3207,
      "step": 2146
    },
    {
      "epoch": 14.189178025609252,
      "grad_norm": 28.77667236328125,
      "learning_rate": 0.00043217821782178217,
      "loss": 2.0841,
      "step": 2147
    },
    {
      "epoch": 14.195786864931847,
      "grad_norm": 36.729618072509766,
      "learning_rate": 0.0004316831683168317,
      "loss": 1.5416,
      "step": 2148
    },
    {
      "epoch": 14.20239570425444,
      "grad_norm": 16.077848434448242,
      "learning_rate": 0.00043118811881188117,
      "loss": 2.0479,
      "step": 2149
    },
    {
      "epoch": 14.209004543577034,
      "grad_norm": 45.768341064453125,
      "learning_rate": 0.0004306930693069307,
      "loss": 2.0481,
      "step": 2150
    },
    {
      "epoch": 14.215613382899628,
      "grad_norm": 20.306554794311523,
      "learning_rate": 0.0004301980198019802,
      "loss": 1.0427,
      "step": 2151
    },
    {
      "epoch": 14.222222222222221,
      "grad_norm": 4.792657375335693,
      "learning_rate": 0.0004297029702970297,
      "loss": 1.3444,
      "step": 2152
    },
    {
      "epoch": 14.228831061544817,
      "grad_norm": 35.08754348754883,
      "learning_rate": 0.0004292079207920792,
      "loss": 1.4323,
      "step": 2153
    },
    {
      "epoch": 14.23543990086741,
      "grad_norm": 44.03108215332031,
      "learning_rate": 0.0004287128712871287,
      "loss": 4.2441,
      "step": 2154
    },
    {
      "epoch": 14.242048740190004,
      "grad_norm": 13.503800392150879,
      "learning_rate": 0.0004282178217821782,
      "loss": 1.8995,
      "step": 2155
    },
    {
      "epoch": 14.248657579512598,
      "grad_norm": 32.202392578125,
      "learning_rate": 0.00042772277227722774,
      "loss": 3.1435,
      "step": 2156
    },
    {
      "epoch": 14.255266418835191,
      "grad_norm": 99.27269744873047,
      "learning_rate": 0.0004272277227722772,
      "loss": 2.8878,
      "step": 2157
    },
    {
      "epoch": 14.261875258157787,
      "grad_norm": 36.977928161621094,
      "learning_rate": 0.00042673267326732674,
      "loss": 2.0269,
      "step": 2158
    },
    {
      "epoch": 14.26848409748038,
      "grad_norm": 38.062599182128906,
      "learning_rate": 0.0004262376237623762,
      "loss": 1.2576,
      "step": 2159
    },
    {
      "epoch": 14.275092936802974,
      "grad_norm": 123.35408782958984,
      "learning_rate": 0.00042574257425742574,
      "loss": 5.3993,
      "step": 2160
    },
    {
      "epoch": 14.281701776125567,
      "grad_norm": 32.84893035888672,
      "learning_rate": 0.00042524752475247527,
      "loss": 3.0087,
      "step": 2161
    },
    {
      "epoch": 14.288310615448163,
      "grad_norm": 35.71953582763672,
      "learning_rate": 0.00042475247524752474,
      "loss": 5.0982,
      "step": 2162
    },
    {
      "epoch": 14.294919454770756,
      "grad_norm": 42.15387725830078,
      "learning_rate": 0.00042425742574257427,
      "loss": 0.9556,
      "step": 2163
    },
    {
      "epoch": 14.30152829409335,
      "grad_norm": 18.590665817260742,
      "learning_rate": 0.00042376237623762374,
      "loss": 1.6677,
      "step": 2164
    },
    {
      "epoch": 14.308137133415944,
      "grad_norm": 3.3282594680786133,
      "learning_rate": 0.00042326732673267326,
      "loss": 3.6065,
      "step": 2165
    },
    {
      "epoch": 14.314745972738537,
      "grad_norm": 26.91520118713379,
      "learning_rate": 0.0004227722772277228,
      "loss": 1.427,
      "step": 2166
    },
    {
      "epoch": 14.321354812061132,
      "grad_norm": 33.6234016418457,
      "learning_rate": 0.00042227722772277226,
      "loss": 3.8056,
      "step": 2167
    },
    {
      "epoch": 14.327963651383726,
      "grad_norm": 22.388675689697266,
      "learning_rate": 0.0004217821782178218,
      "loss": 0.7907,
      "step": 2168
    },
    {
      "epoch": 14.33457249070632,
      "grad_norm": 41.91952896118164,
      "learning_rate": 0.00042128712871287126,
      "loss": 1.5706,
      "step": 2169
    },
    {
      "epoch": 14.341181330028913,
      "grad_norm": 50.885005950927734,
      "learning_rate": 0.0004207920792079208,
      "loss": 1.9943,
      "step": 2170
    },
    {
      "epoch": 14.347790169351507,
      "grad_norm": 22.07234764099121,
      "learning_rate": 0.0004202970297029703,
      "loss": 1.1653,
      "step": 2171
    },
    {
      "epoch": 14.354399008674102,
      "grad_norm": 30.683353424072266,
      "learning_rate": 0.0004198019801980198,
      "loss": 1.7136,
      "step": 2172
    },
    {
      "epoch": 14.361007847996696,
      "grad_norm": 66.46265411376953,
      "learning_rate": 0.0004193069306930693,
      "loss": 1.6916,
      "step": 2173
    },
    {
      "epoch": 14.36761668731929,
      "grad_norm": 65.75214385986328,
      "learning_rate": 0.0004188118811881188,
      "loss": 1.3757,
      "step": 2174
    },
    {
      "epoch": 14.374225526641883,
      "grad_norm": 30.982929229736328,
      "learning_rate": 0.0004183168316831683,
      "loss": 3.6533,
      "step": 2175
    },
    {
      "epoch": 14.380834365964478,
      "grad_norm": 11.386688232421875,
      "learning_rate": 0.00041782178217821784,
      "loss": 2.9406,
      "step": 2176
    },
    {
      "epoch": 14.387443205287072,
      "grad_norm": 6.675998210906982,
      "learning_rate": 0.0004173267326732673,
      "loss": 4.123,
      "step": 2177
    },
    {
      "epoch": 14.394052044609666,
      "grad_norm": 4.640300750732422,
      "learning_rate": 0.00041683168316831683,
      "loss": 1.6142,
      "step": 2178
    },
    {
      "epoch": 14.40066088393226,
      "grad_norm": 36.15965270996094,
      "learning_rate": 0.0004163366336633663,
      "loss": 1.8862,
      "step": 2179
    },
    {
      "epoch": 14.407269723254853,
      "grad_norm": 61.587379455566406,
      "learning_rate": 0.00041584158415841583,
      "loss": 2.1246,
      "step": 2180
    },
    {
      "epoch": 14.413878562577448,
      "grad_norm": 31.29016876220703,
      "learning_rate": 0.00041534653465346536,
      "loss": 2.1466,
      "step": 2181
    },
    {
      "epoch": 14.420487401900042,
      "grad_norm": 55.95884704589844,
      "learning_rate": 0.00041485148514851483,
      "loss": 3.5778,
      "step": 2182
    },
    {
      "epoch": 14.427096241222635,
      "grad_norm": 37.87739181518555,
      "learning_rate": 0.00041435643564356436,
      "loss": 2.7743,
      "step": 2183
    },
    {
      "epoch": 14.433705080545229,
      "grad_norm": 47.61522674560547,
      "learning_rate": 0.00041386138613861383,
      "loss": 2.6275,
      "step": 2184
    },
    {
      "epoch": 14.440313919867823,
      "grad_norm": 74.5545654296875,
      "learning_rate": 0.00041336633663366335,
      "loss": 3.1839,
      "step": 2185
    },
    {
      "epoch": 14.446922759190418,
      "grad_norm": 16.763681411743164,
      "learning_rate": 0.0004128712871287129,
      "loss": 1.6238,
      "step": 2186
    },
    {
      "epoch": 14.453531598513012,
      "grad_norm": 5.762125015258789,
      "learning_rate": 0.00041237623762376235,
      "loss": 2.2073,
      "step": 2187
    },
    {
      "epoch": 14.460140437835605,
      "grad_norm": 44.831443786621094,
      "learning_rate": 0.0004118811881188119,
      "loss": 1.7155,
      "step": 2188
    },
    {
      "epoch": 14.466749277158199,
      "grad_norm": 24.578813552856445,
      "learning_rate": 0.00041138613861386135,
      "loss": 1.3431,
      "step": 2189
    },
    {
      "epoch": 14.473358116480792,
      "grad_norm": 39.19758224487305,
      "learning_rate": 0.0004108910891089109,
      "loss": 2.5416,
      "step": 2190
    },
    {
      "epoch": 14.479966955803388,
      "grad_norm": 21.992109298706055,
      "learning_rate": 0.0004103960396039604,
      "loss": 1.5195,
      "step": 2191
    },
    {
      "epoch": 14.486575795125981,
      "grad_norm": 54.09615707397461,
      "learning_rate": 0.0004099009900990099,
      "loss": 1.1316,
      "step": 2192
    },
    {
      "epoch": 14.493184634448575,
      "grad_norm": 43.99484634399414,
      "learning_rate": 0.0004094059405940594,
      "loss": 1.8114,
      "step": 2193
    },
    {
      "epoch": 14.499793473771168,
      "grad_norm": 24.65328025817871,
      "learning_rate": 0.0004089108910891089,
      "loss": 1.3618,
      "step": 2194
    },
    {
      "epoch": 14.506402313093762,
      "grad_norm": 28.339801788330078,
      "learning_rate": 0.0004084158415841584,
      "loss": 2.1787,
      "step": 2195
    },
    {
      "epoch": 14.513011152416357,
      "grad_norm": 37.33393096923828,
      "learning_rate": 0.0004079207920792079,
      "loss": 0.9187,
      "step": 2196
    },
    {
      "epoch": 14.519619991738951,
      "grad_norm": 1.4004995822906494,
      "learning_rate": 0.0004074257425742574,
      "loss": 0.4598,
      "step": 2197
    },
    {
      "epoch": 14.526228831061545,
      "grad_norm": 31.23549461364746,
      "learning_rate": 0.0004069306930693069,
      "loss": 2.3788,
      "step": 2198
    },
    {
      "epoch": 14.532837670384138,
      "grad_norm": 37.46989059448242,
      "learning_rate": 0.0004064356435643564,
      "loss": 1.8403,
      "step": 2199
    },
    {
      "epoch": 14.539446509706734,
      "grad_norm": 84.51969909667969,
      "learning_rate": 0.000405940594059406,
      "loss": 4.1361,
      "step": 2200
    },
    {
      "epoch": 14.546055349029327,
      "grad_norm": 33.996192932128906,
      "learning_rate": 0.0004054455445544555,
      "loss": 3.109,
      "step": 2201
    },
    {
      "epoch": 14.55266418835192,
      "grad_norm": 3.7668402194976807,
      "learning_rate": 0.000404950495049505,
      "loss": 0.5343,
      "step": 2202
    },
    {
      "epoch": 14.559273027674514,
      "grad_norm": 19.57500648498535,
      "learning_rate": 0.0004044554455445545,
      "loss": 1.2751,
      "step": 2203
    },
    {
      "epoch": 14.565881866997108,
      "grad_norm": 23.340375900268555,
      "learning_rate": 0.00040396039603960397,
      "loss": 1.5876,
      "step": 2204
    },
    {
      "epoch": 14.572490706319703,
      "grad_norm": 37.932830810546875,
      "learning_rate": 0.0004034653465346535,
      "loss": 3.0418,
      "step": 2205
    },
    {
      "epoch": 14.579099545642297,
      "grad_norm": 68.00335693359375,
      "learning_rate": 0.000402970297029703,
      "loss": 3.1244,
      "step": 2206
    },
    {
      "epoch": 14.58570838496489,
      "grad_norm": 47.28579330444336,
      "learning_rate": 0.0004024752475247525,
      "loss": 2.3244,
      "step": 2207
    },
    {
      "epoch": 14.592317224287484,
      "grad_norm": 40.1423454284668,
      "learning_rate": 0.000401980198019802,
      "loss": 1.6895,
      "step": 2208
    },
    {
      "epoch": 14.598926063610078,
      "grad_norm": 30.195438385009766,
      "learning_rate": 0.0004014851485148515,
      "loss": 1.0676,
      "step": 2209
    },
    {
      "epoch": 14.605534902932673,
      "grad_norm": 39.01425552368164,
      "learning_rate": 0.000400990099009901,
      "loss": 1.9722,
      "step": 2210
    },
    {
      "epoch": 14.612143742255267,
      "grad_norm": 17.24755096435547,
      "learning_rate": 0.00040049504950495055,
      "loss": 1.3237,
      "step": 2211
    },
    {
      "epoch": 14.61875258157786,
      "grad_norm": 10.696417808532715,
      "learning_rate": 0.0004,
      "loss": 0.9149,
      "step": 2212
    },
    {
      "epoch": 14.625361420900454,
      "grad_norm": 14.648222923278809,
      "learning_rate": 0.00039950495049504955,
      "loss": 1.1154,
      "step": 2213
    },
    {
      "epoch": 14.63197026022305,
      "grad_norm": 14.407464981079102,
      "learning_rate": 0.000399009900990099,
      "loss": 1.1569,
      "step": 2214
    },
    {
      "epoch": 14.638579099545643,
      "grad_norm": 24.777416229248047,
      "learning_rate": 0.00039851485148514854,
      "loss": 0.6041,
      "step": 2215
    },
    {
      "epoch": 14.645187938868236,
      "grad_norm": 41.08605194091797,
      "learning_rate": 0.00039801980198019807,
      "loss": 5.1546,
      "step": 2216
    },
    {
      "epoch": 14.65179677819083,
      "grad_norm": 4.455591678619385,
      "learning_rate": 0.00039752475247524754,
      "loss": 2.7046,
      "step": 2217
    },
    {
      "epoch": 14.658405617513424,
      "grad_norm": 3.3008217811584473,
      "learning_rate": 0.00039702970297029707,
      "loss": 1.2694,
      "step": 2218
    },
    {
      "epoch": 14.665014456836019,
      "grad_norm": 27.77801513671875,
      "learning_rate": 0.00039653465346534654,
      "loss": 2.357,
      "step": 2219
    },
    {
      "epoch": 14.671623296158613,
      "grad_norm": 12.521749496459961,
      "learning_rate": 0.00039603960396039607,
      "loss": 1.5688,
      "step": 2220
    },
    {
      "epoch": 14.678232135481206,
      "grad_norm": 28.184221267700195,
      "learning_rate": 0.0003955445544554456,
      "loss": 2.719,
      "step": 2221
    },
    {
      "epoch": 14.6848409748038,
      "grad_norm": 41.444541931152344,
      "learning_rate": 0.00039504950495049506,
      "loss": 1.4167,
      "step": 2222
    },
    {
      "epoch": 14.691449814126393,
      "grad_norm": 20.942331314086914,
      "learning_rate": 0.0003945544554455446,
      "loss": 0.9942,
      "step": 2223
    },
    {
      "epoch": 14.698058653448989,
      "grad_norm": 11.45166301727295,
      "learning_rate": 0.00039405940594059406,
      "loss": 0.9777,
      "step": 2224
    },
    {
      "epoch": 14.704667492771582,
      "grad_norm": 21.449796676635742,
      "learning_rate": 0.0003935643564356436,
      "loss": 2.2254,
      "step": 2225
    },
    {
      "epoch": 14.711276332094176,
      "grad_norm": 41.07752990722656,
      "learning_rate": 0.0003930693069306931,
      "loss": 1.5479,
      "step": 2226
    },
    {
      "epoch": 14.71788517141677,
      "grad_norm": 23.35324478149414,
      "learning_rate": 0.0003925742574257426,
      "loss": 1.2909,
      "step": 2227
    },
    {
      "epoch": 14.724494010739363,
      "grad_norm": 17.394556045532227,
      "learning_rate": 0.0003920792079207921,
      "loss": 1.4432,
      "step": 2228
    },
    {
      "epoch": 14.731102850061959,
      "grad_norm": 48.55097198486328,
      "learning_rate": 0.0003915841584158416,
      "loss": 3.0091,
      "step": 2229
    },
    {
      "epoch": 14.737711689384552,
      "grad_norm": 20.05437469482422,
      "learning_rate": 0.0003910891089108911,
      "loss": 1.2784,
      "step": 2230
    },
    {
      "epoch": 14.744320528707146,
      "grad_norm": 43.044952392578125,
      "learning_rate": 0.00039059405940594064,
      "loss": 2.7761,
      "step": 2231
    },
    {
      "epoch": 14.75092936802974,
      "grad_norm": 57.56447982788086,
      "learning_rate": 0.0003900990099009901,
      "loss": 2.2135,
      "step": 2232
    },
    {
      "epoch": 14.757538207352333,
      "grad_norm": 12.922962188720703,
      "learning_rate": 0.00038960396039603964,
      "loss": 1.6207,
      "step": 2233
    },
    {
      "epoch": 14.764147046674928,
      "grad_norm": 9.7582426071167,
      "learning_rate": 0.0003891089108910891,
      "loss": 2.9151,
      "step": 2234
    },
    {
      "epoch": 14.770755885997522,
      "grad_norm": 12.552844047546387,
      "learning_rate": 0.00038861386138613863,
      "loss": 3.0753,
      "step": 2235
    },
    {
      "epoch": 14.777364725320115,
      "grad_norm": 8.300320625305176,
      "learning_rate": 0.00038811881188118816,
      "loss": 0.7686,
      "step": 2236
    },
    {
      "epoch": 14.783973564642709,
      "grad_norm": 28.15553855895996,
      "learning_rate": 0.00038762376237623763,
      "loss": 2.7694,
      "step": 2237
    },
    {
      "epoch": 14.790582403965303,
      "grad_norm": 9.70584774017334,
      "learning_rate": 0.00038712871287128716,
      "loss": 2.0157,
      "step": 2238
    },
    {
      "epoch": 14.797191243287898,
      "grad_norm": 71.75676727294922,
      "learning_rate": 0.00038663366336633663,
      "loss": 3.4166,
      "step": 2239
    },
    {
      "epoch": 14.803800082610492,
      "grad_norm": 40.846946716308594,
      "learning_rate": 0.00038613861386138616,
      "loss": 2.1303,
      "step": 2240
    },
    {
      "epoch": 14.810408921933085,
      "grad_norm": 40.93817901611328,
      "learning_rate": 0.0003856435643564357,
      "loss": 1.7496,
      "step": 2241
    },
    {
      "epoch": 14.817017761255679,
      "grad_norm": 8.170833587646484,
      "learning_rate": 0.00038514851485148515,
      "loss": 2.43,
      "step": 2242
    },
    {
      "epoch": 14.823626600578274,
      "grad_norm": 12.642616271972656,
      "learning_rate": 0.0003846534653465347,
      "loss": 3.6934,
      "step": 2243
    },
    {
      "epoch": 14.830235439900868,
      "grad_norm": 14.124732971191406,
      "learning_rate": 0.00038415841584158415,
      "loss": 2.7865,
      "step": 2244
    },
    {
      "epoch": 14.836844279223461,
      "grad_norm": 9.235584259033203,
      "learning_rate": 0.0003836633663366337,
      "loss": 3.7875,
      "step": 2245
    },
    {
      "epoch": 14.843453118546055,
      "grad_norm": 19.002077102661133,
      "learning_rate": 0.0003831683168316832,
      "loss": 1.4108,
      "step": 2246
    },
    {
      "epoch": 14.850061957868649,
      "grad_norm": 9.683306694030762,
      "learning_rate": 0.0003826732673267327,
      "loss": 2.8184,
      "step": 2247
    },
    {
      "epoch": 14.856670797191244,
      "grad_norm": 50.71754455566406,
      "learning_rate": 0.0003821782178217822,
      "loss": 3.1089,
      "step": 2248
    },
    {
      "epoch": 14.863279636513838,
      "grad_norm": 17.367919921875,
      "learning_rate": 0.0003816831683168317,
      "loss": 1.3272,
      "step": 2249
    },
    {
      "epoch": 14.869888475836431,
      "grad_norm": 76.62954711914062,
      "learning_rate": 0.0003811881188118812,
      "loss": 3.2673,
      "step": 2250
    },
    {
      "epoch": 14.876497315159025,
      "grad_norm": 27.21489715576172,
      "learning_rate": 0.00038069306930693073,
      "loss": 1.6301,
      "step": 2251
    },
    {
      "epoch": 14.88310615448162,
      "grad_norm": 7.540471076965332,
      "learning_rate": 0.0003801980198019802,
      "loss": 1.4263,
      "step": 2252
    },
    {
      "epoch": 14.889714993804214,
      "grad_norm": 16.20201301574707,
      "learning_rate": 0.0003797029702970297,
      "loss": 2.5075,
      "step": 2253
    },
    {
      "epoch": 14.896323833126807,
      "grad_norm": 15.15147876739502,
      "learning_rate": 0.0003792079207920792,
      "loss": 2.4651,
      "step": 2254
    },
    {
      "epoch": 14.902932672449401,
      "grad_norm": 17.418487548828125,
      "learning_rate": 0.0003787128712871287,
      "loss": 1.2605,
      "step": 2255
    },
    {
      "epoch": 14.909541511771994,
      "grad_norm": 32.91868591308594,
      "learning_rate": 0.00037821782178217825,
      "loss": 2.7475,
      "step": 2256
    },
    {
      "epoch": 14.91615035109459,
      "grad_norm": 25.060461044311523,
      "learning_rate": 0.0003777227722772277,
      "loss": 2.0328,
      "step": 2257
    },
    {
      "epoch": 14.922759190417183,
      "grad_norm": 43.18060302734375,
      "learning_rate": 0.00037722772277227725,
      "loss": 1.9174,
      "step": 2258
    },
    {
      "epoch": 14.929368029739777,
      "grad_norm": 56.40352249145508,
      "learning_rate": 0.0003767326732673267,
      "loss": 2.3928,
      "step": 2259
    },
    {
      "epoch": 14.93597686906237,
      "grad_norm": 65.73020935058594,
      "learning_rate": 0.00037623762376237625,
      "loss": 2.4936,
      "step": 2260
    },
    {
      "epoch": 14.942585708384964,
      "grad_norm": 2.0341055393218994,
      "learning_rate": 0.00037574257425742577,
      "loss": 0.5182,
      "step": 2261
    },
    {
      "epoch": 14.94919454770756,
      "grad_norm": 13.787793159484863,
      "learning_rate": 0.00037524752475247524,
      "loss": 4.8278,
      "step": 2262
    },
    {
      "epoch": 14.955803387030153,
      "grad_norm": 59.020835876464844,
      "learning_rate": 0.00037475247524752477,
      "loss": 2.3894,
      "step": 2263
    },
    {
      "epoch": 14.962412226352747,
      "grad_norm": 5.307520389556885,
      "learning_rate": 0.00037425742574257424,
      "loss": 1.5905,
      "step": 2264
    },
    {
      "epoch": 14.96902106567534,
      "grad_norm": 28.9787654876709,
      "learning_rate": 0.00037376237623762377,
      "loss": 0.888,
      "step": 2265
    },
    {
      "epoch": 14.975629904997934,
      "grad_norm": 45.03548812866211,
      "learning_rate": 0.0003732673267326733,
      "loss": 1.272,
      "step": 2266
    },
    {
      "epoch": 14.98223874432053,
      "grad_norm": 63.02690124511719,
      "learning_rate": 0.00037277227722772277,
      "loss": 2.4093,
      "step": 2267
    },
    {
      "epoch": 14.988847583643123,
      "grad_norm": 21.809349060058594,
      "learning_rate": 0.0003722772277227723,
      "loss": 3.1205,
      "step": 2268
    },
    {
      "epoch": 14.995456422965717,
      "grad_norm": 24.322668075561523,
      "learning_rate": 0.00037178217821782177,
      "loss": 2.3722,
      "step": 2269
    },
    {
      "epoch": 14.995456422965717,
      "eval_validation_error_bar": 0.04720556005139196,
      "eval_validation_loss": 6.575355052947998,
      "eval_validation_pearsonr": 0.587888723548703,
      "eval_validation_rmse": 2.5642454624176025,
      "eval_validation_runtime": 33.4559,
      "eval_validation_samples_per_second": 6.068,
      "eval_validation_spearman": 0.6238617128127815,
      "eval_validation_steps_per_second": 6.068,
      "step": 2269
    },
    {
      "epoch": 14.995456422965717,
      "eval_test_error_bar": 0.038434918265774956,
      "eval_test_loss": 6.896957874298096,
      "eval_test_pearsonr": 0.5968844445716658,
      "eval_test_rmse": 2.6262059211730957,
      "eval_test_runtime": 41.9315,
      "eval_test_samples_per_second": 7.775,
      "eval_test_spearman": 0.6039384652237828,
      "eval_test_steps_per_second": 7.775,
      "step": 2269
    },
    {
      "epoch": 15.00206526228831,
      "grad_norm": 7.934340953826904,
      "learning_rate": 0.0003712871287128713,
      "loss": 0.9155,
      "step": 2270
    },
    {
      "epoch": 15.008674101610904,
      "grad_norm": 55.8042106628418,
      "learning_rate": 0.0003707920792079208,
      "loss": 2.3364,
      "step": 2271
    },
    {
      "epoch": 15.0152829409335,
      "grad_norm": 6.509779453277588,
      "learning_rate": 0.0003702970297029703,
      "loss": 3.8554,
      "step": 2272
    },
    {
      "epoch": 15.021891780256093,
      "grad_norm": 9.37089729309082,
      "learning_rate": 0.0003698019801980198,
      "loss": 4.0794,
      "step": 2273
    },
    {
      "epoch": 15.028500619578686,
      "grad_norm": 50.93193817138672,
      "learning_rate": 0.0003693069306930693,
      "loss": 1.4289,
      "step": 2274
    },
    {
      "epoch": 15.03510945890128,
      "grad_norm": 8.21444320678711,
      "learning_rate": 0.0003688118811881188,
      "loss": 1.0796,
      "step": 2275
    },
    {
      "epoch": 15.041718298223875,
      "grad_norm": 89.9701156616211,
      "learning_rate": 0.00036831683168316834,
      "loss": 4.1436,
      "step": 2276
    },
    {
      "epoch": 15.048327137546469,
      "grad_norm": 69.22146606445312,
      "learning_rate": 0.0003678217821782178,
      "loss": 2.4177,
      "step": 2277
    },
    {
      "epoch": 15.054935976869062,
      "grad_norm": 41.188411712646484,
      "learning_rate": 0.00036732673267326734,
      "loss": 3.1928,
      "step": 2278
    },
    {
      "epoch": 15.061544816191656,
      "grad_norm": 36.5525016784668,
      "learning_rate": 0.0003668316831683168,
      "loss": 1.1892,
      "step": 2279
    },
    {
      "epoch": 15.06815365551425,
      "grad_norm": 57.41218566894531,
      "learning_rate": 0.00036633663366336634,
      "loss": 1.9092,
      "step": 2280
    },
    {
      "epoch": 15.074762494836845,
      "grad_norm": 29.6232967376709,
      "learning_rate": 0.00036584158415841586,
      "loss": 1.9022,
      "step": 2281
    },
    {
      "epoch": 15.081371334159439,
      "grad_norm": 40.647254943847656,
      "learning_rate": 0.00036534653465346533,
      "loss": 3.1687,
      "step": 2282
    },
    {
      "epoch": 15.087980173482032,
      "grad_norm": 2.797527551651001,
      "learning_rate": 0.00036485148514851486,
      "loss": 1.0244,
      "step": 2283
    },
    {
      "epoch": 15.094589012804626,
      "grad_norm": 46.37339401245117,
      "learning_rate": 0.00036435643564356433,
      "loss": 0.9266,
      "step": 2284
    },
    {
      "epoch": 15.10119785212722,
      "grad_norm": 19.99844741821289,
      "learning_rate": 0.00036386138613861386,
      "loss": 1.2295,
      "step": 2285
    },
    {
      "epoch": 15.107806691449815,
      "grad_norm": 6.480037689208984,
      "learning_rate": 0.0003633663366336634,
      "loss": 1.0875,
      "step": 2286
    },
    {
      "epoch": 15.114415530772408,
      "grad_norm": 33.85908508300781,
      "learning_rate": 0.00036287128712871286,
      "loss": 2.3128,
      "step": 2287
    },
    {
      "epoch": 15.121024370095002,
      "grad_norm": 39.31748580932617,
      "learning_rate": 0.0003623762376237624,
      "loss": 2.5732,
      "step": 2288
    },
    {
      "epoch": 15.127633209417596,
      "grad_norm": 62.26349639892578,
      "learning_rate": 0.00036188118811881186,
      "loss": 2.0037,
      "step": 2289
    },
    {
      "epoch": 15.13424204874019,
      "grad_norm": 31.649986267089844,
      "learning_rate": 0.0003613861386138614,
      "loss": 1.0569,
      "step": 2290
    },
    {
      "epoch": 15.140850888062785,
      "grad_norm": 39.301631927490234,
      "learning_rate": 0.0003608910891089109,
      "loss": 2.853,
      "step": 2291
    },
    {
      "epoch": 15.147459727385378,
      "grad_norm": 25.329641342163086,
      "learning_rate": 0.0003603960396039604,
      "loss": 1.8193,
      "step": 2292
    },
    {
      "epoch": 15.154068566707972,
      "grad_norm": 21.02166748046875,
      "learning_rate": 0.0003599009900990099,
      "loss": 1.7834,
      "step": 2293
    },
    {
      "epoch": 15.160677406030565,
      "grad_norm": 37.381038665771484,
      "learning_rate": 0.0003594059405940594,
      "loss": 1.6839,
      "step": 2294
    },
    {
      "epoch": 15.16728624535316,
      "grad_norm": 6.771316051483154,
      "learning_rate": 0.0003589108910891089,
      "loss": 0.7437,
      "step": 2295
    },
    {
      "epoch": 15.173895084675754,
      "grad_norm": 24.747028350830078,
      "learning_rate": 0.00035841584158415843,
      "loss": 1.0444,
      "step": 2296
    },
    {
      "epoch": 15.180503923998348,
      "grad_norm": 17.816679000854492,
      "learning_rate": 0.0003579207920792079,
      "loss": 1.917,
      "step": 2297
    },
    {
      "epoch": 15.187112763320942,
      "grad_norm": 32.17369079589844,
      "learning_rate": 0.00035742574257425743,
      "loss": 2.466,
      "step": 2298
    },
    {
      "epoch": 15.193721602643535,
      "grad_norm": 39.00347137451172,
      "learning_rate": 0.0003569306930693069,
      "loss": 0.7893,
      "step": 2299
    },
    {
      "epoch": 15.20033044196613,
      "grad_norm": 57.66627883911133,
      "learning_rate": 0.0003564356435643564,
      "loss": 1.5509,
      "step": 2300
    },
    {
      "epoch": 15.206939281288724,
      "grad_norm": 93.28933715820312,
      "learning_rate": 0.00035594059405940595,
      "loss": 5.1366,
      "step": 2301
    },
    {
      "epoch": 15.213548120611318,
      "grad_norm": 42.0654411315918,
      "learning_rate": 0.0003554455445544554,
      "loss": 1.2861,
      "step": 2302
    },
    {
      "epoch": 15.220156959933911,
      "grad_norm": 102.54287719726562,
      "learning_rate": 0.00035495049504950495,
      "loss": 4.5532,
      "step": 2303
    },
    {
      "epoch": 15.226765799256505,
      "grad_norm": 50.33461380004883,
      "learning_rate": 0.0003544554455445544,
      "loss": 1.448,
      "step": 2304
    },
    {
      "epoch": 15.2333746385791,
      "grad_norm": 33.6910285949707,
      "learning_rate": 0.00035396039603960395,
      "loss": 1.7531,
      "step": 2305
    },
    {
      "epoch": 15.239983477901694,
      "grad_norm": 23.219491958618164,
      "learning_rate": 0.0003534653465346535,
      "loss": 2.6348,
      "step": 2306
    },
    {
      "epoch": 15.246592317224287,
      "grad_norm": 33.294864654541016,
      "learning_rate": 0.00035297029702970295,
      "loss": 2.0637,
      "step": 2307
    },
    {
      "epoch": 15.253201156546881,
      "grad_norm": 14.8230562210083,
      "learning_rate": 0.0003524752475247525,
      "loss": 1.8012,
      "step": 2308
    },
    {
      "epoch": 15.259809995869475,
      "grad_norm": 22.45210838317871,
      "learning_rate": 0.00035198019801980195,
      "loss": 0.9408,
      "step": 2309
    },
    {
      "epoch": 15.26641883519207,
      "grad_norm": 20.731197357177734,
      "learning_rate": 0.00035148514851485147,
      "loss": 1.0553,
      "step": 2310
    },
    {
      "epoch": 15.273027674514664,
      "grad_norm": 13.947145462036133,
      "learning_rate": 0.000350990099009901,
      "loss": 3.4372,
      "step": 2311
    },
    {
      "epoch": 15.279636513837257,
      "grad_norm": 4.279252529144287,
      "learning_rate": 0.00035049504950495047,
      "loss": 1.1604,
      "step": 2312
    },
    {
      "epoch": 15.28624535315985,
      "grad_norm": 41.921241760253906,
      "learning_rate": 0.00035,
      "loss": 1.8293,
      "step": 2313
    },
    {
      "epoch": 15.292854192482444,
      "grad_norm": 35.5682258605957,
      "learning_rate": 0.00034950495049504947,
      "loss": 1.7109,
      "step": 2314
    },
    {
      "epoch": 15.29946303180504,
      "grad_norm": 29.84684944152832,
      "learning_rate": 0.000349009900990099,
      "loss": 1.6533,
      "step": 2315
    },
    {
      "epoch": 15.306071871127633,
      "grad_norm": 60.41758346557617,
      "learning_rate": 0.0003485148514851485,
      "loss": 4.7913,
      "step": 2316
    },
    {
      "epoch": 15.312680710450227,
      "grad_norm": 76.37924194335938,
      "learning_rate": 0.000348019801980198,
      "loss": 4.2321,
      "step": 2317
    },
    {
      "epoch": 15.31928954977282,
      "grad_norm": 10.663030624389648,
      "learning_rate": 0.0003475247524752475,
      "loss": 1.4152,
      "step": 2318
    },
    {
      "epoch": 15.325898389095416,
      "grad_norm": 29.265541076660156,
      "learning_rate": 0.000347029702970297,
      "loss": 1.1316,
      "step": 2319
    },
    {
      "epoch": 15.33250722841801,
      "grad_norm": 28.243942260742188,
      "learning_rate": 0.0003465346534653465,
      "loss": 2.336,
      "step": 2320
    },
    {
      "epoch": 15.339116067740603,
      "grad_norm": 17.15973472595215,
      "learning_rate": 0.00034603960396039604,
      "loss": 2.9135,
      "step": 2321
    },
    {
      "epoch": 15.345724907063197,
      "grad_norm": 62.194374084472656,
      "learning_rate": 0.0003455445544554455,
      "loss": 2.4472,
      "step": 2322
    },
    {
      "epoch": 15.35233374638579,
      "grad_norm": 34.5069465637207,
      "learning_rate": 0.00034504950495049504,
      "loss": 1.1309,
      "step": 2323
    },
    {
      "epoch": 15.358942585708386,
      "grad_norm": 45.36354064941406,
      "learning_rate": 0.0003445544554455445,
      "loss": 1.8753,
      "step": 2324
    },
    {
      "epoch": 15.36555142503098,
      "grad_norm": 8.061612129211426,
      "learning_rate": 0.00034405940594059404,
      "loss": 0.8208,
      "step": 2325
    },
    {
      "epoch": 15.372160264353573,
      "grad_norm": 25.69966697692871,
      "learning_rate": 0.0003435643564356436,
      "loss": 2.9176,
      "step": 2326
    },
    {
      "epoch": 15.378769103676166,
      "grad_norm": 24.723003387451172,
      "learning_rate": 0.0003430693069306931,
      "loss": 1.9736,
      "step": 2327
    },
    {
      "epoch": 15.38537794299876,
      "grad_norm": 13.214640617370605,
      "learning_rate": 0.0003425742574257426,
      "loss": 1.2746,
      "step": 2328
    },
    {
      "epoch": 15.391986782321355,
      "grad_norm": 8.170849800109863,
      "learning_rate": 0.0003420792079207921,
      "loss": 1.1293,
      "step": 2329
    },
    {
      "epoch": 15.398595621643949,
      "grad_norm": 3.612386703491211,
      "learning_rate": 0.0003415841584158416,
      "loss": 1.3023,
      "step": 2330
    },
    {
      "epoch": 15.405204460966543,
      "grad_norm": 7.915157318115234,
      "learning_rate": 0.00034108910891089114,
      "loss": 2.4522,
      "step": 2331
    },
    {
      "epoch": 15.411813300289136,
      "grad_norm": 10.840989112854004,
      "learning_rate": 0.0003405940594059406,
      "loss": 1.1926,
      "step": 2332
    },
    {
      "epoch": 15.418422139611732,
      "grad_norm": 27.475202560424805,
      "learning_rate": 0.00034009900990099014,
      "loss": 1.4052,
      "step": 2333
    },
    {
      "epoch": 15.425030978934325,
      "grad_norm": 33.57026290893555,
      "learning_rate": 0.0003396039603960396,
      "loss": 1.7147,
      "step": 2334
    },
    {
      "epoch": 15.431639818256919,
      "grad_norm": 98.2521743774414,
      "learning_rate": 0.00033910891089108914,
      "loss": 4.853,
      "step": 2335
    },
    {
      "epoch": 15.438248657579512,
      "grad_norm": 20.688907623291016,
      "learning_rate": 0.00033861386138613867,
      "loss": 3.7961,
      "step": 2336
    },
    {
      "epoch": 15.444857496902106,
      "grad_norm": 42.06363296508789,
      "learning_rate": 0.00033811881188118814,
      "loss": 2.3294,
      "step": 2337
    },
    {
      "epoch": 15.451466336224701,
      "grad_norm": 69.92433166503906,
      "learning_rate": 0.00033762376237623766,
      "loss": 2.8616,
      "step": 2338
    },
    {
      "epoch": 15.458075175547295,
      "grad_norm": 59.715660095214844,
      "learning_rate": 0.00033712871287128714,
      "loss": 3.3864,
      "step": 2339
    },
    {
      "epoch": 15.464684014869889,
      "grad_norm": 5.387300491333008,
      "learning_rate": 0.00033663366336633666,
      "loss": 1.1917,
      "step": 2340
    },
    {
      "epoch": 15.471292854192482,
      "grad_norm": 23.221397399902344,
      "learning_rate": 0.0003361386138613862,
      "loss": 0.8543,
      "step": 2341
    },
    {
      "epoch": 15.477901693515076,
      "grad_norm": 13.043522834777832,
      "learning_rate": 0.00033564356435643566,
      "loss": 2.3929,
      "step": 2342
    },
    {
      "epoch": 15.484510532837671,
      "grad_norm": 14.730237007141113,
      "learning_rate": 0.0003351485148514852,
      "loss": 2.3422,
      "step": 2343
    },
    {
      "epoch": 15.491119372160265,
      "grad_norm": 15.262551307678223,
      "learning_rate": 0.00033465346534653466,
      "loss": 3.9764,
      "step": 2344
    },
    {
      "epoch": 15.497728211482858,
      "grad_norm": 11.969230651855469,
      "learning_rate": 0.0003341584158415842,
      "loss": 0.8084,
      "step": 2345
    },
    {
      "epoch": 15.504337050805452,
      "grad_norm": 40.140655517578125,
      "learning_rate": 0.0003336633663366337,
      "loss": 1.6834,
      "step": 2346
    },
    {
      "epoch": 15.510945890128045,
      "grad_norm": 26.588207244873047,
      "learning_rate": 0.0003331683168316832,
      "loss": 0.7619,
      "step": 2347
    },
    {
      "epoch": 15.51755472945064,
      "grad_norm": 45.21682357788086,
      "learning_rate": 0.0003326732673267327,
      "loss": 1.1014,
      "step": 2348
    },
    {
      "epoch": 15.524163568773234,
      "grad_norm": 21.456636428833008,
      "learning_rate": 0.0003321782178217822,
      "loss": 2.6904,
      "step": 2349
    },
    {
      "epoch": 15.530772408095828,
      "grad_norm": 8.917325973510742,
      "learning_rate": 0.0003316831683168317,
      "loss": 1.4213,
      "step": 2350
    },
    {
      "epoch": 15.537381247418422,
      "grad_norm": 3.8710179328918457,
      "learning_rate": 0.00033118811881188123,
      "loss": 1.1602,
      "step": 2351
    },
    {
      "epoch": 15.543990086741015,
      "grad_norm": 17.20782470703125,
      "learning_rate": 0.0003306930693069307,
      "loss": 2.2129,
      "step": 2352
    },
    {
      "epoch": 15.55059892606361,
      "grad_norm": 27.75878143310547,
      "learning_rate": 0.00033019801980198023,
      "loss": 2.7498,
      "step": 2353
    },
    {
      "epoch": 15.557207765386204,
      "grad_norm": 2.0965592861175537,
      "learning_rate": 0.0003297029702970297,
      "loss": 1.4083,
      "step": 2354
    },
    {
      "epoch": 15.563816604708798,
      "grad_norm": 27.342220306396484,
      "learning_rate": 0.00032920792079207923,
      "loss": 1.418,
      "step": 2355
    },
    {
      "epoch": 15.570425444031391,
      "grad_norm": 36.481285095214844,
      "learning_rate": 0.00032871287128712876,
      "loss": 1.9948,
      "step": 2356
    },
    {
      "epoch": 15.577034283353987,
      "grad_norm": 39.42628479003906,
      "learning_rate": 0.00032821782178217823,
      "loss": 1.3014,
      "step": 2357
    },
    {
      "epoch": 15.58364312267658,
      "grad_norm": 24.49742889404297,
      "learning_rate": 0.00032772277227722775,
      "loss": 2.9302,
      "step": 2358
    },
    {
      "epoch": 15.590251961999174,
      "grad_norm": 47.11391830444336,
      "learning_rate": 0.0003272277227722772,
      "loss": 1.4673,
      "step": 2359
    },
    {
      "epoch": 15.596860801321768,
      "grad_norm": 69.14271545410156,
      "learning_rate": 0.00032673267326732675,
      "loss": 2.1678,
      "step": 2360
    },
    {
      "epoch": 15.603469640644361,
      "grad_norm": 20.08934783935547,
      "learning_rate": 0.0003262376237623763,
      "loss": 1.7881,
      "step": 2361
    },
    {
      "epoch": 15.610078479966957,
      "grad_norm": 28.218276977539062,
      "learning_rate": 0.00032574257425742575,
      "loss": 3.9281,
      "step": 2362
    },
    {
      "epoch": 15.61668731928955,
      "grad_norm": 21.88436508178711,
      "learning_rate": 0.0003252475247524753,
      "loss": 0.7841,
      "step": 2363
    },
    {
      "epoch": 15.623296158612144,
      "grad_norm": 9.514214515686035,
      "learning_rate": 0.00032475247524752475,
      "loss": 1.7618,
      "step": 2364
    },
    {
      "epoch": 15.629904997934737,
      "grad_norm": 23.820756912231445,
      "learning_rate": 0.0003242574257425743,
      "loss": 0.7933,
      "step": 2365
    },
    {
      "epoch": 15.636513837257331,
      "grad_norm": 3.7380597591400146,
      "learning_rate": 0.0003237623762376238,
      "loss": 2.0328,
      "step": 2366
    },
    {
      "epoch": 15.643122676579926,
      "grad_norm": 45.35713577270508,
      "learning_rate": 0.00032326732673267327,
      "loss": 2.6255,
      "step": 2367
    },
    {
      "epoch": 15.64973151590252,
      "grad_norm": 23.431915283203125,
      "learning_rate": 0.0003227722772277228,
      "loss": 1.7389,
      "step": 2368
    },
    {
      "epoch": 15.656340355225113,
      "grad_norm": 3.937608242034912,
      "learning_rate": 0.00032227722772277227,
      "loss": 3.1074,
      "step": 2369
    },
    {
      "epoch": 15.662949194547707,
      "grad_norm": 5.180346965789795,
      "learning_rate": 0.0003217821782178218,
      "loss": 1.3512,
      "step": 2370
    },
    {
      "epoch": 15.669558033870302,
      "grad_norm": 29.28465461730957,
      "learning_rate": 0.0003212871287128713,
      "loss": 1.356,
      "step": 2371
    },
    {
      "epoch": 15.676166873192896,
      "grad_norm": 4.281296730041504,
      "learning_rate": 0.0003207920792079208,
      "loss": 0.7682,
      "step": 2372
    },
    {
      "epoch": 15.68277571251549,
      "grad_norm": 5.4793500900268555,
      "learning_rate": 0.0003202970297029703,
      "loss": 2.2969,
      "step": 2373
    },
    {
      "epoch": 15.689384551838083,
      "grad_norm": 21.260658264160156,
      "learning_rate": 0.0003198019801980198,
      "loss": 4.0358,
      "step": 2374
    },
    {
      "epoch": 15.695993391160677,
      "grad_norm": 4.159359455108643,
      "learning_rate": 0.0003193069306930693,
      "loss": 1.1116,
      "step": 2375
    },
    {
      "epoch": 15.702602230483272,
      "grad_norm": 7.728134632110596,
      "learning_rate": 0.00031881188118811885,
      "loss": 0.8259,
      "step": 2376
    },
    {
      "epoch": 15.709211069805866,
      "grad_norm": 64.21559143066406,
      "learning_rate": 0.0003183168316831683,
      "loss": 4.1603,
      "step": 2377
    },
    {
      "epoch": 15.71581990912846,
      "grad_norm": 2.4337260723114014,
      "learning_rate": 0.00031782178217821784,
      "loss": 4.6793,
      "step": 2378
    },
    {
      "epoch": 15.722428748451053,
      "grad_norm": 16.391937255859375,
      "learning_rate": 0.0003173267326732673,
      "loss": 2.8419,
      "step": 2379
    },
    {
      "epoch": 15.729037587773647,
      "grad_norm": 7.878892421722412,
      "learning_rate": 0.00031683168316831684,
      "loss": 2.1458,
      "step": 2380
    },
    {
      "epoch": 15.735646427096242,
      "grad_norm": 57.74260330200195,
      "learning_rate": 0.00031633663366336637,
      "loss": 1.8761,
      "step": 2381
    },
    {
      "epoch": 15.742255266418836,
      "grad_norm": 65.77620697021484,
      "learning_rate": 0.00031584158415841584,
      "loss": 2.5758,
      "step": 2382
    },
    {
      "epoch": 15.74886410574143,
      "grad_norm": 55.23103713989258,
      "learning_rate": 0.00031534653465346537,
      "loss": 1.8659,
      "step": 2383
    },
    {
      "epoch": 15.755472945064023,
      "grad_norm": 74.38789367675781,
      "learning_rate": 0.00031485148514851484,
      "loss": 3.4552,
      "step": 2384
    },
    {
      "epoch": 15.762081784386616,
      "grad_norm": 62.369136810302734,
      "learning_rate": 0.00031435643564356436,
      "loss": 2.5573,
      "step": 2385
    },
    {
      "epoch": 15.768690623709212,
      "grad_norm": 24.262178421020508,
      "learning_rate": 0.0003138613861386139,
      "loss": 2.5468,
      "step": 2386
    },
    {
      "epoch": 15.775299463031805,
      "grad_norm": 23.864665985107422,
      "learning_rate": 0.00031336633663366336,
      "loss": 2.435,
      "step": 2387
    },
    {
      "epoch": 15.781908302354399,
      "grad_norm": 61.23102569580078,
      "learning_rate": 0.0003128712871287129,
      "loss": 2.972,
      "step": 2388
    },
    {
      "epoch": 15.788517141676992,
      "grad_norm": 23.606840133666992,
      "learning_rate": 0.00031237623762376236,
      "loss": 0.9613,
      "step": 2389
    },
    {
      "epoch": 15.795125980999586,
      "grad_norm": 69.56999206542969,
      "learning_rate": 0.0003118811881188119,
      "loss": 2.7087,
      "step": 2390
    },
    {
      "epoch": 15.801734820322181,
      "grad_norm": 50.96492004394531,
      "learning_rate": 0.0003113861386138614,
      "loss": 2.6426,
      "step": 2391
    },
    {
      "epoch": 15.808343659644775,
      "grad_norm": 20.52617835998535,
      "learning_rate": 0.0003108910891089109,
      "loss": 2.0745,
      "step": 2392
    },
    {
      "epoch": 15.814952498967369,
      "grad_norm": 47.66118240356445,
      "learning_rate": 0.0003103960396039604,
      "loss": 2.6011,
      "step": 2393
    },
    {
      "epoch": 15.821561338289962,
      "grad_norm": 48.08406066894531,
      "learning_rate": 0.0003099009900990099,
      "loss": 3.9973,
      "step": 2394
    },
    {
      "epoch": 15.828170177612558,
      "grad_norm": 11.163599967956543,
      "learning_rate": 0.0003094059405940594,
      "loss": 1.1798,
      "step": 2395
    },
    {
      "epoch": 15.834779016935151,
      "grad_norm": 26.849477767944336,
      "learning_rate": 0.00030891089108910894,
      "loss": 5.2041,
      "step": 2396
    },
    {
      "epoch": 15.841387856257745,
      "grad_norm": 2.8926033973693848,
      "learning_rate": 0.0003084158415841584,
      "loss": 1.2723,
      "step": 2397
    },
    {
      "epoch": 15.847996695580338,
      "grad_norm": 2.238440990447998,
      "learning_rate": 0.00030792079207920793,
      "loss": 1.6621,
      "step": 2398
    },
    {
      "epoch": 15.854605534902932,
      "grad_norm": 88.81188201904297,
      "learning_rate": 0.0003074257425742574,
      "loss": 5.5763,
      "step": 2399
    },
    {
      "epoch": 15.861214374225527,
      "grad_norm": 34.7932243347168,
      "learning_rate": 0.00030693069306930693,
      "loss": 1.7695,
      "step": 2400
    },
    {
      "epoch": 15.867823213548121,
      "grad_norm": 89.9743881225586,
      "learning_rate": 0.00030643564356435646,
      "loss": 5.6036,
      "step": 2401
    },
    {
      "epoch": 15.874432052870715,
      "grad_norm": 98.76710510253906,
      "learning_rate": 0.00030594059405940593,
      "loss": 4.6604,
      "step": 2402
    },
    {
      "epoch": 15.881040892193308,
      "grad_norm": 70.37601470947266,
      "learning_rate": 0.00030544554455445546,
      "loss": 2.9155,
      "step": 2403
    },
    {
      "epoch": 15.887649731515902,
      "grad_norm": 32.730079650878906,
      "learning_rate": 0.00030495049504950493,
      "loss": 1.3992,
      "step": 2404
    },
    {
      "epoch": 15.894258570838497,
      "grad_norm": 61.33016586303711,
      "learning_rate": 0.00030445544554455445,
      "loss": 2.5108,
      "step": 2405
    },
    {
      "epoch": 15.90086741016109,
      "grad_norm": 25.491458892822266,
      "learning_rate": 0.000303960396039604,
      "loss": 1.2832,
      "step": 2406
    },
    {
      "epoch": 15.907476249483684,
      "grad_norm": 24.73944854736328,
      "learning_rate": 0.00030346534653465345,
      "loss": 1.6331,
      "step": 2407
    },
    {
      "epoch": 15.914085088806278,
      "grad_norm": 1.09538996219635,
      "learning_rate": 0.000302970297029703,
      "loss": 0.5214,
      "step": 2408
    },
    {
      "epoch": 15.920693928128873,
      "grad_norm": 25.14503288269043,
      "learning_rate": 0.00030247524752475245,
      "loss": 2.0164,
      "step": 2409
    },
    {
      "epoch": 15.927302767451467,
      "grad_norm": 9.675232887268066,
      "learning_rate": 0.000301980198019802,
      "loss": 0.9992,
      "step": 2410
    },
    {
      "epoch": 15.93391160677406,
      "grad_norm": 38.365657806396484,
      "learning_rate": 0.0003014851485148515,
      "loss": 2.1422,
      "step": 2411
    },
    {
      "epoch": 15.940520446096654,
      "grad_norm": 11.414800643920898,
      "learning_rate": 0.000300990099009901,
      "loss": 4.2411,
      "step": 2412
    },
    {
      "epoch": 15.947129285419248,
      "grad_norm": 36.927425384521484,
      "learning_rate": 0.0003004950495049505,
      "loss": 1.4204,
      "step": 2413
    },
    {
      "epoch": 15.953738124741843,
      "grad_norm": 28.801420211791992,
      "learning_rate": 0.0003,
      "loss": 1.8376,
      "step": 2414
    },
    {
      "epoch": 15.960346964064437,
      "grad_norm": 32.708351135253906,
      "learning_rate": 0.0002995049504950495,
      "loss": 2.2493,
      "step": 2415
    },
    {
      "epoch": 15.96695580338703,
      "grad_norm": 15.623011589050293,
      "learning_rate": 0.000299009900990099,
      "loss": 1.5742,
      "step": 2416
    },
    {
      "epoch": 15.973564642709624,
      "grad_norm": 8.986160278320312,
      "learning_rate": 0.0002985148514851485,
      "loss": 1.4016,
      "step": 2417
    },
    {
      "epoch": 15.980173482032217,
      "grad_norm": 29.839141845703125,
      "learning_rate": 0.000298019801980198,
      "loss": 1.4516,
      "step": 2418
    },
    {
      "epoch": 15.986782321354813,
      "grad_norm": 35.43769454956055,
      "learning_rate": 0.0002975247524752475,
      "loss": 1.9425,
      "step": 2419
    },
    {
      "epoch": 15.993391160677406,
      "grad_norm": 2.7413828372955322,
      "learning_rate": 0.000297029702970297,
      "loss": 1.126,
      "step": 2420
    },
    {
      "epoch": 16.0,
      "grad_norm": 20.29161262512207,
      "learning_rate": 0.00029653465346534655,
      "loss": 1.3885,
      "step": 2421
    },
    {
      "epoch": 16.0,
      "eval_validation_error_bar": 0.04555434430699963,
      "eval_validation_loss": 6.291665077209473,
      "eval_validation_pearsonr": 0.5979009485784781,
      "eval_validation_rmse": 2.508319139480591,
      "eval_validation_runtime": 33.5182,
      "eval_validation_samples_per_second": 6.056,
      "eval_validation_spearman": 0.643077750684041,
      "eval_validation_steps_per_second": 6.056,
      "step": 2421
    },
    {
      "epoch": 16.0,
      "eval_test_error_bar": 0.03977600377907646,
      "eval_test_loss": 6.626171112060547,
      "eval_test_pearsonr": 0.5807776344241996,
      "eval_test_rmse": 2.5741350650787354,
      "eval_test_runtime": 38.7694,
      "eval_test_samples_per_second": 8.409,
      "eval_test_spearman": 0.5822053472605045,
      "eval_test_steps_per_second": 8.409,
      "step": 2421
    },
    {
      "epoch": 16.006608839322595,
      "grad_norm": 3.8544466495513916,
      "learning_rate": 0.000296039603960396,
      "loss": 1.4287,
      "step": 2422
    },
    {
      "epoch": 16.013217678645187,
      "grad_norm": 44.2552375793457,
      "learning_rate": 0.00029554455445544555,
      "loss": 3.7427,
      "step": 2423
    },
    {
      "epoch": 16.019826517967783,
      "grad_norm": 47.38496398925781,
      "learning_rate": 0.000295049504950495,
      "loss": 1.4002,
      "step": 2424
    },
    {
      "epoch": 16.026435357290374,
      "grad_norm": 62.60868453979492,
      "learning_rate": 0.00029455445544554455,
      "loss": 2.5728,
      "step": 2425
    },
    {
      "epoch": 16.03304419661297,
      "grad_norm": 52.81795120239258,
      "learning_rate": 0.00029405940594059407,
      "loss": 2.6786,
      "step": 2426
    },
    {
      "epoch": 16.039653035935565,
      "grad_norm": 1.770580768585205,
      "learning_rate": 0.00029356435643564354,
      "loss": 1.1973,
      "step": 2427
    },
    {
      "epoch": 16.046261875258157,
      "grad_norm": 16.08218002319336,
      "learning_rate": 0.00029306930693069307,
      "loss": 0.8998,
      "step": 2428
    },
    {
      "epoch": 16.052870714580752,
      "grad_norm": 26.517019271850586,
      "learning_rate": 0.00029257425742574254,
      "loss": 2.6522,
      "step": 2429
    },
    {
      "epoch": 16.059479553903344,
      "grad_norm": 14.922633171081543,
      "learning_rate": 0.00029207920792079207,
      "loss": 2.8141,
      "step": 2430
    },
    {
      "epoch": 16.06608839322594,
      "grad_norm": 17.340574264526367,
      "learning_rate": 0.0002915841584158416,
      "loss": 1.0079,
      "step": 2431
    },
    {
      "epoch": 16.072697232548535,
      "grad_norm": 8.764781951904297,
      "learning_rate": 0.00029108910891089107,
      "loss": 0.6982,
      "step": 2432
    },
    {
      "epoch": 16.079306071871127,
      "grad_norm": 9.633369445800781,
      "learning_rate": 0.0002905940594059406,
      "loss": 3.237,
      "step": 2433
    },
    {
      "epoch": 16.085914911193722,
      "grad_norm": 9.70299243927002,
      "learning_rate": 0.00029009900990099006,
      "loss": 1.1641,
      "step": 2434
    },
    {
      "epoch": 16.092523750516314,
      "grad_norm": 3.1760833263397217,
      "learning_rate": 0.0002896039603960396,
      "loss": 2.5132,
      "step": 2435
    },
    {
      "epoch": 16.09913258983891,
      "grad_norm": 27.655548095703125,
      "learning_rate": 0.0002891089108910891,
      "loss": 0.809,
      "step": 2436
    },
    {
      "epoch": 16.105741429161505,
      "grad_norm": 23.44120216369629,
      "learning_rate": 0.0002886138613861386,
      "loss": 0.6461,
      "step": 2437
    },
    {
      "epoch": 16.112350268484096,
      "grad_norm": 37.67338943481445,
      "learning_rate": 0.0002881188118811881,
      "loss": 1.4183,
      "step": 2438
    },
    {
      "epoch": 16.118959107806692,
      "grad_norm": 58.98454284667969,
      "learning_rate": 0.0002876237623762376,
      "loss": 3.2385,
      "step": 2439
    },
    {
      "epoch": 16.125567947129284,
      "grad_norm": 18.10268783569336,
      "learning_rate": 0.0002871287128712871,
      "loss": 0.6284,
      "step": 2440
    },
    {
      "epoch": 16.13217678645188,
      "grad_norm": 37.17951965332031,
      "learning_rate": 0.00028663366336633664,
      "loss": 5.0878,
      "step": 2441
    },
    {
      "epoch": 16.138785625774474,
      "grad_norm": 17.3552188873291,
      "learning_rate": 0.0002861386138613861,
      "loss": 2.4423,
      "step": 2442
    },
    {
      "epoch": 16.145394465097066,
      "grad_norm": 28.438308715820312,
      "learning_rate": 0.00028564356435643564,
      "loss": 2.0712,
      "step": 2443
    },
    {
      "epoch": 16.15200330441966,
      "grad_norm": 94.73759460449219,
      "learning_rate": 0.0002851485148514851,
      "loss": 3.3794,
      "step": 2444
    },
    {
      "epoch": 16.158612143742257,
      "grad_norm": 41.129127502441406,
      "learning_rate": 0.00028465346534653464,
      "loss": 1.0536,
      "step": 2445
    },
    {
      "epoch": 16.16522098306485,
      "grad_norm": 17.8563175201416,
      "learning_rate": 0.00028415841584158416,
      "loss": 1.0073,
      "step": 2446
    },
    {
      "epoch": 16.171829822387444,
      "grad_norm": 4.58151912689209,
      "learning_rate": 0.00028366336633663363,
      "loss": 3.5006,
      "step": 2447
    },
    {
      "epoch": 16.178438661710036,
      "grad_norm": 18.579605102539062,
      "learning_rate": 0.00028316831683168316,
      "loss": 1.2318,
      "step": 2448
    },
    {
      "epoch": 16.18504750103263,
      "grad_norm": 19.076147079467773,
      "learning_rate": 0.00028267326732673263,
      "loss": 2.1646,
      "step": 2449
    },
    {
      "epoch": 16.191656340355227,
      "grad_norm": 2.6353302001953125,
      "learning_rate": 0.00028217821782178216,
      "loss": 1.6377,
      "step": 2450
    },
    {
      "epoch": 16.19826517967782,
      "grad_norm": 50.41582107543945,
      "learning_rate": 0.0002816831683168317,
      "loss": 2.1591,
      "step": 2451
    },
    {
      "epoch": 16.204874019000414,
      "grad_norm": 16.889026641845703,
      "learning_rate": 0.0002811881188118812,
      "loss": 4.0258,
      "step": 2452
    },
    {
      "epoch": 16.211482858323006,
      "grad_norm": 5.887624740600586,
      "learning_rate": 0.00028069306930693074,
      "loss": 0.9767,
      "step": 2453
    },
    {
      "epoch": 16.2180916976456,
      "grad_norm": 24.86347198486328,
      "learning_rate": 0.0002801980198019802,
      "loss": 2.1913,
      "step": 2454
    },
    {
      "epoch": 16.224700536968196,
      "grad_norm": 25.924314498901367,
      "learning_rate": 0.00027970297029702973,
      "loss": 1.0395,
      "step": 2455
    },
    {
      "epoch": 16.23130937629079,
      "grad_norm": 37.91470718383789,
      "learning_rate": 0.00027920792079207926,
      "loss": 2.0491,
      "step": 2456
    },
    {
      "epoch": 16.237918215613384,
      "grad_norm": 4.639711856842041,
      "learning_rate": 0.00027871287128712873,
      "loss": 0.8941,
      "step": 2457
    },
    {
      "epoch": 16.244527054935975,
      "grad_norm": 32.88643264770508,
      "learning_rate": 0.00027821782178217826,
      "loss": 4.4505,
      "step": 2458
    },
    {
      "epoch": 16.25113589425857,
      "grad_norm": 2.8607735633850098,
      "learning_rate": 0.00027772277227722773,
      "loss": 0.663,
      "step": 2459
    },
    {
      "epoch": 16.257744733581166,
      "grad_norm": 49.52323913574219,
      "learning_rate": 0.00027722772277227726,
      "loss": 1.7997,
      "step": 2460
    },
    {
      "epoch": 16.264353572903758,
      "grad_norm": 7.504029750823975,
      "learning_rate": 0.0002767326732673268,
      "loss": 1.2967,
      "step": 2461
    },
    {
      "epoch": 16.270962412226353,
      "grad_norm": 19.82781982421875,
      "learning_rate": 0.00027623762376237626,
      "loss": 1.192,
      "step": 2462
    },
    {
      "epoch": 16.277571251548945,
      "grad_norm": 22.209604263305664,
      "learning_rate": 0.0002757425742574258,
      "loss": 1.5667,
      "step": 2463
    },
    {
      "epoch": 16.28418009087154,
      "grad_norm": 17.083894729614258,
      "learning_rate": 0.00027524752475247525,
      "loss": 1.3086,
      "step": 2464
    },
    {
      "epoch": 16.290788930194136,
      "grad_norm": 36.85042190551758,
      "learning_rate": 0.0002747524752475248,
      "loss": 5.0749,
      "step": 2465
    },
    {
      "epoch": 16.297397769516728,
      "grad_norm": 10.396291732788086,
      "learning_rate": 0.0002742574257425743,
      "loss": 0.9567,
      "step": 2466
    },
    {
      "epoch": 16.304006608839323,
      "grad_norm": 1.186310887336731,
      "learning_rate": 0.0002737623762376238,
      "loss": 2.5787,
      "step": 2467
    },
    {
      "epoch": 16.310615448161915,
      "grad_norm": 23.672767639160156,
      "learning_rate": 0.0002732673267326733,
      "loss": 2.6889,
      "step": 2468
    },
    {
      "epoch": 16.31722428748451,
      "grad_norm": 24.4705753326416,
      "learning_rate": 0.0002727722772277228,
      "loss": 2.1673,
      "step": 2469
    },
    {
      "epoch": 16.323833126807106,
      "grad_norm": 17.67445945739746,
      "learning_rate": 0.0002722772277227723,
      "loss": 3.7408,
      "step": 2470
    },
    {
      "epoch": 16.330441966129698,
      "grad_norm": 26.394474029541016,
      "learning_rate": 0.00027178217821782183,
      "loss": 1.209,
      "step": 2471
    },
    {
      "epoch": 16.337050805452293,
      "grad_norm": 26.662277221679688,
      "learning_rate": 0.0002712871287128713,
      "loss": 2.997,
      "step": 2472
    },
    {
      "epoch": 16.343659644774885,
      "grad_norm": 29.371570587158203,
      "learning_rate": 0.0002707920792079208,
      "loss": 2.5252,
      "step": 2473
    },
    {
      "epoch": 16.35026848409748,
      "grad_norm": 18.547767639160156,
      "learning_rate": 0.0002702970297029703,
      "loss": 2.7731,
      "step": 2474
    },
    {
      "epoch": 16.356877323420075,
      "grad_norm": 5.970235347747803,
      "learning_rate": 0.0002698019801980198,
      "loss": 0.988,
      "step": 2475
    },
    {
      "epoch": 16.363486162742667,
      "grad_norm": 45.19923782348633,
      "learning_rate": 0.00026930693069306935,
      "loss": 1.2225,
      "step": 2476
    },
    {
      "epoch": 16.370095002065263,
      "grad_norm": 34.2479362487793,
      "learning_rate": 0.0002688118811881188,
      "loss": 0.8898,
      "step": 2477
    },
    {
      "epoch": 16.376703841387855,
      "grad_norm": 35.195457458496094,
      "learning_rate": 0.00026831683168316835,
      "loss": 2.4086,
      "step": 2478
    },
    {
      "epoch": 16.38331268071045,
      "grad_norm": 3.9653775691986084,
      "learning_rate": 0.0002678217821782178,
      "loss": 1.2129,
      "step": 2479
    },
    {
      "epoch": 16.389921520033045,
      "grad_norm": 8.623233795166016,
      "learning_rate": 0.00026732673267326735,
      "loss": 1.447,
      "step": 2480
    },
    {
      "epoch": 16.396530359355637,
      "grad_norm": 6.39840841293335,
      "learning_rate": 0.0002668316831683169,
      "loss": 1.602,
      "step": 2481
    },
    {
      "epoch": 16.403139198678232,
      "grad_norm": 2.2063395977020264,
      "learning_rate": 0.00026633663366336635,
      "loss": 1.6293,
      "step": 2482
    },
    {
      "epoch": 16.409748038000828,
      "grad_norm": 12.110243797302246,
      "learning_rate": 0.00026584158415841587,
      "loss": 2.8765,
      "step": 2483
    },
    {
      "epoch": 16.41635687732342,
      "grad_norm": 26.66626739501953,
      "learning_rate": 0.00026534653465346534,
      "loss": 1.9047,
      "step": 2484
    },
    {
      "epoch": 16.422965716646015,
      "grad_norm": 4.39765739440918,
      "learning_rate": 0.00026485148514851487,
      "loss": 2.6674,
      "step": 2485
    },
    {
      "epoch": 16.429574555968607,
      "grad_norm": 20.287784576416016,
      "learning_rate": 0.0002643564356435644,
      "loss": 0.8934,
      "step": 2486
    },
    {
      "epoch": 16.436183395291202,
      "grad_norm": 3.108666181564331,
      "learning_rate": 0.00026386138613861387,
      "loss": 0.94,
      "step": 2487
    },
    {
      "epoch": 16.442792234613798,
      "grad_norm": 8.880340576171875,
      "learning_rate": 0.0002633663366336634,
      "loss": 0.8775,
      "step": 2488
    },
    {
      "epoch": 16.44940107393639,
      "grad_norm": 63.01260757446289,
      "learning_rate": 0.00026287128712871287,
      "loss": 3.6445,
      "step": 2489
    },
    {
      "epoch": 16.456009913258985,
      "grad_norm": 26.389448165893555,
      "learning_rate": 0.0002623762376237624,
      "loss": 0.5451,
      "step": 2490
    },
    {
      "epoch": 16.462618752581577,
      "grad_norm": 36.326881408691406,
      "learning_rate": 0.0002618811881188119,
      "loss": 2.01,
      "step": 2491
    },
    {
      "epoch": 16.469227591904172,
      "grad_norm": 22.082958221435547,
      "learning_rate": 0.0002613861386138614,
      "loss": 2.0851,
      "step": 2492
    },
    {
      "epoch": 16.475836431226767,
      "grad_norm": 28.086551666259766,
      "learning_rate": 0.0002608910891089109,
      "loss": 5.3077,
      "step": 2493
    },
    {
      "epoch": 16.48244527054936,
      "grad_norm": 15.398637771606445,
      "learning_rate": 0.0002603960396039604,
      "loss": 2.9798,
      "step": 2494
    },
    {
      "epoch": 16.489054109871955,
      "grad_norm": 11.104146957397461,
      "learning_rate": 0.0002599009900990099,
      "loss": 1.5678,
      "step": 2495
    },
    {
      "epoch": 16.495662949194546,
      "grad_norm": 41.69784164428711,
      "learning_rate": 0.00025940594059405944,
      "loss": 2.93,
      "step": 2496
    },
    {
      "epoch": 16.50227178851714,
      "grad_norm": 21.627378463745117,
      "learning_rate": 0.0002589108910891089,
      "loss": 0.8843,
      "step": 2497
    },
    {
      "epoch": 16.508880627839737,
      "grad_norm": 36.30007553100586,
      "learning_rate": 0.00025841584158415844,
      "loss": 1.7889,
      "step": 2498
    },
    {
      "epoch": 16.51548946716233,
      "grad_norm": 32.687713623046875,
      "learning_rate": 0.0002579207920792079,
      "loss": 1.054,
      "step": 2499
    },
    {
      "epoch": 16.522098306484924,
      "grad_norm": 9.5940580368042,
      "learning_rate": 0.00025742574257425744,
      "loss": 1.3636,
      "step": 2500
    },
    {
      "epoch": 16.528707145807516,
      "grad_norm": 31.543275833129883,
      "learning_rate": 0.00025693069306930696,
      "loss": 1.7061,
      "step": 2501
    },
    {
      "epoch": 16.53531598513011,
      "grad_norm": 21.320669174194336,
      "learning_rate": 0.00025643564356435644,
      "loss": 4.4888,
      "step": 2502
    },
    {
      "epoch": 16.541924824452707,
      "grad_norm": 37.05403137207031,
      "learning_rate": 0.00025594059405940596,
      "loss": 1.7124,
      "step": 2503
    },
    {
      "epoch": 16.5485336637753,
      "grad_norm": 18.274702072143555,
      "learning_rate": 0.00025544554455445543,
      "loss": 1.659,
      "step": 2504
    },
    {
      "epoch": 16.555142503097894,
      "grad_norm": 49.58381652832031,
      "learning_rate": 0.00025495049504950496,
      "loss": 1.8787,
      "step": 2505
    },
    {
      "epoch": 16.561751342420486,
      "grad_norm": 4.8134918212890625,
      "learning_rate": 0.0002544554455445545,
      "loss": 1.5073,
      "step": 2506
    },
    {
      "epoch": 16.56836018174308,
      "grad_norm": 10.647686004638672,
      "learning_rate": 0.00025396039603960396,
      "loss": 1.2859,
      "step": 2507
    },
    {
      "epoch": 16.574969021065677,
      "grad_norm": 6.425694465637207,
      "learning_rate": 0.0002534653465346535,
      "loss": 1.4071,
      "step": 2508
    },
    {
      "epoch": 16.58157786038827,
      "grad_norm": 2.440181016921997,
      "learning_rate": 0.00025297029702970296,
      "loss": 1.0405,
      "step": 2509
    },
    {
      "epoch": 16.588186699710864,
      "grad_norm": 45.4635009765625,
      "learning_rate": 0.0002524752475247525,
      "loss": 2.099,
      "step": 2510
    },
    {
      "epoch": 16.594795539033456,
      "grad_norm": 49.766197204589844,
      "learning_rate": 0.000251980198019802,
      "loss": 1.6613,
      "step": 2511
    },
    {
      "epoch": 16.60140437835605,
      "grad_norm": 7.081394195556641,
      "learning_rate": 0.0002514851485148515,
      "loss": 2.1729,
      "step": 2512
    },
    {
      "epoch": 16.608013217678646,
      "grad_norm": 18.223966598510742,
      "learning_rate": 0.000250990099009901,
      "loss": 1.0322,
      "step": 2513
    },
    {
      "epoch": 16.614622057001238,
      "grad_norm": 19.199359893798828,
      "learning_rate": 0.0002504950495049505,
      "loss": 1.791,
      "step": 2514
    },
    {
      "epoch": 16.621230896323834,
      "grad_norm": 8.770240783691406,
      "learning_rate": 0.00025,
      "loss": 1.3275,
      "step": 2515
    },
    {
      "epoch": 16.627839735646425,
      "grad_norm": 3.3144357204437256,
      "learning_rate": 0.00024950495049504953,
      "loss": 2.0177,
      "step": 2516
    },
    {
      "epoch": 16.63444857496902,
      "grad_norm": 15.727787971496582,
      "learning_rate": 0.000249009900990099,
      "loss": 1.6564,
      "step": 2517
    },
    {
      "epoch": 16.641057414291616,
      "grad_norm": 17.371540069580078,
      "learning_rate": 0.00024851485148514853,
      "loss": 3.1,
      "step": 2518
    },
    {
      "epoch": 16.647666253614208,
      "grad_norm": 40.77007293701172,
      "learning_rate": 0.000248019801980198,
      "loss": 2.4818,
      "step": 2519
    },
    {
      "epoch": 16.654275092936803,
      "grad_norm": 25.545007705688477,
      "learning_rate": 0.00024752475247524753,
      "loss": 1.3853,
      "step": 2520
    },
    {
      "epoch": 16.660883932259395,
      "grad_norm": 4.881875514984131,
      "learning_rate": 0.00024702970297029705,
      "loss": 1.0488,
      "step": 2521
    },
    {
      "epoch": 16.66749277158199,
      "grad_norm": 2.739640951156616,
      "learning_rate": 0.0002465346534653465,
      "loss": 1.4389,
      "step": 2522
    },
    {
      "epoch": 16.674101610904586,
      "grad_norm": 42.526824951171875,
      "learning_rate": 0.00024603960396039605,
      "loss": 0.9273,
      "step": 2523
    },
    {
      "epoch": 16.680710450227178,
      "grad_norm": 22.171960830688477,
      "learning_rate": 0.0002455445544554455,
      "loss": 3.4926,
      "step": 2524
    },
    {
      "epoch": 16.687319289549773,
      "grad_norm": 30.343111038208008,
      "learning_rate": 0.00024504950495049505,
      "loss": 1.1669,
      "step": 2525
    },
    {
      "epoch": 16.69392812887237,
      "grad_norm": 9.298480033874512,
      "learning_rate": 0.0002445544554455446,
      "loss": 1.7263,
      "step": 2526
    },
    {
      "epoch": 16.70053696819496,
      "grad_norm": 5.565040111541748,
      "learning_rate": 0.00024405940594059405,
      "loss": 2.5875,
      "step": 2527
    },
    {
      "epoch": 16.707145807517556,
      "grad_norm": 3.975184917449951,
      "learning_rate": 0.00024356435643564357,
      "loss": 2.221,
      "step": 2528
    },
    {
      "epoch": 16.713754646840147,
      "grad_norm": 48.428070068359375,
      "learning_rate": 0.00024306930693069307,
      "loss": 2.1151,
      "step": 2529
    },
    {
      "epoch": 16.720363486162743,
      "grad_norm": 62.357818603515625,
      "learning_rate": 0.00024257425742574257,
      "loss": 2.742,
      "step": 2530
    },
    {
      "epoch": 16.726972325485338,
      "grad_norm": 34.63710021972656,
      "learning_rate": 0.00024207920792079207,
      "loss": 2.2186,
      "step": 2531
    },
    {
      "epoch": 16.73358116480793,
      "grad_norm": 59.31560516357422,
      "learning_rate": 0.00024158415841584157,
      "loss": 2.0871,
      "step": 2532
    },
    {
      "epoch": 16.740190004130525,
      "grad_norm": 41.557186126708984,
      "learning_rate": 0.0002410891089108911,
      "loss": 1.3785,
      "step": 2533
    },
    {
      "epoch": 16.746798843453117,
      "grad_norm": 39.40257263183594,
      "learning_rate": 0.0002405940594059406,
      "loss": 2.7397,
      "step": 2534
    },
    {
      "epoch": 16.753407682775713,
      "grad_norm": 9.549002647399902,
      "learning_rate": 0.0002400990099009901,
      "loss": 2.0264,
      "step": 2535
    },
    {
      "epoch": 16.760016522098308,
      "grad_norm": 31.58450698852539,
      "learning_rate": 0.0002396039603960396,
      "loss": 1.5996,
      "step": 2536
    },
    {
      "epoch": 16.7666253614209,
      "grad_norm": 42.5382194519043,
      "learning_rate": 0.0002391089108910891,
      "loss": 1.2687,
      "step": 2537
    },
    {
      "epoch": 16.773234200743495,
      "grad_norm": 9.929630279541016,
      "learning_rate": 0.00023861386138613862,
      "loss": 1.5974,
      "step": 2538
    },
    {
      "epoch": 16.779843040066087,
      "grad_norm": 23.991498947143555,
      "learning_rate": 0.00023811881188118812,
      "loss": 0.8636,
      "step": 2539
    },
    {
      "epoch": 16.786451879388682,
      "grad_norm": 21.637956619262695,
      "learning_rate": 0.00023762376237623762,
      "loss": 2.7555,
      "step": 2540
    },
    {
      "epoch": 16.793060718711278,
      "grad_norm": 13.544805526733398,
      "learning_rate": 0.00023712871287128712,
      "loss": 1.0687,
      "step": 2541
    },
    {
      "epoch": 16.79966955803387,
      "grad_norm": 92.4874038696289,
      "learning_rate": 0.00023663366336633662,
      "loss": 3.0063,
      "step": 2542
    },
    {
      "epoch": 16.806278397356465,
      "grad_norm": 30.65534210205078,
      "learning_rate": 0.00023613861386138614,
      "loss": 1.7565,
      "step": 2543
    },
    {
      "epoch": 16.812887236679057,
      "grad_norm": 9.063794136047363,
      "learning_rate": 0.00023564356435643564,
      "loss": 1.4646,
      "step": 2544
    },
    {
      "epoch": 16.819496076001652,
      "grad_norm": 10.140843391418457,
      "learning_rate": 0.00023514851485148514,
      "loss": 1.8591,
      "step": 2545
    },
    {
      "epoch": 16.826104915324247,
      "grad_norm": 12.546367645263672,
      "learning_rate": 0.00023465346534653464,
      "loss": 1.0537,
      "step": 2546
    },
    {
      "epoch": 16.83271375464684,
      "grad_norm": 19.033504486083984,
      "learning_rate": 0.00023415841584158417,
      "loss": 2.3745,
      "step": 2547
    },
    {
      "epoch": 16.839322593969435,
      "grad_norm": 28.407180786132812,
      "learning_rate": 0.0002336633663366337,
      "loss": 6.9586,
      "step": 2548
    },
    {
      "epoch": 16.845931433292026,
      "grad_norm": 41.56696701049805,
      "learning_rate": 0.0002331683168316832,
      "loss": 2.2256,
      "step": 2549
    },
    {
      "epoch": 16.852540272614622,
      "grad_norm": 3.1732006072998047,
      "learning_rate": 0.0002326732673267327,
      "loss": 0.3912,
      "step": 2550
    },
    {
      "epoch": 16.859149111937217,
      "grad_norm": 13.26814079284668,
      "learning_rate": 0.0002321782178217822,
      "loss": 1.7367,
      "step": 2551
    },
    {
      "epoch": 16.86575795125981,
      "grad_norm": 60.00957489013672,
      "learning_rate": 0.0002316831683168317,
      "loss": 2.0047,
      "step": 2552
    },
    {
      "epoch": 16.872366790582404,
      "grad_norm": 17.520902633666992,
      "learning_rate": 0.00023118811881188121,
      "loss": 2.7471,
      "step": 2553
    },
    {
      "epoch": 16.878975629904996,
      "grad_norm": 36.634796142578125,
      "learning_rate": 0.00023069306930693071,
      "loss": 1.5029,
      "step": 2554
    },
    {
      "epoch": 16.88558446922759,
      "grad_norm": 36.592933654785156,
      "learning_rate": 0.0002301980198019802,
      "loss": 1.8802,
      "step": 2555
    },
    {
      "epoch": 16.892193308550187,
      "grad_norm": 7.651791572570801,
      "learning_rate": 0.0002297029702970297,
      "loss": 1.6937,
      "step": 2556
    },
    {
      "epoch": 16.89880214787278,
      "grad_norm": 14.553776741027832,
      "learning_rate": 0.0002292079207920792,
      "loss": 2.0453,
      "step": 2557
    },
    {
      "epoch": 16.905410987195374,
      "grad_norm": 7.656619548797607,
      "learning_rate": 0.00022871287128712874,
      "loss": 1.3632,
      "step": 2558
    },
    {
      "epoch": 16.91201982651797,
      "grad_norm": 50.91068649291992,
      "learning_rate": 0.00022821782178217824,
      "loss": 2.0083,
      "step": 2559
    },
    {
      "epoch": 16.91862866584056,
      "grad_norm": 12.375900268554688,
      "learning_rate": 0.00022772277227722774,
      "loss": 3.3781,
      "step": 2560
    },
    {
      "epoch": 16.925237505163157,
      "grad_norm": 64.07430267333984,
      "learning_rate": 0.00022722772277227723,
      "loss": 3.2899,
      "step": 2561
    },
    {
      "epoch": 16.93184634448575,
      "grad_norm": 2.585822105407715,
      "learning_rate": 0.00022673267326732673,
      "loss": 1.432,
      "step": 2562
    },
    {
      "epoch": 16.938455183808344,
      "grad_norm": 29.468198776245117,
      "learning_rate": 0.00022623762376237626,
      "loss": 1.3639,
      "step": 2563
    },
    {
      "epoch": 16.94506402313094,
      "grad_norm": 6.895501613616943,
      "learning_rate": 0.00022574257425742576,
      "loss": 1.4877,
      "step": 2564
    },
    {
      "epoch": 16.95167286245353,
      "grad_norm": 40.825836181640625,
      "learning_rate": 0.00022524752475247526,
      "loss": 1.6451,
      "step": 2565
    },
    {
      "epoch": 16.958281701776126,
      "grad_norm": 12.813186645507812,
      "learning_rate": 0.00022475247524752476,
      "loss": 1.5109,
      "step": 2566
    },
    {
      "epoch": 16.96489054109872,
      "grad_norm": 3.2352473735809326,
      "learning_rate": 0.00022425742574257426,
      "loss": 1.4368,
      "step": 2567
    },
    {
      "epoch": 16.971499380421314,
      "grad_norm": 6.813577651977539,
      "learning_rate": 0.00022376237623762378,
      "loss": 0.9056,
      "step": 2568
    },
    {
      "epoch": 16.97810821974391,
      "grad_norm": 9.048025131225586,
      "learning_rate": 0.00022326732673267328,
      "loss": 1.3947,
      "step": 2569
    },
    {
      "epoch": 16.9847170590665,
      "grad_norm": 24.844404220581055,
      "learning_rate": 0.00022277227722772278,
      "loss": 1.9828,
      "step": 2570
    },
    {
      "epoch": 16.991325898389096,
      "grad_norm": 4.853248119354248,
      "learning_rate": 0.00022227722772277228,
      "loss": 2.7983,
      "step": 2571
    },
    {
      "epoch": 16.997934737711688,
      "grad_norm": 22.024459838867188,
      "learning_rate": 0.00022178217821782178,
      "loss": 1.9041,
      "step": 2572
    },
    {
      "epoch": 16.997934737711688,
      "eval_validation_error_bar": 0.04735678966157507,
      "eval_validation_loss": 8.033064842224121,
      "eval_validation_pearsonr": 0.6110920932653278,
      "eval_validation_rmse": 2.834266185760498,
      "eval_validation_runtime": 33.4507,
      "eval_validation_samples_per_second": 6.069,
      "eval_validation_spearman": 0.6220599627051854,
      "eval_validation_steps_per_second": 6.069,
      "step": 2572
    },
    {
      "epoch": 16.997934737711688,
      "eval_test_error_bar": 0.04146237611699912,
      "eval_test_loss": 9.334917068481445,
      "eval_test_pearsonr": 0.540784695004204,
      "eval_test_rmse": 3.055309534072876,
      "eval_test_runtime": 41.0986,
      "eval_test_samples_per_second": 7.932,
      "eval_test_spearman": 0.5532044191703105,
      "eval_test_steps_per_second": 7.932,
      "step": 2572
    },
    {
      "epoch": 17.004543577034283,
      "grad_norm": 33.19658279418945,
      "learning_rate": 0.0002212871287128713,
      "loss": 1.1854,
      "step": 2573
    },
    {
      "epoch": 17.01115241635688,
      "grad_norm": 63.620548248291016,
      "learning_rate": 0.0002207920792079208,
      "loss": 2.9051,
      "step": 2574
    },
    {
      "epoch": 17.01776125567947,
      "grad_norm": 46.85334014892578,
      "learning_rate": 0.0002202970297029703,
      "loss": 2.023,
      "step": 2575
    },
    {
      "epoch": 17.024370095002066,
      "grad_norm": 40.02857208251953,
      "learning_rate": 0.0002198019801980198,
      "loss": 1.2742,
      "step": 2576
    },
    {
      "epoch": 17.030978934324658,
      "grad_norm": 11.218994140625,
      "learning_rate": 0.0002193069306930693,
      "loss": 1.3916,
      "step": 2577
    },
    {
      "epoch": 17.037587773647253,
      "grad_norm": 10.629220962524414,
      "learning_rate": 0.00021881188118811883,
      "loss": 1.9823,
      "step": 2578
    },
    {
      "epoch": 17.04419661296985,
      "grad_norm": 14.594325065612793,
      "learning_rate": 0.00021831683168316833,
      "loss": 0.9309,
      "step": 2579
    },
    {
      "epoch": 17.05080545229244,
      "grad_norm": 6.026414394378662,
      "learning_rate": 0.00021782178217821783,
      "loss": 1.6486,
      "step": 2580
    },
    {
      "epoch": 17.057414291615036,
      "grad_norm": 13.437203407287598,
      "learning_rate": 0.00021732673267326732,
      "loss": 0.4904,
      "step": 2581
    },
    {
      "epoch": 17.064023130937628,
      "grad_norm": 17.49003791809082,
      "learning_rate": 0.00021683168316831682,
      "loss": 1.3859,
      "step": 2582
    },
    {
      "epoch": 17.070631970260223,
      "grad_norm": 47.23699188232422,
      "learning_rate": 0.00021633663366336635,
      "loss": 1.2905,
      "step": 2583
    },
    {
      "epoch": 17.07724080958282,
      "grad_norm": 49.96689224243164,
      "learning_rate": 0.00021584158415841585,
      "loss": 2.4168,
      "step": 2584
    },
    {
      "epoch": 17.08384964890541,
      "grad_norm": 1.6794915199279785,
      "learning_rate": 0.00021534653465346535,
      "loss": 2.2744,
      "step": 2585
    },
    {
      "epoch": 17.090458488228006,
      "grad_norm": 8.064468383789062,
      "learning_rate": 0.00021485148514851485,
      "loss": 1.9508,
      "step": 2586
    },
    {
      "epoch": 17.097067327550597,
      "grad_norm": 13.067030906677246,
      "learning_rate": 0.00021435643564356435,
      "loss": 0.6667,
      "step": 2587
    },
    {
      "epoch": 17.103676166873193,
      "grad_norm": 25.95210838317871,
      "learning_rate": 0.00021386138613861387,
      "loss": 2.2444,
      "step": 2588
    },
    {
      "epoch": 17.110285006195788,
      "grad_norm": 48.471309661865234,
      "learning_rate": 0.00021336633663366337,
      "loss": 1.5183,
      "step": 2589
    },
    {
      "epoch": 17.11689384551838,
      "grad_norm": 12.041172981262207,
      "learning_rate": 0.00021287128712871287,
      "loss": 1.4324,
      "step": 2590
    },
    {
      "epoch": 17.123502684840975,
      "grad_norm": 26.95757293701172,
      "learning_rate": 0.00021237623762376237,
      "loss": 3.4405,
      "step": 2591
    },
    {
      "epoch": 17.130111524163567,
      "grad_norm": 15.010412216186523,
      "learning_rate": 0.00021188118811881187,
      "loss": 1.3113,
      "step": 2592
    },
    {
      "epoch": 17.136720363486162,
      "grad_norm": 5.806622505187988,
      "learning_rate": 0.0002113861386138614,
      "loss": 1.1534,
      "step": 2593
    },
    {
      "epoch": 17.143329202808758,
      "grad_norm": 12.25032901763916,
      "learning_rate": 0.0002108910891089109,
      "loss": 4.9726,
      "step": 2594
    },
    {
      "epoch": 17.14993804213135,
      "grad_norm": 33.1480598449707,
      "learning_rate": 0.0002103960396039604,
      "loss": 1.4719,
      "step": 2595
    },
    {
      "epoch": 17.156546881453945,
      "grad_norm": 18.03196144104004,
      "learning_rate": 0.0002099009900990099,
      "loss": 0.8581,
      "step": 2596
    },
    {
      "epoch": 17.163155720776537,
      "grad_norm": 4.604014873504639,
      "learning_rate": 0.0002094059405940594,
      "loss": 4.1005,
      "step": 2597
    },
    {
      "epoch": 17.169764560099132,
      "grad_norm": 37.48226547241211,
      "learning_rate": 0.00020891089108910892,
      "loss": 1.4849,
      "step": 2598
    },
    {
      "epoch": 17.176373399421728,
      "grad_norm": 23.112533569335938,
      "learning_rate": 0.00020841584158415842,
      "loss": 3.9208,
      "step": 2599
    },
    {
      "epoch": 17.18298223874432,
      "grad_norm": 4.331175804138184,
      "learning_rate": 0.00020792079207920792,
      "loss": 1.4984,
      "step": 2600
    },
    {
      "epoch": 17.189591078066915,
      "grad_norm": 16.321861267089844,
      "learning_rate": 0.00020742574257425741,
      "loss": 1.9256,
      "step": 2601
    },
    {
      "epoch": 17.19619991738951,
      "grad_norm": 70.73054504394531,
      "learning_rate": 0.00020693069306930691,
      "loss": 4.3388,
      "step": 2602
    },
    {
      "epoch": 17.202808756712102,
      "grad_norm": 1.4588719606399536,
      "learning_rate": 0.00020643564356435644,
      "loss": 1.5361,
      "step": 2603
    },
    {
      "epoch": 17.209417596034697,
      "grad_norm": 3.0709068775177,
      "learning_rate": 0.00020594059405940594,
      "loss": 1.2732,
      "step": 2604
    },
    {
      "epoch": 17.21602643535729,
      "grad_norm": 18.015661239624023,
      "learning_rate": 0.00020544554455445544,
      "loss": 2.0196,
      "step": 2605
    },
    {
      "epoch": 17.222635274679885,
      "grad_norm": 21.633010864257812,
      "learning_rate": 0.00020495049504950494,
      "loss": 1.0248,
      "step": 2606
    },
    {
      "epoch": 17.22924411400248,
      "grad_norm": 9.221866607666016,
      "learning_rate": 0.00020445544554455444,
      "loss": 0.5898,
      "step": 2607
    },
    {
      "epoch": 17.23585295332507,
      "grad_norm": 30.650129318237305,
      "learning_rate": 0.00020396039603960396,
      "loss": 1.3969,
      "step": 2608
    },
    {
      "epoch": 17.242461792647667,
      "grad_norm": 12.486345291137695,
      "learning_rate": 0.00020346534653465346,
      "loss": 2.7638,
      "step": 2609
    },
    {
      "epoch": 17.24907063197026,
      "grad_norm": 7.631163120269775,
      "learning_rate": 0.000202970297029703,
      "loss": 1.1308,
      "step": 2610
    },
    {
      "epoch": 17.255679471292854,
      "grad_norm": 26.70197105407715,
      "learning_rate": 0.0002024752475247525,
      "loss": 0.9681,
      "step": 2611
    },
    {
      "epoch": 17.26228831061545,
      "grad_norm": 12.645284652709961,
      "learning_rate": 0.00020198019801980199,
      "loss": 2.2945,
      "step": 2612
    },
    {
      "epoch": 17.26889714993804,
      "grad_norm": 28.477949142456055,
      "learning_rate": 0.0002014851485148515,
      "loss": 1.7515,
      "step": 2613
    },
    {
      "epoch": 17.275505989260637,
      "grad_norm": 38.739620208740234,
      "learning_rate": 0.000200990099009901,
      "loss": 1.558,
      "step": 2614
    },
    {
      "epoch": 17.28211482858323,
      "grad_norm": 52.90571594238281,
      "learning_rate": 0.0002004950495049505,
      "loss": 1.4006,
      "step": 2615
    },
    {
      "epoch": 17.288723667905824,
      "grad_norm": 27.463228225708008,
      "learning_rate": 0.0002,
      "loss": 1.447,
      "step": 2616
    },
    {
      "epoch": 17.29533250722842,
      "grad_norm": 16.692935943603516,
      "learning_rate": 0.0001995049504950495,
      "loss": 1.3304,
      "step": 2617
    },
    {
      "epoch": 17.30194134655101,
      "grad_norm": 15.483634948730469,
      "learning_rate": 0.00019900990099009903,
      "loss": 2.5546,
      "step": 2618
    },
    {
      "epoch": 17.308550185873607,
      "grad_norm": 9.297499656677246,
      "learning_rate": 0.00019851485148514853,
      "loss": 1.498,
      "step": 2619
    },
    {
      "epoch": 17.3151590251962,
      "grad_norm": 7.203725814819336,
      "learning_rate": 0.00019801980198019803,
      "loss": 1.1971,
      "step": 2620
    },
    {
      "epoch": 17.321767864518794,
      "grad_norm": 11.373490333557129,
      "learning_rate": 0.00019752475247524753,
      "loss": 1.4155,
      "step": 2621
    },
    {
      "epoch": 17.32837670384139,
      "grad_norm": 9.063080787658691,
      "learning_rate": 0.00019702970297029703,
      "loss": 1.7865,
      "step": 2622
    },
    {
      "epoch": 17.33498554316398,
      "grad_norm": 8.52499771118164,
      "learning_rate": 0.00019653465346534656,
      "loss": 1.558,
      "step": 2623
    },
    {
      "epoch": 17.341594382486576,
      "grad_norm": 24.986143112182617,
      "learning_rate": 0.00019603960396039606,
      "loss": 2.4221,
      "step": 2624
    },
    {
      "epoch": 17.348203221809168,
      "grad_norm": 22.20477294921875,
      "learning_rate": 0.00019554455445544556,
      "loss": 1.306,
      "step": 2625
    },
    {
      "epoch": 17.354812061131764,
      "grad_norm": 11.09997272491455,
      "learning_rate": 0.00019504950495049505,
      "loss": 1.4866,
      "step": 2626
    },
    {
      "epoch": 17.36142090045436,
      "grad_norm": 1.9306119680404663,
      "learning_rate": 0.00019455445544554455,
      "loss": 0.7627,
      "step": 2627
    },
    {
      "epoch": 17.36802973977695,
      "grad_norm": 10.57491397857666,
      "learning_rate": 0.00019405940594059408,
      "loss": 1.5818,
      "step": 2628
    },
    {
      "epoch": 17.374638579099546,
      "grad_norm": 53.39487838745117,
      "learning_rate": 0.00019356435643564358,
      "loss": 2.5353,
      "step": 2629
    },
    {
      "epoch": 17.381247418422138,
      "grad_norm": 24.979339599609375,
      "learning_rate": 0.00019306930693069308,
      "loss": 1.1714,
      "step": 2630
    },
    {
      "epoch": 17.387856257744733,
      "grad_norm": 3.8213469982147217,
      "learning_rate": 0.00019257425742574258,
      "loss": 1.2178,
      "step": 2631
    },
    {
      "epoch": 17.39446509706733,
      "grad_norm": 70.75932312011719,
      "learning_rate": 0.00019207920792079208,
      "loss": 5.1936,
      "step": 2632
    },
    {
      "epoch": 17.40107393638992,
      "grad_norm": 1.7571394443511963,
      "learning_rate": 0.0001915841584158416,
      "loss": 0.8991,
      "step": 2633
    },
    {
      "epoch": 17.407682775712516,
      "grad_norm": 25.215909957885742,
      "learning_rate": 0.0001910891089108911,
      "loss": 2.0159,
      "step": 2634
    },
    {
      "epoch": 17.41429161503511,
      "grad_norm": 29.1237735748291,
      "learning_rate": 0.0001905940594059406,
      "loss": 2.4794,
      "step": 2635
    },
    {
      "epoch": 17.420900454357703,
      "grad_norm": 24.626914978027344,
      "learning_rate": 0.0001900990099009901,
      "loss": 4.8075,
      "step": 2636
    },
    {
      "epoch": 17.4275092936803,
      "grad_norm": 2.337655544281006,
      "learning_rate": 0.0001896039603960396,
      "loss": 0.7159,
      "step": 2637
    },
    {
      "epoch": 17.43411813300289,
      "grad_norm": 9.208724021911621,
      "learning_rate": 0.00018910891089108913,
      "loss": 3.5448,
      "step": 2638
    },
    {
      "epoch": 17.440726972325486,
      "grad_norm": 4.7195329666137695,
      "learning_rate": 0.00018861386138613862,
      "loss": 1.0627,
      "step": 2639
    },
    {
      "epoch": 17.44733581164808,
      "grad_norm": 5.692669868469238,
      "learning_rate": 0.00018811881188118812,
      "loss": 0.7027,
      "step": 2640
    },
    {
      "epoch": 17.453944650970673,
      "grad_norm": 7.66517448425293,
      "learning_rate": 0.00018762376237623762,
      "loss": 1.6292,
      "step": 2641
    },
    {
      "epoch": 17.460553490293268,
      "grad_norm": 54.12783432006836,
      "learning_rate": 0.00018712871287128712,
      "loss": 2.0042,
      "step": 2642
    },
    {
      "epoch": 17.46716232961586,
      "grad_norm": 32.70081329345703,
      "learning_rate": 0.00018663366336633665,
      "loss": 2.2622,
      "step": 2643
    },
    {
      "epoch": 17.473771168938455,
      "grad_norm": 20.43916130065918,
      "learning_rate": 0.00018613861386138615,
      "loss": 1.489,
      "step": 2644
    },
    {
      "epoch": 17.48038000826105,
      "grad_norm": 37.16001510620117,
      "learning_rate": 0.00018564356435643565,
      "loss": 2.1343,
      "step": 2645
    },
    {
      "epoch": 17.486988847583643,
      "grad_norm": 9.53201675415039,
      "learning_rate": 0.00018514851485148514,
      "loss": 1.5421,
      "step": 2646
    },
    {
      "epoch": 17.493597686906238,
      "grad_norm": 44.23741912841797,
      "learning_rate": 0.00018465346534653464,
      "loss": 1.5632,
      "step": 2647
    },
    {
      "epoch": 17.50020652622883,
      "grad_norm": 6.1620259284973145,
      "learning_rate": 0.00018415841584158417,
      "loss": 1.6067,
      "step": 2648
    },
    {
      "epoch": 17.506815365551425,
      "grad_norm": 44.00352096557617,
      "learning_rate": 0.00018366336633663367,
      "loss": 1.836,
      "step": 2649
    },
    {
      "epoch": 17.51342420487402,
      "grad_norm": 39.47083282470703,
      "learning_rate": 0.00018316831683168317,
      "loss": 0.9296,
      "step": 2650
    },
    {
      "epoch": 17.520033044196612,
      "grad_norm": 16.863510131835938,
      "learning_rate": 0.00018267326732673267,
      "loss": 1.3169,
      "step": 2651
    },
    {
      "epoch": 17.526641883519208,
      "grad_norm": 35.805023193359375,
      "learning_rate": 0.00018217821782178217,
      "loss": 2.4521,
      "step": 2652
    },
    {
      "epoch": 17.5332507228418,
      "grad_norm": 81.86076354980469,
      "learning_rate": 0.0001816831683168317,
      "loss": 3.6309,
      "step": 2653
    },
    {
      "epoch": 17.539859562164395,
      "grad_norm": 35.83192825317383,
      "learning_rate": 0.0001811881188118812,
      "loss": 3.5837,
      "step": 2654
    },
    {
      "epoch": 17.54646840148699,
      "grad_norm": 65.95736694335938,
      "learning_rate": 0.0001806930693069307,
      "loss": 1.5885,
      "step": 2655
    },
    {
      "epoch": 17.553077240809582,
      "grad_norm": 18.449481964111328,
      "learning_rate": 0.0001801980198019802,
      "loss": 1.5327,
      "step": 2656
    },
    {
      "epoch": 17.559686080132177,
      "grad_norm": 20.89415168762207,
      "learning_rate": 0.0001797029702970297,
      "loss": 1.1384,
      "step": 2657
    },
    {
      "epoch": 17.56629491945477,
      "grad_norm": 7.105195045471191,
      "learning_rate": 0.00017920792079207922,
      "loss": 2.0769,
      "step": 2658
    },
    {
      "epoch": 17.572903758777365,
      "grad_norm": 10.896079063415527,
      "learning_rate": 0.00017871287128712871,
      "loss": 1.9185,
      "step": 2659
    },
    {
      "epoch": 17.57951259809996,
      "grad_norm": 8.035941123962402,
      "learning_rate": 0.0001782178217821782,
      "loss": 3.9909,
      "step": 2660
    },
    {
      "epoch": 17.586121437422552,
      "grad_norm": 19.4853572845459,
      "learning_rate": 0.0001777227722772277,
      "loss": 1.8147,
      "step": 2661
    },
    {
      "epoch": 17.592730276745147,
      "grad_norm": 43.82174301147461,
      "learning_rate": 0.0001772277227722772,
      "loss": 1.3281,
      "step": 2662
    },
    {
      "epoch": 17.59933911606774,
      "grad_norm": 41.606910705566406,
      "learning_rate": 0.00017673267326732674,
      "loss": 2.0207,
      "step": 2663
    },
    {
      "epoch": 17.605947955390334,
      "grad_norm": 55.40056610107422,
      "learning_rate": 0.00017623762376237624,
      "loss": 3.0847,
      "step": 2664
    },
    {
      "epoch": 17.61255679471293,
      "grad_norm": 6.805355548858643,
      "learning_rate": 0.00017574257425742574,
      "loss": 3.2115,
      "step": 2665
    },
    {
      "epoch": 17.61916563403552,
      "grad_norm": 11.715743064880371,
      "learning_rate": 0.00017524752475247524,
      "loss": 1.8639,
      "step": 2666
    },
    {
      "epoch": 17.625774473358117,
      "grad_norm": 37.0993537902832,
      "learning_rate": 0.00017475247524752473,
      "loss": 3.2947,
      "step": 2667
    },
    {
      "epoch": 17.63238331268071,
      "grad_norm": 5.1254987716674805,
      "learning_rate": 0.00017425742574257426,
      "loss": 2.0532,
      "step": 2668
    },
    {
      "epoch": 17.638992152003304,
      "grad_norm": 24.215490341186523,
      "learning_rate": 0.00017376237623762376,
      "loss": 2.545,
      "step": 2669
    },
    {
      "epoch": 17.6456009913259,
      "grad_norm": 37.90879821777344,
      "learning_rate": 0.00017326732673267326,
      "loss": 1.5012,
      "step": 2670
    },
    {
      "epoch": 17.65220983064849,
      "grad_norm": 11.489707946777344,
      "learning_rate": 0.00017277227722772276,
      "loss": 1.1168,
      "step": 2671
    },
    {
      "epoch": 17.658818669971087,
      "grad_norm": 24.518085479736328,
      "learning_rate": 0.00017227722772277226,
      "loss": 2.5113,
      "step": 2672
    },
    {
      "epoch": 17.66542750929368,
      "grad_norm": 18.291955947875977,
      "learning_rate": 0.0001717821782178218,
      "loss": 0.7391,
      "step": 2673
    },
    {
      "epoch": 17.672036348616274,
      "grad_norm": 8.689311027526855,
      "learning_rate": 0.0001712871287128713,
      "loss": 1.1008,
      "step": 2674
    },
    {
      "epoch": 17.67864518793887,
      "grad_norm": 12.178410530090332,
      "learning_rate": 0.0001707920792079208,
      "loss": 2.5508,
      "step": 2675
    },
    {
      "epoch": 17.68525402726146,
      "grad_norm": 1.7815570831298828,
      "learning_rate": 0.0001702970297029703,
      "loss": 1.1469,
      "step": 2676
    },
    {
      "epoch": 17.691862866584056,
      "grad_norm": 16.08141326904297,
      "learning_rate": 0.0001698019801980198,
      "loss": 1.1168,
      "step": 2677
    },
    {
      "epoch": 17.698471705906652,
      "grad_norm": 51.02910614013672,
      "learning_rate": 0.00016930693069306933,
      "loss": 2.7534,
      "step": 2678
    },
    {
      "epoch": 17.705080545229244,
      "grad_norm": 23.60573387145996,
      "learning_rate": 0.00016881188118811883,
      "loss": 2.1577,
      "step": 2679
    },
    {
      "epoch": 17.71168938455184,
      "grad_norm": 31.04443359375,
      "learning_rate": 0.00016831683168316833,
      "loss": 1.5404,
      "step": 2680
    },
    {
      "epoch": 17.71829822387443,
      "grad_norm": 4.446820259094238,
      "learning_rate": 0.00016782178217821783,
      "loss": 2.6605,
      "step": 2681
    },
    {
      "epoch": 17.724907063197026,
      "grad_norm": 7.388591289520264,
      "learning_rate": 0.00016732673267326733,
      "loss": 2.6613,
      "step": 2682
    },
    {
      "epoch": 17.73151590251962,
      "grad_norm": 40.79913330078125,
      "learning_rate": 0.00016683168316831686,
      "loss": 2.7174,
      "step": 2683
    },
    {
      "epoch": 17.738124741842213,
      "grad_norm": 7.280989646911621,
      "learning_rate": 0.00016633663366336635,
      "loss": 1.9424,
      "step": 2684
    },
    {
      "epoch": 17.74473358116481,
      "grad_norm": 39.276302337646484,
      "learning_rate": 0.00016584158415841585,
      "loss": 0.9433,
      "step": 2685
    },
    {
      "epoch": 17.7513424204874,
      "grad_norm": 27.769289016723633,
      "learning_rate": 0.00016534653465346535,
      "loss": 2.049,
      "step": 2686
    },
    {
      "epoch": 17.757951259809996,
      "grad_norm": 7.735420227050781,
      "learning_rate": 0.00016485148514851485,
      "loss": 0.7004,
      "step": 2687
    },
    {
      "epoch": 17.76456009913259,
      "grad_norm": 51.82908630371094,
      "learning_rate": 0.00016435643564356438,
      "loss": 1.3907,
      "step": 2688
    },
    {
      "epoch": 17.771168938455183,
      "grad_norm": 19.471342086791992,
      "learning_rate": 0.00016386138613861388,
      "loss": 2.9666,
      "step": 2689
    },
    {
      "epoch": 17.77777777777778,
      "grad_norm": 9.847413063049316,
      "learning_rate": 0.00016336633663366338,
      "loss": 0.9506,
      "step": 2690
    },
    {
      "epoch": 17.78438661710037,
      "grad_norm": 2.8165149688720703,
      "learning_rate": 0.00016287128712871287,
      "loss": 1.2431,
      "step": 2691
    },
    {
      "epoch": 17.790995456422966,
      "grad_norm": 6.291609287261963,
      "learning_rate": 0.00016237623762376237,
      "loss": 1.5841,
      "step": 2692
    },
    {
      "epoch": 17.79760429574556,
      "grad_norm": 6.182544231414795,
      "learning_rate": 0.0001618811881188119,
      "loss": 2.6995,
      "step": 2693
    },
    {
      "epoch": 17.804213135068153,
      "grad_norm": 33.22480773925781,
      "learning_rate": 0.0001613861386138614,
      "loss": 1.6409,
      "step": 2694
    },
    {
      "epoch": 17.81082197439075,
      "grad_norm": 22.916494369506836,
      "learning_rate": 0.0001608910891089109,
      "loss": 1.3058,
      "step": 2695
    },
    {
      "epoch": 17.81743081371334,
      "grad_norm": 13.775362968444824,
      "learning_rate": 0.0001603960396039604,
      "loss": 2.3716,
      "step": 2696
    },
    {
      "epoch": 17.824039653035936,
      "grad_norm": 5.618727684020996,
      "learning_rate": 0.0001599009900990099,
      "loss": 0.947,
      "step": 2697
    },
    {
      "epoch": 17.83064849235853,
      "grad_norm": 2.8584671020507812,
      "learning_rate": 0.00015940594059405942,
      "loss": 2.8882,
      "step": 2698
    },
    {
      "epoch": 17.837257331681123,
      "grad_norm": 27.42976951599121,
      "learning_rate": 0.00015891089108910892,
      "loss": 2.6886,
      "step": 2699
    },
    {
      "epoch": 17.843866171003718,
      "grad_norm": 41.589229583740234,
      "learning_rate": 0.00015841584158415842,
      "loss": 1.5075,
      "step": 2700
    },
    {
      "epoch": 17.85047501032631,
      "grad_norm": 27.860910415649414,
      "learning_rate": 0.00015792079207920792,
      "loss": 1.6939,
      "step": 2701
    },
    {
      "epoch": 17.857083849648905,
      "grad_norm": 13.621009826660156,
      "learning_rate": 0.00015742574257425742,
      "loss": 2.33,
      "step": 2702
    },
    {
      "epoch": 17.8636926889715,
      "grad_norm": 7.827550411224365,
      "learning_rate": 0.00015693069306930695,
      "loss": 0.6019,
      "step": 2703
    },
    {
      "epoch": 17.870301528294092,
      "grad_norm": 35.30428695678711,
      "learning_rate": 0.00015643564356435644,
      "loss": 1.3058,
      "step": 2704
    },
    {
      "epoch": 17.876910367616688,
      "grad_norm": 5.772556781768799,
      "learning_rate": 0.00015594059405940594,
      "loss": 1.9048,
      "step": 2705
    },
    {
      "epoch": 17.88351920693928,
      "grad_norm": 3.3911876678466797,
      "learning_rate": 0.00015544554455445544,
      "loss": 1.0835,
      "step": 2706
    },
    {
      "epoch": 17.890128046261875,
      "grad_norm": 9.662593841552734,
      "learning_rate": 0.00015495049504950494,
      "loss": 2.3752,
      "step": 2707
    },
    {
      "epoch": 17.89673688558447,
      "grad_norm": 54.06884765625,
      "learning_rate": 0.00015445544554455447,
      "loss": 1.8338,
      "step": 2708
    },
    {
      "epoch": 17.903345724907062,
      "grad_norm": 18.844879150390625,
      "learning_rate": 0.00015396039603960397,
      "loss": 2.8297,
      "step": 2709
    },
    {
      "epoch": 17.909954564229658,
      "grad_norm": 9.028003692626953,
      "learning_rate": 0.00015346534653465347,
      "loss": 1.5179,
      "step": 2710
    },
    {
      "epoch": 17.916563403552253,
      "grad_norm": 25.15151023864746,
      "learning_rate": 0.00015297029702970297,
      "loss": 1.1399,
      "step": 2711
    },
    {
      "epoch": 17.923172242874845,
      "grad_norm": 40.3030891418457,
      "learning_rate": 0.00015247524752475246,
      "loss": 2.3656,
      "step": 2712
    },
    {
      "epoch": 17.92978108219744,
      "grad_norm": 33.44586944580078,
      "learning_rate": 0.000151980198019802,
      "loss": 1.4332,
      "step": 2713
    },
    {
      "epoch": 17.936389921520032,
      "grad_norm": 39.27871322631836,
      "learning_rate": 0.0001514851485148515,
      "loss": 2.5099,
      "step": 2714
    },
    {
      "epoch": 17.942998760842627,
      "grad_norm": 14.273612022399902,
      "learning_rate": 0.000150990099009901,
      "loss": 1.9183,
      "step": 2715
    },
    {
      "epoch": 17.949607600165223,
      "grad_norm": 18.34770965576172,
      "learning_rate": 0.0001504950495049505,
      "loss": 0.9043,
      "step": 2716
    },
    {
      "epoch": 17.956216439487815,
      "grad_norm": 18.990917205810547,
      "learning_rate": 0.00015,
      "loss": 1.4254,
      "step": 2717
    },
    {
      "epoch": 17.96282527881041,
      "grad_norm": 39.782012939453125,
      "learning_rate": 0.0001495049504950495,
      "loss": 2.2987,
      "step": 2718
    },
    {
      "epoch": 17.969434118133,
      "grad_norm": 4.194615364074707,
      "learning_rate": 0.000149009900990099,
      "loss": 1.0324,
      "step": 2719
    },
    {
      "epoch": 17.976042957455597,
      "grad_norm": 56.43537902832031,
      "learning_rate": 0.0001485148514851485,
      "loss": 1.5962,
      "step": 2720
    },
    {
      "epoch": 17.982651796778192,
      "grad_norm": 4.928615093231201,
      "learning_rate": 0.000148019801980198,
      "loss": 0.9544,
      "step": 2721
    },
    {
      "epoch": 17.989260636100784,
      "grad_norm": 14.484247207641602,
      "learning_rate": 0.0001475247524752475,
      "loss": 1.9957,
      "step": 2722
    },
    {
      "epoch": 17.99586947542338,
      "grad_norm": 28.56731605529785,
      "learning_rate": 0.00014702970297029704,
      "loss": 3.4741,
      "step": 2723
    },
    {
      "epoch": 17.99586947542338,
      "eval_validation_error_bar": 0.04587323839330377,
      "eval_validation_loss": 6.297769546508789,
      "eval_validation_pearsonr": 0.593331835825078,
      "eval_validation_rmse": 2.509535789489746,
      "eval_validation_runtime": 33.0651,
      "eval_validation_samples_per_second": 6.139,
      "eval_validation_spearman": 0.6394297805219259,
      "eval_validation_steps_per_second": 6.139,
      "step": 2723
    },
    {
      "epoch": 17.99586947542338,
      "eval_test_error_bar": 0.039930174412843314,
      "eval_test_loss": 6.662823677062988,
      "eval_test_pearsonr": 0.568645043348539,
      "eval_test_rmse": 2.581244707107544,
      "eval_test_runtime": 41.5137,
      "eval_test_samples_per_second": 7.853,
      "eval_test_spearman": 0.5796351339903917,
      "eval_test_steps_per_second": 7.853,
      "step": 2723
    },
    {
      "epoch": 18.00247831474597,
      "grad_norm": 51.29472351074219,
      "learning_rate": 0.00014653465346534653,
      "loss": 1.5728,
      "step": 2724
    },
    {
      "epoch": 18.009087154068567,
      "grad_norm": 26.357999801635742,
      "learning_rate": 0.00014603960396039603,
      "loss": 1.6225,
      "step": 2725
    },
    {
      "epoch": 18.015695993391162,
      "grad_norm": 34.66718292236328,
      "learning_rate": 0.00014554455445544553,
      "loss": 1.8924,
      "step": 2726
    },
    {
      "epoch": 18.022304832713754,
      "grad_norm": 17.876426696777344,
      "learning_rate": 0.00014504950495049503,
      "loss": 2.3572,
      "step": 2727
    },
    {
      "epoch": 18.02891367203635,
      "grad_norm": 49.845619201660156,
      "learning_rate": 0.00014455445544554456,
      "loss": 1.7948,
      "step": 2728
    },
    {
      "epoch": 18.03552251135894,
      "grad_norm": 44.66771697998047,
      "learning_rate": 0.00014405940594059406,
      "loss": 2.8427,
      "step": 2729
    },
    {
      "epoch": 18.042131350681537,
      "grad_norm": 19.212051391601562,
      "learning_rate": 0.00014356435643564356,
      "loss": 0.8789,
      "step": 2730
    },
    {
      "epoch": 18.048740190004132,
      "grad_norm": 37.886192321777344,
      "learning_rate": 0.00014306930693069306,
      "loss": 3.4412,
      "step": 2731
    },
    {
      "epoch": 18.055349029326724,
      "grad_norm": 58.55557632446289,
      "learning_rate": 0.00014257425742574255,
      "loss": 1.5544,
      "step": 2732
    },
    {
      "epoch": 18.06195786864932,
      "grad_norm": 28.115558624267578,
      "learning_rate": 0.00014207920792079208,
      "loss": 1.2936,
      "step": 2733
    },
    {
      "epoch": 18.06856670797191,
      "grad_norm": 18.37422752380371,
      "learning_rate": 0.00014158415841584158,
      "loss": 1.3011,
      "step": 2734
    },
    {
      "epoch": 18.075175547294506,
      "grad_norm": 28.326622009277344,
      "learning_rate": 0.00014108910891089108,
      "loss": 0.9296,
      "step": 2735
    },
    {
      "epoch": 18.0817843866171,
      "grad_norm": 19.767805099487305,
      "learning_rate": 0.0001405940594059406,
      "loss": 1.555,
      "step": 2736
    },
    {
      "epoch": 18.088393225939694,
      "grad_norm": 26.710840225219727,
      "learning_rate": 0.0001400990099009901,
      "loss": 2.2102,
      "step": 2737
    },
    {
      "epoch": 18.09500206526229,
      "grad_norm": 61.080078125,
      "learning_rate": 0.00013960396039603963,
      "loss": 1.3119,
      "step": 2738
    },
    {
      "epoch": 18.10161090458488,
      "grad_norm": 28.615673065185547,
      "learning_rate": 0.00013910891089108913,
      "loss": 2.2621,
      "step": 2739
    },
    {
      "epoch": 18.108219743907476,
      "grad_norm": 52.032779693603516,
      "learning_rate": 0.00013861386138613863,
      "loss": 1.4708,
      "step": 2740
    },
    {
      "epoch": 18.11482858323007,
      "grad_norm": 59.020320892333984,
      "learning_rate": 0.00013811881188118813,
      "loss": 2.397,
      "step": 2741
    },
    {
      "epoch": 18.121437422552663,
      "grad_norm": 78.33247375488281,
      "learning_rate": 0.00013762376237623763,
      "loss": 2.7541,
      "step": 2742
    },
    {
      "epoch": 18.12804626187526,
      "grad_norm": 14.89405632019043,
      "learning_rate": 0.00013712871287128715,
      "loss": 1.7095,
      "step": 2743
    },
    {
      "epoch": 18.13465510119785,
      "grad_norm": 0.6988662481307983,
      "learning_rate": 0.00013663366336633665,
      "loss": 0.6516,
      "step": 2744
    },
    {
      "epoch": 18.141263940520446,
      "grad_norm": 10.194669723510742,
      "learning_rate": 0.00013613861386138615,
      "loss": 4.1629,
      "step": 2745
    },
    {
      "epoch": 18.14787277984304,
      "grad_norm": 31.371850967407227,
      "learning_rate": 0.00013564356435643565,
      "loss": 2.0169,
      "step": 2746
    },
    {
      "epoch": 18.154481619165633,
      "grad_norm": 43.730045318603516,
      "learning_rate": 0.00013514851485148515,
      "loss": 1.6712,
      "step": 2747
    },
    {
      "epoch": 18.16109045848823,
      "grad_norm": 51.826725006103516,
      "learning_rate": 0.00013465346534653468,
      "loss": 3.1745,
      "step": 2748
    },
    {
      "epoch": 18.16769929781082,
      "grad_norm": 68.78057098388672,
      "learning_rate": 0.00013415841584158417,
      "loss": 3.1753,
      "step": 2749
    },
    {
      "epoch": 18.174308137133416,
      "grad_norm": 52.75550842285156,
      "learning_rate": 0.00013366336633663367,
      "loss": 1.4789,
      "step": 2750
    },
    {
      "epoch": 18.18091697645601,
      "grad_norm": 45.973384857177734,
      "learning_rate": 0.00013316831683168317,
      "loss": 1.6009,
      "step": 2751
    },
    {
      "epoch": 18.187525815778603,
      "grad_norm": 13.79708480834961,
      "learning_rate": 0.00013267326732673267,
      "loss": 2.4227,
      "step": 2752
    },
    {
      "epoch": 18.194134655101198,
      "grad_norm": 49.41610336303711,
      "learning_rate": 0.0001321782178217822,
      "loss": 1.972,
      "step": 2753
    },
    {
      "epoch": 18.200743494423794,
      "grad_norm": 57.8501091003418,
      "learning_rate": 0.0001316831683168317,
      "loss": 1.5533,
      "step": 2754
    },
    {
      "epoch": 18.207352333746385,
      "grad_norm": 5.075431823730469,
      "learning_rate": 0.0001311881188118812,
      "loss": 1.0854,
      "step": 2755
    },
    {
      "epoch": 18.21396117306898,
      "grad_norm": 6.050015449523926,
      "learning_rate": 0.0001306930693069307,
      "loss": 0.4252,
      "step": 2756
    },
    {
      "epoch": 18.220570012391573,
      "grad_norm": 21.181772232055664,
      "learning_rate": 0.0001301980198019802,
      "loss": 2.1303,
      "step": 2757
    },
    {
      "epoch": 18.227178851714168,
      "grad_norm": 10.906037330627441,
      "learning_rate": 0.00012970297029702972,
      "loss": 2.0851,
      "step": 2758
    },
    {
      "epoch": 18.233787691036763,
      "grad_norm": 25.11798667907715,
      "learning_rate": 0.00012920792079207922,
      "loss": 1.4401,
      "step": 2759
    },
    {
      "epoch": 18.240396530359355,
      "grad_norm": 36.7288932800293,
      "learning_rate": 0.00012871287128712872,
      "loss": 1.8624,
      "step": 2760
    },
    {
      "epoch": 18.24700536968195,
      "grad_norm": 7.840493679046631,
      "learning_rate": 0.00012821782178217822,
      "loss": 1.5104,
      "step": 2761
    },
    {
      "epoch": 18.253614209004542,
      "grad_norm": 63.28215026855469,
      "learning_rate": 0.00012772277227722772,
      "loss": 2.5771,
      "step": 2762
    },
    {
      "epoch": 18.260223048327138,
      "grad_norm": 74.92395782470703,
      "learning_rate": 0.00012722772277227724,
      "loss": 3.6375,
      "step": 2763
    },
    {
      "epoch": 18.266831887649733,
      "grad_norm": 26.680253982543945,
      "learning_rate": 0.00012673267326732674,
      "loss": 1.7567,
      "step": 2764
    },
    {
      "epoch": 18.273440726972325,
      "grad_norm": 15.533699035644531,
      "learning_rate": 0.00012623762376237624,
      "loss": 1.3091,
      "step": 2765
    },
    {
      "epoch": 18.28004956629492,
      "grad_norm": 26.111894607543945,
      "learning_rate": 0.00012574257425742574,
      "loss": 1.9096,
      "step": 2766
    },
    {
      "epoch": 18.286658405617512,
      "grad_norm": 19.26061248779297,
      "learning_rate": 0.00012524752475247524,
      "loss": 1.1755,
      "step": 2767
    },
    {
      "epoch": 18.293267244940107,
      "grad_norm": 41.523536682128906,
      "learning_rate": 0.00012475247524752477,
      "loss": 1.5778,
      "step": 2768
    },
    {
      "epoch": 18.299876084262703,
      "grad_norm": 25.13109016418457,
      "learning_rate": 0.00012425742574257426,
      "loss": 5.3838,
      "step": 2769
    },
    {
      "epoch": 18.306484923585295,
      "grad_norm": 42.390750885009766,
      "learning_rate": 0.00012376237623762376,
      "loss": 3.3625,
      "step": 2770
    },
    {
      "epoch": 18.31309376290789,
      "grad_norm": 5.771508693695068,
      "learning_rate": 0.00012326732673267326,
      "loss": 0.7432,
      "step": 2771
    },
    {
      "epoch": 18.319702602230482,
      "grad_norm": 41.26816940307617,
      "learning_rate": 0.00012277227722772276,
      "loss": 2.6897,
      "step": 2772
    },
    {
      "epoch": 18.326311441553077,
      "grad_norm": 43.2320442199707,
      "learning_rate": 0.0001222772277227723,
      "loss": 2.0442,
      "step": 2773
    },
    {
      "epoch": 18.332920280875673,
      "grad_norm": 18.961912155151367,
      "learning_rate": 0.00012178217821782179,
      "loss": 1.1525,
      "step": 2774
    },
    {
      "epoch": 18.339529120198264,
      "grad_norm": 10.456092834472656,
      "learning_rate": 0.00012128712871287129,
      "loss": 2.0448,
      "step": 2775
    },
    {
      "epoch": 18.34613795952086,
      "grad_norm": 14.294137954711914,
      "learning_rate": 0.00012079207920792079,
      "loss": 1.4616,
      "step": 2776
    },
    {
      "epoch": 18.35274679884345,
      "grad_norm": 11.141308784484863,
      "learning_rate": 0.0001202970297029703,
      "loss": 1.2335,
      "step": 2777
    },
    {
      "epoch": 18.359355638166047,
      "grad_norm": 9.013564109802246,
      "learning_rate": 0.0001198019801980198,
      "loss": 1.3348,
      "step": 2778
    },
    {
      "epoch": 18.365964477488642,
      "grad_norm": 7.378396987915039,
      "learning_rate": 0.00011930693069306931,
      "loss": 1.0277,
      "step": 2779
    },
    {
      "epoch": 18.372573316811234,
      "grad_norm": 6.292361736297607,
      "learning_rate": 0.00011881188118811881,
      "loss": 1.2122,
      "step": 2780
    },
    {
      "epoch": 18.37918215613383,
      "grad_norm": 43.784637451171875,
      "learning_rate": 0.00011831683168316831,
      "loss": 1.3207,
      "step": 2781
    },
    {
      "epoch": 18.38579099545642,
      "grad_norm": 32.118900299072266,
      "learning_rate": 0.00011782178217821782,
      "loss": 2.4302,
      "step": 2782
    },
    {
      "epoch": 18.392399834779017,
      "grad_norm": 28.292865753173828,
      "learning_rate": 0.00011732673267326732,
      "loss": 1.722,
      "step": 2783
    },
    {
      "epoch": 18.399008674101612,
      "grad_norm": 6.270802974700928,
      "learning_rate": 0.00011683168316831685,
      "loss": 0.9126,
      "step": 2784
    },
    {
      "epoch": 18.405617513424204,
      "grad_norm": 4.046255588531494,
      "learning_rate": 0.00011633663366336635,
      "loss": 2.3252,
      "step": 2785
    },
    {
      "epoch": 18.4122263527468,
      "grad_norm": 21.423755645751953,
      "learning_rate": 0.00011584158415841584,
      "loss": 2.2399,
      "step": 2786
    },
    {
      "epoch": 18.41883519206939,
      "grad_norm": 10.36901569366455,
      "learning_rate": 0.00011534653465346536,
      "loss": 1.9007,
      "step": 2787
    },
    {
      "epoch": 18.425444031391986,
      "grad_norm": 28.496932983398438,
      "learning_rate": 0.00011485148514851486,
      "loss": 0.7852,
      "step": 2788
    },
    {
      "epoch": 18.432052870714582,
      "grad_norm": 2.5843923091888428,
      "learning_rate": 0.00011435643564356437,
      "loss": 0.7857,
      "step": 2789
    },
    {
      "epoch": 18.438661710037174,
      "grad_norm": 22.804563522338867,
      "learning_rate": 0.00011386138613861387,
      "loss": 1.8608,
      "step": 2790
    },
    {
      "epoch": 18.44527054935977,
      "grad_norm": 28.787940979003906,
      "learning_rate": 0.00011336633663366337,
      "loss": 1.2515,
      "step": 2791
    },
    {
      "epoch": 18.451879388682364,
      "grad_norm": 51.056766510009766,
      "learning_rate": 0.00011287128712871288,
      "loss": 1.9285,
      "step": 2792
    },
    {
      "epoch": 18.458488228004956,
      "grad_norm": 6.33063268661499,
      "learning_rate": 0.00011237623762376238,
      "loss": 2.0615,
      "step": 2793
    },
    {
      "epoch": 18.46509706732755,
      "grad_norm": 4.171057224273682,
      "learning_rate": 0.00011188118811881189,
      "loss": 1.0479,
      "step": 2794
    },
    {
      "epoch": 18.471705906650143,
      "grad_norm": 66.36598205566406,
      "learning_rate": 0.00011138613861386139,
      "loss": 2.9622,
      "step": 2795
    },
    {
      "epoch": 18.47831474597274,
      "grad_norm": 6.939012050628662,
      "learning_rate": 0.00011089108910891089,
      "loss": 1.3931,
      "step": 2796
    },
    {
      "epoch": 18.484923585295334,
      "grad_norm": 28.661056518554688,
      "learning_rate": 0.0001103960396039604,
      "loss": 2.5737,
      "step": 2797
    },
    {
      "epoch": 18.491532424617926,
      "grad_norm": 43.76016616821289,
      "learning_rate": 0.0001099009900990099,
      "loss": 0.7799,
      "step": 2798
    },
    {
      "epoch": 18.49814126394052,
      "grad_norm": 25.83088493347168,
      "learning_rate": 0.00010940594059405941,
      "loss": 0.7927,
      "step": 2799
    },
    {
      "epoch": 18.504750103263113,
      "grad_norm": 11.921109199523926,
      "learning_rate": 0.00010891089108910891,
      "loss": 2.0656,
      "step": 2800
    },
    {
      "epoch": 18.51135894258571,
      "grad_norm": 21.22542953491211,
      "learning_rate": 0.00010841584158415841,
      "loss": 1.6513,
      "step": 2801
    },
    {
      "epoch": 18.517967781908304,
      "grad_norm": 24.249038696289062,
      "learning_rate": 0.00010792079207920792,
      "loss": 1.3862,
      "step": 2802
    },
    {
      "epoch": 18.524576621230896,
      "grad_norm": 12.633246421813965,
      "learning_rate": 0.00010742574257425742,
      "loss": 2.9554,
      "step": 2803
    },
    {
      "epoch": 18.53118546055349,
      "grad_norm": 8.371476173400879,
      "learning_rate": 0.00010693069306930694,
      "loss": 1.8078,
      "step": 2804
    },
    {
      "epoch": 18.537794299876083,
      "grad_norm": 16.913318634033203,
      "learning_rate": 0.00010643564356435644,
      "loss": 1.564,
      "step": 2805
    },
    {
      "epoch": 18.54440313919868,
      "grad_norm": 27.012779235839844,
      "learning_rate": 0.00010594059405940593,
      "loss": 1.8366,
      "step": 2806
    },
    {
      "epoch": 18.551011978521274,
      "grad_norm": 17.93012237548828,
      "learning_rate": 0.00010544554455445545,
      "loss": 3.2869,
      "step": 2807
    },
    {
      "epoch": 18.557620817843866,
      "grad_norm": 17.531150817871094,
      "learning_rate": 0.00010495049504950495,
      "loss": 2.5802,
      "step": 2808
    },
    {
      "epoch": 18.56422965716646,
      "grad_norm": 7.212887763977051,
      "learning_rate": 0.00010445544554455446,
      "loss": 0.7382,
      "step": 2809
    },
    {
      "epoch": 18.570838496489053,
      "grad_norm": 3.589456796646118,
      "learning_rate": 0.00010396039603960396,
      "loss": 0.5854,
      "step": 2810
    },
    {
      "epoch": 18.577447335811648,
      "grad_norm": 12.385238647460938,
      "learning_rate": 0.00010346534653465346,
      "loss": 0.6609,
      "step": 2811
    },
    {
      "epoch": 18.584056175134243,
      "grad_norm": 20.636926651000977,
      "learning_rate": 0.00010297029702970297,
      "loss": 1.128,
      "step": 2812
    },
    {
      "epoch": 18.590665014456835,
      "grad_norm": 5.887376308441162,
      "learning_rate": 0.00010247524752475247,
      "loss": 1.4053,
      "step": 2813
    },
    {
      "epoch": 18.59727385377943,
      "grad_norm": 74.48736572265625,
      "learning_rate": 0.00010198019801980198,
      "loss": 4.0575,
      "step": 2814
    },
    {
      "epoch": 18.603882693102022,
      "grad_norm": 31.287233352661133,
      "learning_rate": 0.0001014851485148515,
      "loss": 2.0981,
      "step": 2815
    },
    {
      "epoch": 18.610491532424618,
      "grad_norm": 54.19300079345703,
      "learning_rate": 0.00010099009900990099,
      "loss": 2.7237,
      "step": 2816
    },
    {
      "epoch": 18.617100371747213,
      "grad_norm": 1.8900377750396729,
      "learning_rate": 0.0001004950495049505,
      "loss": 1.8438,
      "step": 2817
    },
    {
      "epoch": 18.623709211069805,
      "grad_norm": 53.10007858276367,
      "learning_rate": 0.0001,
      "loss": 1.4862,
      "step": 2818
    },
    {
      "epoch": 18.6303180503924,
      "grad_norm": 37.224700927734375,
      "learning_rate": 9.950495049504952e-05,
      "loss": 1.5226,
      "step": 2819
    },
    {
      "epoch": 18.636926889714992,
      "grad_norm": 76.30201721191406,
      "learning_rate": 9.900990099009902e-05,
      "loss": 4.6476,
      "step": 2820
    },
    {
      "epoch": 18.643535729037588,
      "grad_norm": 70.3912582397461,
      "learning_rate": 9.851485148514852e-05,
      "loss": 1.8921,
      "step": 2821
    },
    {
      "epoch": 18.650144568360183,
      "grad_norm": 67.03377532958984,
      "learning_rate": 9.801980198019803e-05,
      "loss": 1.7613,
      "step": 2822
    },
    {
      "epoch": 18.656753407682775,
      "grad_norm": 43.018314361572266,
      "learning_rate": 9.752475247524753e-05,
      "loss": 1.1835,
      "step": 2823
    },
    {
      "epoch": 18.66336224700537,
      "grad_norm": 16.117237091064453,
      "learning_rate": 9.702970297029704e-05,
      "loss": 1.0542,
      "step": 2824
    },
    {
      "epoch": 18.669971086327962,
      "grad_norm": 48.33161163330078,
      "learning_rate": 9.653465346534654e-05,
      "loss": 1.5873,
      "step": 2825
    },
    {
      "epoch": 18.676579925650557,
      "grad_norm": 8.881660461425781,
      "learning_rate": 9.603960396039604e-05,
      "loss": 2.4777,
      "step": 2826
    },
    {
      "epoch": 18.683188764973153,
      "grad_norm": 3.082862615585327,
      "learning_rate": 9.554455445544555e-05,
      "loss": 0.9721,
      "step": 2827
    },
    {
      "epoch": 18.689797604295745,
      "grad_norm": 9.76497745513916,
      "learning_rate": 9.504950495049505e-05,
      "loss": 1.2953,
      "step": 2828
    },
    {
      "epoch": 18.69640644361834,
      "grad_norm": 19.96575164794922,
      "learning_rate": 9.455445544554456e-05,
      "loss": 1.5837,
      "step": 2829
    },
    {
      "epoch": 18.70301528294093,
      "grad_norm": 29.48478126525879,
      "learning_rate": 9.405940594059406e-05,
      "loss": 0.6849,
      "step": 2830
    },
    {
      "epoch": 18.709624122263527,
      "grad_norm": 26.58634376525879,
      "learning_rate": 9.356435643564356e-05,
      "loss": 2.3306,
      "step": 2831
    },
    {
      "epoch": 18.716232961586122,
      "grad_norm": 12.513553619384766,
      "learning_rate": 9.306930693069307e-05,
      "loss": 1.2868,
      "step": 2832
    },
    {
      "epoch": 18.722841800908714,
      "grad_norm": 34.17407989501953,
      "learning_rate": 9.257425742574257e-05,
      "loss": 2.4292,
      "step": 2833
    },
    {
      "epoch": 18.72945064023131,
      "grad_norm": 40.03488540649414,
      "learning_rate": 9.207920792079209e-05,
      "loss": 1.5873,
      "step": 2834
    },
    {
      "epoch": 18.736059479553905,
      "grad_norm": 10.851339340209961,
      "learning_rate": 9.158415841584158e-05,
      "loss": 1.1989,
      "step": 2835
    },
    {
      "epoch": 18.742668318876497,
      "grad_norm": 22.81993865966797,
      "learning_rate": 9.108910891089108e-05,
      "loss": 1.3763,
      "step": 2836
    },
    {
      "epoch": 18.749277158199092,
      "grad_norm": 4.255749702453613,
      "learning_rate": 9.05940594059406e-05,
      "loss": 1.9018,
      "step": 2837
    },
    {
      "epoch": 18.755885997521684,
      "grad_norm": 16.91993522644043,
      "learning_rate": 9.00990099009901e-05,
      "loss": 2.2212,
      "step": 2838
    },
    {
      "epoch": 18.76249483684428,
      "grad_norm": 7.435080051422119,
      "learning_rate": 8.960396039603961e-05,
      "loss": 1.0679,
      "step": 2839
    },
    {
      "epoch": 18.769103676166875,
      "grad_norm": 3.333303213119507,
      "learning_rate": 8.91089108910891e-05,
      "loss": 3.2551,
      "step": 2840
    },
    {
      "epoch": 18.775712515489467,
      "grad_norm": 25.454450607299805,
      "learning_rate": 8.86138613861386e-05,
      "loss": 1.4838,
      "step": 2841
    },
    {
      "epoch": 18.782321354812062,
      "grad_norm": 3.5185508728027344,
      "learning_rate": 8.811881188118812e-05,
      "loss": 1.7355,
      "step": 2842
    },
    {
      "epoch": 18.788930194134654,
      "grad_norm": 48.42668151855469,
      "learning_rate": 8.762376237623762e-05,
      "loss": 2.2661,
      "step": 2843
    },
    {
      "epoch": 18.79553903345725,
      "grad_norm": 53.161415100097656,
      "learning_rate": 8.712871287128713e-05,
      "loss": 3.1276,
      "step": 2844
    },
    {
      "epoch": 18.802147872779845,
      "grad_norm": 25.02984619140625,
      "learning_rate": 8.663366336633663e-05,
      "loss": 1.8122,
      "step": 2845
    },
    {
      "epoch": 18.808756712102436,
      "grad_norm": 2.847008228302002,
      "learning_rate": 8.613861386138613e-05,
      "loss": 0.4841,
      "step": 2846
    },
    {
      "epoch": 18.81536555142503,
      "grad_norm": 34.13019943237305,
      "learning_rate": 8.564356435643565e-05,
      "loss": 4.9923,
      "step": 2847
    },
    {
      "epoch": 18.821974390747624,
      "grad_norm": 14.318886756896973,
      "learning_rate": 8.514851485148515e-05,
      "loss": 1.8161,
      "step": 2848
    },
    {
      "epoch": 18.82858323007022,
      "grad_norm": 6.367138385772705,
      "learning_rate": 8.465346534653467e-05,
      "loss": 2.9466,
      "step": 2849
    },
    {
      "epoch": 18.835192069392814,
      "grad_norm": 19.40644073486328,
      "learning_rate": 8.415841584158417e-05,
      "loss": 2.5643,
      "step": 2850
    },
    {
      "epoch": 18.841800908715406,
      "grad_norm": 17.57864761352539,
      "learning_rate": 8.366336633663366e-05,
      "loss": 0.7035,
      "step": 2851
    },
    {
      "epoch": 18.848409748038,
      "grad_norm": 18.551532745361328,
      "learning_rate": 8.316831683168318e-05,
      "loss": 0.7553,
      "step": 2852
    },
    {
      "epoch": 18.855018587360593,
      "grad_norm": 33.15359115600586,
      "learning_rate": 8.267326732673268e-05,
      "loss": 1.4476,
      "step": 2853
    },
    {
      "epoch": 18.86162742668319,
      "grad_norm": 11.060651779174805,
      "learning_rate": 8.217821782178219e-05,
      "loss": 0.6643,
      "step": 2854
    },
    {
      "epoch": 18.868236266005784,
      "grad_norm": 28.9317569732666,
      "learning_rate": 8.168316831683169e-05,
      "loss": 5.4854,
      "step": 2855
    },
    {
      "epoch": 18.874845105328376,
      "grad_norm": 28.413124084472656,
      "learning_rate": 8.118811881188119e-05,
      "loss": 1.0618,
      "step": 2856
    },
    {
      "epoch": 18.88145394465097,
      "grad_norm": 9.810441017150879,
      "learning_rate": 8.06930693069307e-05,
      "loss": 0.9752,
      "step": 2857
    },
    {
      "epoch": 18.888062783973563,
      "grad_norm": 32.170265197753906,
      "learning_rate": 8.01980198019802e-05,
      "loss": 1.292,
      "step": 2858
    },
    {
      "epoch": 18.89467162329616,
      "grad_norm": 7.076160430908203,
      "learning_rate": 7.970297029702971e-05,
      "loss": 0.9338,
      "step": 2859
    },
    {
      "epoch": 18.901280462618754,
      "grad_norm": 7.755486488342285,
      "learning_rate": 7.920792079207921e-05,
      "loss": 1.5041,
      "step": 2860
    },
    {
      "epoch": 18.907889301941346,
      "grad_norm": 16.110307693481445,
      "learning_rate": 7.871287128712871e-05,
      "loss": 2.2954,
      "step": 2861
    },
    {
      "epoch": 18.91449814126394,
      "grad_norm": 13.74198055267334,
      "learning_rate": 7.821782178217822e-05,
      "loss": 2.9065,
      "step": 2862
    },
    {
      "epoch": 18.921106980586533,
      "grad_norm": 5.335498332977295,
      "learning_rate": 7.772277227722772e-05,
      "loss": 0.4532,
      "step": 2863
    },
    {
      "epoch": 18.927715819909128,
      "grad_norm": 14.305930137634277,
      "learning_rate": 7.722772277227723e-05,
      "loss": 2.4759,
      "step": 2864
    },
    {
      "epoch": 18.934324659231724,
      "grad_norm": 7.837625503540039,
      "learning_rate": 7.673267326732673e-05,
      "loss": 1.6654,
      "step": 2865
    },
    {
      "epoch": 18.940933498554315,
      "grad_norm": 15.711370468139648,
      "learning_rate": 7.623762376237623e-05,
      "loss": 1.0087,
      "step": 2866
    },
    {
      "epoch": 18.94754233787691,
      "grad_norm": 45.65386199951172,
      "learning_rate": 7.574257425742574e-05,
      "loss": 1.8607,
      "step": 2867
    },
    {
      "epoch": 18.954151177199506,
      "grad_norm": 23.605735778808594,
      "learning_rate": 7.524752475247524e-05,
      "loss": 1.0381,
      "step": 2868
    },
    {
      "epoch": 18.960760016522098,
      "grad_norm": 29.62322998046875,
      "learning_rate": 7.475247524752476e-05,
      "loss": 1.4806,
      "step": 2869
    },
    {
      "epoch": 18.967368855844693,
      "grad_norm": 49.652626037597656,
      "learning_rate": 7.425742574257426e-05,
      "loss": 1.64,
      "step": 2870
    },
    {
      "epoch": 18.973977695167285,
      "grad_norm": 5.588064670562744,
      "learning_rate": 7.376237623762375e-05,
      "loss": 0.9803,
      "step": 2871
    },
    {
      "epoch": 18.98058653448988,
      "grad_norm": 16.926584243774414,
      "learning_rate": 7.326732673267327e-05,
      "loss": 1.0543,
      "step": 2872
    },
    {
      "epoch": 18.987195373812476,
      "grad_norm": 7.173912048339844,
      "learning_rate": 7.277227722772277e-05,
      "loss": 1.1801,
      "step": 2873
    },
    {
      "epoch": 18.993804213135068,
      "grad_norm": 36.01318359375,
      "learning_rate": 7.227722772277228e-05,
      "loss": 2.7408,
      "step": 2874
    },
    {
      "epoch": 18.993804213135068,
      "eval_validation_error_bar": 0.0452673890796655,
      "eval_validation_loss": 7.312039375305176,
      "eval_validation_pearsonr": 0.5878995800619797,
      "eval_validation_rmse": 2.704078197479248,
      "eval_validation_runtime": 33.436,
      "eval_validation_samples_per_second": 6.071,
      "eval_validation_spearman": 0.6463355329247661,
      "eval_validation_steps_per_second": 6.071,
      "step": 2874
    },
    {
      "epoch": 18.993804213135068,
      "eval_test_error_bar": 0.03954129582819714,
      "eval_test_loss": 7.377989768981934,
      "eval_test_pearsonr": 0.5731465509030464,
      "eval_test_rmse": 2.716245412826538,
      "eval_test_runtime": 41.5584,
      "eval_test_samples_per_second": 7.844,
      "eval_test_spearman": 0.5860888094509913,
      "eval_test_steps_per_second": 7.844,
      "step": 2874
    },
    {
      "epoch": 19.000413052457663,
      "grad_norm": 32.82505798339844,
      "learning_rate": 7.178217821782178e-05,
      "loss": 3.4665,
      "step": 2875
    },
    {
      "epoch": 19.007021891780255,
      "grad_norm": 21.513097763061523,
      "learning_rate": 7.128712871287128e-05,
      "loss": 1.4679,
      "step": 2876
    },
    {
      "epoch": 19.01363073110285,
      "grad_norm": 57.41389083862305,
      "learning_rate": 7.079207920792079e-05,
      "loss": 2.4174,
      "step": 2877
    },
    {
      "epoch": 19.020239570425446,
      "grad_norm": 12.549311637878418,
      "learning_rate": 7.02970297029703e-05,
      "loss": 3.0323,
      "step": 2878
    },
    {
      "epoch": 19.026848409748037,
      "grad_norm": 13.143345832824707,
      "learning_rate": 6.980198019801982e-05,
      "loss": 1.2027,
      "step": 2879
    },
    {
      "epoch": 19.033457249070633,
      "grad_norm": 14.579039573669434,
      "learning_rate": 6.930693069306931e-05,
      "loss": 1.7517,
      "step": 2880
    },
    {
      "epoch": 19.040066088393225,
      "grad_norm": 30.160764694213867,
      "learning_rate": 6.881188118811881e-05,
      "loss": 2.1001,
      "step": 2881
    },
    {
      "epoch": 19.04667492771582,
      "grad_norm": 60.23891830444336,
      "learning_rate": 6.831683168316833e-05,
      "loss": 1.8027,
      "step": 2882
    },
    {
      "epoch": 19.053283767038415,
      "grad_norm": 15.624303817749023,
      "learning_rate": 6.782178217821783e-05,
      "loss": 1.6507,
      "step": 2883
    },
    {
      "epoch": 19.059892606361007,
      "grad_norm": 37.13037872314453,
      "learning_rate": 6.732673267326734e-05,
      "loss": 2.1007,
      "step": 2884
    },
    {
      "epoch": 19.066501445683603,
      "grad_norm": 72.96908569335938,
      "learning_rate": 6.683168316831684e-05,
      "loss": 2.5361,
      "step": 2885
    },
    {
      "epoch": 19.073110285006194,
      "grad_norm": 39.23118591308594,
      "learning_rate": 6.633663366336634e-05,
      "loss": 0.9665,
      "step": 2886
    },
    {
      "epoch": 19.07971912432879,
      "grad_norm": 68.33877563476562,
      "learning_rate": 6.584158415841585e-05,
      "loss": 3.3231,
      "step": 2887
    },
    {
      "epoch": 19.086327963651385,
      "grad_norm": 2.9927666187286377,
      "learning_rate": 6.534653465346535e-05,
      "loss": 2.0986,
      "step": 2888
    },
    {
      "epoch": 19.092936802973977,
      "grad_norm": 11.576550483703613,
      "learning_rate": 6.485148514851486e-05,
      "loss": 0.54,
      "step": 2889
    },
    {
      "epoch": 19.099545642296572,
      "grad_norm": 3.1700587272644043,
      "learning_rate": 6.435643564356436e-05,
      "loss": 0.7213,
      "step": 2890
    },
    {
      "epoch": 19.106154481619164,
      "grad_norm": 2.3766181468963623,
      "learning_rate": 6.386138613861386e-05,
      "loss": 1.2468,
      "step": 2891
    },
    {
      "epoch": 19.11276332094176,
      "grad_norm": 9.381993293762207,
      "learning_rate": 6.336633663366337e-05,
      "loss": 1.9657,
      "step": 2892
    },
    {
      "epoch": 19.119372160264355,
      "grad_norm": 29.22027015686035,
      "learning_rate": 6.287128712871287e-05,
      "loss": 4.5722,
      "step": 2893
    },
    {
      "epoch": 19.125980999586947,
      "grad_norm": 17.28097152709961,
      "learning_rate": 6.237623762376238e-05,
      "loss": 2.4719,
      "step": 2894
    },
    {
      "epoch": 19.132589838909542,
      "grad_norm": 6.274415969848633,
      "learning_rate": 6.188118811881188e-05,
      "loss": 0.6423,
      "step": 2895
    },
    {
      "epoch": 19.139198678232134,
      "grad_norm": 5.534066677093506,
      "learning_rate": 6.138613861386138e-05,
      "loss": 1.1001,
      "step": 2896
    },
    {
      "epoch": 19.14580751755473,
      "grad_norm": 2.876871109008789,
      "learning_rate": 6.0891089108910894e-05,
      "loss": 1.5525,
      "step": 2897
    },
    {
      "epoch": 19.152416356877325,
      "grad_norm": 35.8853645324707,
      "learning_rate": 6.039603960396039e-05,
      "loss": 3.1104,
      "step": 2898
    },
    {
      "epoch": 19.159025196199917,
      "grad_norm": 7.636048316955566,
      "learning_rate": 5.99009900990099e-05,
      "loss": 0.9532,
      "step": 2899
    },
    {
      "epoch": 19.165634035522512,
      "grad_norm": 50.75631332397461,
      "learning_rate": 5.9405940594059404e-05,
      "loss": 3.1138,
      "step": 2900
    },
    {
      "epoch": 19.172242874845104,
      "grad_norm": 9.569761276245117,
      "learning_rate": 5.891089108910891e-05,
      "loss": 1.496,
      "step": 2901
    },
    {
      "epoch": 19.1788517141677,
      "grad_norm": 32.274375915527344,
      "learning_rate": 5.841584158415842e-05,
      "loss": 2.1748,
      "step": 2902
    },
    {
      "epoch": 19.185460553490294,
      "grad_norm": 36.279396057128906,
      "learning_rate": 5.792079207920792e-05,
      "loss": 1.098,
      "step": 2903
    },
    {
      "epoch": 19.192069392812886,
      "grad_norm": 19.969999313354492,
      "learning_rate": 5.742574257425743e-05,
      "loss": 1.1378,
      "step": 2904
    },
    {
      "epoch": 19.19867823213548,
      "grad_norm": 17.968830108642578,
      "learning_rate": 5.6930693069306934e-05,
      "loss": 1.9158,
      "step": 2905
    },
    {
      "epoch": 19.205287071458073,
      "grad_norm": 4.072488307952881,
      "learning_rate": 5.643564356435644e-05,
      "loss": 2.1496,
      "step": 2906
    },
    {
      "epoch": 19.21189591078067,
      "grad_norm": 29.848068237304688,
      "learning_rate": 5.5940594059405946e-05,
      "loss": 0.7632,
      "step": 2907
    },
    {
      "epoch": 19.218504750103264,
      "grad_norm": 45.07569885253906,
      "learning_rate": 5.5445544554455445e-05,
      "loss": 1.8142,
      "step": 2908
    },
    {
      "epoch": 19.225113589425856,
      "grad_norm": 21.129255294799805,
      "learning_rate": 5.495049504950495e-05,
      "loss": 1.6967,
      "step": 2909
    },
    {
      "epoch": 19.23172242874845,
      "grad_norm": 31.8297119140625,
      "learning_rate": 5.4455445544554456e-05,
      "loss": 0.9499,
      "step": 2910
    },
    {
      "epoch": 19.238331268071047,
      "grad_norm": 37.08480453491211,
      "learning_rate": 5.396039603960396e-05,
      "loss": 3.4134,
      "step": 2911
    },
    {
      "epoch": 19.24494010739364,
      "grad_norm": 2.670654296875,
      "learning_rate": 5.346534653465347e-05,
      "loss": 0.5213,
      "step": 2912
    },
    {
      "epoch": 19.251548946716234,
      "grad_norm": 45.150657653808594,
      "learning_rate": 5.297029702970297e-05,
      "loss": 1.4525,
      "step": 2913
    },
    {
      "epoch": 19.258157786038826,
      "grad_norm": 21.5753116607666,
      "learning_rate": 5.247524752475247e-05,
      "loss": 2.8486,
      "step": 2914
    },
    {
      "epoch": 19.26476662536142,
      "grad_norm": 6.939838886260986,
      "learning_rate": 5.198019801980198e-05,
      "loss": 0.4647,
      "step": 2915
    },
    {
      "epoch": 19.271375464684017,
      "grad_norm": 2.5727922916412354,
      "learning_rate": 5.1485148514851485e-05,
      "loss": 0.8795,
      "step": 2916
    },
    {
      "epoch": 19.27798430400661,
      "grad_norm": 4.826194763183594,
      "learning_rate": 5.099009900990099e-05,
      "loss": 1.1387,
      "step": 2917
    },
    {
      "epoch": 19.284593143329204,
      "grad_norm": 14.038670539855957,
      "learning_rate": 5.0495049504950497e-05,
      "loss": 3.5053,
      "step": 2918
    },
    {
      "epoch": 19.291201982651796,
      "grad_norm": 3.9075522422790527,
      "learning_rate": 5e-05,
      "loss": 2.0977,
      "step": 2919
    },
    {
      "epoch": 19.29781082197439,
      "grad_norm": 38.767417907714844,
      "learning_rate": 4.950495049504951e-05,
      "loss": 3.9229,
      "step": 2920
    },
    {
      "epoch": 19.304419661296986,
      "grad_norm": 24.25401496887207,
      "learning_rate": 4.9009900990099014e-05,
      "loss": 1.2808,
      "step": 2921
    },
    {
      "epoch": 19.311028500619578,
      "grad_norm": 15.986392974853516,
      "learning_rate": 4.851485148514852e-05,
      "loss": 0.8136,
      "step": 2922
    },
    {
      "epoch": 19.317637339942173,
      "grad_norm": 15.341524124145508,
      "learning_rate": 4.801980198019802e-05,
      "loss": 1.6194,
      "step": 2923
    },
    {
      "epoch": 19.324246179264765,
      "grad_norm": 14.23447322845459,
      "learning_rate": 4.7524752475247525e-05,
      "loss": 2.2948,
      "step": 2924
    },
    {
      "epoch": 19.33085501858736,
      "grad_norm": 4.976494312286377,
      "learning_rate": 4.702970297029703e-05,
      "loss": 1.2111,
      "step": 2925
    },
    {
      "epoch": 19.337463857909956,
      "grad_norm": 16.347177505493164,
      "learning_rate": 4.653465346534654e-05,
      "loss": 2.1714,
      "step": 2926
    },
    {
      "epoch": 19.344072697232548,
      "grad_norm": 10.85441780090332,
      "learning_rate": 4.603960396039604e-05,
      "loss": 1.6091,
      "step": 2927
    },
    {
      "epoch": 19.350681536555143,
      "grad_norm": 20.034624099731445,
      "learning_rate": 4.554455445544554e-05,
      "loss": 2.1975,
      "step": 2928
    },
    {
      "epoch": 19.357290375877735,
      "grad_norm": 3.5751070976257324,
      "learning_rate": 4.504950495049505e-05,
      "loss": 1.0811,
      "step": 2929
    },
    {
      "epoch": 19.36389921520033,
      "grad_norm": 34.85192108154297,
      "learning_rate": 4.455445544554455e-05,
      "loss": 2.5289,
      "step": 2930
    },
    {
      "epoch": 19.370508054522926,
      "grad_norm": 7.999783515930176,
      "learning_rate": 4.405940594059406e-05,
      "loss": 1.7625,
      "step": 2931
    },
    {
      "epoch": 19.377116893845518,
      "grad_norm": 7.741827487945557,
      "learning_rate": 4.3564356435643565e-05,
      "loss": 0.7702,
      "step": 2932
    },
    {
      "epoch": 19.383725733168113,
      "grad_norm": 4.30844783782959,
      "learning_rate": 4.3069306930693064e-05,
      "loss": 0.8686,
      "step": 2933
    },
    {
      "epoch": 19.390334572490705,
      "grad_norm": 2.5624680519104004,
      "learning_rate": 4.257425742574258e-05,
      "loss": 0.6136,
      "step": 2934
    },
    {
      "epoch": 19.3969434118133,
      "grad_norm": 19.337078094482422,
      "learning_rate": 4.207920792079208e-05,
      "loss": 0.8241,
      "step": 2935
    },
    {
      "epoch": 19.403552251135896,
      "grad_norm": 9.70505428314209,
      "learning_rate": 4.158415841584159e-05,
      "loss": 1.5586,
      "step": 2936
    },
    {
      "epoch": 19.410161090458487,
      "grad_norm": 19.405094146728516,
      "learning_rate": 4.1089108910891094e-05,
      "loss": 0.4915,
      "step": 2937
    },
    {
      "epoch": 19.416769929781083,
      "grad_norm": 19.675029754638672,
      "learning_rate": 4.0594059405940594e-05,
      "loss": 2.9548,
      "step": 2938
    },
    {
      "epoch": 19.423378769103675,
      "grad_norm": 11.448352813720703,
      "learning_rate": 4.00990099009901e-05,
      "loss": 0.9663,
      "step": 2939
    },
    {
      "epoch": 19.42998760842627,
      "grad_norm": 22.429208755493164,
      "learning_rate": 3.9603960396039605e-05,
      "loss": 1.3055,
      "step": 2940
    },
    {
      "epoch": 19.436596447748865,
      "grad_norm": 41.31772232055664,
      "learning_rate": 3.910891089108911e-05,
      "loss": 1.6759,
      "step": 2941
    },
    {
      "epoch": 19.443205287071457,
      "grad_norm": 19.348814010620117,
      "learning_rate": 3.861386138613862e-05,
      "loss": 1.0149,
      "step": 2942
    },
    {
      "epoch": 19.449814126394052,
      "grad_norm": 14.150949478149414,
      "learning_rate": 3.8118811881188116e-05,
      "loss": 1.0563,
      "step": 2943
    },
    {
      "epoch": 19.456422965716644,
      "grad_norm": 25.117860794067383,
      "learning_rate": 3.762376237623762e-05,
      "loss": 0.9413,
      "step": 2944
    },
    {
      "epoch": 19.46303180503924,
      "grad_norm": 33.02954864501953,
      "learning_rate": 3.712871287128713e-05,
      "loss": 3.4736,
      "step": 2945
    },
    {
      "epoch": 19.469640644361835,
      "grad_norm": 14.63016128540039,
      "learning_rate": 3.6633663366336634e-05,
      "loss": 2.2733,
      "step": 2946
    },
    {
      "epoch": 19.476249483684427,
      "grad_norm": 8.176822662353516,
      "learning_rate": 3.613861386138614e-05,
      "loss": 1.6291,
      "step": 2947
    },
    {
      "epoch": 19.482858323007022,
      "grad_norm": 7.700885772705078,
      "learning_rate": 3.564356435643564e-05,
      "loss": 0.8564,
      "step": 2948
    },
    {
      "epoch": 19.489467162329618,
      "grad_norm": 37.46479415893555,
      "learning_rate": 3.514851485148515e-05,
      "loss": 6.8479,
      "step": 2949
    },
    {
      "epoch": 19.49607600165221,
      "grad_norm": 33.276710510253906,
      "learning_rate": 3.465346534653466e-05,
      "loss": 1.7114,
      "step": 2950
    },
    {
      "epoch": 19.502684840974805,
      "grad_norm": 4.739072322845459,
      "learning_rate": 3.415841584158416e-05,
      "loss": 1.5167,
      "step": 2951
    },
    {
      "epoch": 19.509293680297397,
      "grad_norm": 10.724979400634766,
      "learning_rate": 3.366336633663367e-05,
      "loss": 0.8678,
      "step": 2952
    },
    {
      "epoch": 19.515902519619992,
      "grad_norm": 34.959468841552734,
      "learning_rate": 3.316831683168317e-05,
      "loss": 1.7403,
      "step": 2953
    },
    {
      "epoch": 19.522511358942587,
      "grad_norm": 5.841096878051758,
      "learning_rate": 3.2673267326732674e-05,
      "loss": 1.0459,
      "step": 2954
    },
    {
      "epoch": 19.52912019826518,
      "grad_norm": 11.958976745605469,
      "learning_rate": 3.217821782178218e-05,
      "loss": 1.1324,
      "step": 2955
    },
    {
      "epoch": 19.535729037587775,
      "grad_norm": 18.423357009887695,
      "learning_rate": 3.1683168316831686e-05,
      "loss": 1.3331,
      "step": 2956
    },
    {
      "epoch": 19.542337876910366,
      "grad_norm": 19.262367248535156,
      "learning_rate": 3.118811881188119e-05,
      "loss": 1.1008,
      "step": 2957
    },
    {
      "epoch": 19.54894671623296,
      "grad_norm": 36.42931365966797,
      "learning_rate": 3.069306930693069e-05,
      "loss": 0.8191,
      "step": 2958
    },
    {
      "epoch": 19.555555555555557,
      "grad_norm": 57.54146957397461,
      "learning_rate": 3.0198019801980196e-05,
      "loss": 1.9839,
      "step": 2959
    },
    {
      "epoch": 19.56216439487815,
      "grad_norm": 4.088990688323975,
      "learning_rate": 2.9702970297029702e-05,
      "loss": 1.5064,
      "step": 2960
    },
    {
      "epoch": 19.568773234200744,
      "grad_norm": 11.34162425994873,
      "learning_rate": 2.920792079207921e-05,
      "loss": 0.8947,
      "step": 2961
    },
    {
      "epoch": 19.575382073523336,
      "grad_norm": 8.424585342407227,
      "learning_rate": 2.8712871287128714e-05,
      "loss": 0.6269,
      "step": 2962
    },
    {
      "epoch": 19.58199091284593,
      "grad_norm": 40.69374084472656,
      "learning_rate": 2.821782178217822e-05,
      "loss": 1.2136,
      "step": 2963
    },
    {
      "epoch": 19.588599752168527,
      "grad_norm": 39.33843994140625,
      "learning_rate": 2.7722772277227722e-05,
      "loss": 2.4948,
      "step": 2964
    },
    {
      "epoch": 19.59520859149112,
      "grad_norm": 17.860227584838867,
      "learning_rate": 2.7227722772277228e-05,
      "loss": 0.8468,
      "step": 2965
    },
    {
      "epoch": 19.601817430813714,
      "grad_norm": 20.733600616455078,
      "learning_rate": 2.6732673267326734e-05,
      "loss": 1.0694,
      "step": 2966
    },
    {
      "epoch": 19.608426270136306,
      "grad_norm": 44.61797332763672,
      "learning_rate": 2.6237623762376237e-05,
      "loss": 1.4852,
      "step": 2967
    },
    {
      "epoch": 19.6150351094589,
      "grad_norm": 31.42056655883789,
      "learning_rate": 2.5742574257425742e-05,
      "loss": 2.6642,
      "step": 2968
    },
    {
      "epoch": 19.621643948781497,
      "grad_norm": 14.633509635925293,
      "learning_rate": 2.5247524752475248e-05,
      "loss": 0.8989,
      "step": 2969
    },
    {
      "epoch": 19.62825278810409,
      "grad_norm": 28.294769287109375,
      "learning_rate": 2.4752475247524754e-05,
      "loss": 1.6117,
      "step": 2970
    },
    {
      "epoch": 19.634861627426684,
      "grad_norm": 52.77681350708008,
      "learning_rate": 2.425742574257426e-05,
      "loss": 4.1328,
      "step": 2971
    },
    {
      "epoch": 19.641470466749276,
      "grad_norm": 22.926729202270508,
      "learning_rate": 2.3762376237623762e-05,
      "loss": 1.1099,
      "step": 2972
    },
    {
      "epoch": 19.64807930607187,
      "grad_norm": 10.402655601501465,
      "learning_rate": 2.326732673267327e-05,
      "loss": 0.7978,
      "step": 2973
    },
    {
      "epoch": 19.654688145394466,
      "grad_norm": 8.596355438232422,
      "learning_rate": 2.277227722772277e-05,
      "loss": 1.6057,
      "step": 2974
    },
    {
      "epoch": 19.661296984717058,
      "grad_norm": 8.812769889831543,
      "learning_rate": 2.2277227722772277e-05,
      "loss": 0.6777,
      "step": 2975
    },
    {
      "epoch": 19.667905824039654,
      "grad_norm": 6.095637321472168,
      "learning_rate": 2.1782178217821783e-05,
      "loss": 1.3315,
      "step": 2976
    },
    {
      "epoch": 19.674514663362245,
      "grad_norm": 13.226275444030762,
      "learning_rate": 2.128712871287129e-05,
      "loss": 1.4389,
      "step": 2977
    },
    {
      "epoch": 19.68112350268484,
      "grad_norm": 16.742578506469727,
      "learning_rate": 2.0792079207920794e-05,
      "loss": 1.2586,
      "step": 2978
    },
    {
      "epoch": 19.687732342007436,
      "grad_norm": 21.555973052978516,
      "learning_rate": 2.0297029702970297e-05,
      "loss": 2.0165,
      "step": 2979
    },
    {
      "epoch": 19.694341181330028,
      "grad_norm": 4.647503852844238,
      "learning_rate": 1.9801980198019803e-05,
      "loss": 0.6913,
      "step": 2980
    },
    {
      "epoch": 19.700950020652623,
      "grad_norm": 10.704000473022461,
      "learning_rate": 1.930693069306931e-05,
      "loss": 0.6116,
      "step": 2981
    },
    {
      "epoch": 19.707558859975215,
      "grad_norm": 32.925132751464844,
      "learning_rate": 1.881188118811881e-05,
      "loss": 1.6599,
      "step": 2982
    },
    {
      "epoch": 19.71416769929781,
      "grad_norm": 11.171935081481934,
      "learning_rate": 1.8316831683168317e-05,
      "loss": 1.7265,
      "step": 2983
    },
    {
      "epoch": 19.720776538620406,
      "grad_norm": 17.518108367919922,
      "learning_rate": 1.782178217821782e-05,
      "loss": 0.9874,
      "step": 2984
    },
    {
      "epoch": 19.727385377942998,
      "grad_norm": 6.024172306060791,
      "learning_rate": 1.732673267326733e-05,
      "loss": 2.7254,
      "step": 2985
    },
    {
      "epoch": 19.733994217265593,
      "grad_norm": 32.711952209472656,
      "learning_rate": 1.6831683168316834e-05,
      "loss": 2.2186,
      "step": 2986
    },
    {
      "epoch": 19.740603056588185,
      "grad_norm": 31.041751861572266,
      "learning_rate": 1.6336633663366337e-05,
      "loss": 1.1692,
      "step": 2987
    },
    {
      "epoch": 19.74721189591078,
      "grad_norm": 20.98719024658203,
      "learning_rate": 1.5841584158415843e-05,
      "loss": 1.1734,
      "step": 2988
    },
    {
      "epoch": 19.753820735233376,
      "grad_norm": 2.8211746215820312,
      "learning_rate": 1.5346534653465345e-05,
      "loss": 1.4182,
      "step": 2989
    },
    {
      "epoch": 19.760429574555967,
      "grad_norm": 27.827133178710938,
      "learning_rate": 1.4851485148514851e-05,
      "loss": 1.8083,
      "step": 2990
    },
    {
      "epoch": 19.767038413878563,
      "grad_norm": 27.682872772216797,
      "learning_rate": 1.4356435643564357e-05,
      "loss": 2.691,
      "step": 2991
    },
    {
      "epoch": 19.77364725320116,
      "grad_norm": 12.843999862670898,
      "learning_rate": 1.3861386138613861e-05,
      "loss": 3.1196,
      "step": 2992
    },
    {
      "epoch": 19.78025609252375,
      "grad_norm": 17.97903060913086,
      "learning_rate": 1.3366336633663367e-05,
      "loss": 1.7309,
      "step": 2993
    },
    {
      "epoch": 19.786864931846345,
      "grad_norm": 12.623098373413086,
      "learning_rate": 1.2871287128712871e-05,
      "loss": 0.9935,
      "step": 2994
    },
    {
      "epoch": 19.793473771168937,
      "grad_norm": 15.25272274017334,
      "learning_rate": 1.2376237623762377e-05,
      "loss": 1.7688,
      "step": 2995
    },
    {
      "epoch": 19.800082610491533,
      "grad_norm": 16.681873321533203,
      "learning_rate": 1.1881188118811881e-05,
      "loss": 0.8823,
      "step": 2996
    },
    {
      "epoch": 19.806691449814128,
      "grad_norm": 39.18128204345703,
      "learning_rate": 1.1386138613861385e-05,
      "loss": 2.4,
      "step": 2997
    },
    {
      "epoch": 19.81330028913672,
      "grad_norm": 8.332417488098145,
      "learning_rate": 1.0891089108910891e-05,
      "loss": 1.3004,
      "step": 2998
    },
    {
      "epoch": 19.819909128459315,
      "grad_norm": 11.533541679382324,
      "learning_rate": 1.0396039603960397e-05,
      "loss": 1.4084,
      "step": 2999
    },
    {
      "epoch": 19.826517967781907,
      "grad_norm": 24.352872848510742,
      "learning_rate": 9.900990099009901e-06,
      "loss": 1.9669,
      "step": 3000
    },
    {
      "epoch": 19.833126807104502,
      "grad_norm": 58.55782699584961,
      "learning_rate": 9.405940594059405e-06,
      "loss": 5.0281,
      "step": 3001
    },
    {
      "epoch": 19.839735646427098,
      "grad_norm": 31.898895263671875,
      "learning_rate": 8.91089108910891e-06,
      "loss": 3.5807,
      "step": 3002
    },
    {
      "epoch": 19.84634448574969,
      "grad_norm": 13.609715461730957,
      "learning_rate": 8.415841584158417e-06,
      "loss": 1.929,
      "step": 3003
    },
    {
      "epoch": 19.852953325072285,
      "grad_norm": 38.96003723144531,
      "learning_rate": 7.920792079207921e-06,
      "loss": 1.6366,
      "step": 3004
    },
    {
      "epoch": 19.859562164394877,
      "grad_norm": 49.49243927001953,
      "learning_rate": 7.4257425742574256e-06,
      "loss": 3.0806,
      "step": 3005
    },
    {
      "epoch": 19.866171003717472,
      "grad_norm": 8.87868595123291,
      "learning_rate": 6.930693069306931e-06,
      "loss": 1.5662,
      "step": 3006
    },
    {
      "epoch": 19.872779843040068,
      "grad_norm": 19.623308181762695,
      "learning_rate": 6.435643564356436e-06,
      "loss": 1.9592,
      "step": 3007
    },
    {
      "epoch": 19.87938868236266,
      "grad_norm": 18.1348934173584,
      "learning_rate": 5.940594059405941e-06,
      "loss": 3.3535,
      "step": 3008
    },
    {
      "epoch": 19.885997521685255,
      "grad_norm": 14.988481521606445,
      "learning_rate": 5.445544554455446e-06,
      "loss": 0.7324,
      "step": 3009
    },
    {
      "epoch": 19.892606361007847,
      "grad_norm": 3.0860092639923096,
      "learning_rate": 4.950495049504951e-06,
      "loss": 0.4155,
      "step": 3010
    },
    {
      "epoch": 19.899215200330442,
      "grad_norm": 27.89711570739746,
      "learning_rate": 4.455445544554455e-06,
      "loss": 4.3009,
      "step": 3011
    },
    {
      "epoch": 19.905824039653037,
      "grad_norm": 27.477130889892578,
      "learning_rate": 3.960396039603961e-06,
      "loss": 2.289,
      "step": 3012
    },
    {
      "epoch": 19.91243287897563,
      "grad_norm": 27.655227661132812,
      "learning_rate": 3.4653465346534653e-06,
      "loss": 4.0492,
      "step": 3013
    },
    {
      "epoch": 19.919041718298224,
      "grad_norm": 10.970453262329102,
      "learning_rate": 2.9702970297029703e-06,
      "loss": 1.1092,
      "step": 3014
    },
    {
      "epoch": 19.925650557620816,
      "grad_norm": 21.131580352783203,
      "learning_rate": 2.4752475247524753e-06,
      "loss": 2.2112,
      "step": 3015
    },
    {
      "epoch": 19.93225939694341,
      "grad_norm": 68.95329284667969,
      "learning_rate": 1.9801980198019803e-06,
      "loss": 4.9298,
      "step": 3016
    },
    {
      "epoch": 19.938868236266007,
      "grad_norm": 14.288858413696289,
      "learning_rate": 1.4851485148514852e-06,
      "loss": 1.6467,
      "step": 3017
    },
    {
      "epoch": 19.9454770755886,
      "grad_norm": 9.057397842407227,
      "learning_rate": 9.900990099009902e-07,
      "loss": 0.9927,
      "step": 3018
    },
    {
      "epoch": 19.952085914911194,
      "grad_norm": 6.804597854614258,
      "learning_rate": 4.950495049504951e-07,
      "loss": 1.1696,
      "step": 3019
    },
    {
      "epoch": 19.958694754233786,
      "grad_norm": 13.009825706481934,
      "learning_rate": 0.0,
      "loss": 2.0865,
      "step": 3020
    },
    {
      "epoch": 19.958694754233786,
      "eval_validation_error_bar": 0.04521257063200632,
      "eval_validation_loss": 7.421983242034912,
      "eval_validation_pearsonr": 0.5896330160088707,
      "eval_validation_rmse": 2.7243316173553467,
      "eval_validation_runtime": 30.4494,
      "eval_validation_samples_per_second": 6.667,
      "eval_validation_spearman": 0.6469552431528565,
      "eval_validation_steps_per_second": 6.667,
      "step": 3020
    },
    {
      "epoch": 19.958694754233786,
      "eval_test_error_bar": 0.03903479534964168,
      "eval_test_loss": 7.391680717468262,
      "eval_test_pearsonr": 0.5824717121016185,
      "eval_test_rmse": 2.718764543533325,
      "eval_test_runtime": 39.7216,
      "eval_test_samples_per_second": 8.207,
      "eval_test_spearman": 0.5943517307867253,
      "eval_test_steps_per_second": 8.207,
      "step": 3020
    }
  ],
  "logging_steps": 1,
  "max_steps": 3020,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": true
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
