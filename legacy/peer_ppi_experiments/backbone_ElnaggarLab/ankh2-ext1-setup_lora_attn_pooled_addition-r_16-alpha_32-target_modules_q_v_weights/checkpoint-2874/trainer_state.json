{
  "best_metric": 0.7450996987550391,
  "best_model_checkpoint": "backbone_ElnaggarLab/ankh2-ext1-setup_lora_attn_pooled_addition-r_16-alpha_32-target_modules_q_v_weights/checkpoint-2874",
  "epoch": 18.993804213135068,
  "eval_steps": 500,
  "global_step": 2874,
  "is_hyper_param_search": false,
  "is_local_process_zero": true,
  "is_world_process_zero": true,
  "log_history": [
    {
      "epoch": 0.00660883932259397,
      "grad_norm": 35.748165130615234,
      "learning_rate": 1e-06,
      "loss": 141.3929,
      "step": 1
    },
    {
      "epoch": 0.01321767864518794,
      "grad_norm": 37.67443084716797,
      "learning_rate": 2e-06,
      "loss": 157.522,
      "step": 2
    },
    {
      "epoch": 0.01982651796778191,
      "grad_norm": 43.364131927490234,
      "learning_rate": 3e-06,
      "loss": 178.2306,
      "step": 3
    },
    {
      "epoch": 0.02643535729037588,
      "grad_norm": 32.681678771972656,
      "learning_rate": 4e-06,
      "loss": 120.3411,
      "step": 4
    },
    {
      "epoch": 0.033044196612969846,
      "grad_norm": 38.484378814697266,
      "learning_rate": 5e-06,
      "loss": 160.8191,
      "step": 5
    },
    {
      "epoch": 0.03965303593556382,
      "grad_norm": 32.97942352294922,
      "learning_rate": 6e-06,
      "loss": 128.693,
      "step": 6
    },
    {
      "epoch": 0.04626187525815779,
      "grad_norm": 38.55534744262695,
      "learning_rate": 7e-06,
      "loss": 147.8348,
      "step": 7
    },
    {
      "epoch": 0.05287071458075176,
      "grad_norm": 36.26316833496094,
      "learning_rate": 8e-06,
      "loss": 149.5284,
      "step": 8
    },
    {
      "epoch": 0.05947955390334572,
      "grad_norm": 34.976409912109375,
      "learning_rate": 9e-06,
      "loss": 137.267,
      "step": 9
    },
    {
      "epoch": 0.06608839322593969,
      "grad_norm": 33.43935012817383,
      "learning_rate": 1e-05,
      "loss": 125.0094,
      "step": 10
    },
    {
      "epoch": 0.07269723254853366,
      "grad_norm": 33.97117614746094,
      "learning_rate": 1.1e-05,
      "loss": 127.2177,
      "step": 11
    },
    {
      "epoch": 0.07930607187112763,
      "grad_norm": 31.927398681640625,
      "learning_rate": 1.2e-05,
      "loss": 121.9375,
      "step": 12
    },
    {
      "epoch": 0.0859149111937216,
      "grad_norm": 37.71814727783203,
      "learning_rate": 1.3e-05,
      "loss": 155.7311,
      "step": 13
    },
    {
      "epoch": 0.09252375051631558,
      "grad_norm": 36.77113342285156,
      "learning_rate": 1.4e-05,
      "loss": 139.2891,
      "step": 14
    },
    {
      "epoch": 0.09913258983890955,
      "grad_norm": 32.08150100708008,
      "learning_rate": 1.5e-05,
      "loss": 120.2596,
      "step": 15
    },
    {
      "epoch": 0.10574142916150352,
      "grad_norm": 35.68916320800781,
      "learning_rate": 1.6e-05,
      "loss": 145.7162,
      "step": 16
    },
    {
      "epoch": 0.11235026848409747,
      "grad_norm": 38.75413513183594,
      "learning_rate": 1.7000000000000003e-05,
      "loss": 161.7779,
      "step": 17
    },
    {
      "epoch": 0.11895910780669144,
      "grad_norm": 36.48552703857422,
      "learning_rate": 1.8e-05,
      "loss": 146.4852,
      "step": 18
    },
    {
      "epoch": 0.12556794712928543,
      "grad_norm": 33.25530242919922,
      "learning_rate": 1.9e-05,
      "loss": 118.99,
      "step": 19
    },
    {
      "epoch": 0.13217678645187939,
      "grad_norm": 36.6989860534668,
      "learning_rate": 2e-05,
      "loss": 137.8633,
      "step": 20
    },
    {
      "epoch": 0.13878562577447337,
      "grad_norm": 37.564083099365234,
      "learning_rate": 2.1000000000000002e-05,
      "loss": 153.732,
      "step": 21
    },
    {
      "epoch": 0.14539446509706733,
      "grad_norm": 40.734012603759766,
      "learning_rate": 2.2e-05,
      "loss": 176.3197,
      "step": 22
    },
    {
      "epoch": 0.15200330441966128,
      "grad_norm": 41.792762756347656,
      "learning_rate": 2.3e-05,
      "loss": 176.3634,
      "step": 23
    },
    {
      "epoch": 0.15861214374225527,
      "grad_norm": 33.8125,
      "learning_rate": 2.4e-05,
      "loss": 125.7689,
      "step": 24
    },
    {
      "epoch": 0.16522098306484923,
      "grad_norm": 36.782039642333984,
      "learning_rate": 2.5e-05,
      "loss": 141.4822,
      "step": 25
    },
    {
      "epoch": 0.1718298223874432,
      "grad_norm": 42.71696090698242,
      "learning_rate": 2.6e-05,
      "loss": 158.1736,
      "step": 26
    },
    {
      "epoch": 0.17843866171003717,
      "grad_norm": 34.726173400878906,
      "learning_rate": 2.7e-05,
      "loss": 125.6149,
      "step": 27
    },
    {
      "epoch": 0.18504750103263115,
      "grad_norm": 37.394325256347656,
      "learning_rate": 2.8e-05,
      "loss": 126.7594,
      "step": 28
    },
    {
      "epoch": 0.1916563403552251,
      "grad_norm": 33.33363342285156,
      "learning_rate": 2.9000000000000004e-05,
      "loss": 99.8325,
      "step": 29
    },
    {
      "epoch": 0.1982651796778191,
      "grad_norm": 39.24846649169922,
      "learning_rate": 3e-05,
      "loss": 127.5564,
      "step": 30
    },
    {
      "epoch": 0.20487401900041305,
      "grad_norm": 40.772857666015625,
      "learning_rate": 3.1e-05,
      "loss": 128.1213,
      "step": 31
    },
    {
      "epoch": 0.21148285832300703,
      "grad_norm": 39.965084075927734,
      "learning_rate": 3.2e-05,
      "loss": 123.6189,
      "step": 32
    },
    {
      "epoch": 0.218091697645601,
      "grad_norm": 39.82828140258789,
      "learning_rate": 3.3e-05,
      "loss": 119.1284,
      "step": 33
    },
    {
      "epoch": 0.22470053696819495,
      "grad_norm": 40.3974609375,
      "learning_rate": 3.4000000000000007e-05,
      "loss": 127.1842,
      "step": 34
    },
    {
      "epoch": 0.23130937629078893,
      "grad_norm": 41.998023986816406,
      "learning_rate": 3.5000000000000004e-05,
      "loss": 130.7252,
      "step": 35
    },
    {
      "epoch": 0.2379182156133829,
      "grad_norm": 46.80624008178711,
      "learning_rate": 3.6e-05,
      "loss": 129.7364,
      "step": 36
    },
    {
      "epoch": 0.24452705493597687,
      "grad_norm": 42.093631744384766,
      "learning_rate": 3.7e-05,
      "loss": 124.4152,
      "step": 37
    },
    {
      "epoch": 0.25113589425857086,
      "grad_norm": 37.271732330322266,
      "learning_rate": 3.8e-05,
      "loss": 87.181,
      "step": 38
    },
    {
      "epoch": 0.2577447335811648,
      "grad_norm": 43.722755432128906,
      "learning_rate": 3.9e-05,
      "loss": 110.3141,
      "step": 39
    },
    {
      "epoch": 0.26435357290375877,
      "grad_norm": 55.05678176879883,
      "learning_rate": 4e-05,
      "loss": 149.0157,
      "step": 40
    },
    {
      "epoch": 0.27096241222635276,
      "grad_norm": 54.27906799316406,
      "learning_rate": 4.1e-05,
      "loss": 131.8932,
      "step": 41
    },
    {
      "epoch": 0.27757125154894674,
      "grad_norm": 49.61307907104492,
      "learning_rate": 4.2000000000000004e-05,
      "loss": 115.4637,
      "step": 42
    },
    {
      "epoch": 0.28418009087154067,
      "grad_norm": 46.87409973144531,
      "learning_rate": 4.2999999999999995e-05,
      "loss": 93.4731,
      "step": 43
    },
    {
      "epoch": 0.29078893019413465,
      "grad_norm": 48.51725387573242,
      "learning_rate": 4.4e-05,
      "loss": 99.9808,
      "step": 44
    },
    {
      "epoch": 0.29739776951672864,
      "grad_norm": 54.02445983886719,
      "learning_rate": 4.4999999999999996e-05,
      "loss": 109.3799,
      "step": 45
    },
    {
      "epoch": 0.30400660883932257,
      "grad_norm": 54.60094451904297,
      "learning_rate": 4.6e-05,
      "loss": 118.7422,
      "step": 46
    },
    {
      "epoch": 0.31061544816191655,
      "grad_norm": 53.67479705810547,
      "learning_rate": 4.7000000000000004e-05,
      "loss": 90.2236,
      "step": 47
    },
    {
      "epoch": 0.31722428748451054,
      "grad_norm": 56.38961410522461,
      "learning_rate": 4.8e-05,
      "loss": 99.1732,
      "step": 48
    },
    {
      "epoch": 0.3238331268071045,
      "grad_norm": 55.93805694580078,
      "learning_rate": 4.9000000000000005e-05,
      "loss": 83.7168,
      "step": 49
    },
    {
      "epoch": 0.33044196612969845,
      "grad_norm": 58.599884033203125,
      "learning_rate": 5e-05,
      "loss": 87.2228,
      "step": 50
    },
    {
      "epoch": 0.33705080545229243,
      "grad_norm": 75.51445007324219,
      "learning_rate": 5.1e-05,
      "loss": 125.2375,
      "step": 51
    },
    {
      "epoch": 0.3436596447748864,
      "grad_norm": 51.415428161621094,
      "learning_rate": 5.2e-05,
      "loss": 61.409,
      "step": 52
    },
    {
      "epoch": 0.3502684840974804,
      "grad_norm": 57.870174407958984,
      "learning_rate": 5.3e-05,
      "loss": 67.2345,
      "step": 53
    },
    {
      "epoch": 0.35687732342007433,
      "grad_norm": 67.57486724853516,
      "learning_rate": 5.4e-05,
      "loss": 94.0593,
      "step": 54
    },
    {
      "epoch": 0.3634861627426683,
      "grad_norm": 66.35248565673828,
      "learning_rate": 5.5e-05,
      "loss": 80.9302,
      "step": 55
    },
    {
      "epoch": 0.3700950020652623,
      "grad_norm": 72.30741119384766,
      "learning_rate": 5.6e-05,
      "loss": 92.215,
      "step": 56
    },
    {
      "epoch": 0.37670384138785623,
      "grad_norm": 72.15620422363281,
      "learning_rate": 5.7e-05,
      "loss": 73.1065,
      "step": 57
    },
    {
      "epoch": 0.3833126807104502,
      "grad_norm": 55.64228057861328,
      "learning_rate": 5.800000000000001e-05,
      "loss": 44.7594,
      "step": 58
    },
    {
      "epoch": 0.3899215200330442,
      "grad_norm": 64.2855453491211,
      "learning_rate": 5.9e-05,
      "loss": 53.6768,
      "step": 59
    },
    {
      "epoch": 0.3965303593556382,
      "grad_norm": 47.56248092651367,
      "learning_rate": 6e-05,
      "loss": 29.9695,
      "step": 60
    },
    {
      "epoch": 0.4031391986782321,
      "grad_norm": 45.991451263427734,
      "learning_rate": 6.1e-05,
      "loss": 32.2101,
      "step": 61
    },
    {
      "epoch": 0.4097480380008261,
      "grad_norm": 51.265235900878906,
      "learning_rate": 6.2e-05,
      "loss": 34.6433,
      "step": 62
    },
    {
      "epoch": 0.4163568773234201,
      "grad_norm": 45.23630142211914,
      "learning_rate": 6.3e-05,
      "loss": 21.0114,
      "step": 63
    },
    {
      "epoch": 0.42296571664601407,
      "grad_norm": 61.055931091308594,
      "learning_rate": 6.4e-05,
      "loss": 32.3219,
      "step": 64
    },
    {
      "epoch": 0.429574555968608,
      "grad_norm": 60.20902633666992,
      "learning_rate": 6.500000000000001e-05,
      "loss": 32.6327,
      "step": 65
    },
    {
      "epoch": 0.436183395291202,
      "grad_norm": 49.12431716918945,
      "learning_rate": 6.6e-05,
      "loss": 17.2249,
      "step": 66
    },
    {
      "epoch": 0.44279223461379597,
      "grad_norm": 51.378273010253906,
      "learning_rate": 6.7e-05,
      "loss": 21.7303,
      "step": 67
    },
    {
      "epoch": 0.4494010739363899,
      "grad_norm": 27.108989715576172,
      "learning_rate": 6.800000000000001e-05,
      "loss": 9.3625,
      "step": 68
    },
    {
      "epoch": 0.4560099132589839,
      "grad_norm": 12.669703483581543,
      "learning_rate": 6.900000000000001e-05,
      "loss": 9.0207,
      "step": 69
    },
    {
      "epoch": 0.46261875258157786,
      "grad_norm": 15.69239330291748,
      "learning_rate": 7.000000000000001e-05,
      "loss": 11.4914,
      "step": 70
    },
    {
      "epoch": 0.46922759190417185,
      "grad_norm": 24.366483688354492,
      "learning_rate": 7.099999999999999e-05,
      "loss": 18.5848,
      "step": 71
    },
    {
      "epoch": 0.4758364312267658,
      "grad_norm": 18.853496551513672,
      "learning_rate": 7.2e-05,
      "loss": 5.2038,
      "step": 72
    },
    {
      "epoch": 0.48244527054935976,
      "grad_norm": 20.55179214477539,
      "learning_rate": 7.3e-05,
      "loss": 9.7317,
      "step": 73
    },
    {
      "epoch": 0.48905410987195375,
      "grad_norm": 9.289280891418457,
      "learning_rate": 7.4e-05,
      "loss": 11.1846,
      "step": 74
    },
    {
      "epoch": 0.49566294919454773,
      "grad_norm": 44.06165313720703,
      "learning_rate": 7.5e-05,
      "loss": 10.7106,
      "step": 75
    },
    {
      "epoch": 0.5022717885171417,
      "grad_norm": 16.56899642944336,
      "learning_rate": 7.6e-05,
      "loss": 10.947,
      "step": 76
    },
    {
      "epoch": 0.5088806278397356,
      "grad_norm": 28.568769454956055,
      "learning_rate": 7.7e-05,
      "loss": 5.8272,
      "step": 77
    },
    {
      "epoch": 0.5154894671623296,
      "grad_norm": 39.30437469482422,
      "learning_rate": 7.8e-05,
      "loss": 10.4342,
      "step": 78
    },
    {
      "epoch": 0.5220983064849236,
      "grad_norm": 51.347999572753906,
      "learning_rate": 7.9e-05,
      "loss": 17.2814,
      "step": 79
    },
    {
      "epoch": 0.5287071458075175,
      "grad_norm": 34.6059455871582,
      "learning_rate": 8e-05,
      "loss": 17.9904,
      "step": 80
    },
    {
      "epoch": 0.5353159851301115,
      "grad_norm": 19.591699600219727,
      "learning_rate": 8.1e-05,
      "loss": 8.3425,
      "step": 81
    },
    {
      "epoch": 0.5419248244527055,
      "grad_norm": 18.937620162963867,
      "learning_rate": 8.2e-05,
      "loss": 10.6903,
      "step": 82
    },
    {
      "epoch": 0.5485336637752994,
      "grad_norm": 18.2437801361084,
      "learning_rate": 8.300000000000001e-05,
      "loss": 8.2167,
      "step": 83
    },
    {
      "epoch": 0.5551425030978935,
      "grad_norm": 7.19502067565918,
      "learning_rate": 8.400000000000001e-05,
      "loss": 9.5813,
      "step": 84
    },
    {
      "epoch": 0.5617513424204874,
      "grad_norm": 20.830415725708008,
      "learning_rate": 8.5e-05,
      "loss": 8.6004,
      "step": 85
    },
    {
      "epoch": 0.5683601817430813,
      "grad_norm": 23.725358963012695,
      "learning_rate": 8.599999999999999e-05,
      "loss": 5.134,
      "step": 86
    },
    {
      "epoch": 0.5749690210656754,
      "grad_norm": 7.557068824768066,
      "learning_rate": 8.7e-05,
      "loss": 7.0854,
      "step": 87
    },
    {
      "epoch": 0.5815778603882693,
      "grad_norm": 16.237567901611328,
      "learning_rate": 8.8e-05,
      "loss": 10.9261,
      "step": 88
    },
    {
      "epoch": 0.5881866997108632,
      "grad_norm": 19.26003646850586,
      "learning_rate": 8.9e-05,
      "loss": 9.5419,
      "step": 89
    },
    {
      "epoch": 0.5947955390334573,
      "grad_norm": 9.314613342285156,
      "learning_rate": 8.999999999999999e-05,
      "loss": 5.629,
      "step": 90
    },
    {
      "epoch": 0.6014043783560512,
      "grad_norm": 5.429922103881836,
      "learning_rate": 9.1e-05,
      "loss": 2.8374,
      "step": 91
    },
    {
      "epoch": 0.6080132176786451,
      "grad_norm": 3.735652446746826,
      "learning_rate": 9.2e-05,
      "loss": 5.5059,
      "step": 92
    },
    {
      "epoch": 0.6146220570012392,
      "grad_norm": 8.300307273864746,
      "learning_rate": 9.3e-05,
      "loss": 8.7064,
      "step": 93
    },
    {
      "epoch": 0.6212308963238331,
      "grad_norm": 19.676366806030273,
      "learning_rate": 9.400000000000001e-05,
      "loss": 11.4883,
      "step": 94
    },
    {
      "epoch": 0.6278397356464271,
      "grad_norm": 42.4295539855957,
      "learning_rate": 9.5e-05,
      "loss": 9.0518,
      "step": 95
    },
    {
      "epoch": 0.6344485749690211,
      "grad_norm": 5.787633895874023,
      "learning_rate": 9.6e-05,
      "loss": 5.4098,
      "step": 96
    },
    {
      "epoch": 0.641057414291615,
      "grad_norm": 6.388309001922607,
      "learning_rate": 9.7e-05,
      "loss": 11.0471,
      "step": 97
    },
    {
      "epoch": 0.647666253614209,
      "grad_norm": 15.560792922973633,
      "learning_rate": 9.800000000000001e-05,
      "loss": 9.387,
      "step": 98
    },
    {
      "epoch": 0.654275092936803,
      "grad_norm": 7.2567644119262695,
      "learning_rate": 9.900000000000001e-05,
      "loss": 7.8882,
      "step": 99
    },
    {
      "epoch": 0.6608839322593969,
      "grad_norm": 9.23438835144043,
      "learning_rate": 0.0001,
      "loss": 8.2285,
      "step": 100
    },
    {
      "epoch": 0.6674927715819909,
      "grad_norm": 3.655066728591919,
      "learning_rate": 0.000101,
      "loss": 3.9944,
      "step": 101
    },
    {
      "epoch": 0.6741016109045849,
      "grad_norm": 9.967253684997559,
      "learning_rate": 0.000102,
      "loss": 9.285,
      "step": 102
    },
    {
      "epoch": 0.6807104502271788,
      "grad_norm": 12.792778968811035,
      "learning_rate": 0.000103,
      "loss": 4.7582,
      "step": 103
    },
    {
      "epoch": 0.6873192895497728,
      "grad_norm": 7.8003997802734375,
      "learning_rate": 0.000104,
      "loss": 4.1439,
      "step": 104
    },
    {
      "epoch": 0.6939281288723668,
      "grad_norm": 3.975492238998413,
      "learning_rate": 0.000105,
      "loss": 3.6945,
      "step": 105
    },
    {
      "epoch": 0.7005369681949608,
      "grad_norm": 25.48697280883789,
      "learning_rate": 0.000106,
      "loss": 8.7684,
      "step": 106
    },
    {
      "epoch": 0.7071458075175547,
      "grad_norm": 15.756147384643555,
      "learning_rate": 0.000107,
      "loss": 3.6233,
      "step": 107
    },
    {
      "epoch": 0.7137546468401487,
      "grad_norm": 3.6314918994903564,
      "learning_rate": 0.000108,
      "loss": 1.6888,
      "step": 108
    },
    {
      "epoch": 0.7203634861627427,
      "grad_norm": 8.375436782836914,
      "learning_rate": 0.000109,
      "loss": 4.8141,
      "step": 109
    },
    {
      "epoch": 0.7269723254853366,
      "grad_norm": 6.049928188323975,
      "learning_rate": 0.00011,
      "loss": 2.5242,
      "step": 110
    },
    {
      "epoch": 0.7335811648079306,
      "grad_norm": 10.563462257385254,
      "learning_rate": 0.000111,
      "loss": 4.5642,
      "step": 111
    },
    {
      "epoch": 0.7401900041305246,
      "grad_norm": 5.622959136962891,
      "learning_rate": 0.000112,
      "loss": 3.5092,
      "step": 112
    },
    {
      "epoch": 0.7467988434531185,
      "grad_norm": 12.743062973022461,
      "learning_rate": 0.00011300000000000001,
      "loss": 6.9593,
      "step": 113
    },
    {
      "epoch": 0.7534076827757125,
      "grad_norm": 4.642171382904053,
      "learning_rate": 0.000114,
      "loss": 5.6069,
      "step": 114
    },
    {
      "epoch": 0.7600165220983065,
      "grad_norm": 10.165061950683594,
      "learning_rate": 0.000115,
      "loss": 3.0832,
      "step": 115
    },
    {
      "epoch": 0.7666253614209004,
      "grad_norm": 4.037095069885254,
      "learning_rate": 0.00011600000000000001,
      "loss": 3.6298,
      "step": 116
    },
    {
      "epoch": 0.7732342007434945,
      "grad_norm": 15.17014217376709,
      "learning_rate": 0.00011700000000000001,
      "loss": 7.7434,
      "step": 117
    },
    {
      "epoch": 0.7798430400660884,
      "grad_norm": 13.280330657958984,
      "learning_rate": 0.000118,
      "loss": 6.2054,
      "step": 118
    },
    {
      "epoch": 0.7864518793886823,
      "grad_norm": 3.338141679763794,
      "learning_rate": 0.00011899999999999999,
      "loss": 3.0767,
      "step": 119
    },
    {
      "epoch": 0.7930607187112764,
      "grad_norm": 25.090028762817383,
      "learning_rate": 0.00012,
      "loss": 5.8568,
      "step": 120
    },
    {
      "epoch": 0.7996695580338703,
      "grad_norm": 5.871990203857422,
      "learning_rate": 0.000121,
      "loss": 3.2611,
      "step": 121
    },
    {
      "epoch": 0.8062783973564642,
      "grad_norm": 15.744503021240234,
      "learning_rate": 0.000122,
      "loss": 3.2225,
      "step": 122
    },
    {
      "epoch": 0.8128872366790583,
      "grad_norm": 10.079967498779297,
      "learning_rate": 0.000123,
      "loss": 6.0737,
      "step": 123
    },
    {
      "epoch": 0.8194960760016522,
      "grad_norm": 1.8483515977859497,
      "learning_rate": 0.000124,
      "loss": 1.2522,
      "step": 124
    },
    {
      "epoch": 0.8261049153242461,
      "grad_norm": 14.87963581085205,
      "learning_rate": 0.000125,
      "loss": 2.7927,
      "step": 125
    },
    {
      "epoch": 0.8327137546468402,
      "grad_norm": 7.69212532043457,
      "learning_rate": 0.000126,
      "loss": 1.9761,
      "step": 126
    },
    {
      "epoch": 0.8393225939694341,
      "grad_norm": 15.967691421508789,
      "learning_rate": 0.000127,
      "loss": 4.1879,
      "step": 127
    },
    {
      "epoch": 0.8459314332920281,
      "grad_norm": 5.759356498718262,
      "learning_rate": 0.000128,
      "loss": 5.6919,
      "step": 128
    },
    {
      "epoch": 0.8525402726146221,
      "grad_norm": 7.463836193084717,
      "learning_rate": 0.00012900000000000002,
      "loss": 5.8163,
      "step": 129
    },
    {
      "epoch": 0.859149111937216,
      "grad_norm": 6.029048442840576,
      "learning_rate": 0.00013000000000000002,
      "loss": 4.4935,
      "step": 130
    },
    {
      "epoch": 0.86575795125981,
      "grad_norm": 8.112872123718262,
      "learning_rate": 0.000131,
      "loss": 4.0191,
      "step": 131
    },
    {
      "epoch": 0.872366790582404,
      "grad_norm": 10.008862495422363,
      "learning_rate": 0.000132,
      "loss": 3.7744,
      "step": 132
    },
    {
      "epoch": 0.8789756299049979,
      "grad_norm": 7.530002117156982,
      "learning_rate": 0.000133,
      "loss": 3.3207,
      "step": 133
    },
    {
      "epoch": 0.8855844692275919,
      "grad_norm": 5.961694717407227,
      "learning_rate": 0.000134,
      "loss": 4.3495,
      "step": 134
    },
    {
      "epoch": 0.8921933085501859,
      "grad_norm": 4.946255683898926,
      "learning_rate": 0.000135,
      "loss": 1.642,
      "step": 135
    },
    {
      "epoch": 0.8988021478727798,
      "grad_norm": 3.1022679805755615,
      "learning_rate": 0.00013600000000000003,
      "loss": 2.9036,
      "step": 136
    },
    {
      "epoch": 0.9054109871953738,
      "grad_norm": 11.44009017944336,
      "learning_rate": 0.00013700000000000002,
      "loss": 5.2145,
      "step": 137
    },
    {
      "epoch": 0.9120198265179678,
      "grad_norm": 7.31787633895874,
      "learning_rate": 0.00013800000000000002,
      "loss": 5.5023,
      "step": 138
    },
    {
      "epoch": 0.9186286658405618,
      "grad_norm": 11.097549438476562,
      "learning_rate": 0.00013900000000000002,
      "loss": 4.2707,
      "step": 139
    },
    {
      "epoch": 0.9252375051631557,
      "grad_norm": 4.220390796661377,
      "learning_rate": 0.00014000000000000001,
      "loss": 1.199,
      "step": 140
    },
    {
      "epoch": 0.9318463444857497,
      "grad_norm": 3.2037205696105957,
      "learning_rate": 0.00014099999999999998,
      "loss": 3.6267,
      "step": 141
    },
    {
      "epoch": 0.9384551838083437,
      "grad_norm": 8.670281410217285,
      "learning_rate": 0.00014199999999999998,
      "loss": 2.6829,
      "step": 142
    },
    {
      "epoch": 0.9450640231309376,
      "grad_norm": 25.133451461791992,
      "learning_rate": 0.00014299999999999998,
      "loss": 6.3109,
      "step": 143
    },
    {
      "epoch": 0.9516728624535316,
      "grad_norm": 14.074664115905762,
      "learning_rate": 0.000144,
      "loss": 4.4422,
      "step": 144
    },
    {
      "epoch": 0.9582817017761256,
      "grad_norm": 6.172621726989746,
      "learning_rate": 0.000145,
      "loss": 4.3984,
      "step": 145
    },
    {
      "epoch": 0.9648905410987195,
      "grad_norm": 4.984298229217529,
      "learning_rate": 0.000146,
      "loss": 5.1234,
      "step": 146
    },
    {
      "epoch": 0.9714993804213135,
      "grad_norm": 4.672379493713379,
      "learning_rate": 0.000147,
      "loss": 2.4043,
      "step": 147
    },
    {
      "epoch": 0.9781082197439075,
      "grad_norm": 8.711668014526367,
      "learning_rate": 0.000148,
      "loss": 2.2212,
      "step": 148
    },
    {
      "epoch": 0.9847170590665014,
      "grad_norm": 2.0737907886505127,
      "learning_rate": 0.000149,
      "loss": 0.9604,
      "step": 149
    },
    {
      "epoch": 0.9913258983890955,
      "grad_norm": 14.093667984008789,
      "learning_rate": 0.00015,
      "loss": 5.4658,
      "step": 150
    },
    {
      "epoch": 0.9979347377116894,
      "grad_norm": 4.3485870361328125,
      "learning_rate": 0.000151,
      "loss": 4.0305,
      "step": 151
    },
    {
      "epoch": 0.9979347377116894,
      "eval_validation_error_bar": 0.05666196551819159,
      "eval_validation_loss": 6.5307393074035645,
      "eval_validation_pearsonr": 0.5340286529174433,
      "eval_validation_rmse": 2.5555310249328613,
      "eval_validation_runtime": 28.6527,
      "eval_validation_samples_per_second": 7.085,
      "eval_validation_spearman": 0.49335174293704703,
      "eval_validation_steps_per_second": 7.085,
      "step": 151
    },
    {
      "epoch": 0.9979347377116894,
      "eval_test_error_bar": 0.054630779095369245,
      "eval_test_loss": 11.339831352233887,
      "eval_test_pearsonr": 0.17759999020436187,
      "eval_test_rmse": 3.367466688156128,
      "eval_test_runtime": 45.8175,
      "eval_test_samples_per_second": 7.115,
      "eval_test_spearman": 0.15493289234430918,
      "eval_test_steps_per_second": 7.115,
      "step": 151
    },
    {
      "epoch": 1.0045435770342834,
      "grad_norm": 13.538692474365234,
      "learning_rate": 0.000152,
      "loss": 5.5697,
      "step": 152
    },
    {
      "epoch": 1.0111524163568772,
      "grad_norm": 14.038132667541504,
      "learning_rate": 0.000153,
      "loss": 3.8194,
      "step": 153
    },
    {
      "epoch": 1.0177612556794713,
      "grad_norm": 2.8885278701782227,
      "learning_rate": 0.000154,
      "loss": 2.4215,
      "step": 154
    },
    {
      "epoch": 1.0243700950020653,
      "grad_norm": 11.783538818359375,
      "learning_rate": 0.000155,
      "loss": 2.2033,
      "step": 155
    },
    {
      "epoch": 1.0309789343246591,
      "grad_norm": 6.7096781730651855,
      "learning_rate": 0.000156,
      "loss": 4.8383,
      "step": 156
    },
    {
      "epoch": 1.0375877736472532,
      "grad_norm": 2.8861472606658936,
      "learning_rate": 0.000157,
      "loss": 1.9193,
      "step": 157
    },
    {
      "epoch": 1.0441966129698472,
      "grad_norm": 13.96933650970459,
      "learning_rate": 0.000158,
      "loss": 9.1163,
      "step": 158
    },
    {
      "epoch": 1.050805452292441,
      "grad_norm": 18.747814178466797,
      "learning_rate": 0.00015900000000000002,
      "loss": 7.6328,
      "step": 159
    },
    {
      "epoch": 1.057414291615035,
      "grad_norm": 25.288814544677734,
      "learning_rate": 0.00016,
      "loss": 6.1429,
      "step": 160
    },
    {
      "epoch": 1.0640231309376291,
      "grad_norm": 14.10446548461914,
      "learning_rate": 0.000161,
      "loss": 5.7742,
      "step": 161
    },
    {
      "epoch": 1.070631970260223,
      "grad_norm": 10.463335037231445,
      "learning_rate": 0.000162,
      "loss": 5.9731,
      "step": 162
    },
    {
      "epoch": 1.077240809582817,
      "grad_norm": 17.1007022857666,
      "learning_rate": 0.000163,
      "loss": 3.1578,
      "step": 163
    },
    {
      "epoch": 1.083849648905411,
      "grad_norm": 4.537623882293701,
      "learning_rate": 0.000164,
      "loss": 6.9073,
      "step": 164
    },
    {
      "epoch": 1.090458488228005,
      "grad_norm": 14.290506362915039,
      "learning_rate": 0.000165,
      "loss": 6.4466,
      "step": 165
    },
    {
      "epoch": 1.0970673275505989,
      "grad_norm": 21.19244956970215,
      "learning_rate": 0.00016600000000000002,
      "loss": 6.5266,
      "step": 166
    },
    {
      "epoch": 1.103676166873193,
      "grad_norm": 7.676146984100342,
      "learning_rate": 0.00016700000000000002,
      "loss": 1.6644,
      "step": 167
    },
    {
      "epoch": 1.110285006195787,
      "grad_norm": 25.04113006591797,
      "learning_rate": 0.00016800000000000002,
      "loss": 3.3631,
      "step": 168
    },
    {
      "epoch": 1.1168938455183808,
      "grad_norm": 12.540902137756348,
      "learning_rate": 0.00016900000000000002,
      "loss": 4.5376,
      "step": 169
    },
    {
      "epoch": 1.1235026848409748,
      "grad_norm": 19.373750686645508,
      "learning_rate": 0.00017,
      "loss": 3.7215,
      "step": 170
    },
    {
      "epoch": 1.1301115241635689,
      "grad_norm": 10.90507984161377,
      "learning_rate": 0.000171,
      "loss": 4.1645,
      "step": 171
    },
    {
      "epoch": 1.1367203634861627,
      "grad_norm": 9.371406555175781,
      "learning_rate": 0.00017199999999999998,
      "loss": 5.0032,
      "step": 172
    },
    {
      "epoch": 1.1433292028087567,
      "grad_norm": 20.51617431640625,
      "learning_rate": 0.000173,
      "loss": 6.223,
      "step": 173
    },
    {
      "epoch": 1.1499380421313508,
      "grad_norm": 4.056406497955322,
      "learning_rate": 0.000174,
      "loss": 3.1102,
      "step": 174
    },
    {
      "epoch": 1.1565468814539446,
      "grad_norm": 8.09680461883545,
      "learning_rate": 0.000175,
      "loss": 2.5442,
      "step": 175
    },
    {
      "epoch": 1.1631557207765386,
      "grad_norm": 18.386775970458984,
      "learning_rate": 0.000176,
      "loss": 3.8809,
      "step": 176
    },
    {
      "epoch": 1.1697645600991327,
      "grad_norm": 18.955673217773438,
      "learning_rate": 0.000177,
      "loss": 6.0321,
      "step": 177
    },
    {
      "epoch": 1.1763733994217265,
      "grad_norm": 9.91081714630127,
      "learning_rate": 0.000178,
      "loss": 3.5868,
      "step": 178
    },
    {
      "epoch": 1.1829822387443205,
      "grad_norm": 5.445245742797852,
      "learning_rate": 0.000179,
      "loss": 4.8431,
      "step": 179
    },
    {
      "epoch": 1.1895910780669146,
      "grad_norm": 1.679659128189087,
      "learning_rate": 0.00017999999999999998,
      "loss": 2.6377,
      "step": 180
    },
    {
      "epoch": 1.1961999173895084,
      "grad_norm": 8.27901840209961,
      "learning_rate": 0.000181,
      "loss": 4.9671,
      "step": 181
    },
    {
      "epoch": 1.2028087567121024,
      "grad_norm": 4.387126922607422,
      "learning_rate": 0.000182,
      "loss": 5.2091,
      "step": 182
    },
    {
      "epoch": 1.2094175960346965,
      "grad_norm": 10.992569923400879,
      "learning_rate": 0.000183,
      "loss": 2.9804,
      "step": 183
    },
    {
      "epoch": 1.2160264353572905,
      "grad_norm": 7.370335578918457,
      "learning_rate": 0.000184,
      "loss": 6.1407,
      "step": 184
    },
    {
      "epoch": 1.2226352746798843,
      "grad_norm": 12.327367782592773,
      "learning_rate": 0.000185,
      "loss": 2.9071,
      "step": 185
    },
    {
      "epoch": 1.2292441140024783,
      "grad_norm": 13.808316230773926,
      "learning_rate": 0.000186,
      "loss": 4.5126,
      "step": 186
    },
    {
      "epoch": 1.2358529533250722,
      "grad_norm": 7.055421829223633,
      "learning_rate": 0.000187,
      "loss": 5.6231,
      "step": 187
    },
    {
      "epoch": 1.2424617926476662,
      "grad_norm": 3.297778367996216,
      "learning_rate": 0.00018800000000000002,
      "loss": 2.4105,
      "step": 188
    },
    {
      "epoch": 1.2490706319702602,
      "grad_norm": 3.5234248638153076,
      "learning_rate": 0.000189,
      "loss": 4.6064,
      "step": 189
    },
    {
      "epoch": 1.2556794712928543,
      "grad_norm": 5.6089372634887695,
      "learning_rate": 0.00019,
      "loss": 4.8616,
      "step": 190
    },
    {
      "epoch": 1.262288310615448,
      "grad_norm": 4.63966178894043,
      "learning_rate": 0.000191,
      "loss": 1.7962,
      "step": 191
    },
    {
      "epoch": 1.2688971499380421,
      "grad_norm": 8.024625778198242,
      "learning_rate": 0.000192,
      "loss": 4.3432,
      "step": 192
    },
    {
      "epoch": 1.275505989260636,
      "grad_norm": 17.52340316772461,
      "learning_rate": 0.000193,
      "loss": 4.4037,
      "step": 193
    },
    {
      "epoch": 1.28211482858323,
      "grad_norm": 10.835038185119629,
      "learning_rate": 0.000194,
      "loss": 4.3512,
      "step": 194
    },
    {
      "epoch": 1.288723667905824,
      "grad_norm": 3.6204049587249756,
      "learning_rate": 0.00019500000000000002,
      "loss": 1.4675,
      "step": 195
    },
    {
      "epoch": 1.295332507228418,
      "grad_norm": 16.020408630371094,
      "learning_rate": 0.00019600000000000002,
      "loss": 2.666,
      "step": 196
    },
    {
      "epoch": 1.301941346551012,
      "grad_norm": 3.9923648834228516,
      "learning_rate": 0.00019700000000000002,
      "loss": 3.0394,
      "step": 197
    },
    {
      "epoch": 1.308550185873606,
      "grad_norm": 17.611902236938477,
      "learning_rate": 0.00019800000000000002,
      "loss": 5.0589,
      "step": 198
    },
    {
      "epoch": 1.3151590251962,
      "grad_norm": 6.654346942901611,
      "learning_rate": 0.000199,
      "loss": 2.7166,
      "step": 199
    },
    {
      "epoch": 1.3217678645187938,
      "grad_norm": 8.89234733581543,
      "learning_rate": 0.0002,
      "loss": 4.228,
      "step": 200
    },
    {
      "epoch": 1.3283767038413878,
      "grad_norm": 12.81808853149414,
      "learning_rate": 0.000201,
      "loss": 3.5457,
      "step": 201
    },
    {
      "epoch": 1.3349855431639819,
      "grad_norm": 6.9085307121276855,
      "learning_rate": 0.000202,
      "loss": 2.435,
      "step": 202
    },
    {
      "epoch": 1.341594382486576,
      "grad_norm": 11.401111602783203,
      "learning_rate": 0.00020300000000000003,
      "loss": 4.2534,
      "step": 203
    },
    {
      "epoch": 1.3482032218091697,
      "grad_norm": 3.5113813877105713,
      "learning_rate": 0.000204,
      "loss": 2.6196,
      "step": 204
    },
    {
      "epoch": 1.3548120611317638,
      "grad_norm": 14.28053092956543,
      "learning_rate": 0.000205,
      "loss": 4.8305,
      "step": 205
    },
    {
      "epoch": 1.3614209004543576,
      "grad_norm": 9.523927688598633,
      "learning_rate": 0.000206,
      "loss": 3.8571,
      "step": 206
    },
    {
      "epoch": 1.3680297397769516,
      "grad_norm": 7.181851387023926,
      "learning_rate": 0.000207,
      "loss": 5.5602,
      "step": 207
    },
    {
      "epoch": 1.3746385790995457,
      "grad_norm": 10.796745300292969,
      "learning_rate": 0.000208,
      "loss": 2.1845,
      "step": 208
    },
    {
      "epoch": 1.3812474184221397,
      "grad_norm": 2.8647103309631348,
      "learning_rate": 0.00020899999999999998,
      "loss": 2.1245,
      "step": 209
    },
    {
      "epoch": 1.3878562577447335,
      "grad_norm": 5.438963413238525,
      "learning_rate": 0.00021,
      "loss": 3.7944,
      "step": 210
    },
    {
      "epoch": 1.3944650970673276,
      "grad_norm": 11.935525894165039,
      "learning_rate": 0.000211,
      "loss": 8.0196,
      "step": 211
    },
    {
      "epoch": 1.4010739363899214,
      "grad_norm": 16.447921752929688,
      "learning_rate": 0.000212,
      "loss": 5.3209,
      "step": 212
    },
    {
      "epoch": 1.4076827757125154,
      "grad_norm": 5.0088276863098145,
      "learning_rate": 0.000213,
      "loss": 1.9993,
      "step": 213
    },
    {
      "epoch": 1.4142916150351095,
      "grad_norm": 18.047996520996094,
      "learning_rate": 0.000214,
      "loss": 3.988,
      "step": 214
    },
    {
      "epoch": 1.4209004543577035,
      "grad_norm": 21.064041137695312,
      "learning_rate": 0.000215,
      "loss": 5.7719,
      "step": 215
    },
    {
      "epoch": 1.4275092936802973,
      "grad_norm": 19.307058334350586,
      "learning_rate": 0.000216,
      "loss": 7.831,
      "step": 216
    },
    {
      "epoch": 1.4341181330028914,
      "grad_norm": 12.835506439208984,
      "learning_rate": 0.00021700000000000002,
      "loss": 1.4738,
      "step": 217
    },
    {
      "epoch": 1.4407269723254854,
      "grad_norm": 4.571415424346924,
      "learning_rate": 0.000218,
      "loss": 3.2932,
      "step": 218
    },
    {
      "epoch": 1.4473358116480792,
      "grad_norm": 11.67414665222168,
      "learning_rate": 0.000219,
      "loss": 2.5114,
      "step": 219
    },
    {
      "epoch": 1.4539446509706733,
      "grad_norm": 17.630908966064453,
      "learning_rate": 0.00022,
      "loss": 3.1841,
      "step": 220
    },
    {
      "epoch": 1.4605534902932673,
      "grad_norm": 12.235572814941406,
      "learning_rate": 0.000221,
      "loss": 1.6305,
      "step": 221
    },
    {
      "epoch": 1.4671623296158613,
      "grad_norm": 13.773517608642578,
      "learning_rate": 0.000222,
      "loss": 3.9127,
      "step": 222
    },
    {
      "epoch": 1.4737711689384552,
      "grad_norm": 13.466087341308594,
      "learning_rate": 0.000223,
      "loss": 4.7729,
      "step": 223
    },
    {
      "epoch": 1.4803800082610492,
      "grad_norm": 10.358912467956543,
      "learning_rate": 0.000224,
      "loss": 4.4135,
      "step": 224
    },
    {
      "epoch": 1.486988847583643,
      "grad_norm": 19.29961395263672,
      "learning_rate": 0.00022500000000000002,
      "loss": 3.2965,
      "step": 225
    },
    {
      "epoch": 1.493597686906237,
      "grad_norm": 28.499794006347656,
      "learning_rate": 0.00022600000000000002,
      "loss": 8.1544,
      "step": 226
    },
    {
      "epoch": 1.500206526228831,
      "grad_norm": 17.040237426757812,
      "learning_rate": 0.00022700000000000002,
      "loss": 2.7396,
      "step": 227
    },
    {
      "epoch": 1.5068153655514251,
      "grad_norm": 13.98397445678711,
      "learning_rate": 0.000228,
      "loss": 2.471,
      "step": 228
    },
    {
      "epoch": 1.513424204874019,
      "grad_norm": 11.42086410522461,
      "learning_rate": 0.000229,
      "loss": 4.7848,
      "step": 229
    },
    {
      "epoch": 1.520033044196613,
      "grad_norm": 23.38556671142578,
      "learning_rate": 0.00023,
      "loss": 5.8737,
      "step": 230
    },
    {
      "epoch": 1.5266418835192068,
      "grad_norm": 23.99523162841797,
      "learning_rate": 0.000231,
      "loss": 3.3035,
      "step": 231
    },
    {
      "epoch": 1.5332507228418009,
      "grad_norm": 15.304787635803223,
      "learning_rate": 0.00023200000000000003,
      "loss": 4.9743,
      "step": 232
    },
    {
      "epoch": 1.539859562164395,
      "grad_norm": 11.319670677185059,
      "learning_rate": 0.00023300000000000003,
      "loss": 3.1549,
      "step": 233
    },
    {
      "epoch": 1.546468401486989,
      "grad_norm": 18.541217803955078,
      "learning_rate": 0.00023400000000000002,
      "loss": 3.4967,
      "step": 234
    },
    {
      "epoch": 1.553077240809583,
      "grad_norm": 26.242895126342773,
      "learning_rate": 0.000235,
      "loss": 4.2506,
      "step": 235
    },
    {
      "epoch": 1.5596860801321768,
      "grad_norm": 4.198838233947754,
      "learning_rate": 0.000236,
      "loss": 1.7831,
      "step": 236
    },
    {
      "epoch": 1.5662949194547706,
      "grad_norm": 7.520010948181152,
      "learning_rate": 0.000237,
      "loss": 1.8787,
      "step": 237
    },
    {
      "epoch": 1.5729037587773647,
      "grad_norm": 19.989089965820312,
      "learning_rate": 0.00023799999999999998,
      "loss": 3.6531,
      "step": 238
    },
    {
      "epoch": 1.5795125980999587,
      "grad_norm": 39.881996154785156,
      "learning_rate": 0.00023899999999999998,
      "loss": 9.6029,
      "step": 239
    },
    {
      "epoch": 1.5861214374225527,
      "grad_norm": 24.598663330078125,
      "learning_rate": 0.00024,
      "loss": 5.0935,
      "step": 240
    },
    {
      "epoch": 1.5927302767451468,
      "grad_norm": 20.1588077545166,
      "learning_rate": 0.000241,
      "loss": 4.3642,
      "step": 241
    },
    {
      "epoch": 1.5993391160677406,
      "grad_norm": 17.521814346313477,
      "learning_rate": 0.000242,
      "loss": 4.706,
      "step": 242
    },
    {
      "epoch": 1.6059479553903344,
      "grad_norm": 4.8933539390563965,
      "learning_rate": 0.000243,
      "loss": 4.4073,
      "step": 243
    },
    {
      "epoch": 1.6125567947129285,
      "grad_norm": 6.533393383026123,
      "learning_rate": 0.000244,
      "loss": 3.0704,
      "step": 244
    },
    {
      "epoch": 1.6191656340355225,
      "grad_norm": 15.329740524291992,
      "learning_rate": 0.000245,
      "loss": 3.7294,
      "step": 245
    },
    {
      "epoch": 1.6257744733581165,
      "grad_norm": 7.863405704498291,
      "learning_rate": 0.000246,
      "loss": 2.3134,
      "step": 246
    },
    {
      "epoch": 1.6323833126807106,
      "grad_norm": 22.102664947509766,
      "learning_rate": 0.000247,
      "loss": 6.0971,
      "step": 247
    },
    {
      "epoch": 1.6389921520033044,
      "grad_norm": 8.980663299560547,
      "learning_rate": 0.000248,
      "loss": 3.6153,
      "step": 248
    },
    {
      "epoch": 1.6456009913258984,
      "grad_norm": 5.0459136962890625,
      "learning_rate": 0.000249,
      "loss": 4.9948,
      "step": 249
    },
    {
      "epoch": 1.6522098306484923,
      "grad_norm": 13.428962707519531,
      "learning_rate": 0.00025,
      "loss": 10.687,
      "step": 250
    },
    {
      "epoch": 1.6588186699710863,
      "grad_norm": 22.692407608032227,
      "learning_rate": 0.00025100000000000003,
      "loss": 4.7764,
      "step": 251
    },
    {
      "epoch": 1.6654275092936803,
      "grad_norm": 11.736815452575684,
      "learning_rate": 0.000252,
      "loss": 3.4217,
      "step": 252
    },
    {
      "epoch": 1.6720363486162744,
      "grad_norm": 20.544837951660156,
      "learning_rate": 0.000253,
      "loss": 3.678,
      "step": 253
    },
    {
      "epoch": 1.6786451879388682,
      "grad_norm": 11.943928718566895,
      "learning_rate": 0.000254,
      "loss": 2.0493,
      "step": 254
    },
    {
      "epoch": 1.6852540272614622,
      "grad_norm": 9.43830680847168,
      "learning_rate": 0.000255,
      "loss": 3.8512,
      "step": 255
    },
    {
      "epoch": 1.691862866584056,
      "grad_norm": 17.71424102783203,
      "learning_rate": 0.000256,
      "loss": 5.9227,
      "step": 256
    },
    {
      "epoch": 1.69847170590665,
      "grad_norm": 3.989474296569824,
      "learning_rate": 0.000257,
      "loss": 3.4456,
      "step": 257
    },
    {
      "epoch": 1.7050805452292441,
      "grad_norm": 18.969614028930664,
      "learning_rate": 0.00025800000000000004,
      "loss": 3.3926,
      "step": 258
    },
    {
      "epoch": 1.7116893845518382,
      "grad_norm": 13.683009147644043,
      "learning_rate": 0.000259,
      "loss": 3.6172,
      "step": 259
    },
    {
      "epoch": 1.7182982238744322,
      "grad_norm": 36.66935729980469,
      "learning_rate": 0.00026000000000000003,
      "loss": 12.1935,
      "step": 260
    },
    {
      "epoch": 1.724907063197026,
      "grad_norm": 14.368993759155273,
      "learning_rate": 0.000261,
      "loss": 4.2252,
      "step": 261
    },
    {
      "epoch": 1.7315159025196198,
      "grad_norm": 3.1088340282440186,
      "learning_rate": 0.000262,
      "loss": 4.1544,
      "step": 262
    },
    {
      "epoch": 1.7381247418422139,
      "grad_norm": 10.956188201904297,
      "learning_rate": 0.000263,
      "loss": 1.655,
      "step": 263
    },
    {
      "epoch": 1.744733581164808,
      "grad_norm": 16.131277084350586,
      "learning_rate": 0.000264,
      "loss": 3.0707,
      "step": 264
    },
    {
      "epoch": 1.751342420487402,
      "grad_norm": 7.999231338500977,
      "learning_rate": 0.00026500000000000004,
      "loss": 2.9158,
      "step": 265
    },
    {
      "epoch": 1.757951259809996,
      "grad_norm": 7.589114665985107,
      "learning_rate": 0.000266,
      "loss": 5.7878,
      "step": 266
    },
    {
      "epoch": 1.7645600991325898,
      "grad_norm": 19.792871475219727,
      "learning_rate": 0.00026700000000000004,
      "loss": 3.8226,
      "step": 267
    },
    {
      "epoch": 1.7711689384551839,
      "grad_norm": 25.981643676757812,
      "learning_rate": 0.000268,
      "loss": 7.1657,
      "step": 268
    },
    {
      "epoch": 1.7777777777777777,
      "grad_norm": 20.03606414794922,
      "learning_rate": 0.00026900000000000003,
      "loss": 4.3381,
      "step": 269
    },
    {
      "epoch": 1.7843866171003717,
      "grad_norm": 9.212739944458008,
      "learning_rate": 0.00027,
      "loss": 4.2593,
      "step": 270
    },
    {
      "epoch": 1.7909954564229658,
      "grad_norm": 8.471381187438965,
      "learning_rate": 0.00027100000000000003,
      "loss": 1.6425,
      "step": 271
    },
    {
      "epoch": 1.7976042957455598,
      "grad_norm": 4.708523273468018,
      "learning_rate": 0.00027200000000000005,
      "loss": 4.8605,
      "step": 272
    },
    {
      "epoch": 1.8042131350681536,
      "grad_norm": 5.102662086486816,
      "learning_rate": 0.000273,
      "loss": 1.7445,
      "step": 273
    },
    {
      "epoch": 1.8108219743907477,
      "grad_norm": 3.473365068435669,
      "learning_rate": 0.00027400000000000005,
      "loss": 1.6728,
      "step": 274
    },
    {
      "epoch": 1.8174308137133415,
      "grad_norm": 26.18375587463379,
      "learning_rate": 0.000275,
      "loss": 3.4037,
      "step": 275
    },
    {
      "epoch": 1.8240396530359355,
      "grad_norm": 12.676069259643555,
      "learning_rate": 0.00027600000000000004,
      "loss": 2.1683,
      "step": 276
    },
    {
      "epoch": 1.8306484923585296,
      "grad_norm": 3.4928572177886963,
      "learning_rate": 0.000277,
      "loss": 3.3716,
      "step": 277
    },
    {
      "epoch": 1.8372573316811236,
      "grad_norm": 4.785274505615234,
      "learning_rate": 0.00027800000000000004,
      "loss": 5.4323,
      "step": 278
    },
    {
      "epoch": 1.8438661710037176,
      "grad_norm": 2.9717495441436768,
      "learning_rate": 0.000279,
      "loss": 1.6405,
      "step": 279
    },
    {
      "epoch": 1.8504750103263115,
      "grad_norm": 8.639911651611328,
      "learning_rate": 0.00028000000000000003,
      "loss": 2.4278,
      "step": 280
    },
    {
      "epoch": 1.8570838496489053,
      "grad_norm": 6.391762733459473,
      "learning_rate": 0.00028100000000000005,
      "loss": 3.4367,
      "step": 281
    },
    {
      "epoch": 1.8636926889714993,
      "grad_norm": 11.785794258117676,
      "learning_rate": 0.00028199999999999997,
      "loss": 5.2499,
      "step": 282
    },
    {
      "epoch": 1.8703015282940934,
      "grad_norm": 16.382089614868164,
      "learning_rate": 0.000283,
      "loss": 8.9253,
      "step": 283
    },
    {
      "epoch": 1.8769103676166874,
      "grad_norm": 17.624990463256836,
      "learning_rate": 0.00028399999999999996,
      "loss": 6.1489,
      "step": 284
    },
    {
      "epoch": 1.8835192069392814,
      "grad_norm": 18.348052978515625,
      "learning_rate": 0.000285,
      "loss": 4.5116,
      "step": 285
    },
    {
      "epoch": 1.8901280462618752,
      "grad_norm": 15.962248802185059,
      "learning_rate": 0.00028599999999999996,
      "loss": 3.6849,
      "step": 286
    },
    {
      "epoch": 1.896736885584469,
      "grad_norm": 12.41230297088623,
      "learning_rate": 0.000287,
      "loss": 3.5705,
      "step": 287
    },
    {
      "epoch": 1.903345724907063,
      "grad_norm": 23.58102035522461,
      "learning_rate": 0.000288,
      "loss": 6.8281,
      "step": 288
    },
    {
      "epoch": 1.9099545642296571,
      "grad_norm": 22.72820472717285,
      "learning_rate": 0.000289,
      "loss": 3.8412,
      "step": 289
    },
    {
      "epoch": 1.9165634035522512,
      "grad_norm": 18.543439865112305,
      "learning_rate": 0.00029,
      "loss": 3.0713,
      "step": 290
    },
    {
      "epoch": 1.9231722428748452,
      "grad_norm": 12.92105770111084,
      "learning_rate": 0.00029099999999999997,
      "loss": 3.8257,
      "step": 291
    },
    {
      "epoch": 1.929781082197439,
      "grad_norm": 4.880592346191406,
      "learning_rate": 0.000292,
      "loss": 5.516,
      "step": 292
    },
    {
      "epoch": 1.936389921520033,
      "grad_norm": 11.095439910888672,
      "learning_rate": 0.00029299999999999997,
      "loss": 2.2127,
      "step": 293
    },
    {
      "epoch": 1.942998760842627,
      "grad_norm": 21.117870330810547,
      "learning_rate": 0.000294,
      "loss": 6.3122,
      "step": 294
    },
    {
      "epoch": 1.949607600165221,
      "grad_norm": 8.287490844726562,
      "learning_rate": 0.000295,
      "loss": 3.7239,
      "step": 295
    },
    {
      "epoch": 1.956216439487815,
      "grad_norm": 4.969232559204102,
      "learning_rate": 0.000296,
      "loss": 1.8293,
      "step": 296
    },
    {
      "epoch": 1.962825278810409,
      "grad_norm": 17.50139808654785,
      "learning_rate": 0.000297,
      "loss": 6.288,
      "step": 297
    },
    {
      "epoch": 1.9694341181330028,
      "grad_norm": 8.849578857421875,
      "learning_rate": 0.000298,
      "loss": 3.2186,
      "step": 298
    },
    {
      "epoch": 1.9760429574555969,
      "grad_norm": 3.3673038482666016,
      "learning_rate": 0.000299,
      "loss": 1.2888,
      "step": 299
    },
    {
      "epoch": 1.9826517967781907,
      "grad_norm": 4.223812103271484,
      "learning_rate": 0.0003,
      "loss": 3.3045,
      "step": 300
    },
    {
      "epoch": 1.9892606361007847,
      "grad_norm": 12.70642375946045,
      "learning_rate": 0.000301,
      "loss": 4.7668,
      "step": 301
    },
    {
      "epoch": 1.9958694754233788,
      "grad_norm": 23.63724708557129,
      "learning_rate": 0.000302,
      "loss": 3.4982,
      "step": 302
    },
    {
      "epoch": 1.9958694754233788,
      "eval_validation_error_bar": 0.051147802365939896,
      "eval_validation_loss": 6.695489406585693,
      "eval_validation_pearsonr": 0.5225464676157386,
      "eval_validation_rmse": 2.587564468383789,
      "eval_validation_runtime": 28.5406,
      "eval_validation_samples_per_second": 7.113,
      "eval_validation_spearman": 0.5742927843948468,
      "eval_validation_steps_per_second": 7.113,
      "step": 302
    },
    {
      "epoch": 1.9958694754233788,
      "eval_test_error_bar": 0.049849305915037716,
      "eval_test_loss": 9.352303504943848,
      "eval_test_pearsonr": 0.36927131815476233,
      "eval_test_rmse": 3.0581536293029785,
      "eval_test_runtime": 45.8163,
      "eval_test_samples_per_second": 7.115,
      "eval_test_spearman": 0.3637940234214226,
      "eval_test_steps_per_second": 7.115,
      "step": 302
    },
    {
      "epoch": 2.002478314745973,
      "grad_norm": 18.703550338745117,
      "learning_rate": 0.000303,
      "loss": 4.3792,
      "step": 303
    },
    {
      "epoch": 2.009087154068567,
      "grad_norm": 17.975515365600586,
      "learning_rate": 0.000304,
      "loss": 3.9345,
      "step": 304
    },
    {
      "epoch": 2.0156959933911605,
      "grad_norm": 14.904781341552734,
      "learning_rate": 0.000305,
      "loss": 3.2138,
      "step": 305
    },
    {
      "epoch": 2.0223048327137545,
      "grad_norm": 14.644723892211914,
      "learning_rate": 0.000306,
      "loss": 4.2205,
      "step": 306
    },
    {
      "epoch": 2.0289136720363485,
      "grad_norm": 25.03099250793457,
      "learning_rate": 0.000307,
      "loss": 7.7427,
      "step": 307
    },
    {
      "epoch": 2.0355225113589426,
      "grad_norm": 22.62333869934082,
      "learning_rate": 0.000308,
      "loss": 4.3769,
      "step": 308
    },
    {
      "epoch": 2.0421313506815366,
      "grad_norm": 9.055937767028809,
      "learning_rate": 0.00030900000000000003,
      "loss": 2.6954,
      "step": 309
    },
    {
      "epoch": 2.0487401900041307,
      "grad_norm": 6.27015495300293,
      "learning_rate": 0.00031,
      "loss": 2.1156,
      "step": 310
    },
    {
      "epoch": 2.0553490293267247,
      "grad_norm": 9.177102088928223,
      "learning_rate": 0.000311,
      "loss": 2.4988,
      "step": 311
    },
    {
      "epoch": 2.0619578686493183,
      "grad_norm": 2.7642009258270264,
      "learning_rate": 0.000312,
      "loss": 1.684,
      "step": 312
    },
    {
      "epoch": 2.0685667079719123,
      "grad_norm": 8.702192306518555,
      "learning_rate": 0.000313,
      "loss": 2.4054,
      "step": 313
    },
    {
      "epoch": 2.0751755472945064,
      "grad_norm": 8.507268905639648,
      "learning_rate": 0.000314,
      "loss": 10.0344,
      "step": 314
    },
    {
      "epoch": 2.0817843866171004,
      "grad_norm": 11.196288108825684,
      "learning_rate": 0.000315,
      "loss": 1.5585,
      "step": 315
    },
    {
      "epoch": 2.0883932259396945,
      "grad_norm": 19.948272705078125,
      "learning_rate": 0.000316,
      "loss": 5.0367,
      "step": 316
    },
    {
      "epoch": 2.0950020652622885,
      "grad_norm": 17.87925148010254,
      "learning_rate": 0.000317,
      "loss": 8.2824,
      "step": 317
    },
    {
      "epoch": 2.101610904584882,
      "grad_norm": 4.38392448425293,
      "learning_rate": 0.00031800000000000003,
      "loss": 2.9183,
      "step": 318
    },
    {
      "epoch": 2.108219743907476,
      "grad_norm": 5.99341344833374,
      "learning_rate": 0.000319,
      "loss": 2.944,
      "step": 319
    },
    {
      "epoch": 2.11482858323007,
      "grad_norm": 6.85900354385376,
      "learning_rate": 0.00032,
      "loss": 2.5646,
      "step": 320
    },
    {
      "epoch": 2.121437422552664,
      "grad_norm": 5.56048583984375,
      "learning_rate": 0.000321,
      "loss": 2.6658,
      "step": 321
    },
    {
      "epoch": 2.1280462618752582,
      "grad_norm": 15.290207862854004,
      "learning_rate": 0.000322,
      "loss": 5.9555,
      "step": 322
    },
    {
      "epoch": 2.1346551011978523,
      "grad_norm": 4.1606764793396,
      "learning_rate": 0.000323,
      "loss": 1.5746,
      "step": 323
    },
    {
      "epoch": 2.141263940520446,
      "grad_norm": 3.065084934234619,
      "learning_rate": 0.000324,
      "loss": 1.9625,
      "step": 324
    },
    {
      "epoch": 2.14787277984304,
      "grad_norm": 2.865459442138672,
      "learning_rate": 0.00032500000000000004,
      "loss": 1.4431,
      "step": 325
    },
    {
      "epoch": 2.154481619165634,
      "grad_norm": 5.421199321746826,
      "learning_rate": 0.000326,
      "loss": 1.5882,
      "step": 326
    },
    {
      "epoch": 2.161090458488228,
      "grad_norm": 12.513401985168457,
      "learning_rate": 0.00032700000000000003,
      "loss": 11.8665,
      "step": 327
    },
    {
      "epoch": 2.167699297810822,
      "grad_norm": 11.751251220703125,
      "learning_rate": 0.000328,
      "loss": 1.5981,
      "step": 328
    },
    {
      "epoch": 2.174308137133416,
      "grad_norm": 4.586784362792969,
      "learning_rate": 0.00032900000000000003,
      "loss": 5.6022,
      "step": 329
    },
    {
      "epoch": 2.18091697645601,
      "grad_norm": 25.882307052612305,
      "learning_rate": 0.00033,
      "loss": 4.4027,
      "step": 330
    },
    {
      "epoch": 2.1875258157786037,
      "grad_norm": 7.799858093261719,
      "learning_rate": 0.000331,
      "loss": 4.8443,
      "step": 331
    },
    {
      "epoch": 2.1941346551011978,
      "grad_norm": 20.307716369628906,
      "learning_rate": 0.00033200000000000005,
      "loss": 10.2231,
      "step": 332
    },
    {
      "epoch": 2.200743494423792,
      "grad_norm": 5.383415699005127,
      "learning_rate": 0.000333,
      "loss": 4.7366,
      "step": 333
    },
    {
      "epoch": 2.207352333746386,
      "grad_norm": 3.346407890319824,
      "learning_rate": 0.00033400000000000004,
      "loss": 1.4654,
      "step": 334
    },
    {
      "epoch": 2.21396117306898,
      "grad_norm": 12.73434829711914,
      "learning_rate": 0.000335,
      "loss": 2.0694,
      "step": 335
    },
    {
      "epoch": 2.220570012391574,
      "grad_norm": 4.4843363761901855,
      "learning_rate": 0.00033600000000000004,
      "loss": 2.1797,
      "step": 336
    },
    {
      "epoch": 2.2271788517141675,
      "grad_norm": 17.908418655395508,
      "learning_rate": 0.000337,
      "loss": 2.0263,
      "step": 337
    },
    {
      "epoch": 2.2337876910367616,
      "grad_norm": 4.0609283447265625,
      "learning_rate": 0.00033800000000000003,
      "loss": 2.585,
      "step": 338
    },
    {
      "epoch": 2.2403965303593556,
      "grad_norm": 8.410004615783691,
      "learning_rate": 0.00033900000000000005,
      "loss": 3.6182,
      "step": 339
    },
    {
      "epoch": 2.2470053696819496,
      "grad_norm": 6.80307149887085,
      "learning_rate": 0.00034,
      "loss": 1.6886,
      "step": 340
    },
    {
      "epoch": 2.2536142090045437,
      "grad_norm": 15.26396656036377,
      "learning_rate": 0.00034100000000000005,
      "loss": 4.4748,
      "step": 341
    },
    {
      "epoch": 2.2602230483271377,
      "grad_norm": 5.135189056396484,
      "learning_rate": 0.000342,
      "loss": 3.1883,
      "step": 342
    },
    {
      "epoch": 2.2668318876497313,
      "grad_norm": 11.283451080322266,
      "learning_rate": 0.00034300000000000004,
      "loss": 2.6821,
      "step": 343
    },
    {
      "epoch": 2.2734407269723254,
      "grad_norm": 12.048463821411133,
      "learning_rate": 0.00034399999999999996,
      "loss": 2.8991,
      "step": 344
    },
    {
      "epoch": 2.2800495662949194,
      "grad_norm": 7.717197418212891,
      "learning_rate": 0.000345,
      "loss": 2.6353,
      "step": 345
    },
    {
      "epoch": 2.2866584056175134,
      "grad_norm": 2.6548287868499756,
      "learning_rate": 0.000346,
      "loss": 0.9084,
      "step": 346
    },
    {
      "epoch": 2.2932672449401075,
      "grad_norm": 5.407320976257324,
      "learning_rate": 0.000347,
      "loss": 2.8213,
      "step": 347
    },
    {
      "epoch": 2.2998760842627015,
      "grad_norm": 11.059679985046387,
      "learning_rate": 0.000348,
      "loss": 2.6958,
      "step": 348
    },
    {
      "epoch": 2.3064849235852956,
      "grad_norm": 9.207076072692871,
      "learning_rate": 0.00034899999999999997,
      "loss": 1.5753,
      "step": 349
    },
    {
      "epoch": 2.313093762907889,
      "grad_norm": 12.234478950500488,
      "learning_rate": 0.00035,
      "loss": 2.4925,
      "step": 350
    },
    {
      "epoch": 2.319702602230483,
      "grad_norm": 4.731940746307373,
      "learning_rate": 0.00035099999999999997,
      "loss": 3.6852,
      "step": 351
    },
    {
      "epoch": 2.3263114415530772,
      "grad_norm": 13.800137519836426,
      "learning_rate": 0.000352,
      "loss": 4.9245,
      "step": 352
    },
    {
      "epoch": 2.3329202808756713,
      "grad_norm": 7.960721969604492,
      "learning_rate": 0.00035299999999999996,
      "loss": 5.2659,
      "step": 353
    },
    {
      "epoch": 2.3395291201982653,
      "grad_norm": 4.628958225250244,
      "learning_rate": 0.000354,
      "loss": 3.413,
      "step": 354
    },
    {
      "epoch": 2.346137959520859,
      "grad_norm": 15.879509925842285,
      "learning_rate": 0.000355,
      "loss": 4.7963,
      "step": 355
    },
    {
      "epoch": 2.352746798843453,
      "grad_norm": 7.185343265533447,
      "learning_rate": 0.000356,
      "loss": 4.8365,
      "step": 356
    },
    {
      "epoch": 2.359355638166047,
      "grad_norm": 5.695821285247803,
      "learning_rate": 0.000357,
      "loss": 2.4314,
      "step": 357
    },
    {
      "epoch": 2.365964477488641,
      "grad_norm": 8.25489330291748,
      "learning_rate": 0.000358,
      "loss": 2.3035,
      "step": 358
    },
    {
      "epoch": 2.372573316811235,
      "grad_norm": 13.064390182495117,
      "learning_rate": 0.000359,
      "loss": 1.9134,
      "step": 359
    },
    {
      "epoch": 2.379182156133829,
      "grad_norm": 16.200782775878906,
      "learning_rate": 0.00035999999999999997,
      "loss": 3.1106,
      "step": 360
    },
    {
      "epoch": 2.385790995456423,
      "grad_norm": 2.7522010803222656,
      "learning_rate": 0.000361,
      "loss": 1.6649,
      "step": 361
    },
    {
      "epoch": 2.3923998347790167,
      "grad_norm": 14.276188850402832,
      "learning_rate": 0.000362,
      "loss": 2.2696,
      "step": 362
    },
    {
      "epoch": 2.399008674101611,
      "grad_norm": 8.25570297241211,
      "learning_rate": 0.000363,
      "loss": 3.4425,
      "step": 363
    },
    {
      "epoch": 2.405617513424205,
      "grad_norm": 6.691510200500488,
      "learning_rate": 0.000364,
      "loss": 1.8147,
      "step": 364
    },
    {
      "epoch": 2.412226352746799,
      "grad_norm": 11.238167762756348,
      "learning_rate": 0.000365,
      "loss": 2.2727,
      "step": 365
    },
    {
      "epoch": 2.418835192069393,
      "grad_norm": 20.193674087524414,
      "learning_rate": 0.000366,
      "loss": 5.0727,
      "step": 366
    },
    {
      "epoch": 2.425444031391987,
      "grad_norm": 15.805102348327637,
      "learning_rate": 0.000367,
      "loss": 2.5777,
      "step": 367
    },
    {
      "epoch": 2.432052870714581,
      "grad_norm": 15.195159912109375,
      "learning_rate": 0.000368,
      "loss": 2.8169,
      "step": 368
    },
    {
      "epoch": 2.4386617100371746,
      "grad_norm": 7.461302757263184,
      "learning_rate": 0.000369,
      "loss": 3.4571,
      "step": 369
    },
    {
      "epoch": 2.4452705493597686,
      "grad_norm": 6.439919948577881,
      "learning_rate": 0.00037,
      "loss": 1.0437,
      "step": 370
    },
    {
      "epoch": 2.4518793886823627,
      "grad_norm": 32.52141571044922,
      "learning_rate": 0.000371,
      "loss": 5.1968,
      "step": 371
    },
    {
      "epoch": 2.4584882280049567,
      "grad_norm": 24.44283103942871,
      "learning_rate": 0.000372,
      "loss": 2.3048,
      "step": 372
    },
    {
      "epoch": 2.4650970673275507,
      "grad_norm": 3.4613473415374756,
      "learning_rate": 0.000373,
      "loss": 4.0618,
      "step": 373
    },
    {
      "epoch": 2.4717059066501443,
      "grad_norm": 30.248491287231445,
      "learning_rate": 0.000374,
      "loss": 4.6122,
      "step": 374
    },
    {
      "epoch": 2.4783147459727384,
      "grad_norm": 36.857810974121094,
      "learning_rate": 0.000375,
      "loss": 6.295,
      "step": 375
    },
    {
      "epoch": 2.4849235852953324,
      "grad_norm": 15.288082122802734,
      "learning_rate": 0.00037600000000000003,
      "loss": 2.9128,
      "step": 376
    },
    {
      "epoch": 2.4915324246179265,
      "grad_norm": 11.262530326843262,
      "learning_rate": 0.000377,
      "loss": 2.3981,
      "step": 377
    },
    {
      "epoch": 2.4981412639405205,
      "grad_norm": 8.249707221984863,
      "learning_rate": 0.000378,
      "loss": 7.277,
      "step": 378
    },
    {
      "epoch": 2.5047501032631145,
      "grad_norm": 6.906107425689697,
      "learning_rate": 0.000379,
      "loss": 4.194,
      "step": 379
    },
    {
      "epoch": 2.5113589425857086,
      "grad_norm": 15.312091827392578,
      "learning_rate": 0.00038,
      "loss": 5.7828,
      "step": 380
    },
    {
      "epoch": 2.517967781908302,
      "grad_norm": 21.67150115966797,
      "learning_rate": 0.000381,
      "loss": 4.8982,
      "step": 381
    },
    {
      "epoch": 2.524576621230896,
      "grad_norm": 24.58738136291504,
      "learning_rate": 0.000382,
      "loss": 6.5531,
      "step": 382
    },
    {
      "epoch": 2.5311854605534903,
      "grad_norm": 11.240447998046875,
      "learning_rate": 0.00038300000000000004,
      "loss": 4.7151,
      "step": 383
    },
    {
      "epoch": 2.5377942998760843,
      "grad_norm": 12.444747924804688,
      "learning_rate": 0.000384,
      "loss": 4.6039,
      "step": 384
    },
    {
      "epoch": 2.5444031391986783,
      "grad_norm": 15.972332954406738,
      "learning_rate": 0.00038500000000000003,
      "loss": 3.1816,
      "step": 385
    },
    {
      "epoch": 2.551011978521272,
      "grad_norm": 6.008676528930664,
      "learning_rate": 0.000386,
      "loss": 2.6835,
      "step": 386
    },
    {
      "epoch": 2.5576208178438664,
      "grad_norm": 37.771766662597656,
      "learning_rate": 0.00038700000000000003,
      "loss": 5.8001,
      "step": 387
    },
    {
      "epoch": 2.56422965716646,
      "grad_norm": 34.87837600708008,
      "learning_rate": 0.000388,
      "loss": 10.3979,
      "step": 388
    },
    {
      "epoch": 2.570838496489054,
      "grad_norm": 14.167595863342285,
      "learning_rate": 0.000389,
      "loss": 3.9647,
      "step": 389
    },
    {
      "epoch": 2.577447335811648,
      "grad_norm": 13.224630355834961,
      "learning_rate": 0.00039000000000000005,
      "loss": 3.835,
      "step": 390
    },
    {
      "epoch": 2.584056175134242,
      "grad_norm": 12.78200912475586,
      "learning_rate": 0.000391,
      "loss": 1.3011,
      "step": 391
    },
    {
      "epoch": 2.590665014456836,
      "grad_norm": 4.626237869262695,
      "learning_rate": 0.00039200000000000004,
      "loss": 5.5389,
      "step": 392
    },
    {
      "epoch": 2.5972738537794298,
      "grad_norm": 15.538337707519531,
      "learning_rate": 0.000393,
      "loss": 3.8067,
      "step": 393
    },
    {
      "epoch": 2.603882693102024,
      "grad_norm": 10.846513748168945,
      "learning_rate": 0.00039400000000000004,
      "loss": 3.4899,
      "step": 394
    },
    {
      "epoch": 2.610491532424618,
      "grad_norm": 7.150426864624023,
      "learning_rate": 0.000395,
      "loss": 3.9539,
      "step": 395
    },
    {
      "epoch": 2.617100371747212,
      "grad_norm": 12.173439979553223,
      "learning_rate": 0.00039600000000000003,
      "loss": 3.2748,
      "step": 396
    },
    {
      "epoch": 2.623709211069806,
      "grad_norm": 22.871414184570312,
      "learning_rate": 0.00039700000000000005,
      "loss": 5.3965,
      "step": 397
    },
    {
      "epoch": 2.6303180503924,
      "grad_norm": 27.364931106567383,
      "learning_rate": 0.000398,
      "loss": 6.122,
      "step": 398
    },
    {
      "epoch": 2.636926889714994,
      "grad_norm": 22.31220245361328,
      "learning_rate": 0.00039900000000000005,
      "loss": 2.3195,
      "step": 399
    },
    {
      "epoch": 2.6435357290375876,
      "grad_norm": 16.246862411499023,
      "learning_rate": 0.0004,
      "loss": 3.5971,
      "step": 400
    },
    {
      "epoch": 2.6501445683601816,
      "grad_norm": 7.93843936920166,
      "learning_rate": 0.00040100000000000004,
      "loss": 4.1468,
      "step": 401
    },
    {
      "epoch": 2.6567534076827757,
      "grad_norm": 12.39631462097168,
      "learning_rate": 0.000402,
      "loss": 3.0865,
      "step": 402
    },
    {
      "epoch": 2.6633622470053697,
      "grad_norm": 3.360212564468384,
      "learning_rate": 0.00040300000000000004,
      "loss": 3.2113,
      "step": 403
    },
    {
      "epoch": 2.6699710863279638,
      "grad_norm": 17.262378692626953,
      "learning_rate": 0.000404,
      "loss": 3.0973,
      "step": 404
    },
    {
      "epoch": 2.6765799256505574,
      "grad_norm": 17.89883041381836,
      "learning_rate": 0.00040500000000000003,
      "loss": 4.1952,
      "step": 405
    },
    {
      "epoch": 2.683188764973152,
      "grad_norm": 9.630233764648438,
      "learning_rate": 0.00040600000000000006,
      "loss": 2.5633,
      "step": 406
    },
    {
      "epoch": 2.6897976042957454,
      "grad_norm": 9.163969039916992,
      "learning_rate": 0.00040699999999999997,
      "loss": 3.0836,
      "step": 407
    },
    {
      "epoch": 2.6964064436183395,
      "grad_norm": 10.19416618347168,
      "learning_rate": 0.000408,
      "loss": 1.5889,
      "step": 408
    },
    {
      "epoch": 2.7030152829409335,
      "grad_norm": 7.721503734588623,
      "learning_rate": 0.00040899999999999997,
      "loss": 3.1906,
      "step": 409
    },
    {
      "epoch": 2.7096241222635276,
      "grad_norm": 6.1781840324401855,
      "learning_rate": 0.00041,
      "loss": 3.0541,
      "step": 410
    },
    {
      "epoch": 2.7162329615861216,
      "grad_norm": 5.906196594238281,
      "learning_rate": 0.00041099999999999996,
      "loss": 1.15,
      "step": 411
    },
    {
      "epoch": 2.722841800908715,
      "grad_norm": 12.684672355651855,
      "learning_rate": 0.000412,
      "loss": 2.6559,
      "step": 412
    },
    {
      "epoch": 2.7294506402313092,
      "grad_norm": 4.270727634429932,
      "learning_rate": 0.000413,
      "loss": 3.8813,
      "step": 413
    },
    {
      "epoch": 2.7360594795539033,
      "grad_norm": 17.017093658447266,
      "learning_rate": 0.000414,
      "loss": 5.2044,
      "step": 414
    },
    {
      "epoch": 2.7426683188764973,
      "grad_norm": 3.639636516571045,
      "learning_rate": 0.000415,
      "loss": 1.7406,
      "step": 415
    },
    {
      "epoch": 2.7492771581990914,
      "grad_norm": 24.090417861938477,
      "learning_rate": 0.000416,
      "loss": 3.6168,
      "step": 416
    },
    {
      "epoch": 2.7558859975216854,
      "grad_norm": 6.0813398361206055,
      "learning_rate": 0.000417,
      "loss": 2.1125,
      "step": 417
    },
    {
      "epoch": 2.7624948368442794,
      "grad_norm": 5.096335411071777,
      "learning_rate": 0.00041799999999999997,
      "loss": 3.741,
      "step": 418
    },
    {
      "epoch": 2.769103676166873,
      "grad_norm": 3.0644359588623047,
      "learning_rate": 0.000419,
      "loss": 1.0598,
      "step": 419
    },
    {
      "epoch": 2.775712515489467,
      "grad_norm": 6.900911808013916,
      "learning_rate": 0.00042,
      "loss": 3.4349,
      "step": 420
    },
    {
      "epoch": 2.782321354812061,
      "grad_norm": 4.489596366882324,
      "learning_rate": 0.000421,
      "loss": 2.0234,
      "step": 421
    },
    {
      "epoch": 2.788930194134655,
      "grad_norm": 20.707542419433594,
      "learning_rate": 0.000422,
      "loss": 8.4497,
      "step": 422
    },
    {
      "epoch": 2.795539033457249,
      "grad_norm": 2.43642520904541,
      "learning_rate": 0.000423,
      "loss": 2.9962,
      "step": 423
    },
    {
      "epoch": 2.802147872779843,
      "grad_norm": 12.966730117797852,
      "learning_rate": 0.000424,
      "loss": 1.6715,
      "step": 424
    },
    {
      "epoch": 2.8087567121024373,
      "grad_norm": 4.45335054397583,
      "learning_rate": 0.000425,
      "loss": 1.0973,
      "step": 425
    },
    {
      "epoch": 2.815365551425031,
      "grad_norm": 2.909074544906616,
      "learning_rate": 0.000426,
      "loss": 1.8934,
      "step": 426
    },
    {
      "epoch": 2.821974390747625,
      "grad_norm": 10.782883644104004,
      "learning_rate": 0.000427,
      "loss": 5.1579,
      "step": 427
    },
    {
      "epoch": 2.828583230070219,
      "grad_norm": 10.20760726928711,
      "learning_rate": 0.000428,
      "loss": 4.5935,
      "step": 428
    },
    {
      "epoch": 2.835192069392813,
      "grad_norm": 9.564854621887207,
      "learning_rate": 0.000429,
      "loss": 3.0813,
      "step": 429
    },
    {
      "epoch": 2.841800908715407,
      "grad_norm": 12.39664363861084,
      "learning_rate": 0.00043,
      "loss": 2.6329,
      "step": 430
    },
    {
      "epoch": 2.8484097480380006,
      "grad_norm": 7.339967727661133,
      "learning_rate": 0.000431,
      "loss": 1.3818,
      "step": 431
    },
    {
      "epoch": 2.8550185873605947,
      "grad_norm": 4.95277738571167,
      "learning_rate": 0.000432,
      "loss": 5.8802,
      "step": 432
    },
    {
      "epoch": 2.8616274266831887,
      "grad_norm": 4.055518627166748,
      "learning_rate": 0.000433,
      "loss": 3.052,
      "step": 433
    },
    {
      "epoch": 2.8682362660057827,
      "grad_norm": 8.504851341247559,
      "learning_rate": 0.00043400000000000003,
      "loss": 1.9342,
      "step": 434
    },
    {
      "epoch": 2.874845105328377,
      "grad_norm": 5.89473819732666,
      "learning_rate": 0.000435,
      "loss": 2.8045,
      "step": 435
    },
    {
      "epoch": 2.881453944650971,
      "grad_norm": 12.079147338867188,
      "learning_rate": 0.000436,
      "loss": 2.8453,
      "step": 436
    },
    {
      "epoch": 2.888062783973565,
      "grad_norm": 12.539996147155762,
      "learning_rate": 0.000437,
      "loss": 2.0052,
      "step": 437
    },
    {
      "epoch": 2.8946716232961585,
      "grad_norm": 7.995632648468018,
      "learning_rate": 0.000438,
      "loss": 2.0637,
      "step": 438
    },
    {
      "epoch": 2.9012804626187525,
      "grad_norm": 19.121179580688477,
      "learning_rate": 0.000439,
      "loss": 5.0276,
      "step": 439
    },
    {
      "epoch": 2.9078893019413465,
      "grad_norm": 3.8724381923675537,
      "learning_rate": 0.00044,
      "loss": 1.4778,
      "step": 440
    },
    {
      "epoch": 2.9144981412639406,
      "grad_norm": 10.367012977600098,
      "learning_rate": 0.000441,
      "loss": 1.925,
      "step": 441
    },
    {
      "epoch": 2.9211069805865346,
      "grad_norm": 7.825291156768799,
      "learning_rate": 0.000442,
      "loss": 2.3896,
      "step": 442
    },
    {
      "epoch": 2.927715819909128,
      "grad_norm": 5.714006423950195,
      "learning_rate": 0.00044300000000000003,
      "loss": 1.6249,
      "step": 443
    },
    {
      "epoch": 2.9343246592317227,
      "grad_norm": 6.139378070831299,
      "learning_rate": 0.000444,
      "loss": 1.9734,
      "step": 444
    },
    {
      "epoch": 2.9409334985543163,
      "grad_norm": 7.162132263183594,
      "learning_rate": 0.00044500000000000003,
      "loss": 3.5937,
      "step": 445
    },
    {
      "epoch": 2.9475423378769103,
      "grad_norm": 18.60979652404785,
      "learning_rate": 0.000446,
      "loss": 3.0349,
      "step": 446
    },
    {
      "epoch": 2.9541511771995044,
      "grad_norm": 8.351168632507324,
      "learning_rate": 0.000447,
      "loss": 6.8053,
      "step": 447
    },
    {
      "epoch": 2.9607600165220984,
      "grad_norm": 3.3530259132385254,
      "learning_rate": 0.000448,
      "loss": 2.2076,
      "step": 448
    },
    {
      "epoch": 2.9673688558446925,
      "grad_norm": 2.050079584121704,
      "learning_rate": 0.000449,
      "loss": 1.8335,
      "step": 449
    },
    {
      "epoch": 2.973977695167286,
      "grad_norm": 6.368945598602295,
      "learning_rate": 0.00045000000000000004,
      "loss": 2.582,
      "step": 450
    },
    {
      "epoch": 2.98058653448988,
      "grad_norm": 17.835451126098633,
      "learning_rate": 0.000451,
      "loss": 6.5415,
      "step": 451
    },
    {
      "epoch": 2.987195373812474,
      "grad_norm": 7.939521789550781,
      "learning_rate": 0.00045200000000000004,
      "loss": 2.1425,
      "step": 452
    },
    {
      "epoch": 2.993804213135068,
      "grad_norm": 3.4324214458465576,
      "learning_rate": 0.000453,
      "loss": 2.566,
      "step": 453
    },
    {
      "epoch": 2.993804213135068,
      "eval_validation_error_bar": 0.044061530097136606,
      "eval_validation_loss": 5.6085591316223145,
      "eval_validation_pearsonr": 0.6275133481083676,
      "eval_validation_rmse": 2.368239641189575,
      "eval_validation_runtime": 28.5631,
      "eval_validation_samples_per_second": 7.107,
      "eval_validation_spearman": 0.6597776502681006,
      "eval_validation_steps_per_second": 7.107,
      "step": 453
    },
    {
      "epoch": 2.993804213135068,
      "eval_test_error_bar": 0.03944993533089407,
      "eval_test_loss": 6.600106716156006,
      "eval_test_pearsonr": 0.5752939869699176,
      "eval_test_rmse": 2.5690672397613525,
      "eval_test_runtime": 45.7828,
      "eval_test_samples_per_second": 7.121,
      "eval_test_spearman": 0.5875910013454027,
      "eval_test_steps_per_second": 7.121,
      "step": 453
    },
    {
      "epoch": 3.000413052457662,
      "grad_norm": 2.9334545135498047,
      "learning_rate": 0.00045400000000000003,
      "loss": 1.7802,
      "step": 454
    },
    {
      "epoch": 3.0070218917802563,
      "grad_norm": 9.938041687011719,
      "learning_rate": 0.000455,
      "loss": 2.4457,
      "step": 455
    },
    {
      "epoch": 3.01363073110285,
      "grad_norm": 6.333775043487549,
      "learning_rate": 0.000456,
      "loss": 2.4839,
      "step": 456
    },
    {
      "epoch": 3.020239570425444,
      "grad_norm": 4.341709613800049,
      "learning_rate": 0.00045700000000000005,
      "loss": 4.5851,
      "step": 457
    },
    {
      "epoch": 3.026848409748038,
      "grad_norm": 13.633971214294434,
      "learning_rate": 0.000458,
      "loss": 6.0801,
      "step": 458
    },
    {
      "epoch": 3.033457249070632,
      "grad_norm": 15.919368743896484,
      "learning_rate": 0.00045900000000000004,
      "loss": 1.668,
      "step": 459
    },
    {
      "epoch": 3.040066088393226,
      "grad_norm": 20.364770889282227,
      "learning_rate": 0.00046,
      "loss": 4.2413,
      "step": 460
    },
    {
      "epoch": 3.04667492771582,
      "grad_norm": 7.975681304931641,
      "learning_rate": 0.00046100000000000004,
      "loss": 2.2336,
      "step": 461
    },
    {
      "epoch": 3.053283767038414,
      "grad_norm": 19.001155853271484,
      "learning_rate": 0.000462,
      "loss": 3.5245,
      "step": 462
    },
    {
      "epoch": 3.0598926063610077,
      "grad_norm": 5.490711688995361,
      "learning_rate": 0.00046300000000000003,
      "loss": 2.2702,
      "step": 463
    },
    {
      "epoch": 3.0665014456836017,
      "grad_norm": 23.742712020874023,
      "learning_rate": 0.00046400000000000006,
      "loss": 6.0376,
      "step": 464
    },
    {
      "epoch": 3.0731102850061958,
      "grad_norm": 7.651044845581055,
      "learning_rate": 0.000465,
      "loss": 1.4186,
      "step": 465
    },
    {
      "epoch": 3.07971912432879,
      "grad_norm": 6.345998764038086,
      "learning_rate": 0.00046600000000000005,
      "loss": 1.7157,
      "step": 466
    },
    {
      "epoch": 3.086327963651384,
      "grad_norm": 4.286613941192627,
      "learning_rate": 0.000467,
      "loss": 3.7962,
      "step": 467
    },
    {
      "epoch": 3.092936802973978,
      "grad_norm": 2.4312877655029297,
      "learning_rate": 0.00046800000000000005,
      "loss": 3.2208,
      "step": 468
    },
    {
      "epoch": 3.0995456422965715,
      "grad_norm": 2.610250473022461,
      "learning_rate": 0.00046899999999999996,
      "loss": 1.5253,
      "step": 469
    },
    {
      "epoch": 3.1061544816191655,
      "grad_norm": 5.858495235443115,
      "learning_rate": 0.00047,
      "loss": 1.7899,
      "step": 470
    },
    {
      "epoch": 3.1127633209417596,
      "grad_norm": 15.303365707397461,
      "learning_rate": 0.000471,
      "loss": 6.3493,
      "step": 471
    },
    {
      "epoch": 3.1193721602643536,
      "grad_norm": 10.275395393371582,
      "learning_rate": 0.000472,
      "loss": 2.1974,
      "step": 472
    },
    {
      "epoch": 3.1259809995869476,
      "grad_norm": 19.673246383666992,
      "learning_rate": 0.000473,
      "loss": 3.5719,
      "step": 473
    },
    {
      "epoch": 3.1325898389095417,
      "grad_norm": 21.137723922729492,
      "learning_rate": 0.000474,
      "loss": 4.064,
      "step": 474
    },
    {
      "epoch": 3.1391986782321353,
      "grad_norm": 22.601539611816406,
      "learning_rate": 0.000475,
      "loss": 2.6414,
      "step": 475
    },
    {
      "epoch": 3.1458075175547293,
      "grad_norm": 9.748615264892578,
      "learning_rate": 0.00047599999999999997,
      "loss": 3.4941,
      "step": 476
    },
    {
      "epoch": 3.1524163568773234,
      "grad_norm": 11.619293212890625,
      "learning_rate": 0.000477,
      "loss": 2.8872,
      "step": 477
    },
    {
      "epoch": 3.1590251961999174,
      "grad_norm": 9.910652160644531,
      "learning_rate": 0.00047799999999999996,
      "loss": 4.5471,
      "step": 478
    },
    {
      "epoch": 3.1656340355225114,
      "grad_norm": 31.12544059753418,
      "learning_rate": 0.000479,
      "loss": 9.3121,
      "step": 479
    },
    {
      "epoch": 3.1722428748451055,
      "grad_norm": 5.523082733154297,
      "learning_rate": 0.00048,
      "loss": 4.3646,
      "step": 480
    },
    {
      "epoch": 3.178851714167699,
      "grad_norm": 8.869428634643555,
      "learning_rate": 0.000481,
      "loss": 7.4463,
      "step": 481
    },
    {
      "epoch": 3.185460553490293,
      "grad_norm": 6.276830673217773,
      "learning_rate": 0.000482,
      "loss": 4.5024,
      "step": 482
    },
    {
      "epoch": 3.192069392812887,
      "grad_norm": 5.345962047576904,
      "learning_rate": 0.000483,
      "loss": 2.8966,
      "step": 483
    },
    {
      "epoch": 3.198678232135481,
      "grad_norm": 9.721397399902344,
      "learning_rate": 0.000484,
      "loss": 2.0489,
      "step": 484
    },
    {
      "epoch": 3.2052870714580752,
      "grad_norm": 6.695647716522217,
      "learning_rate": 0.00048499999999999997,
      "loss": 2.1481,
      "step": 485
    },
    {
      "epoch": 3.2118959107806693,
      "grad_norm": 9.283907890319824,
      "learning_rate": 0.000486,
      "loss": 2.5614,
      "step": 486
    },
    {
      "epoch": 3.2185047501032633,
      "grad_norm": 2.90702223777771,
      "learning_rate": 0.000487,
      "loss": 1.8452,
      "step": 487
    },
    {
      "epoch": 3.225113589425857,
      "grad_norm": 11.747685432434082,
      "learning_rate": 0.000488,
      "loss": 1.1485,
      "step": 488
    },
    {
      "epoch": 3.231722428748451,
      "grad_norm": 31.18033790588379,
      "learning_rate": 0.000489,
      "loss": 5.3275,
      "step": 489
    },
    {
      "epoch": 3.238331268071045,
      "grad_norm": 22.715717315673828,
      "learning_rate": 0.00049,
      "loss": 3.2618,
      "step": 490
    },
    {
      "epoch": 3.244940107393639,
      "grad_norm": 4.156953811645508,
      "learning_rate": 0.000491,
      "loss": 4.4457,
      "step": 491
    },
    {
      "epoch": 3.251548946716233,
      "grad_norm": 8.001054763793945,
      "learning_rate": 0.000492,
      "loss": 3.1221,
      "step": 492
    },
    {
      "epoch": 3.258157786038827,
      "grad_norm": 17.591571807861328,
      "learning_rate": 0.0004930000000000001,
      "loss": 2.2526,
      "step": 493
    },
    {
      "epoch": 3.264766625361421,
      "grad_norm": 4.600484848022461,
      "learning_rate": 0.000494,
      "loss": 4.9644,
      "step": 494
    },
    {
      "epoch": 3.2713754646840147,
      "grad_norm": 2.1887407302856445,
      "learning_rate": 0.000495,
      "loss": 1.0006,
      "step": 495
    },
    {
      "epoch": 3.277984304006609,
      "grad_norm": 4.762485980987549,
      "learning_rate": 0.000496,
      "loss": 3.7659,
      "step": 496
    },
    {
      "epoch": 3.284593143329203,
      "grad_norm": 13.652588844299316,
      "learning_rate": 0.000497,
      "loss": 2.4952,
      "step": 497
    },
    {
      "epoch": 3.291201982651797,
      "grad_norm": 2.99870228767395,
      "learning_rate": 0.000498,
      "loss": 2.3496,
      "step": 498
    },
    {
      "epoch": 3.297810821974391,
      "grad_norm": 15.184549331665039,
      "learning_rate": 0.000499,
      "loss": 4.4293,
      "step": 499
    },
    {
      "epoch": 3.3044196612969845,
      "grad_norm": 4.901586055755615,
      "learning_rate": 0.0005,
      "loss": 4.3306,
      "step": 500
    },
    {
      "epoch": 3.3110285006195785,
      "grad_norm": 11.621200561523438,
      "learning_rate": 0.000501,
      "loss": 2.0354,
      "step": 501
    },
    {
      "epoch": 3.3176373399421726,
      "grad_norm": 4.686629772186279,
      "learning_rate": 0.0005020000000000001,
      "loss": 4.5954,
      "step": 502
    },
    {
      "epoch": 3.3242461792647666,
      "grad_norm": 5.589987754821777,
      "learning_rate": 0.000503,
      "loss": 4.1055,
      "step": 503
    },
    {
      "epoch": 3.3308550185873607,
      "grad_norm": 18.28173828125,
      "learning_rate": 0.000504,
      "loss": 6.5096,
      "step": 504
    },
    {
      "epoch": 3.3374638579099547,
      "grad_norm": 13.692075729370117,
      "learning_rate": 0.000505,
      "loss": 3.6841,
      "step": 505
    },
    {
      "epoch": 3.3440726972325487,
      "grad_norm": 10.406557083129883,
      "learning_rate": 0.000506,
      "loss": 3.3714,
      "step": 506
    },
    {
      "epoch": 3.3506815365551423,
      "grad_norm": 30.65392303466797,
      "learning_rate": 0.000507,
      "loss": 7.1676,
      "step": 507
    },
    {
      "epoch": 3.3572903758777364,
      "grad_norm": 28.138837814331055,
      "learning_rate": 0.000508,
      "loss": 5.5201,
      "step": 508
    },
    {
      "epoch": 3.3638992152003304,
      "grad_norm": 13.454057693481445,
      "learning_rate": 0.000509,
      "loss": 7.4162,
      "step": 509
    },
    {
      "epoch": 3.3705080545229245,
      "grad_norm": 9.993574142456055,
      "learning_rate": 0.00051,
      "loss": 2.9651,
      "step": 510
    },
    {
      "epoch": 3.3771168938455185,
      "grad_norm": 20.85352325439453,
      "learning_rate": 0.0005110000000000001,
      "loss": 5.5451,
      "step": 511
    },
    {
      "epoch": 3.3837257331681125,
      "grad_norm": 20.208011627197266,
      "learning_rate": 0.000512,
      "loss": 3.8604,
      "step": 512
    },
    {
      "epoch": 3.390334572490706,
      "grad_norm": 22.92020606994629,
      "learning_rate": 0.000513,
      "loss": 2.8753,
      "step": 513
    },
    {
      "epoch": 3.3969434118133,
      "grad_norm": 17.53255271911621,
      "learning_rate": 0.000514,
      "loss": 2.9142,
      "step": 514
    },
    {
      "epoch": 3.403552251135894,
      "grad_norm": 16.115196228027344,
      "learning_rate": 0.000515,
      "loss": 3.4039,
      "step": 515
    },
    {
      "epoch": 3.4101610904584883,
      "grad_norm": 17.864683151245117,
      "learning_rate": 0.0005160000000000001,
      "loss": 2.9808,
      "step": 516
    },
    {
      "epoch": 3.4167699297810823,
      "grad_norm": 4.677165508270264,
      "learning_rate": 0.000517,
      "loss": 1.7328,
      "step": 517
    },
    {
      "epoch": 3.4233787691036763,
      "grad_norm": 13.396838188171387,
      "learning_rate": 0.000518,
      "loss": 2.8064,
      "step": 518
    },
    {
      "epoch": 3.42998760842627,
      "grad_norm": 12.562353134155273,
      "learning_rate": 0.000519,
      "loss": 2.2651,
      "step": 519
    },
    {
      "epoch": 3.436596447748864,
      "grad_norm": 7.437530994415283,
      "learning_rate": 0.0005200000000000001,
      "loss": 2.0784,
      "step": 520
    },
    {
      "epoch": 3.443205287071458,
      "grad_norm": 1.7641664743423462,
      "learning_rate": 0.000521,
      "loss": 0.756,
      "step": 521
    },
    {
      "epoch": 3.449814126394052,
      "grad_norm": 2.686786651611328,
      "learning_rate": 0.000522,
      "loss": 1.6686,
      "step": 522
    },
    {
      "epoch": 3.456422965716646,
      "grad_norm": 2.305577039718628,
      "learning_rate": 0.000523,
      "loss": 0.5629,
      "step": 523
    },
    {
      "epoch": 3.46303180503924,
      "grad_norm": 3.6883633136749268,
      "learning_rate": 0.000524,
      "loss": 2.1776,
      "step": 524
    },
    {
      "epoch": 3.469640644361834,
      "grad_norm": 14.946252822875977,
      "learning_rate": 0.0005250000000000001,
      "loss": 2.5631,
      "step": 525
    },
    {
      "epoch": 3.4762494836844278,
      "grad_norm": 7.961697578430176,
      "learning_rate": 0.000526,
      "loss": 5.1181,
      "step": 526
    },
    {
      "epoch": 3.482858323007022,
      "grad_norm": 2.523790121078491,
      "learning_rate": 0.000527,
      "loss": 2.1894,
      "step": 527
    },
    {
      "epoch": 3.489467162329616,
      "grad_norm": 8.655985832214355,
      "learning_rate": 0.000528,
      "loss": 1.6135,
      "step": 528
    },
    {
      "epoch": 3.49607600165221,
      "grad_norm": 3.2367069721221924,
      "learning_rate": 0.0005290000000000001,
      "loss": 2.4845,
      "step": 529
    },
    {
      "epoch": 3.502684840974804,
      "grad_norm": 7.391529083251953,
      "learning_rate": 0.0005300000000000001,
      "loss": 2.4154,
      "step": 530
    },
    {
      "epoch": 3.5092936802973975,
      "grad_norm": 14.505146980285645,
      "learning_rate": 0.000531,
      "loss": 4.7653,
      "step": 531
    },
    {
      "epoch": 3.515902519619992,
      "grad_norm": 6.526435375213623,
      "learning_rate": 0.000532,
      "loss": 1.3188,
      "step": 532
    },
    {
      "epoch": 3.5225113589425856,
      "grad_norm": 14.647379875183105,
      "learning_rate": 0.000533,
      "loss": 2.8217,
      "step": 533
    },
    {
      "epoch": 3.5291201982651796,
      "grad_norm": 17.093687057495117,
      "learning_rate": 0.0005340000000000001,
      "loss": 3.578,
      "step": 534
    },
    {
      "epoch": 3.5357290375877737,
      "grad_norm": 15.15427017211914,
      "learning_rate": 0.000535,
      "loss": 4.1482,
      "step": 535
    },
    {
      "epoch": 3.5423378769103677,
      "grad_norm": 3.5772106647491455,
      "learning_rate": 0.000536,
      "loss": 1.2872,
      "step": 536
    },
    {
      "epoch": 3.5489467162329618,
      "grad_norm": 3.7056519985198975,
      "learning_rate": 0.000537,
      "loss": 1.905,
      "step": 537
    },
    {
      "epoch": 3.5555555555555554,
      "grad_norm": 8.758008003234863,
      "learning_rate": 0.0005380000000000001,
      "loss": 3.5029,
      "step": 538
    },
    {
      "epoch": 3.5621643948781494,
      "grad_norm": 3.6642136573791504,
      "learning_rate": 0.0005390000000000001,
      "loss": 1.4167,
      "step": 539
    },
    {
      "epoch": 3.5687732342007434,
      "grad_norm": 9.083643913269043,
      "learning_rate": 0.00054,
      "loss": 5.7254,
      "step": 540
    },
    {
      "epoch": 3.5753820735233375,
      "grad_norm": 5.960034370422363,
      "learning_rate": 0.000541,
      "loss": 1.6112,
      "step": 541
    },
    {
      "epoch": 3.5819909128459315,
      "grad_norm": 30.778453826904297,
      "learning_rate": 0.0005420000000000001,
      "loss": 4.8479,
      "step": 542
    },
    {
      "epoch": 3.5885997521685256,
      "grad_norm": 22.301219940185547,
      "learning_rate": 0.0005430000000000001,
      "loss": 3.5105,
      "step": 543
    },
    {
      "epoch": 3.5952085914911196,
      "grad_norm": 26.226566314697266,
      "learning_rate": 0.0005440000000000001,
      "loss": 5.1696,
      "step": 544
    },
    {
      "epoch": 3.601817430813713,
      "grad_norm": 3.6014552116394043,
      "learning_rate": 0.000545,
      "loss": 1.883,
      "step": 545
    },
    {
      "epoch": 3.6084262701363072,
      "grad_norm": 26.9484806060791,
      "learning_rate": 0.000546,
      "loss": 7.3808,
      "step": 546
    },
    {
      "epoch": 3.6150351094589013,
      "grad_norm": 3.2192859649658203,
      "learning_rate": 0.0005470000000000001,
      "loss": 1.5486,
      "step": 547
    },
    {
      "epoch": 3.6216439487814953,
      "grad_norm": 7.319751739501953,
      "learning_rate": 0.0005480000000000001,
      "loss": 1.2721,
      "step": 548
    },
    {
      "epoch": 3.6282527881040894,
      "grad_norm": 3.308501958847046,
      "learning_rate": 0.000549,
      "loss": 1.1823,
      "step": 549
    },
    {
      "epoch": 3.634861627426683,
      "grad_norm": 6.894944190979004,
      "learning_rate": 0.00055,
      "loss": 3.9071,
      "step": 550
    },
    {
      "epoch": 3.6414704667492774,
      "grad_norm": 27.16449546813965,
      "learning_rate": 0.0005510000000000001,
      "loss": 4.0077,
      "step": 551
    },
    {
      "epoch": 3.648079306071871,
      "grad_norm": 39.109066009521484,
      "learning_rate": 0.0005520000000000001,
      "loss": 9.6608,
      "step": 552
    },
    {
      "epoch": 3.654688145394465,
      "grad_norm": 32.84786605834961,
      "learning_rate": 0.0005530000000000001,
      "loss": 5.9015,
      "step": 553
    },
    {
      "epoch": 3.661296984717059,
      "grad_norm": 45.30339431762695,
      "learning_rate": 0.000554,
      "loss": 8.1324,
      "step": 554
    },
    {
      "epoch": 3.667905824039653,
      "grad_norm": 33.4998664855957,
      "learning_rate": 0.000555,
      "loss": 4.851,
      "step": 555
    },
    {
      "epoch": 3.674514663362247,
      "grad_norm": 2.677546739578247,
      "learning_rate": 0.0005560000000000001,
      "loss": 1.8016,
      "step": 556
    },
    {
      "epoch": 3.681123502684841,
      "grad_norm": 27.250614166259766,
      "learning_rate": 0.0005570000000000001,
      "loss": 7.0921,
      "step": 557
    },
    {
      "epoch": 3.687732342007435,
      "grad_norm": 18.4937744140625,
      "learning_rate": 0.000558,
      "loss": 4.172,
      "step": 558
    },
    {
      "epoch": 3.694341181330029,
      "grad_norm": 11.87242317199707,
      "learning_rate": 0.000559,
      "loss": 2.0165,
      "step": 559
    },
    {
      "epoch": 3.700950020652623,
      "grad_norm": 8.04447078704834,
      "learning_rate": 0.0005600000000000001,
      "loss": 2.1229,
      "step": 560
    },
    {
      "epoch": 3.707558859975217,
      "grad_norm": 3.9544785022735596,
      "learning_rate": 0.0005610000000000001,
      "loss": 1.8959,
      "step": 561
    },
    {
      "epoch": 3.714167699297811,
      "grad_norm": 9.487521171569824,
      "learning_rate": 0.0005620000000000001,
      "loss": 3.8564,
      "step": 562
    },
    {
      "epoch": 3.720776538620405,
      "grad_norm": 18.35784912109375,
      "learning_rate": 0.0005629999999999999,
      "loss": 6.6271,
      "step": 563
    },
    {
      "epoch": 3.7273853779429986,
      "grad_norm": 6.970381259918213,
      "learning_rate": 0.0005639999999999999,
      "loss": 2.4598,
      "step": 564
    },
    {
      "epoch": 3.7339942172655927,
      "grad_norm": 11.360921859741211,
      "learning_rate": 0.000565,
      "loss": 2.7548,
      "step": 565
    },
    {
      "epoch": 3.7406030565881867,
      "grad_norm": 14.060375213623047,
      "learning_rate": 0.000566,
      "loss": 2.6174,
      "step": 566
    },
    {
      "epoch": 3.7472118959107807,
      "grad_norm": 2.431889772415161,
      "learning_rate": 0.000567,
      "loss": 1.2721,
      "step": 567
    },
    {
      "epoch": 3.753820735233375,
      "grad_norm": 3.783634662628174,
      "learning_rate": 0.0005679999999999999,
      "loss": 3.5128,
      "step": 568
    },
    {
      "epoch": 3.7604295745559684,
      "grad_norm": 7.389451503753662,
      "learning_rate": 0.000569,
      "loss": 1.7948,
      "step": 569
    },
    {
      "epoch": 3.7670384138785624,
      "grad_norm": 2.553067445755005,
      "learning_rate": 0.00057,
      "loss": 2.5279,
      "step": 570
    },
    {
      "epoch": 3.7736472532011565,
      "grad_norm": 15.667737007141113,
      "learning_rate": 0.000571,
      "loss": 4.0526,
      "step": 571
    },
    {
      "epoch": 3.7802560925237505,
      "grad_norm": 3.5625908374786377,
      "learning_rate": 0.0005719999999999999,
      "loss": 2.7684,
      "step": 572
    },
    {
      "epoch": 3.7868649318463445,
      "grad_norm": 9.361464500427246,
      "learning_rate": 0.0005729999999999999,
      "loss": 1.205,
      "step": 573
    },
    {
      "epoch": 3.7934737711689386,
      "grad_norm": 3.4569506645202637,
      "learning_rate": 0.000574,
      "loss": 2.0501,
      "step": 574
    },
    {
      "epoch": 3.8000826104915326,
      "grad_norm": 2.538386106491089,
      "learning_rate": 0.000575,
      "loss": 0.7272,
      "step": 575
    },
    {
      "epoch": 3.806691449814126,
      "grad_norm": 7.969133377075195,
      "learning_rate": 0.000576,
      "loss": 2.2918,
      "step": 576
    },
    {
      "epoch": 3.8133002891367203,
      "grad_norm": 18.991744995117188,
      "learning_rate": 0.0005769999999999999,
      "loss": 4.9372,
      "step": 577
    },
    {
      "epoch": 3.8199091284593143,
      "grad_norm": 19.09930419921875,
      "learning_rate": 0.000578,
      "loss": 2.9532,
      "step": 578
    },
    {
      "epoch": 3.8265179677819083,
      "grad_norm": 7.108707427978516,
      "learning_rate": 0.000579,
      "loss": 3.0541,
      "step": 579
    },
    {
      "epoch": 3.8331268071045024,
      "grad_norm": 3.28066086769104,
      "learning_rate": 0.00058,
      "loss": 4.0572,
      "step": 580
    },
    {
      "epoch": 3.839735646427096,
      "grad_norm": 9.863729476928711,
      "learning_rate": 0.0005809999999999999,
      "loss": 2.3758,
      "step": 581
    },
    {
      "epoch": 3.8463444857496905,
      "grad_norm": 6.682586669921875,
      "learning_rate": 0.0005819999999999999,
      "loss": 2.2662,
      "step": 582
    },
    {
      "epoch": 3.852953325072284,
      "grad_norm": 4.188199043273926,
      "learning_rate": 0.000583,
      "loss": 3.7152,
      "step": 583
    },
    {
      "epoch": 3.859562164394878,
      "grad_norm": 4.536573886871338,
      "learning_rate": 0.000584,
      "loss": 1.2731,
      "step": 584
    },
    {
      "epoch": 3.866171003717472,
      "grad_norm": 22.614072799682617,
      "learning_rate": 0.000585,
      "loss": 5.0804,
      "step": 585
    },
    {
      "epoch": 3.872779843040066,
      "grad_norm": 28.552688598632812,
      "learning_rate": 0.0005859999999999999,
      "loss": 3.7292,
      "step": 586
    },
    {
      "epoch": 3.87938868236266,
      "grad_norm": 15.715262413024902,
      "learning_rate": 0.000587,
      "loss": 3.1803,
      "step": 587
    },
    {
      "epoch": 3.885997521685254,
      "grad_norm": 2.3433098793029785,
      "learning_rate": 0.000588,
      "loss": 1.2722,
      "step": 588
    },
    {
      "epoch": 3.892606361007848,
      "grad_norm": 3.1081666946411133,
      "learning_rate": 0.000589,
      "loss": 3.1058,
      "step": 589
    },
    {
      "epoch": 3.899215200330442,
      "grad_norm": 9.160849571228027,
      "learning_rate": 0.00059,
      "loss": 4.1043,
      "step": 590
    },
    {
      "epoch": 3.905824039653036,
      "grad_norm": 20.14559555053711,
      "learning_rate": 0.0005909999999999999,
      "loss": 3.8227,
      "step": 591
    },
    {
      "epoch": 3.91243287897563,
      "grad_norm": 16.2360897064209,
      "learning_rate": 0.000592,
      "loss": 3.2144,
      "step": 592
    },
    {
      "epoch": 3.919041718298224,
      "grad_norm": 2.4249916076660156,
      "learning_rate": 0.000593,
      "loss": 1.0926,
      "step": 593
    },
    {
      "epoch": 3.925650557620818,
      "grad_norm": 6.55070686340332,
      "learning_rate": 0.000594,
      "loss": 2.1391,
      "step": 594
    },
    {
      "epoch": 3.9322593969434116,
      "grad_norm": 20.242700576782227,
      "learning_rate": 0.0005949999999999999,
      "loss": 4.004,
      "step": 595
    },
    {
      "epoch": 3.9388682362660057,
      "grad_norm": 13.390576362609863,
      "learning_rate": 0.000596,
      "loss": 2.342,
      "step": 596
    },
    {
      "epoch": 3.9454770755885997,
      "grad_norm": 9.47349739074707,
      "learning_rate": 0.000597,
      "loss": 1.9239,
      "step": 597
    },
    {
      "epoch": 3.9520859149111938,
      "grad_norm": 24.064186096191406,
      "learning_rate": 0.000598,
      "loss": 6.3329,
      "step": 598
    },
    {
      "epoch": 3.958694754233788,
      "grad_norm": 23.569774627685547,
      "learning_rate": 0.000599,
      "loss": 5.4667,
      "step": 599
    },
    {
      "epoch": 3.9653035935563814,
      "grad_norm": 26.01599884033203,
      "learning_rate": 0.0006,
      "loss": 6.4338,
      "step": 600
    },
    {
      "epoch": 3.971912432878976,
      "grad_norm": 15.618474006652832,
      "learning_rate": 0.000601,
      "loss": 6.9771,
      "step": 601
    },
    {
      "epoch": 3.9785212722015695,
      "grad_norm": 2.622739315032959,
      "learning_rate": 0.000602,
      "loss": 1.2566,
      "step": 602
    },
    {
      "epoch": 3.9851301115241635,
      "grad_norm": 32.36685562133789,
      "learning_rate": 0.000603,
      "loss": 6.2059,
      "step": 603
    },
    {
      "epoch": 3.9917389508467576,
      "grad_norm": 22.258325576782227,
      "learning_rate": 0.000604,
      "loss": 3.1256,
      "step": 604
    },
    {
      "epoch": 3.9983477901693516,
      "grad_norm": 28.564495086669922,
      "learning_rate": 0.000605,
      "loss": 5.9208,
      "step": 605
    },
    {
      "epoch": 3.9983477901693516,
      "eval_validation_error_bar": 0.040016643656583036,
      "eval_validation_loss": 8.7880859375,
      "eval_validation_pearsonr": 0.6491857250490861,
      "eval_validation_rmse": 2.964470624923706,
      "eval_validation_runtime": 28.541,
      "eval_validation_samples_per_second": 7.113,
      "eval_validation_spearman": 0.7022306699211228,
      "eval_validation_steps_per_second": 7.113,
      "step": 605
    },
    {
      "epoch": 3.9983477901693516,
      "eval_test_error_bar": 0.035502015549994416,
      "eval_test_loss": 8.114365577697754,
      "eval_test_pearsonr": 0.6409566695377837,
      "eval_test_rmse": 2.8485724925994873,
      "eval_test_runtime": 45.8986,
      "eval_test_samples_per_second": 7.103,
      "eval_test_spearman": 0.6480347419748573,
      "eval_test_steps_per_second": 7.103,
      "step": 605
    },
    {
      "epoch": 4.004956629491946,
      "grad_norm": 28.826053619384766,
      "learning_rate": 0.000606,
      "loss": 8.7582,
      "step": 606
    },
    {
      "epoch": 4.011565468814539,
      "grad_norm": 6.0052595138549805,
      "learning_rate": 0.000607,
      "loss": 5.2873,
      "step": 607
    },
    {
      "epoch": 4.018174308137134,
      "grad_norm": 4.462308883666992,
      "learning_rate": 0.000608,
      "loss": 2.4813,
      "step": 608
    },
    {
      "epoch": 4.024783147459727,
      "grad_norm": 7.217889785766602,
      "learning_rate": 0.000609,
      "loss": 2.6725,
      "step": 609
    },
    {
      "epoch": 4.031391986782321,
      "grad_norm": 11.411592483520508,
      "learning_rate": 0.00061,
      "loss": 2.6181,
      "step": 610
    },
    {
      "epoch": 4.038000826104915,
      "grad_norm": 9.634354591369629,
      "learning_rate": 0.000611,
      "loss": 2.2073,
      "step": 611
    },
    {
      "epoch": 4.044609665427509,
      "grad_norm": 9.205455780029297,
      "learning_rate": 0.000612,
      "loss": 1.4645,
      "step": 612
    },
    {
      "epoch": 4.0512185047501035,
      "grad_norm": 11.905104637145996,
      "learning_rate": 0.000613,
      "loss": 4.632,
      "step": 613
    },
    {
      "epoch": 4.057827344072697,
      "grad_norm": 21.663789749145508,
      "learning_rate": 0.000614,
      "loss": 2.798,
      "step": 614
    },
    {
      "epoch": 4.064436183395292,
      "grad_norm": 13.713974952697754,
      "learning_rate": 0.000615,
      "loss": 1.9894,
      "step": 615
    },
    {
      "epoch": 4.071045022717885,
      "grad_norm": 7.6898112297058105,
      "learning_rate": 0.000616,
      "loss": 3.3206,
      "step": 616
    },
    {
      "epoch": 4.077653862040479,
      "grad_norm": 4.310020446777344,
      "learning_rate": 0.000617,
      "loss": 1.313,
      "step": 617
    },
    {
      "epoch": 4.084262701363073,
      "grad_norm": 2.2648065090179443,
      "learning_rate": 0.0006180000000000001,
      "loss": 3.2881,
      "step": 618
    },
    {
      "epoch": 4.090871540685667,
      "grad_norm": 7.941735744476318,
      "learning_rate": 0.000619,
      "loss": 2.2303,
      "step": 619
    },
    {
      "epoch": 4.097480380008261,
      "grad_norm": 4.085305690765381,
      "learning_rate": 0.00062,
      "loss": 3.3164,
      "step": 620
    },
    {
      "epoch": 4.104089219330855,
      "grad_norm": 5.478601455688477,
      "learning_rate": 0.000621,
      "loss": 2.6439,
      "step": 621
    },
    {
      "epoch": 4.110698058653449,
      "grad_norm": 5.675262928009033,
      "learning_rate": 0.000622,
      "loss": 3.3833,
      "step": 622
    },
    {
      "epoch": 4.117306897976043,
      "grad_norm": 25.8039608001709,
      "learning_rate": 0.000623,
      "loss": 4.7442,
      "step": 623
    },
    {
      "epoch": 4.123915737298637,
      "grad_norm": 31.717409133911133,
      "learning_rate": 0.000624,
      "loss": 4.1456,
      "step": 624
    },
    {
      "epoch": 4.130524576621231,
      "grad_norm": 34.58102035522461,
      "learning_rate": 0.000625,
      "loss": 4.6286,
      "step": 625
    },
    {
      "epoch": 4.137133415943825,
      "grad_norm": 22.12694549560547,
      "learning_rate": 0.000626,
      "loss": 2.8869,
      "step": 626
    },
    {
      "epoch": 4.143742255266419,
      "grad_norm": 8.188796043395996,
      "learning_rate": 0.0006270000000000001,
      "loss": 2.013,
      "step": 627
    },
    {
      "epoch": 4.150351094589013,
      "grad_norm": 10.794617652893066,
      "learning_rate": 0.000628,
      "loss": 3.4668,
      "step": 628
    },
    {
      "epoch": 4.156959933911606,
      "grad_norm": 13.410228729248047,
      "learning_rate": 0.000629,
      "loss": 1.4705,
      "step": 629
    },
    {
      "epoch": 4.163568773234201,
      "grad_norm": 2.626436471939087,
      "learning_rate": 0.00063,
      "loss": 1.4529,
      "step": 630
    },
    {
      "epoch": 4.170177612556794,
      "grad_norm": 11.158178329467773,
      "learning_rate": 0.000631,
      "loss": 4.1064,
      "step": 631
    },
    {
      "epoch": 4.176786451879389,
      "grad_norm": 7.53707218170166,
      "learning_rate": 0.000632,
      "loss": 2.1085,
      "step": 632
    },
    {
      "epoch": 4.1833952912019825,
      "grad_norm": 16.56095314025879,
      "learning_rate": 0.000633,
      "loss": 2.5679,
      "step": 633
    },
    {
      "epoch": 4.190004130524577,
      "grad_norm": 12.092546463012695,
      "learning_rate": 0.000634,
      "loss": 3.7503,
      "step": 634
    },
    {
      "epoch": 4.196612969847171,
      "grad_norm": 12.870993614196777,
      "learning_rate": 0.000635,
      "loss": 1.5889,
      "step": 635
    },
    {
      "epoch": 4.203221809169764,
      "grad_norm": 15.230000495910645,
      "learning_rate": 0.0006360000000000001,
      "loss": 3.1772,
      "step": 636
    },
    {
      "epoch": 4.209830648492359,
      "grad_norm": 11.775079727172852,
      "learning_rate": 0.000637,
      "loss": 2.8032,
      "step": 637
    },
    {
      "epoch": 4.216439487814952,
      "grad_norm": 10.400849342346191,
      "learning_rate": 0.000638,
      "loss": 3.3177,
      "step": 638
    },
    {
      "epoch": 4.223048327137547,
      "grad_norm": 11.776761054992676,
      "learning_rate": 0.000639,
      "loss": 2.9936,
      "step": 639
    },
    {
      "epoch": 4.22965716646014,
      "grad_norm": 3.6932880878448486,
      "learning_rate": 0.00064,
      "loss": 2.958,
      "step": 640
    },
    {
      "epoch": 4.236266005782735,
      "grad_norm": 26.463041305541992,
      "learning_rate": 0.0006410000000000001,
      "loss": 4.8074,
      "step": 641
    },
    {
      "epoch": 4.242874845105328,
      "grad_norm": 13.103002548217773,
      "learning_rate": 0.000642,
      "loss": 1.5704,
      "step": 642
    },
    {
      "epoch": 4.249483684427922,
      "grad_norm": 3.345702648162842,
      "learning_rate": 0.000643,
      "loss": 1.8636,
      "step": 643
    },
    {
      "epoch": 4.2560925237505165,
      "grad_norm": 10.895915985107422,
      "learning_rate": 0.000644,
      "loss": 1.3392,
      "step": 644
    },
    {
      "epoch": 4.26270136307311,
      "grad_norm": 9.788516998291016,
      "learning_rate": 0.0006450000000000001,
      "loss": 1.0247,
      "step": 645
    },
    {
      "epoch": 4.269310202395705,
      "grad_norm": 8.610209465026855,
      "learning_rate": 0.000646,
      "loss": 2.0207,
      "step": 646
    },
    {
      "epoch": 4.275919041718298,
      "grad_norm": 6.92677640914917,
      "learning_rate": 0.000647,
      "loss": 1.7952,
      "step": 647
    },
    {
      "epoch": 4.282527881040892,
      "grad_norm": 10.233587265014648,
      "learning_rate": 0.000648,
      "loss": 3.0269,
      "step": 648
    },
    {
      "epoch": 4.289136720363486,
      "grad_norm": 5.079732894897461,
      "learning_rate": 0.0006490000000000001,
      "loss": 2.4335,
      "step": 649
    },
    {
      "epoch": 4.29574555968608,
      "grad_norm": 3.7418007850646973,
      "learning_rate": 0.0006500000000000001,
      "loss": 3.8477,
      "step": 650
    },
    {
      "epoch": 4.302354399008674,
      "grad_norm": 25.325151443481445,
      "learning_rate": 0.000651,
      "loss": 3.9867,
      "step": 651
    },
    {
      "epoch": 4.308963238331268,
      "grad_norm": 22.000608444213867,
      "learning_rate": 0.000652,
      "loss": 6.5303,
      "step": 652
    },
    {
      "epoch": 4.315572077653862,
      "grad_norm": 7.141408920288086,
      "learning_rate": 0.000653,
      "loss": 2.6289,
      "step": 653
    },
    {
      "epoch": 4.322180916976456,
      "grad_norm": 19.39324951171875,
      "learning_rate": 0.0006540000000000001,
      "loss": 3.7113,
      "step": 654
    },
    {
      "epoch": 4.32878975629905,
      "grad_norm": 20.3648681640625,
      "learning_rate": 0.0006550000000000001,
      "loss": 2.3684,
      "step": 655
    },
    {
      "epoch": 4.335398595621644,
      "grad_norm": 1.7572472095489502,
      "learning_rate": 0.000656,
      "loss": 1.2249,
      "step": 656
    },
    {
      "epoch": 4.342007434944238,
      "grad_norm": 4.270747184753418,
      "learning_rate": 0.000657,
      "loss": 3.5539,
      "step": 657
    },
    {
      "epoch": 4.348616274266832,
      "grad_norm": 7.8724164962768555,
      "learning_rate": 0.0006580000000000001,
      "loss": 2.6472,
      "step": 658
    },
    {
      "epoch": 4.355225113589426,
      "grad_norm": 2.7845025062561035,
      "learning_rate": 0.0006590000000000001,
      "loss": 2.0947,
      "step": 659
    },
    {
      "epoch": 4.36183395291202,
      "grad_norm": 17.766082763671875,
      "learning_rate": 0.00066,
      "loss": 3.5416,
      "step": 660
    },
    {
      "epoch": 4.368442792234614,
      "grad_norm": 28.630292892456055,
      "learning_rate": 0.000661,
      "loss": 6.8231,
      "step": 661
    },
    {
      "epoch": 4.375051631557207,
      "grad_norm": 10.777400970458984,
      "learning_rate": 0.000662,
      "loss": 1.3802,
      "step": 662
    },
    {
      "epoch": 4.381660470879802,
      "grad_norm": 9.026957511901855,
      "learning_rate": 0.0006630000000000001,
      "loss": 3.2052,
      "step": 663
    },
    {
      "epoch": 4.3882693102023955,
      "grad_norm": 16.134679794311523,
      "learning_rate": 0.0006640000000000001,
      "loss": 3.0035,
      "step": 664
    },
    {
      "epoch": 4.39487814952499,
      "grad_norm": 25.763032913208008,
      "learning_rate": 0.000665,
      "loss": 3.322,
      "step": 665
    },
    {
      "epoch": 4.401486988847584,
      "grad_norm": 26.2971248626709,
      "learning_rate": 0.000666,
      "loss": 2.8646,
      "step": 666
    },
    {
      "epoch": 4.408095828170177,
      "grad_norm": 8.126446723937988,
      "learning_rate": 0.0006670000000000001,
      "loss": 3.9648,
      "step": 667
    },
    {
      "epoch": 4.414704667492772,
      "grad_norm": 8.589643478393555,
      "learning_rate": 0.0006680000000000001,
      "loss": 2.6318,
      "step": 668
    },
    {
      "epoch": 4.421313506815365,
      "grad_norm": 8.380062103271484,
      "learning_rate": 0.0006690000000000001,
      "loss": 2.8473,
      "step": 669
    },
    {
      "epoch": 4.42792234613796,
      "grad_norm": 30.09082794189453,
      "learning_rate": 0.00067,
      "loss": 6.0337,
      "step": 670
    },
    {
      "epoch": 4.434531185460553,
      "grad_norm": 27.416973114013672,
      "learning_rate": 0.000671,
      "loss": 7.9895,
      "step": 671
    },
    {
      "epoch": 4.441140024783148,
      "grad_norm": 23.736156463623047,
      "learning_rate": 0.0006720000000000001,
      "loss": 3.4731,
      "step": 672
    },
    {
      "epoch": 4.447748864105741,
      "grad_norm": 18.651103973388672,
      "learning_rate": 0.0006730000000000001,
      "loss": 4.589,
      "step": 673
    },
    {
      "epoch": 4.454357703428335,
      "grad_norm": 16.971160888671875,
      "learning_rate": 0.000674,
      "loss": 2.574,
      "step": 674
    },
    {
      "epoch": 4.4609665427509295,
      "grad_norm": 15.366538047790527,
      "learning_rate": 0.000675,
      "loss": 2.0153,
      "step": 675
    },
    {
      "epoch": 4.467575382073523,
      "grad_norm": 31.60304069519043,
      "learning_rate": 0.0006760000000000001,
      "loss": 4.1165,
      "step": 676
    },
    {
      "epoch": 4.474184221396118,
      "grad_norm": 23.849943161010742,
      "learning_rate": 0.0006770000000000001,
      "loss": 3.5337,
      "step": 677
    },
    {
      "epoch": 4.480793060718711,
      "grad_norm": 17.3436279296875,
      "learning_rate": 0.0006780000000000001,
      "loss": 2.6684,
      "step": 678
    },
    {
      "epoch": 4.487401900041306,
      "grad_norm": 14.463078498840332,
      "learning_rate": 0.000679,
      "loss": 2.1035,
      "step": 679
    },
    {
      "epoch": 4.494010739363899,
      "grad_norm": 14.509608268737793,
      "learning_rate": 0.00068,
      "loss": 4.0112,
      "step": 680
    },
    {
      "epoch": 4.500619578686493,
      "grad_norm": 18.13889503479004,
      "learning_rate": 0.0006810000000000001,
      "loss": 2.8907,
      "step": 681
    },
    {
      "epoch": 4.507228418009087,
      "grad_norm": 4.121090412139893,
      "learning_rate": 0.0006820000000000001,
      "loss": 4.7364,
      "step": 682
    },
    {
      "epoch": 4.513837257331681,
      "grad_norm": 31.716474533081055,
      "learning_rate": 0.000683,
      "loss": 6.7962,
      "step": 683
    },
    {
      "epoch": 4.520446096654275,
      "grad_norm": 8.514046669006348,
      "learning_rate": 0.000684,
      "loss": 1.9011,
      "step": 684
    },
    {
      "epoch": 4.527054935976869,
      "grad_norm": 9.991146087646484,
      "learning_rate": 0.0006850000000000001,
      "loss": 0.775,
      "step": 685
    },
    {
      "epoch": 4.533663775299463,
      "grad_norm": 11.254150390625,
      "learning_rate": 0.0006860000000000001,
      "loss": 1.4631,
      "step": 686
    },
    {
      "epoch": 4.540272614622057,
      "grad_norm": 3.617269277572632,
      "learning_rate": 0.0006870000000000001,
      "loss": 3.5174,
      "step": 687
    },
    {
      "epoch": 4.546881453944651,
      "grad_norm": 26.848493576049805,
      "learning_rate": 0.0006879999999999999,
      "loss": 7.9288,
      "step": 688
    },
    {
      "epoch": 4.553490293267245,
      "grad_norm": 26.197429656982422,
      "learning_rate": 0.0006889999999999999,
      "loss": 4.2832,
      "step": 689
    },
    {
      "epoch": 4.560099132589839,
      "grad_norm": 22.664587020874023,
      "learning_rate": 0.00069,
      "loss": 7.3456,
      "step": 690
    },
    {
      "epoch": 4.566707971912432,
      "grad_norm": 11.794614791870117,
      "learning_rate": 0.000691,
      "loss": 1.9622,
      "step": 691
    },
    {
      "epoch": 4.573316811235027,
      "grad_norm": 28.442054748535156,
      "learning_rate": 0.000692,
      "loss": 7.3587,
      "step": 692
    },
    {
      "epoch": 4.5799256505576205,
      "grad_norm": 8.86270523071289,
      "learning_rate": 0.0006929999999999999,
      "loss": 8.2015,
      "step": 693
    },
    {
      "epoch": 4.586534489880215,
      "grad_norm": 50.19654846191406,
      "learning_rate": 0.000694,
      "loss": 9.0723,
      "step": 694
    },
    {
      "epoch": 4.5931433292028085,
      "grad_norm": 19.678110122680664,
      "learning_rate": 0.000695,
      "loss": 3.7925,
      "step": 695
    },
    {
      "epoch": 4.599752168525403,
      "grad_norm": 7.2186079025268555,
      "learning_rate": 0.000696,
      "loss": 3.9609,
      "step": 696
    },
    {
      "epoch": 4.606361007847997,
      "grad_norm": 5.762016296386719,
      "learning_rate": 0.0006969999999999999,
      "loss": 1.547,
      "step": 697
    },
    {
      "epoch": 4.612969847170591,
      "grad_norm": 17.82166862487793,
      "learning_rate": 0.0006979999999999999,
      "loss": 4.8272,
      "step": 698
    },
    {
      "epoch": 4.619578686493185,
      "grad_norm": 14.362499237060547,
      "learning_rate": 0.000699,
      "loss": 3.4387,
      "step": 699
    },
    {
      "epoch": 4.626187525815778,
      "grad_norm": 7.054169654846191,
      "learning_rate": 0.0007,
      "loss": 1.4101,
      "step": 700
    },
    {
      "epoch": 4.632796365138373,
      "grad_norm": 14.686450004577637,
      "learning_rate": 0.000701,
      "loss": 3.7131,
      "step": 701
    },
    {
      "epoch": 4.639405204460966,
      "grad_norm": 14.843250274658203,
      "learning_rate": 0.0007019999999999999,
      "loss": 2.1162,
      "step": 702
    },
    {
      "epoch": 4.646014043783561,
      "grad_norm": 33.891624450683594,
      "learning_rate": 0.000703,
      "loss": 5.2907,
      "step": 703
    },
    {
      "epoch": 4.6526228831061545,
      "grad_norm": 53.92638397216797,
      "learning_rate": 0.000704,
      "loss": 9.3517,
      "step": 704
    },
    {
      "epoch": 4.659231722428748,
      "grad_norm": 24.973209381103516,
      "learning_rate": 0.000705,
      "loss": 4.3562,
      "step": 705
    },
    {
      "epoch": 4.6658405617513425,
      "grad_norm": 25.29639434814453,
      "learning_rate": 0.0007059999999999999,
      "loss": 2.6488,
      "step": 706
    },
    {
      "epoch": 4.672449401073936,
      "grad_norm": 8.294290542602539,
      "learning_rate": 0.000707,
      "loss": 2.9948,
      "step": 707
    },
    {
      "epoch": 4.679058240396531,
      "grad_norm": 3.1974751949310303,
      "learning_rate": 0.000708,
      "loss": 2.4441,
      "step": 708
    },
    {
      "epoch": 4.685667079719124,
      "grad_norm": 14.325614929199219,
      "learning_rate": 0.000709,
      "loss": 2.6252,
      "step": 709
    },
    {
      "epoch": 4.692275919041718,
      "grad_norm": 24.213354110717773,
      "learning_rate": 0.00071,
      "loss": 5.6483,
      "step": 710
    },
    {
      "epoch": 4.698884758364312,
      "grad_norm": 22.8067684173584,
      "learning_rate": 0.0007109999999999999,
      "loss": 3.0653,
      "step": 711
    },
    {
      "epoch": 4.705493597686906,
      "grad_norm": 2.309006929397583,
      "learning_rate": 0.000712,
      "loss": 1.6111,
      "step": 712
    },
    {
      "epoch": 4.7121024370095,
      "grad_norm": 28.663774490356445,
      "learning_rate": 0.000713,
      "loss": 5.899,
      "step": 713
    },
    {
      "epoch": 4.718711276332094,
      "grad_norm": 9.997549057006836,
      "learning_rate": 0.000714,
      "loss": 3.1039,
      "step": 714
    },
    {
      "epoch": 4.7253201156546885,
      "grad_norm": 4.544032096862793,
      "learning_rate": 0.000715,
      "loss": 5.7342,
      "step": 715
    },
    {
      "epoch": 4.731928954977282,
      "grad_norm": 7.889768123626709,
      "learning_rate": 0.000716,
      "loss": 6.1632,
      "step": 716
    },
    {
      "epoch": 4.7385377942998765,
      "grad_norm": 14.561813354492188,
      "learning_rate": 0.000717,
      "loss": 4.0136,
      "step": 717
    },
    {
      "epoch": 4.74514663362247,
      "grad_norm": 6.873262405395508,
      "learning_rate": 0.000718,
      "loss": 2.5977,
      "step": 718
    },
    {
      "epoch": 4.751755472945064,
      "grad_norm": 3.4689245223999023,
      "learning_rate": 0.000719,
      "loss": 4.1066,
      "step": 719
    },
    {
      "epoch": 4.758364312267658,
      "grad_norm": 4.393742084503174,
      "learning_rate": 0.0007199999999999999,
      "loss": 2.4675,
      "step": 720
    },
    {
      "epoch": 4.764973151590252,
      "grad_norm": 6.951828479766846,
      "learning_rate": 0.000721,
      "loss": 2.7541,
      "step": 721
    },
    {
      "epoch": 4.771581990912846,
      "grad_norm": 20.7460994720459,
      "learning_rate": 0.000722,
      "loss": 4.315,
      "step": 722
    },
    {
      "epoch": 4.77819083023544,
      "grad_norm": 29.882822036743164,
      "learning_rate": 0.000723,
      "loss": 4.7339,
      "step": 723
    },
    {
      "epoch": 4.7847996695580335,
      "grad_norm": 14.816218376159668,
      "learning_rate": 0.000724,
      "loss": 3.4509,
      "step": 724
    },
    {
      "epoch": 4.791408508880628,
      "grad_norm": 12.921623229980469,
      "learning_rate": 0.000725,
      "loss": 3.3225,
      "step": 725
    },
    {
      "epoch": 4.798017348203222,
      "grad_norm": 29.33332061767578,
      "learning_rate": 0.000726,
      "loss": 10.8893,
      "step": 726
    },
    {
      "epoch": 4.804626187525816,
      "grad_norm": 4.710474491119385,
      "learning_rate": 0.000727,
      "loss": 1.4438,
      "step": 727
    },
    {
      "epoch": 4.81123502684841,
      "grad_norm": 3.689492702484131,
      "learning_rate": 0.000728,
      "loss": 3.802,
      "step": 728
    },
    {
      "epoch": 4.817843866171003,
      "grad_norm": 25.148746490478516,
      "learning_rate": 0.000729,
      "loss": 3.7805,
      "step": 729
    },
    {
      "epoch": 4.824452705493598,
      "grad_norm": 38.393253326416016,
      "learning_rate": 0.00073,
      "loss": 7.3574,
      "step": 730
    },
    {
      "epoch": 4.831061544816191,
      "grad_norm": 14.468674659729004,
      "learning_rate": 0.000731,
      "loss": 3.685,
      "step": 731
    },
    {
      "epoch": 4.837670384138786,
      "grad_norm": 5.181061744689941,
      "learning_rate": 0.000732,
      "loss": 2.4328,
      "step": 732
    },
    {
      "epoch": 4.844279223461379,
      "grad_norm": 31.78403091430664,
      "learning_rate": 0.000733,
      "loss": 5.2595,
      "step": 733
    },
    {
      "epoch": 4.850888062783974,
      "grad_norm": 20.136993408203125,
      "learning_rate": 0.000734,
      "loss": 6.039,
      "step": 734
    },
    {
      "epoch": 4.8574969021065675,
      "grad_norm": 31.50390625,
      "learning_rate": 0.000735,
      "loss": 6.9126,
      "step": 735
    },
    {
      "epoch": 4.864105741429162,
      "grad_norm": 23.8400936126709,
      "learning_rate": 0.000736,
      "loss": 4.0498,
      "step": 736
    },
    {
      "epoch": 4.870714580751756,
      "grad_norm": 2.4356038570404053,
      "learning_rate": 0.000737,
      "loss": 3.0062,
      "step": 737
    },
    {
      "epoch": 4.877323420074349,
      "grad_norm": 12.69481086730957,
      "learning_rate": 0.000738,
      "loss": 2.7495,
      "step": 738
    },
    {
      "epoch": 4.883932259396944,
      "grad_norm": 27.248476028442383,
      "learning_rate": 0.000739,
      "loss": 4.7202,
      "step": 739
    },
    {
      "epoch": 4.890541098719537,
      "grad_norm": 34.4566764831543,
      "learning_rate": 0.00074,
      "loss": 5.0014,
      "step": 740
    },
    {
      "epoch": 4.897149938042132,
      "grad_norm": 25.116859436035156,
      "learning_rate": 0.000741,
      "loss": 4.5255,
      "step": 741
    },
    {
      "epoch": 4.903758777364725,
      "grad_norm": 4.568933963775635,
      "learning_rate": 0.000742,
      "loss": 2.0284,
      "step": 742
    },
    {
      "epoch": 4.910367616687319,
      "grad_norm": 3.2399721145629883,
      "learning_rate": 0.0007430000000000001,
      "loss": 3.7258,
      "step": 743
    },
    {
      "epoch": 4.916976456009913,
      "grad_norm": 38.34724044799805,
      "learning_rate": 0.000744,
      "loss": 10.031,
      "step": 744
    },
    {
      "epoch": 4.923585295332507,
      "grad_norm": 34.461185455322266,
      "learning_rate": 0.000745,
      "loss": 9.2277,
      "step": 745
    },
    {
      "epoch": 4.9301941346551015,
      "grad_norm": 42.97383499145508,
      "learning_rate": 0.000746,
      "loss": 11.2737,
      "step": 746
    },
    {
      "epoch": 4.936802973977695,
      "grad_norm": 33.25004577636719,
      "learning_rate": 0.000747,
      "loss": 7.3187,
      "step": 747
    },
    {
      "epoch": 4.943411813300289,
      "grad_norm": 22.578062057495117,
      "learning_rate": 0.000748,
      "loss": 3.8126,
      "step": 748
    },
    {
      "epoch": 4.950020652622883,
      "grad_norm": 5.2870402336120605,
      "learning_rate": 0.000749,
      "loss": 2.5781,
      "step": 749
    },
    {
      "epoch": 4.956629491945477,
      "grad_norm": 34.9913444519043,
      "learning_rate": 0.00075,
      "loss": 6.4178,
      "step": 750
    },
    {
      "epoch": 4.963238331268071,
      "grad_norm": 47.70172882080078,
      "learning_rate": 0.000751,
      "loss": 11.8722,
      "step": 751
    },
    {
      "epoch": 4.969847170590665,
      "grad_norm": 62.07699203491211,
      "learning_rate": 0.0007520000000000001,
      "loss": 12.9142,
      "step": 752
    },
    {
      "epoch": 4.976456009913259,
      "grad_norm": 65.32978820800781,
      "learning_rate": 0.000753,
      "loss": 14.2602,
      "step": 753
    },
    {
      "epoch": 4.983064849235853,
      "grad_norm": 56.056251525878906,
      "learning_rate": 0.000754,
      "loss": 11.5362,
      "step": 754
    },
    {
      "epoch": 4.989673688558447,
      "grad_norm": 25.43136215209961,
      "learning_rate": 0.000755,
      "loss": 4.3652,
      "step": 755
    },
    {
      "epoch": 4.996282527881041,
      "grad_norm": 2.0174782276153564,
      "learning_rate": 0.000756,
      "loss": 1.338,
      "step": 756
    },
    {
      "epoch": 4.996282527881041,
      "eval_validation_error_bar": 0.04410375005728065,
      "eval_validation_loss": 4.721327781677246,
      "eval_validation_pearsonr": 0.6454199904965833,
      "eval_validation_rmse": 2.1728615760803223,
      "eval_validation_runtime": 28.5697,
      "eval_validation_samples_per_second": 7.105,
      "eval_validation_spearman": 0.6593135848542414,
      "eval_validation_steps_per_second": 7.105,
      "step": 756
    },
    {
      "epoch": 4.996282527881041,
      "eval_test_error_bar": 0.04216977394707502,
      "eval_test_loss": 6.897706508636475,
      "eval_test_pearsonr": 0.5192703202556732,
      "eval_test_rmse": 2.6263484954833984,
      "eval_test_runtime": 45.7888,
      "eval_test_samples_per_second": 7.12,
      "eval_test_spearman": 0.5404113294264121,
      "eval_test_steps_per_second": 7.12,
      "step": 756
    },
    {
      "epoch": 5.002891367203635,
      "grad_norm": 16.36760902404785,
      "learning_rate": 0.000757,
      "loss": 4.2334,
      "step": 757
    },
    {
      "epoch": 5.009500206526229,
      "grad_norm": 19.31294822692871,
      "learning_rate": 0.000758,
      "loss": 5.2106,
      "step": 758
    },
    {
      "epoch": 5.016109045848823,
      "grad_norm": 19.279375076293945,
      "learning_rate": 0.000759,
      "loss": 4.244,
      "step": 759
    },
    {
      "epoch": 5.022717885171417,
      "grad_norm": 10.073622703552246,
      "learning_rate": 0.00076,
      "loss": 2.1298,
      "step": 760
    },
    {
      "epoch": 5.029326724494011,
      "grad_norm": 4.245323181152344,
      "learning_rate": 0.0007610000000000001,
      "loss": 2.3164,
      "step": 761
    },
    {
      "epoch": 5.035935563816604,
      "grad_norm": 26.768686294555664,
      "learning_rate": 0.000762,
      "loss": 3.7753,
      "step": 762
    },
    {
      "epoch": 5.042544403139199,
      "grad_norm": 36.80234909057617,
      "learning_rate": 0.000763,
      "loss": 5.0053,
      "step": 763
    },
    {
      "epoch": 5.049153242461792,
      "grad_norm": 35.69483947753906,
      "learning_rate": 0.000764,
      "loss": 4.0698,
      "step": 764
    },
    {
      "epoch": 5.055762081784387,
      "grad_norm": 24.344337463378906,
      "learning_rate": 0.0007650000000000001,
      "loss": 2.9996,
      "step": 765
    },
    {
      "epoch": 5.0623709211069805,
      "grad_norm": 9.591136932373047,
      "learning_rate": 0.0007660000000000001,
      "loss": 3.2627,
      "step": 766
    },
    {
      "epoch": 5.068979760429575,
      "grad_norm": 12.571455001831055,
      "learning_rate": 0.000767,
      "loss": 2.7497,
      "step": 767
    },
    {
      "epoch": 5.075588599752169,
      "grad_norm": 5.540163993835449,
      "learning_rate": 0.000768,
      "loss": 1.9189,
      "step": 768
    },
    {
      "epoch": 5.082197439074762,
      "grad_norm": 18.31424331665039,
      "learning_rate": 0.000769,
      "loss": 3.8274,
      "step": 769
    },
    {
      "epoch": 5.088806278397357,
      "grad_norm": 14.824689865112305,
      "learning_rate": 0.0007700000000000001,
      "loss": 3.3319,
      "step": 770
    },
    {
      "epoch": 5.09541511771995,
      "grad_norm": 6.102456569671631,
      "learning_rate": 0.000771,
      "loss": 1.7407,
      "step": 771
    },
    {
      "epoch": 5.102023957042545,
      "grad_norm": 1.8367031812667847,
      "learning_rate": 0.000772,
      "loss": 1.4186,
      "step": 772
    },
    {
      "epoch": 5.108632796365138,
      "grad_norm": 23.56466293334961,
      "learning_rate": 0.000773,
      "loss": 2.939,
      "step": 773
    },
    {
      "epoch": 5.115241635687732,
      "grad_norm": 14.70210075378418,
      "learning_rate": 0.0007740000000000001,
      "loss": 3.2757,
      "step": 774
    },
    {
      "epoch": 5.121850475010326,
      "grad_norm": 9.739595413208008,
      "learning_rate": 0.0007750000000000001,
      "loss": 3.5266,
      "step": 775
    },
    {
      "epoch": 5.12845931433292,
      "grad_norm": 8.505229949951172,
      "learning_rate": 0.000776,
      "loss": 2.839,
      "step": 776
    },
    {
      "epoch": 5.1350681536555145,
      "grad_norm": 16.70718002319336,
      "learning_rate": 0.000777,
      "loss": 3.6082,
      "step": 777
    },
    {
      "epoch": 5.141676992978108,
      "grad_norm": 18.063201904296875,
      "learning_rate": 0.000778,
      "loss": 2.7449,
      "step": 778
    },
    {
      "epoch": 5.148285832300703,
      "grad_norm": 19.740610122680664,
      "learning_rate": 0.0007790000000000001,
      "loss": 3.2649,
      "step": 779
    },
    {
      "epoch": 5.154894671623296,
      "grad_norm": 4.066902160644531,
      "learning_rate": 0.0007800000000000001,
      "loss": 3.5282,
      "step": 780
    },
    {
      "epoch": 5.16150351094589,
      "grad_norm": 25.09629249572754,
      "learning_rate": 0.000781,
      "loss": 5.1918,
      "step": 781
    },
    {
      "epoch": 5.168112350268484,
      "grad_norm": 29.151851654052734,
      "learning_rate": 0.000782,
      "loss": 5.0793,
      "step": 782
    },
    {
      "epoch": 5.174721189591078,
      "grad_norm": 25.278440475463867,
      "learning_rate": 0.0007830000000000001,
      "loss": 3.99,
      "step": 783
    },
    {
      "epoch": 5.181330028913672,
      "grad_norm": 3.690858840942383,
      "learning_rate": 0.0007840000000000001,
      "loss": 1.2557,
      "step": 784
    },
    {
      "epoch": 5.187938868236266,
      "grad_norm": 15.724656105041504,
      "learning_rate": 0.000785,
      "loss": 4.9587,
      "step": 785
    },
    {
      "epoch": 5.1945477075588595,
      "grad_norm": 9.035588264465332,
      "learning_rate": 0.000786,
      "loss": 4.4077,
      "step": 786
    },
    {
      "epoch": 5.201156546881454,
      "grad_norm": 11.698019981384277,
      "learning_rate": 0.000787,
      "loss": 6.1492,
      "step": 787
    },
    {
      "epoch": 5.207765386204048,
      "grad_norm": 13.90818977355957,
      "learning_rate": 0.0007880000000000001,
      "loss": 2.8168,
      "step": 788
    },
    {
      "epoch": 5.214374225526642,
      "grad_norm": 5.345459938049316,
      "learning_rate": 0.0007890000000000001,
      "loss": 3.6043,
      "step": 789
    },
    {
      "epoch": 5.220983064849236,
      "grad_norm": 19.061845779418945,
      "learning_rate": 0.00079,
      "loss": 1.7512,
      "step": 790
    },
    {
      "epoch": 5.22759190417183,
      "grad_norm": 45.83580780029297,
      "learning_rate": 0.000791,
      "loss": 6.9533,
      "step": 791
    },
    {
      "epoch": 5.234200743494424,
      "grad_norm": 39.646812438964844,
      "learning_rate": 0.0007920000000000001,
      "loss": 6.99,
      "step": 792
    },
    {
      "epoch": 5.240809582817017,
      "grad_norm": 29.543365478515625,
      "learning_rate": 0.0007930000000000001,
      "loss": 10.2849,
      "step": 793
    },
    {
      "epoch": 5.247418422139612,
      "grad_norm": 15.067418098449707,
      "learning_rate": 0.0007940000000000001,
      "loss": 3.9162,
      "step": 794
    },
    {
      "epoch": 5.2540272614622054,
      "grad_norm": 11.77410888671875,
      "learning_rate": 0.000795,
      "loss": 4.7851,
      "step": 795
    },
    {
      "epoch": 5.2606361007848,
      "grad_norm": 24.33804702758789,
      "learning_rate": 0.000796,
      "loss": 8.0484,
      "step": 796
    },
    {
      "epoch": 5.2672449401073935,
      "grad_norm": 24.28472328186035,
      "learning_rate": 0.0007970000000000001,
      "loss": 5.0181,
      "step": 797
    },
    {
      "epoch": 5.273853779429988,
      "grad_norm": 16.745271682739258,
      "learning_rate": 0.0007980000000000001,
      "loss": 2.6476,
      "step": 798
    },
    {
      "epoch": 5.280462618752582,
      "grad_norm": 24.644376754760742,
      "learning_rate": 0.000799,
      "loss": 5.4445,
      "step": 799
    },
    {
      "epoch": 5.287071458075175,
      "grad_norm": 12.527566909790039,
      "learning_rate": 0.0008,
      "loss": 2.3242,
      "step": 800
    },
    {
      "epoch": 5.29368029739777,
      "grad_norm": 22.045684814453125,
      "learning_rate": 0.0008010000000000001,
      "loss": 2.8376,
      "step": 801
    },
    {
      "epoch": 5.300289136720363,
      "grad_norm": 34.19100570678711,
      "learning_rate": 0.0008020000000000001,
      "loss": 6.3279,
      "step": 802
    },
    {
      "epoch": 5.306897976042958,
      "grad_norm": 27.355714797973633,
      "learning_rate": 0.0008030000000000001,
      "loss": 4.2123,
      "step": 803
    },
    {
      "epoch": 5.313506815365551,
      "grad_norm": 20.927522659301758,
      "learning_rate": 0.000804,
      "loss": 3.7015,
      "step": 804
    },
    {
      "epoch": 5.320115654688145,
      "grad_norm": 3.260037899017334,
      "learning_rate": 0.000805,
      "loss": 3.9115,
      "step": 805
    },
    {
      "epoch": 5.326724494010739,
      "grad_norm": 20.667707443237305,
      "learning_rate": 0.0008060000000000001,
      "loss": 3.484,
      "step": 806
    },
    {
      "epoch": 5.333333333333333,
      "grad_norm": 29.010210037231445,
      "learning_rate": 0.0008070000000000001,
      "loss": 10.1599,
      "step": 807
    },
    {
      "epoch": 5.3399421726559275,
      "grad_norm": 30.65773582458496,
      "learning_rate": 0.000808,
      "loss": 6.472,
      "step": 808
    },
    {
      "epoch": 5.346551011978521,
      "grad_norm": 26.897382736206055,
      "learning_rate": 0.000809,
      "loss": 5.1391,
      "step": 809
    },
    {
      "epoch": 5.353159851301116,
      "grad_norm": 5.624751567840576,
      "learning_rate": 0.0008100000000000001,
      "loss": 2.8639,
      "step": 810
    },
    {
      "epoch": 5.359768690623709,
      "grad_norm": 18.18777847290039,
      "learning_rate": 0.0008110000000000001,
      "loss": 5.4858,
      "step": 811
    },
    {
      "epoch": 5.366377529946303,
      "grad_norm": 35.907257080078125,
      "learning_rate": 0.0008120000000000001,
      "loss": 6.7692,
      "step": 812
    },
    {
      "epoch": 5.372986369268897,
      "grad_norm": 54.567501068115234,
      "learning_rate": 0.0008129999999999999,
      "loss": 10.8821,
      "step": 813
    },
    {
      "epoch": 5.379595208591491,
      "grad_norm": 60.6588249206543,
      "learning_rate": 0.0008139999999999999,
      "loss": 11.7719,
      "step": 814
    },
    {
      "epoch": 5.386204047914085,
      "grad_norm": 31.63907241821289,
      "learning_rate": 0.000815,
      "loss": 5.29,
      "step": 815
    },
    {
      "epoch": 5.392812887236679,
      "grad_norm": 4.371053695678711,
      "learning_rate": 0.000816,
      "loss": 1.8475,
      "step": 816
    },
    {
      "epoch": 5.399421726559273,
      "grad_norm": 5.536308765411377,
      "learning_rate": 0.000817,
      "loss": 1.2967,
      "step": 817
    },
    {
      "epoch": 5.406030565881867,
      "grad_norm": 16.46468734741211,
      "learning_rate": 0.0008179999999999999,
      "loss": 3.4136,
      "step": 818
    },
    {
      "epoch": 5.412639405204461,
      "grad_norm": 28.10634994506836,
      "learning_rate": 0.000819,
      "loss": 7.3199,
      "step": 819
    },
    {
      "epoch": 5.419248244527055,
      "grad_norm": 14.983222961425781,
      "learning_rate": 0.00082,
      "loss": 2.4402,
      "step": 820
    },
    {
      "epoch": 5.425857083849649,
      "grad_norm": 11.365706443786621,
      "learning_rate": 0.000821,
      "loss": 2.126,
      "step": 821
    },
    {
      "epoch": 5.432465923172243,
      "grad_norm": 21.699092864990234,
      "learning_rate": 0.0008219999999999999,
      "loss": 2.7732,
      "step": 822
    },
    {
      "epoch": 5.439074762494837,
      "grad_norm": 20.326515197753906,
      "learning_rate": 0.000823,
      "loss": 4.7595,
      "step": 823
    },
    {
      "epoch": 5.44568360181743,
      "grad_norm": 25.93533706665039,
      "learning_rate": 0.000824,
      "loss": 4.2905,
      "step": 824
    },
    {
      "epoch": 5.452292441140025,
      "grad_norm": 6.168569087982178,
      "learning_rate": 0.000825,
      "loss": 1.896,
      "step": 825
    },
    {
      "epoch": 5.4589012804626185,
      "grad_norm": 8.118659973144531,
      "learning_rate": 0.000826,
      "loss": 4.9405,
      "step": 826
    },
    {
      "epoch": 5.465510119785213,
      "grad_norm": 1.3495603799819946,
      "learning_rate": 0.0008269999999999999,
      "loss": 1.0752,
      "step": 827
    },
    {
      "epoch": 5.4721189591078065,
      "grad_norm": 7.073380470275879,
      "learning_rate": 0.000828,
      "loss": 2.1856,
      "step": 828
    },
    {
      "epoch": 5.478727798430401,
      "grad_norm": 11.417887687683105,
      "learning_rate": 0.000829,
      "loss": 2.2827,
      "step": 829
    },
    {
      "epoch": 5.485336637752995,
      "grad_norm": 4.401050090789795,
      "learning_rate": 0.00083,
      "loss": 0.8104,
      "step": 830
    },
    {
      "epoch": 5.491945477075588,
      "grad_norm": 4.355268478393555,
      "learning_rate": 0.0008309999999999999,
      "loss": 2.1373,
      "step": 831
    },
    {
      "epoch": 5.498554316398183,
      "grad_norm": 20.834110260009766,
      "learning_rate": 0.000832,
      "loss": 2.8602,
      "step": 832
    },
    {
      "epoch": 5.505163155720776,
      "grad_norm": 5.9736409187316895,
      "learning_rate": 0.000833,
      "loss": 0.9013,
      "step": 833
    },
    {
      "epoch": 5.511771995043371,
      "grad_norm": 3.593498706817627,
      "learning_rate": 0.000834,
      "loss": 1.8089,
      "step": 834
    },
    {
      "epoch": 5.518380834365964,
      "grad_norm": 10.782246589660645,
      "learning_rate": 0.000835,
      "loss": 2.8854,
      "step": 835
    },
    {
      "epoch": 5.524989673688559,
      "grad_norm": 12.201860427856445,
      "learning_rate": 0.0008359999999999999,
      "loss": 4.7073,
      "step": 836
    },
    {
      "epoch": 5.5315985130111525,
      "grad_norm": 4.791100025177002,
      "learning_rate": 0.000837,
      "loss": 1.0266,
      "step": 837
    },
    {
      "epoch": 5.538207352333746,
      "grad_norm": 27.23432731628418,
      "learning_rate": 0.000838,
      "loss": 5.6266,
      "step": 838
    },
    {
      "epoch": 5.5448161916563405,
      "grad_norm": 11.445752143859863,
      "learning_rate": 0.000839,
      "loss": 2.4073,
      "step": 839
    },
    {
      "epoch": 5.551425030978934,
      "grad_norm": 9.604763984680176,
      "learning_rate": 0.00084,
      "loss": 1.9758,
      "step": 840
    },
    {
      "epoch": 5.558033870301529,
      "grad_norm": 4.977237224578857,
      "learning_rate": 0.000841,
      "loss": 3.2511,
      "step": 841
    },
    {
      "epoch": 5.564642709624122,
      "grad_norm": 5.917466163635254,
      "learning_rate": 0.000842,
      "loss": 6.6035,
      "step": 842
    },
    {
      "epoch": 5.571251548946716,
      "grad_norm": 15.074358940124512,
      "learning_rate": 0.000843,
      "loss": 2.6999,
      "step": 843
    },
    {
      "epoch": 5.57786038826931,
      "grad_norm": 5.227051734924316,
      "learning_rate": 0.000844,
      "loss": 2.3124,
      "step": 844
    },
    {
      "epoch": 5.584469227591904,
      "grad_norm": 19.835542678833008,
      "learning_rate": 0.0008449999999999999,
      "loss": 6.5566,
      "step": 845
    },
    {
      "epoch": 5.591078066914498,
      "grad_norm": 23.495113372802734,
      "learning_rate": 0.000846,
      "loss": 3.0594,
      "step": 846
    },
    {
      "epoch": 5.597686906237092,
      "grad_norm": 21.026247024536133,
      "learning_rate": 0.000847,
      "loss": 3.5225,
      "step": 847
    },
    {
      "epoch": 5.6042957455596865,
      "grad_norm": 26.8349666595459,
      "learning_rate": 0.000848,
      "loss": 4.6973,
      "step": 848
    },
    {
      "epoch": 5.61090458488228,
      "grad_norm": 17.93657684326172,
      "learning_rate": 0.000849,
      "loss": 4.3395,
      "step": 849
    },
    {
      "epoch": 5.617513424204874,
      "grad_norm": 11.979737281799316,
      "learning_rate": 0.00085,
      "loss": 2.7547,
      "step": 850
    },
    {
      "epoch": 5.624122263527468,
      "grad_norm": 3.4620020389556885,
      "learning_rate": 0.000851,
      "loss": 3.0042,
      "step": 851
    },
    {
      "epoch": 5.630731102850062,
      "grad_norm": 11.764915466308594,
      "learning_rate": 0.000852,
      "loss": 3.0613,
      "step": 852
    },
    {
      "epoch": 5.637339942172656,
      "grad_norm": 3.3690905570983887,
      "learning_rate": 0.000853,
      "loss": 1.3513,
      "step": 853
    },
    {
      "epoch": 5.64394878149525,
      "grad_norm": 6.827093124389648,
      "learning_rate": 0.000854,
      "loss": 2.5903,
      "step": 854
    },
    {
      "epoch": 5.650557620817844,
      "grad_norm": 7.682412624359131,
      "learning_rate": 0.000855,
      "loss": 2.585,
      "step": 855
    },
    {
      "epoch": 5.657166460140438,
      "grad_norm": 3.068117618560791,
      "learning_rate": 0.000856,
      "loss": 1.7856,
      "step": 856
    },
    {
      "epoch": 5.6637752994630315,
      "grad_norm": 3.7752997875213623,
      "learning_rate": 0.000857,
      "loss": 3.4758,
      "step": 857
    },
    {
      "epoch": 5.670384138785626,
      "grad_norm": 2.128080129623413,
      "learning_rate": 0.000858,
      "loss": 1.43,
      "step": 858
    },
    {
      "epoch": 5.67699297810822,
      "grad_norm": 20.294511795043945,
      "learning_rate": 0.000859,
      "loss": 6.1328,
      "step": 859
    },
    {
      "epoch": 5.683601817430814,
      "grad_norm": 2.7313616275787354,
      "learning_rate": 0.00086,
      "loss": 1.824,
      "step": 860
    },
    {
      "epoch": 5.690210656753408,
      "grad_norm": 8.183487892150879,
      "learning_rate": 0.000861,
      "loss": 2.5025,
      "step": 861
    },
    {
      "epoch": 5.696819496076001,
      "grad_norm": 11.287320137023926,
      "learning_rate": 0.000862,
      "loss": 3.8707,
      "step": 862
    },
    {
      "epoch": 5.703428335398596,
      "grad_norm": 17.002086639404297,
      "learning_rate": 0.000863,
      "loss": 5.6218,
      "step": 863
    },
    {
      "epoch": 5.710037174721189,
      "grad_norm": 4.010396480560303,
      "learning_rate": 0.000864,
      "loss": 4.8026,
      "step": 864
    },
    {
      "epoch": 5.716646014043784,
      "grad_norm": 14.783037185668945,
      "learning_rate": 0.000865,
      "loss": 5.8886,
      "step": 865
    },
    {
      "epoch": 5.723254853366377,
      "grad_norm": 11.779346466064453,
      "learning_rate": 0.000866,
      "loss": 2.5757,
      "step": 866
    },
    {
      "epoch": 5.729863692688972,
      "grad_norm": 5.381911277770996,
      "learning_rate": 0.000867,
      "loss": 1.5231,
      "step": 867
    },
    {
      "epoch": 5.7364725320115655,
      "grad_norm": 8.853459358215332,
      "learning_rate": 0.0008680000000000001,
      "loss": 2.5497,
      "step": 868
    },
    {
      "epoch": 5.743081371334159,
      "grad_norm": 6.836781978607178,
      "learning_rate": 0.000869,
      "loss": 3.5399,
      "step": 869
    },
    {
      "epoch": 5.749690210656754,
      "grad_norm": 11.881866455078125,
      "learning_rate": 0.00087,
      "loss": 2.6191,
      "step": 870
    },
    {
      "epoch": 5.756299049979347,
      "grad_norm": 16.611499786376953,
      "learning_rate": 0.000871,
      "loss": 5.7236,
      "step": 871
    },
    {
      "epoch": 5.762907889301942,
      "grad_norm": 11.36954116821289,
      "learning_rate": 0.000872,
      "loss": 2.343,
      "step": 872
    },
    {
      "epoch": 5.769516728624535,
      "grad_norm": 17.135272979736328,
      "learning_rate": 0.000873,
      "loss": 5.2704,
      "step": 873
    },
    {
      "epoch": 5.77612556794713,
      "grad_norm": 15.081204414367676,
      "learning_rate": 0.000874,
      "loss": 4.4237,
      "step": 874
    },
    {
      "epoch": 5.782734407269723,
      "grad_norm": 11.699586868286133,
      "learning_rate": 0.000875,
      "loss": 3.6574,
      "step": 875
    },
    {
      "epoch": 5.789343246592317,
      "grad_norm": 10.923605918884277,
      "learning_rate": 0.000876,
      "loss": 3.506,
      "step": 876
    },
    {
      "epoch": 5.795952085914911,
      "grad_norm": 5.999563694000244,
      "learning_rate": 0.0008770000000000001,
      "loss": 2.3944,
      "step": 877
    },
    {
      "epoch": 5.802560925237505,
      "grad_norm": 3.2422471046447754,
      "learning_rate": 0.000878,
      "loss": 1.6765,
      "step": 878
    },
    {
      "epoch": 5.8091697645600995,
      "grad_norm": 12.142829895019531,
      "learning_rate": 0.000879,
      "loss": 1.1811,
      "step": 879
    },
    {
      "epoch": 5.815778603882693,
      "grad_norm": 16.12786102294922,
      "learning_rate": 0.00088,
      "loss": 1.5236,
      "step": 880
    },
    {
      "epoch": 5.822387443205287,
      "grad_norm": 5.945392608642578,
      "learning_rate": 0.0008810000000000001,
      "loss": 2.9522,
      "step": 881
    },
    {
      "epoch": 5.828996282527881,
      "grad_norm": 2.6380820274353027,
      "learning_rate": 0.000882,
      "loss": 2.5063,
      "step": 882
    },
    {
      "epoch": 5.835605121850475,
      "grad_norm": 35.028167724609375,
      "learning_rate": 0.000883,
      "loss": 4.7491,
      "step": 883
    },
    {
      "epoch": 5.842213961173069,
      "grad_norm": 12.13742446899414,
      "learning_rate": 0.000884,
      "loss": 1.6812,
      "step": 884
    },
    {
      "epoch": 5.848822800495663,
      "grad_norm": 6.990658283233643,
      "learning_rate": 0.000885,
      "loss": 1.8057,
      "step": 885
    },
    {
      "epoch": 5.855431639818256,
      "grad_norm": 23.256855010986328,
      "learning_rate": 0.0008860000000000001,
      "loss": 2.9776,
      "step": 886
    },
    {
      "epoch": 5.862040479140851,
      "grad_norm": 8.640291213989258,
      "learning_rate": 0.000887,
      "loss": 2.5227,
      "step": 887
    },
    {
      "epoch": 5.8686493184634445,
      "grad_norm": 24.31588363647461,
      "learning_rate": 0.000888,
      "loss": 3.2624,
      "step": 888
    },
    {
      "epoch": 5.875258157786039,
      "grad_norm": 28.80879020690918,
      "learning_rate": 0.000889,
      "loss": 5.0419,
      "step": 889
    },
    {
      "epoch": 5.881866997108633,
      "grad_norm": 6.177342891693115,
      "learning_rate": 0.0008900000000000001,
      "loss": 1.4245,
      "step": 890
    },
    {
      "epoch": 5.888475836431227,
      "grad_norm": 12.399629592895508,
      "learning_rate": 0.0008910000000000001,
      "loss": 0.837,
      "step": 891
    },
    {
      "epoch": 5.895084675753821,
      "grad_norm": 8.391780853271484,
      "learning_rate": 0.000892,
      "loss": 4.8987,
      "step": 892
    },
    {
      "epoch": 5.901693515076415,
      "grad_norm": 8.898083686828613,
      "learning_rate": 0.000893,
      "loss": 2.8477,
      "step": 893
    },
    {
      "epoch": 5.908302354399009,
      "grad_norm": 9.847358703613281,
      "learning_rate": 0.000894,
      "loss": 4.4639,
      "step": 894
    },
    {
      "epoch": 5.914911193721602,
      "grad_norm": 21.89811134338379,
      "learning_rate": 0.0008950000000000001,
      "loss": 3.0488,
      "step": 895
    },
    {
      "epoch": 5.921520033044197,
      "grad_norm": 19.19521141052246,
      "learning_rate": 0.000896,
      "loss": 5.3415,
      "step": 896
    },
    {
      "epoch": 5.92812887236679,
      "grad_norm": 3.4038186073303223,
      "learning_rate": 0.000897,
      "loss": 1.5032,
      "step": 897
    },
    {
      "epoch": 5.934737711689385,
      "grad_norm": 10.186772346496582,
      "learning_rate": 0.000898,
      "loss": 4.6107,
      "step": 898
    },
    {
      "epoch": 5.9413465510119785,
      "grad_norm": 13.251185417175293,
      "learning_rate": 0.0008990000000000001,
      "loss": 1.5205,
      "step": 899
    },
    {
      "epoch": 5.947955390334572,
      "grad_norm": 13.661590576171875,
      "learning_rate": 0.0009000000000000001,
      "loss": 2.2741,
      "step": 900
    },
    {
      "epoch": 5.954564229657167,
      "grad_norm": 6.178734302520752,
      "learning_rate": 0.000901,
      "loss": 3.9868,
      "step": 901
    },
    {
      "epoch": 5.96117306897976,
      "grad_norm": 5.2873101234436035,
      "learning_rate": 0.000902,
      "loss": 1.9651,
      "step": 902
    },
    {
      "epoch": 5.967781908302355,
      "grad_norm": 5.785032749176025,
      "learning_rate": 0.000903,
      "loss": 4.3152,
      "step": 903
    },
    {
      "epoch": 5.974390747624948,
      "grad_norm": 3.0367846488952637,
      "learning_rate": 0.0009040000000000001,
      "loss": 1.6183,
      "step": 904
    },
    {
      "epoch": 5.980999586947542,
      "grad_norm": 11.430082321166992,
      "learning_rate": 0.0009050000000000001,
      "loss": 1.0234,
      "step": 905
    },
    {
      "epoch": 5.987608426270136,
      "grad_norm": 7.14699649810791,
      "learning_rate": 0.000906,
      "loss": 1.4759,
      "step": 906
    },
    {
      "epoch": 5.99421726559273,
      "grad_norm": 3.5138206481933594,
      "learning_rate": 0.000907,
      "loss": 1.4229,
      "step": 907
    },
    {
      "epoch": 5.99421726559273,
      "eval_validation_error_bar": 0.036686561665657945,
      "eval_validation_loss": 4.72916316986084,
      "eval_validation_pearsonr": 0.6766839927868642,
      "eval_validation_rmse": 2.17466402053833,
      "eval_validation_runtime": 28.5483,
      "eval_validation_samples_per_second": 7.111,
      "eval_validation_spearman": 0.7345596040770442,
      "eval_validation_steps_per_second": 7.111,
      "step": 907
    },
    {
      "epoch": 5.99421726559273,
      "eval_test_error_bar": 0.036378650081956426,
      "eval_test_loss": 6.034022331237793,
      "eval_test_pearsonr": 0.6179964102404176,
      "eval_test_rmse": 2.4564247131347656,
      "eval_test_runtime": 45.8237,
      "eval_test_samples_per_second": 7.114,
      "eval_test_spearman": 0.635302479364155,
      "eval_test_steps_per_second": 7.114,
      "step": 907
    },
    {
      "epoch": 6.000826104915324,
      "grad_norm": 7.525970935821533,
      "learning_rate": 0.0009080000000000001,
      "loss": 3.733,
      "step": 908
    },
    {
      "epoch": 6.007434944237918,
      "grad_norm": 3.071733236312866,
      "learning_rate": 0.0009090000000000001,
      "loss": 1.0501,
      "step": 909
    },
    {
      "epoch": 6.0140437835605125,
      "grad_norm": 1.9098316431045532,
      "learning_rate": 0.00091,
      "loss": 2.0385,
      "step": 910
    },
    {
      "epoch": 6.020652622883106,
      "grad_norm": 14.86328125,
      "learning_rate": 0.000911,
      "loss": 2.3305,
      "step": 911
    },
    {
      "epoch": 6.0272614622057,
      "grad_norm": 5.60001802444458,
      "learning_rate": 0.000912,
      "loss": 2.3323,
      "step": 912
    },
    {
      "epoch": 6.033870301528294,
      "grad_norm": 6.151859283447266,
      "learning_rate": 0.0009130000000000001,
      "loss": 3.3044,
      "step": 913
    },
    {
      "epoch": 6.040479140850888,
      "grad_norm": 28.548141479492188,
      "learning_rate": 0.0009140000000000001,
      "loss": 3.4302,
      "step": 914
    },
    {
      "epoch": 6.047087980173482,
      "grad_norm": 4.409938335418701,
      "learning_rate": 0.000915,
      "loss": 2.4629,
      "step": 915
    },
    {
      "epoch": 6.053696819496076,
      "grad_norm": 12.596384048461914,
      "learning_rate": 0.000916,
      "loss": 2.5074,
      "step": 916
    },
    {
      "epoch": 6.06030565881867,
      "grad_norm": 4.130216598510742,
      "learning_rate": 0.0009170000000000001,
      "loss": 1.1998,
      "step": 917
    },
    {
      "epoch": 6.066914498141264,
      "grad_norm": 12.856152534484863,
      "learning_rate": 0.0009180000000000001,
      "loss": 2.841,
      "step": 918
    },
    {
      "epoch": 6.0735233374638575,
      "grad_norm": 6.026784420013428,
      "learning_rate": 0.0009190000000000001,
      "loss": 2.5032,
      "step": 919
    },
    {
      "epoch": 6.080132176786452,
      "grad_norm": 1.69933021068573,
      "learning_rate": 0.00092,
      "loss": 2.6123,
      "step": 920
    },
    {
      "epoch": 6.086741016109046,
      "grad_norm": 2.307453155517578,
      "learning_rate": 0.000921,
      "loss": 2.3531,
      "step": 921
    },
    {
      "epoch": 6.09334985543164,
      "grad_norm": 11.968597412109375,
      "learning_rate": 0.0009220000000000001,
      "loss": 2.6729,
      "step": 922
    },
    {
      "epoch": 6.099958694754234,
      "grad_norm": 10.322785377502441,
      "learning_rate": 0.0009230000000000001,
      "loss": 1.5564,
      "step": 923
    },
    {
      "epoch": 6.106567534076828,
      "grad_norm": 15.276613235473633,
      "learning_rate": 0.000924,
      "loss": 5.6105,
      "step": 924
    },
    {
      "epoch": 6.113176373399422,
      "grad_norm": 16.879623413085938,
      "learning_rate": 0.000925,
      "loss": 3.1263,
      "step": 925
    },
    {
      "epoch": 6.119785212722015,
      "grad_norm": 16.145872116088867,
      "learning_rate": 0.0009260000000000001,
      "loss": 2.5482,
      "step": 926
    },
    {
      "epoch": 6.12639405204461,
      "grad_norm": 23.17640495300293,
      "learning_rate": 0.0009270000000000001,
      "loss": 3.3297,
      "step": 927
    },
    {
      "epoch": 6.1330028913672034,
      "grad_norm": 7.583490371704102,
      "learning_rate": 0.0009280000000000001,
      "loss": 2.3296,
      "step": 928
    },
    {
      "epoch": 6.139611730689798,
      "grad_norm": 5.469784259796143,
      "learning_rate": 0.000929,
      "loss": 1.6103,
      "step": 929
    },
    {
      "epoch": 6.1462205700123915,
      "grad_norm": 10.311237335205078,
      "learning_rate": 0.00093,
      "loss": 2.7724,
      "step": 930
    },
    {
      "epoch": 6.152829409334985,
      "grad_norm": 15.203948020935059,
      "learning_rate": 0.0009310000000000001,
      "loss": 1.7696,
      "step": 931
    },
    {
      "epoch": 6.15943824865758,
      "grad_norm": 10.686838150024414,
      "learning_rate": 0.0009320000000000001,
      "loss": 2.5582,
      "step": 932
    },
    {
      "epoch": 6.166047087980173,
      "grad_norm": 14.245025634765625,
      "learning_rate": 0.000933,
      "loss": 2.0663,
      "step": 933
    },
    {
      "epoch": 6.172655927302768,
      "grad_norm": 4.36591911315918,
      "learning_rate": 0.000934,
      "loss": 3.6453,
      "step": 934
    },
    {
      "epoch": 6.179264766625361,
      "grad_norm": 24.115379333496094,
      "learning_rate": 0.0009350000000000001,
      "loss": 2.8271,
      "step": 935
    },
    {
      "epoch": 6.185873605947956,
      "grad_norm": 15.7901029586792,
      "learning_rate": 0.0009360000000000001,
      "loss": 4.1282,
      "step": 936
    },
    {
      "epoch": 6.192482445270549,
      "grad_norm": 3.46610164642334,
      "learning_rate": 0.0009370000000000001,
      "loss": 2.2504,
      "step": 937
    },
    {
      "epoch": 6.199091284593143,
      "grad_norm": 26.733871459960938,
      "learning_rate": 0.0009379999999999999,
      "loss": 5.9439,
      "step": 938
    },
    {
      "epoch": 6.205700123915737,
      "grad_norm": 15.821023941040039,
      "learning_rate": 0.000939,
      "loss": 3.2604,
      "step": 939
    },
    {
      "epoch": 6.212308963238331,
      "grad_norm": 38.264686584472656,
      "learning_rate": 0.00094,
      "loss": 4.0838,
      "step": 940
    },
    {
      "epoch": 6.2189178025609255,
      "grad_norm": 13.773303985595703,
      "learning_rate": 0.000941,
      "loss": 2.4896,
      "step": 941
    },
    {
      "epoch": 6.225526641883519,
      "grad_norm": 8.313718795776367,
      "learning_rate": 0.000942,
      "loss": 3.2192,
      "step": 942
    },
    {
      "epoch": 6.232135481206114,
      "grad_norm": 4.563786029815674,
      "learning_rate": 0.0009429999999999999,
      "loss": 1.3437,
      "step": 943
    },
    {
      "epoch": 6.238744320528707,
      "grad_norm": 6.925875186920166,
      "learning_rate": 0.000944,
      "loss": 2.4131,
      "step": 944
    },
    {
      "epoch": 6.245353159851301,
      "grad_norm": 3.851262331008911,
      "learning_rate": 0.000945,
      "loss": 1.8112,
      "step": 945
    },
    {
      "epoch": 6.251961999173895,
      "grad_norm": 22.291418075561523,
      "learning_rate": 0.000946,
      "loss": 3.0124,
      "step": 946
    },
    {
      "epoch": 6.258570838496489,
      "grad_norm": 39.98407745361328,
      "learning_rate": 0.0009469999999999999,
      "loss": 8.0822,
      "step": 947
    },
    {
      "epoch": 6.265179677819083,
      "grad_norm": 17.96928596496582,
      "learning_rate": 0.000948,
      "loss": 1.9396,
      "step": 948
    },
    {
      "epoch": 6.271788517141677,
      "grad_norm": 16.72977638244629,
      "learning_rate": 0.000949,
      "loss": 3.8396,
      "step": 949
    },
    {
      "epoch": 6.2783973564642706,
      "grad_norm": 3.751314163208008,
      "learning_rate": 0.00095,
      "loss": 2.0127,
      "step": 950
    },
    {
      "epoch": 6.285006195786865,
      "grad_norm": 37.86372756958008,
      "learning_rate": 0.000951,
      "loss": 9.3067,
      "step": 951
    },
    {
      "epoch": 6.291615035109459,
      "grad_norm": 47.90216064453125,
      "learning_rate": 0.0009519999999999999,
      "loss": 12.1378,
      "step": 952
    },
    {
      "epoch": 6.298223874432053,
      "grad_norm": 46.45568084716797,
      "learning_rate": 0.000953,
      "loss": 12.6472,
      "step": 953
    },
    {
      "epoch": 6.304832713754647,
      "grad_norm": 45.996795654296875,
      "learning_rate": 0.000954,
      "loss": 13.9957,
      "step": 954
    },
    {
      "epoch": 6.311441553077241,
      "grad_norm": 19.64645004272461,
      "learning_rate": 0.000955,
      "loss": 4.8449,
      "step": 955
    },
    {
      "epoch": 6.318050392399835,
      "grad_norm": 29.739200592041016,
      "learning_rate": 0.0009559999999999999,
      "loss": 6.1336,
      "step": 956
    },
    {
      "epoch": 6.324659231722428,
      "grad_norm": 41.49315643310547,
      "learning_rate": 0.000957,
      "loss": 5.2414,
      "step": 957
    },
    {
      "epoch": 6.331268071045023,
      "grad_norm": 66.96644592285156,
      "learning_rate": 0.000958,
      "loss": 10.6677,
      "step": 958
    },
    {
      "epoch": 6.3378769103676165,
      "grad_norm": 61.61403274536133,
      "learning_rate": 0.000959,
      "loss": 9.2476,
      "step": 959
    },
    {
      "epoch": 6.344485749690211,
      "grad_norm": 61.03202438354492,
      "learning_rate": 0.00096,
      "loss": 9.5713,
      "step": 960
    },
    {
      "epoch": 6.3510945890128045,
      "grad_norm": 63.89182662963867,
      "learning_rate": 0.0009609999999999999,
      "loss": 11.088,
      "step": 961
    },
    {
      "epoch": 6.357703428335398,
      "grad_norm": 44.89146041870117,
      "learning_rate": 0.000962,
      "loss": 6.3122,
      "step": 962
    },
    {
      "epoch": 6.364312267657993,
      "grad_norm": 8.070258140563965,
      "learning_rate": 0.000963,
      "loss": 2.8525,
      "step": 963
    },
    {
      "epoch": 6.370921106980586,
      "grad_norm": 11.168596267700195,
      "learning_rate": 0.000964,
      "loss": 2.3113,
      "step": 964
    },
    {
      "epoch": 6.377529946303181,
      "grad_norm": 36.74414825439453,
      "learning_rate": 0.000965,
      "loss": 6.5068,
      "step": 965
    },
    {
      "epoch": 6.384138785625774,
      "grad_norm": 28.460956573486328,
      "learning_rate": 0.000966,
      "loss": 6.6503,
      "step": 966
    },
    {
      "epoch": 6.390747624948369,
      "grad_norm": 25.888500213623047,
      "learning_rate": 0.000967,
      "loss": 4.4571,
      "step": 967
    },
    {
      "epoch": 6.397356464270962,
      "grad_norm": 17.442569732666016,
      "learning_rate": 0.000968,
      "loss": 3.6119,
      "step": 968
    },
    {
      "epoch": 6.403965303593556,
      "grad_norm": 12.407635688781738,
      "learning_rate": 0.000969,
      "loss": 1.7227,
      "step": 969
    },
    {
      "epoch": 6.4105741429161505,
      "grad_norm": 15.90398120880127,
      "learning_rate": 0.0009699999999999999,
      "loss": 1.6391,
      "step": 970
    },
    {
      "epoch": 6.417182982238744,
      "grad_norm": 32.972801208496094,
      "learning_rate": 0.000971,
      "loss": 3.5168,
      "step": 971
    },
    {
      "epoch": 6.4237918215613385,
      "grad_norm": 50.1110954284668,
      "learning_rate": 0.000972,
      "loss": 5.9213,
      "step": 972
    },
    {
      "epoch": 6.430400660883932,
      "grad_norm": 77.44674682617188,
      "learning_rate": 0.000973,
      "loss": 13.653,
      "step": 973
    },
    {
      "epoch": 6.437009500206527,
      "grad_norm": 36.56364059448242,
      "learning_rate": 0.000974,
      "loss": 5.5126,
      "step": 974
    },
    {
      "epoch": 6.44361833952912,
      "grad_norm": 6.429546356201172,
      "learning_rate": 0.000975,
      "loss": 5.3611,
      "step": 975
    },
    {
      "epoch": 6.450227178851714,
      "grad_norm": 6.379133701324463,
      "learning_rate": 0.000976,
      "loss": 2.3403,
      "step": 976
    },
    {
      "epoch": 6.456836018174308,
      "grad_norm": 18.460895538330078,
      "learning_rate": 0.000977,
      "loss": 2.766,
      "step": 977
    },
    {
      "epoch": 6.463444857496902,
      "grad_norm": 20.213350296020508,
      "learning_rate": 0.000978,
      "loss": 5.1254,
      "step": 978
    },
    {
      "epoch": 6.470053696819496,
      "grad_norm": 12.456457138061523,
      "learning_rate": 0.000979,
      "loss": 1.4942,
      "step": 979
    },
    {
      "epoch": 6.47666253614209,
      "grad_norm": 5.0907511711120605,
      "learning_rate": 0.00098,
      "loss": 0.7394,
      "step": 980
    },
    {
      "epoch": 6.483271375464684,
      "grad_norm": 16.221141815185547,
      "learning_rate": 0.000981,
      "loss": 2.4656,
      "step": 981
    },
    {
      "epoch": 6.489880214787278,
      "grad_norm": 38.546897888183594,
      "learning_rate": 0.000982,
      "loss": 6.5833,
      "step": 982
    },
    {
      "epoch": 6.496489054109872,
      "grad_norm": 40.13234329223633,
      "learning_rate": 0.000983,
      "loss": 5.1336,
      "step": 983
    },
    {
      "epoch": 6.503097893432466,
      "grad_norm": 42.55814743041992,
      "learning_rate": 0.000984,
      "loss": 5.9151,
      "step": 984
    },
    {
      "epoch": 6.50970673275506,
      "grad_norm": 16.040664672851562,
      "learning_rate": 0.000985,
      "loss": 2.2506,
      "step": 985
    },
    {
      "epoch": 6.516315572077654,
      "grad_norm": 8.188600540161133,
      "learning_rate": 0.0009860000000000001,
      "loss": 1.5824,
      "step": 986
    },
    {
      "epoch": 6.522924411400248,
      "grad_norm": 6.920970916748047,
      "learning_rate": 0.000987,
      "loss": 4.2209,
      "step": 987
    },
    {
      "epoch": 6.529533250722842,
      "grad_norm": 6.602786064147949,
      "learning_rate": 0.000988,
      "loss": 1.7637,
      "step": 988
    },
    {
      "epoch": 6.536142090045436,
      "grad_norm": 4.728143215179443,
      "learning_rate": 0.000989,
      "loss": 2.5296,
      "step": 989
    },
    {
      "epoch": 6.5427509293680295,
      "grad_norm": 1.278894305229187,
      "learning_rate": 0.00099,
      "loss": 0.5429,
      "step": 990
    },
    {
      "epoch": 6.549359768690624,
      "grad_norm": 7.293804168701172,
      "learning_rate": 0.000991,
      "loss": 1.4541,
      "step": 991
    },
    {
      "epoch": 6.555968608013218,
      "grad_norm": 1.9878835678100586,
      "learning_rate": 0.000992,
      "loss": 1.4755,
      "step": 992
    },
    {
      "epoch": 6.562577447335812,
      "grad_norm": 17.28281021118164,
      "learning_rate": 0.000993,
      "loss": 3.1659,
      "step": 993
    },
    {
      "epoch": 6.569186286658406,
      "grad_norm": 11.894604682922363,
      "learning_rate": 0.000994,
      "loss": 2.148,
      "step": 994
    },
    {
      "epoch": 6.575795125980999,
      "grad_norm": 4.6717529296875,
      "learning_rate": 0.000995,
      "loss": 3.881,
      "step": 995
    },
    {
      "epoch": 6.582403965303594,
      "grad_norm": 25.85169792175293,
      "learning_rate": 0.000996,
      "loss": 5.6771,
      "step": 996
    },
    {
      "epoch": 6.589012804626187,
      "grad_norm": 8.003761291503906,
      "learning_rate": 0.000997,
      "loss": 3.8777,
      "step": 997
    },
    {
      "epoch": 6.595621643948782,
      "grad_norm": 5.9420552253723145,
      "learning_rate": 0.000998,
      "loss": 1.8093,
      "step": 998
    },
    {
      "epoch": 6.602230483271375,
      "grad_norm": 4.434621334075928,
      "learning_rate": 0.000999,
      "loss": 3.474,
      "step": 999
    },
    {
      "epoch": 6.608839322593969,
      "grad_norm": 25.131372451782227,
      "learning_rate": 0.001,
      "loss": 4.3564,
      "step": 1000
    },
    {
      "epoch": 6.6154481619165635,
      "grad_norm": 3.187282085418701,
      "learning_rate": 0.0009995049504950494,
      "loss": 3.7115,
      "step": 1001
    },
    {
      "epoch": 6.622057001239157,
      "grad_norm": 25.551687240600586,
      "learning_rate": 0.000999009900990099,
      "loss": 4.7091,
      "step": 1002
    },
    {
      "epoch": 6.628665840561752,
      "grad_norm": 12.369058609008789,
      "learning_rate": 0.0009985148514851485,
      "loss": 2.3414,
      "step": 1003
    },
    {
      "epoch": 6.635274679884345,
      "grad_norm": 2.043790340423584,
      "learning_rate": 0.0009980198019801981,
      "loss": 4.7652,
      "step": 1004
    },
    {
      "epoch": 6.64188351920694,
      "grad_norm": 26.470443725585938,
      "learning_rate": 0.0009975247524752475,
      "loss": 3.935,
      "step": 1005
    },
    {
      "epoch": 6.648492358529533,
      "grad_norm": 39.12077713012695,
      "learning_rate": 0.000997029702970297,
      "loss": 6.0388,
      "step": 1006
    },
    {
      "epoch": 6.655101197852127,
      "grad_norm": 24.23871421813965,
      "learning_rate": 0.0009965346534653466,
      "loss": 3.4015,
      "step": 1007
    },
    {
      "epoch": 6.661710037174721,
      "grad_norm": 32.31462860107422,
      "learning_rate": 0.000996039603960396,
      "loss": 4.6115,
      "step": 1008
    },
    {
      "epoch": 6.668318876497315,
      "grad_norm": 14.291285514831543,
      "learning_rate": 0.0009955445544554456,
      "loss": 5.459,
      "step": 1009
    },
    {
      "epoch": 6.674927715819909,
      "grad_norm": 14.228595733642578,
      "learning_rate": 0.000995049504950495,
      "loss": 7.8533,
      "step": 1010
    },
    {
      "epoch": 6.681536555142503,
      "grad_norm": 4.02821683883667,
      "learning_rate": 0.0009945544554455445,
      "loss": 5.5588,
      "step": 1011
    },
    {
      "epoch": 6.6881453944650975,
      "grad_norm": 21.769624710083008,
      "learning_rate": 0.0009940594059405941,
      "loss": 4.0347,
      "step": 1012
    },
    {
      "epoch": 6.694754233787691,
      "grad_norm": 3.1096396446228027,
      "learning_rate": 0.0009935643564356435,
      "loss": 2.7937,
      "step": 1013
    },
    {
      "epoch": 6.701363073110285,
      "grad_norm": 21.215299606323242,
      "learning_rate": 0.0009930693069306932,
      "loss": 3.2623,
      "step": 1014
    },
    {
      "epoch": 6.707971912432879,
      "grad_norm": 4.176438331604004,
      "learning_rate": 0.0009925742574257426,
      "loss": 3.3354,
      "step": 1015
    },
    {
      "epoch": 6.714580751755473,
      "grad_norm": 4.1290602684021,
      "learning_rate": 0.000992079207920792,
      "loss": 2.2306,
      "step": 1016
    },
    {
      "epoch": 6.721189591078067,
      "grad_norm": 7.445465564727783,
      "learning_rate": 0.0009915841584158416,
      "loss": 1.9367,
      "step": 1017
    },
    {
      "epoch": 6.727798430400661,
      "grad_norm": 12.394011497497559,
      "learning_rate": 0.000991089108910891,
      "loss": 2.4933,
      "step": 1018
    },
    {
      "epoch": 6.734407269723254,
      "grad_norm": 4.860274791717529,
      "learning_rate": 0.0009905940594059407,
      "loss": 1.2199,
      "step": 1019
    },
    {
      "epoch": 6.741016109045849,
      "grad_norm": 10.714902877807617,
      "learning_rate": 0.0009900990099009901,
      "loss": 1.179,
      "step": 1020
    },
    {
      "epoch": 6.7476249483684425,
      "grad_norm": 30.7912540435791,
      "learning_rate": 0.0009896039603960395,
      "loss": 4.3778,
      "step": 1021
    },
    {
      "epoch": 6.754233787691037,
      "grad_norm": 29.389293670654297,
      "learning_rate": 0.0009891089108910892,
      "loss": 4.7346,
      "step": 1022
    },
    {
      "epoch": 6.760842627013631,
      "grad_norm": 20.17000389099121,
      "learning_rate": 0.0009886138613861386,
      "loss": 2.0337,
      "step": 1023
    },
    {
      "epoch": 6.767451466336225,
      "grad_norm": 16.612308502197266,
      "learning_rate": 0.0009881188118811882,
      "loss": 3.7072,
      "step": 1024
    },
    {
      "epoch": 6.774060305658819,
      "grad_norm": 5.503606796264648,
      "learning_rate": 0.0009876237623762376,
      "loss": 1.2707,
      "step": 1025
    },
    {
      "epoch": 6.780669144981412,
      "grad_norm": 22.5395450592041,
      "learning_rate": 0.000987128712871287,
      "loss": 3.5016,
      "step": 1026
    },
    {
      "epoch": 6.787277984304007,
      "grad_norm": 18.919879913330078,
      "learning_rate": 0.0009866336633663367,
      "loss": 3.4217,
      "step": 1027
    },
    {
      "epoch": 6.7938868236266,
      "grad_norm": 14.761368751525879,
      "learning_rate": 0.000986138613861386,
      "loss": 3.1565,
      "step": 1028
    },
    {
      "epoch": 6.800495662949195,
      "grad_norm": 5.291067600250244,
      "learning_rate": 0.0009856435643564357,
      "loss": 3.9097,
      "step": 1029
    },
    {
      "epoch": 6.807104502271788,
      "grad_norm": 7.819884300231934,
      "learning_rate": 0.0009851485148514852,
      "loss": 5.2877,
      "step": 1030
    },
    {
      "epoch": 6.813713341594383,
      "grad_norm": 16.75922393798828,
      "learning_rate": 0.0009846534653465346,
      "loss": 2.7218,
      "step": 1031
    },
    {
      "epoch": 6.8203221809169765,
      "grad_norm": 22.66811180114746,
      "learning_rate": 0.0009841584158415842,
      "loss": 5.6177,
      "step": 1032
    },
    {
      "epoch": 6.82693102023957,
      "grad_norm": 12.154626846313477,
      "learning_rate": 0.0009836633663366336,
      "loss": 1.9967,
      "step": 1033
    },
    {
      "epoch": 6.833539859562165,
      "grad_norm": 10.237649917602539,
      "learning_rate": 0.0009831683168316833,
      "loss": 1.9752,
      "step": 1034
    },
    {
      "epoch": 6.840148698884758,
      "grad_norm": 1.8727368116378784,
      "learning_rate": 0.0009826732673267327,
      "loss": 0.6668,
      "step": 1035
    },
    {
      "epoch": 6.846757538207353,
      "grad_norm": 16.74131202697754,
      "learning_rate": 0.000982178217821782,
      "loss": 1.8375,
      "step": 1036
    },
    {
      "epoch": 6.853366377529946,
      "grad_norm": 30.00929832458496,
      "learning_rate": 0.0009816831683168317,
      "loss": 3.8521,
      "step": 1037
    },
    {
      "epoch": 6.85997521685254,
      "grad_norm": 18.715906143188477,
      "learning_rate": 0.0009811881188118811,
      "loss": 3.3518,
      "step": 1038
    },
    {
      "epoch": 6.866584056175134,
      "grad_norm": 3.793933391571045,
      "learning_rate": 0.0009806930693069308,
      "loss": 1.8165,
      "step": 1039
    },
    {
      "epoch": 6.873192895497728,
      "grad_norm": 15.85431957244873,
      "learning_rate": 0.0009801980198019802,
      "loss": 7.6491,
      "step": 1040
    },
    {
      "epoch": 6.879801734820322,
      "grad_norm": 11.031473159790039,
      "learning_rate": 0.0009797029702970296,
      "loss": 2.454,
      "step": 1041
    },
    {
      "epoch": 6.886410574142916,
      "grad_norm": 4.972548484802246,
      "learning_rate": 0.0009792079207920793,
      "loss": 2.6287,
      "step": 1042
    },
    {
      "epoch": 6.8930194134655105,
      "grad_norm": 15.754006385803223,
      "learning_rate": 0.0009787128712871287,
      "loss": 5.079,
      "step": 1043
    },
    {
      "epoch": 6.899628252788104,
      "grad_norm": 4.331609725952148,
      "learning_rate": 0.0009782178217821783,
      "loss": 1.4828,
      "step": 1044
    },
    {
      "epoch": 6.906237092110698,
      "grad_norm": 19.717693328857422,
      "learning_rate": 0.0009777227722772277,
      "loss": 5.8911,
      "step": 1045
    },
    {
      "epoch": 6.912845931433292,
      "grad_norm": 19.61189842224121,
      "learning_rate": 0.0009772277227722771,
      "loss": 3.6797,
      "step": 1046
    },
    {
      "epoch": 6.919454770755886,
      "grad_norm": 5.63738489151001,
      "learning_rate": 0.0009767326732673268,
      "loss": 2.0748,
      "step": 1047
    },
    {
      "epoch": 6.92606361007848,
      "grad_norm": 3.8791394233703613,
      "learning_rate": 0.0009762376237623762,
      "loss": 1.4918,
      "step": 1048
    },
    {
      "epoch": 6.932672449401074,
      "grad_norm": 11.641627311706543,
      "learning_rate": 0.0009757425742574257,
      "loss": 1.4972,
      "step": 1049
    },
    {
      "epoch": 6.939281288723668,
      "grad_norm": 13.294610023498535,
      "learning_rate": 0.0009752475247524752,
      "loss": 4.4905,
      "step": 1050
    },
    {
      "epoch": 6.945890128046262,
      "grad_norm": 27.391353607177734,
      "learning_rate": 0.0009747524752475248,
      "loss": 2.9831,
      "step": 1051
    },
    {
      "epoch": 6.9524989673688555,
      "grad_norm": 22.072463989257812,
      "learning_rate": 0.0009742574257425743,
      "loss": 4.8692,
      "step": 1052
    },
    {
      "epoch": 6.95910780669145,
      "grad_norm": 17.073274612426758,
      "learning_rate": 0.0009737623762376237,
      "loss": 1.6728,
      "step": 1053
    },
    {
      "epoch": 6.965716646014044,
      "grad_norm": 16.043556213378906,
      "learning_rate": 0.0009732673267326732,
      "loss": 1.7681,
      "step": 1054
    },
    {
      "epoch": 6.972325485336638,
      "grad_norm": 19.463302612304688,
      "learning_rate": 0.0009727722772277228,
      "loss": 3.3352,
      "step": 1055
    },
    {
      "epoch": 6.978934324659232,
      "grad_norm": 11.971573829650879,
      "learning_rate": 0.0009722772277227723,
      "loss": 2.7589,
      "step": 1056
    },
    {
      "epoch": 6.985543163981825,
      "grad_norm": 14.783883094787598,
      "learning_rate": 0.0009717821782178218,
      "loss": 2.0706,
      "step": 1057
    },
    {
      "epoch": 6.99215200330442,
      "grad_norm": 9.213553428649902,
      "learning_rate": 0.0009712871287128712,
      "loss": 1.6123,
      "step": 1058
    },
    {
      "epoch": 6.998760842627013,
      "grad_norm": 11.271685600280762,
      "learning_rate": 0.0009707920792079208,
      "loss": 2.4169,
      "step": 1059
    },
    {
      "epoch": 6.998760842627013,
      "eval_validation_error_bar": 0.037742847033575506,
      "eval_validation_loss": 4.185112476348877,
      "eval_validation_pearsonr": 0.6959063538565461,
      "eval_validation_rmse": 2.0457546710968018,
      "eval_validation_runtime": 28.5006,
      "eval_validation_samples_per_second": 7.123,
      "eval_validation_spearman": 0.7245352173319161,
      "eval_validation_steps_per_second": 7.123,
      "step": 1059
    },
    {
      "epoch": 6.998760842627013,
      "eval_test_error_bar": 0.040818597718420196,
      "eval_test_loss": 7.208502292633057,
      "eval_test_pearsonr": 0.49735230002934405,
      "eval_test_rmse": 2.6848654747009277,
      "eval_test_runtime": 45.8108,
      "eval_test_samples_per_second": 7.116,
      "eval_test_spearman": 0.5645129419189012,
      "eval_test_steps_per_second": 7.116,
      "step": 1059
    },
    {
      "epoch": 7.005369681949608,
      "grad_norm": 9.476448059082031,
      "learning_rate": 0.0009702970297029703,
      "loss": 3.5847,
      "step": 1060
    },
    {
      "epoch": 7.0119785212722014,
      "grad_norm": 11.203858375549316,
      "learning_rate": 0.0009698019801980198,
      "loss": 2.2632,
      "step": 1061
    },
    {
      "epoch": 7.018587360594796,
      "grad_norm": 9.824871063232422,
      "learning_rate": 0.0009693069306930693,
      "loss": 1.9916,
      "step": 1062
    },
    {
      "epoch": 7.0251961999173895,
      "grad_norm": 13.292434692382812,
      "learning_rate": 0.0009688118811881188,
      "loss": 3.7277,
      "step": 1063
    },
    {
      "epoch": 7.031805039239983,
      "grad_norm": 5.473818778991699,
      "learning_rate": 0.0009683168316831683,
      "loss": 1.1816,
      "step": 1064
    },
    {
      "epoch": 7.038413878562578,
      "grad_norm": 27.11549186706543,
      "learning_rate": 0.0009678217821782178,
      "loss": 2.7556,
      "step": 1065
    },
    {
      "epoch": 7.045022717885171,
      "grad_norm": 20.324960708618164,
      "learning_rate": 0.0009673267326732673,
      "loss": 1.8232,
      "step": 1066
    },
    {
      "epoch": 7.051631557207766,
      "grad_norm": 1.5396523475646973,
      "learning_rate": 0.0009668316831683169,
      "loss": 0.7945,
      "step": 1067
    },
    {
      "epoch": 7.058240396530359,
      "grad_norm": 26.206809997558594,
      "learning_rate": 0.0009663366336633663,
      "loss": 3.1234,
      "step": 1068
    },
    {
      "epoch": 7.064849235852954,
      "grad_norm": 15.14804458618164,
      "learning_rate": 0.0009658415841584158,
      "loss": 2.2272,
      "step": 1069
    },
    {
      "epoch": 7.071458075175547,
      "grad_norm": 3.7977824211120605,
      "learning_rate": 0.0009653465346534653,
      "loss": 1.6669,
      "step": 1070
    },
    {
      "epoch": 7.078066914498141,
      "grad_norm": 7.976273059844971,
      "learning_rate": 0.0009648514851485149,
      "loss": 0.8968,
      "step": 1071
    },
    {
      "epoch": 7.0846757538207354,
      "grad_norm": 7.100482940673828,
      "learning_rate": 0.0009643564356435644,
      "loss": 1.8789,
      "step": 1072
    },
    {
      "epoch": 7.091284593143329,
      "grad_norm": 8.214008331298828,
      "learning_rate": 0.0009638613861386138,
      "loss": 1.749,
      "step": 1073
    },
    {
      "epoch": 7.0978934324659235,
      "grad_norm": 13.360978126525879,
      "learning_rate": 0.0009633663366336633,
      "loss": 2.5108,
      "step": 1074
    },
    {
      "epoch": 7.104502271788517,
      "grad_norm": 3.4604554176330566,
      "learning_rate": 0.0009628712871287129,
      "loss": 2.2784,
      "step": 1075
    },
    {
      "epoch": 7.111111111111111,
      "grad_norm": 17.274389266967773,
      "learning_rate": 0.0009623762376237624,
      "loss": 2.5445,
      "step": 1076
    },
    {
      "epoch": 7.117719950433705,
      "grad_norm": 16.416412353515625,
      "learning_rate": 0.0009618811881188119,
      "loss": 1.7426,
      "step": 1077
    },
    {
      "epoch": 7.124328789756299,
      "grad_norm": 11.185354232788086,
      "learning_rate": 0.0009613861386138613,
      "loss": 2.6003,
      "step": 1078
    },
    {
      "epoch": 7.130937629078893,
      "grad_norm": 5.102348327636719,
      "learning_rate": 0.0009608910891089109,
      "loss": 3.5228,
      "step": 1079
    },
    {
      "epoch": 7.137546468401487,
      "grad_norm": 14.552569389343262,
      "learning_rate": 0.0009603960396039604,
      "loss": 2.0068,
      "step": 1080
    },
    {
      "epoch": 7.144155307724081,
      "grad_norm": 18.69921875,
      "learning_rate": 0.0009599009900990099,
      "loss": 1.4416,
      "step": 1081
    },
    {
      "epoch": 7.150764147046675,
      "grad_norm": 5.0687031745910645,
      "learning_rate": 0.0009594059405940594,
      "loss": 1.2466,
      "step": 1082
    },
    {
      "epoch": 7.1573729863692686,
      "grad_norm": 22.260839462280273,
      "learning_rate": 0.0009589108910891089,
      "loss": 2.7127,
      "step": 1083
    },
    {
      "epoch": 7.163981825691863,
      "grad_norm": 2.818303108215332,
      "learning_rate": 0.0009584158415841584,
      "loss": 3.7094,
      "step": 1084
    },
    {
      "epoch": 7.170590665014457,
      "grad_norm": 8.153510093688965,
      "learning_rate": 0.0009579207920792079,
      "loss": 2.4861,
      "step": 1085
    },
    {
      "epoch": 7.177199504337051,
      "grad_norm": 16.015169143676758,
      "learning_rate": 0.0009574257425742574,
      "loss": 3.4299,
      "step": 1086
    },
    {
      "epoch": 7.183808343659645,
      "grad_norm": 2.6139214038848877,
      "learning_rate": 0.000956930693069307,
      "loss": 3.8018,
      "step": 1087
    },
    {
      "epoch": 7.190417182982239,
      "grad_norm": 9.48239517211914,
      "learning_rate": 0.0009564356435643564,
      "loss": 3.3183,
      "step": 1088
    },
    {
      "epoch": 7.197026022304833,
      "grad_norm": 23.96441650390625,
      "learning_rate": 0.0009559405940594059,
      "loss": 2.7957,
      "step": 1089
    },
    {
      "epoch": 7.203634861627426,
      "grad_norm": 8.140054702758789,
      "learning_rate": 0.0009554455445544554,
      "loss": 3.8177,
      "step": 1090
    },
    {
      "epoch": 7.210243700950021,
      "grad_norm": 2.224050521850586,
      "learning_rate": 0.000954950495049505,
      "loss": 1.6749,
      "step": 1091
    },
    {
      "epoch": 7.2168525402726145,
      "grad_norm": 20.991737365722656,
      "learning_rate": 0.0009544554455445545,
      "loss": 3.7791,
      "step": 1092
    },
    {
      "epoch": 7.223461379595209,
      "grad_norm": 15.138482093811035,
      "learning_rate": 0.0009539603960396039,
      "loss": 1.7346,
      "step": 1093
    },
    {
      "epoch": 7.2300702189178025,
      "grad_norm": 14.960205078125,
      "learning_rate": 0.0009534653465346534,
      "loss": 3.9532,
      "step": 1094
    },
    {
      "epoch": 7.236679058240396,
      "grad_norm": 3.4464406967163086,
      "learning_rate": 0.000952970297029703,
      "loss": 1.0328,
      "step": 1095
    },
    {
      "epoch": 7.243287897562991,
      "grad_norm": 6.990484237670898,
      "learning_rate": 0.0009524752475247525,
      "loss": 2.9936,
      "step": 1096
    },
    {
      "epoch": 7.249896736885584,
      "grad_norm": 27.844905853271484,
      "learning_rate": 0.000951980198019802,
      "loss": 4.0816,
      "step": 1097
    },
    {
      "epoch": 7.256505576208179,
      "grad_norm": 27.45221519470215,
      "learning_rate": 0.0009514851485148514,
      "loss": 3.475,
      "step": 1098
    },
    {
      "epoch": 7.263114415530772,
      "grad_norm": 15.23755931854248,
      "learning_rate": 0.0009509900990099009,
      "loss": 3.3716,
      "step": 1099
    },
    {
      "epoch": 7.269723254853367,
      "grad_norm": 23.502199172973633,
      "learning_rate": 0.0009504950495049505,
      "loss": 4.115,
      "step": 1100
    },
    {
      "epoch": 7.27633209417596,
      "grad_norm": 17.328231811523438,
      "learning_rate": 0.00095,
      "loss": 2.6995,
      "step": 1101
    },
    {
      "epoch": 7.282940933498554,
      "grad_norm": 20.86073875427246,
      "learning_rate": 0.0009495049504950495,
      "loss": 2.436,
      "step": 1102
    },
    {
      "epoch": 7.2895497728211485,
      "grad_norm": 30.46039390563965,
      "learning_rate": 0.0009490099009900989,
      "loss": 3.9705,
      "step": 1103
    },
    {
      "epoch": 7.296158612143742,
      "grad_norm": 22.11568832397461,
      "learning_rate": 0.0009485148514851485,
      "loss": 4.3135,
      "step": 1104
    },
    {
      "epoch": 7.3027674514663365,
      "grad_norm": 19.398229598999023,
      "learning_rate": 0.000948019801980198,
      "loss": 2.2646,
      "step": 1105
    },
    {
      "epoch": 7.30937629078893,
      "grad_norm": 8.910177230834961,
      "learning_rate": 0.0009475247524752475,
      "loss": 4.0173,
      "step": 1106
    },
    {
      "epoch": 7.315985130111525,
      "grad_norm": 33.1898307800293,
      "learning_rate": 0.000947029702970297,
      "loss": 5.428,
      "step": 1107
    },
    {
      "epoch": 7.322593969434118,
      "grad_norm": 25.85342025756836,
      "learning_rate": 0.0009465346534653465,
      "loss": 3.9238,
      "step": 1108
    },
    {
      "epoch": 7.329202808756712,
      "grad_norm": 16.663837432861328,
      "learning_rate": 0.000946039603960396,
      "loss": 1.3853,
      "step": 1109
    },
    {
      "epoch": 7.335811648079306,
      "grad_norm": 7.502276420593262,
      "learning_rate": 0.0009455445544554455,
      "loss": 2.7807,
      "step": 1110
    },
    {
      "epoch": 7.3424204874019,
      "grad_norm": 7.06001615524292,
      "learning_rate": 0.000945049504950495,
      "loss": 4.0808,
      "step": 1111
    },
    {
      "epoch": 7.349029326724494,
      "grad_norm": 32.354251861572266,
      "learning_rate": 0.0009445544554455446,
      "loss": 3.4659,
      "step": 1112
    },
    {
      "epoch": 7.355638166047088,
      "grad_norm": 33.454681396484375,
      "learning_rate": 0.000944059405940594,
      "loss": 4.0488,
      "step": 1113
    },
    {
      "epoch": 7.362247005369682,
      "grad_norm": 18.736743927001953,
      "learning_rate": 0.0009435643564356435,
      "loss": 1.1513,
      "step": 1114
    },
    {
      "epoch": 7.368855844692276,
      "grad_norm": 10.541533470153809,
      "learning_rate": 0.000943069306930693,
      "loss": 1.2002,
      "step": 1115
    },
    {
      "epoch": 7.37546468401487,
      "grad_norm": 2.8706459999084473,
      "learning_rate": 0.0009425742574257426,
      "loss": 4.9591,
      "step": 1116
    },
    {
      "epoch": 7.382073523337464,
      "grad_norm": 14.834343910217285,
      "learning_rate": 0.0009420792079207921,
      "loss": 2.5444,
      "step": 1117
    },
    {
      "epoch": 7.388682362660058,
      "grad_norm": 29.72985076904297,
      "learning_rate": 0.0009415841584158415,
      "loss": 5.5971,
      "step": 1118
    },
    {
      "epoch": 7.395291201982651,
      "grad_norm": 26.24844741821289,
      "learning_rate": 0.000941089108910891,
      "loss": 2.3646,
      "step": 1119
    },
    {
      "epoch": 7.401900041305246,
      "grad_norm": 4.666548252105713,
      "learning_rate": 0.0009405940594059406,
      "loss": 0.9368,
      "step": 1120
    },
    {
      "epoch": 7.408508880627839,
      "grad_norm": 3.5282247066497803,
      "learning_rate": 0.0009400990099009901,
      "loss": 2.4082,
      "step": 1121
    },
    {
      "epoch": 7.415117719950434,
      "grad_norm": 10.612327575683594,
      "learning_rate": 0.0009396039603960396,
      "loss": 2.2793,
      "step": 1122
    },
    {
      "epoch": 7.4217265592730275,
      "grad_norm": 12.595359802246094,
      "learning_rate": 0.000939108910891089,
      "loss": 1.3197,
      "step": 1123
    },
    {
      "epoch": 7.428335398595622,
      "grad_norm": 14.668562889099121,
      "learning_rate": 0.0009386138613861386,
      "loss": 1.9796,
      "step": 1124
    },
    {
      "epoch": 7.434944237918216,
      "grad_norm": 5.066205024719238,
      "learning_rate": 0.0009381188118811881,
      "loss": 1.7196,
      "step": 1125
    },
    {
      "epoch": 7.44155307724081,
      "grad_norm": 36.518550872802734,
      "learning_rate": 0.0009376237623762376,
      "loss": 5.0698,
      "step": 1126
    },
    {
      "epoch": 7.448161916563404,
      "grad_norm": 10.8688383102417,
      "learning_rate": 0.0009371287128712872,
      "loss": 1.7671,
      "step": 1127
    },
    {
      "epoch": 7.454770755885997,
      "grad_norm": 26.990493774414062,
      "learning_rate": 0.0009366336633663367,
      "loss": 4.745,
      "step": 1128
    },
    {
      "epoch": 7.461379595208592,
      "grad_norm": 18.39974594116211,
      "learning_rate": 0.0009361386138613862,
      "loss": 2.1583,
      "step": 1129
    },
    {
      "epoch": 7.467988434531185,
      "grad_norm": 8.655479431152344,
      "learning_rate": 0.0009356435643564357,
      "loss": 2.3113,
      "step": 1130
    },
    {
      "epoch": 7.47459727385378,
      "grad_norm": 14.542308807373047,
      "learning_rate": 0.0009351485148514852,
      "loss": 2.4707,
      "step": 1131
    },
    {
      "epoch": 7.481206113176373,
      "grad_norm": 4.597560405731201,
      "learning_rate": 0.0009346534653465348,
      "loss": 3.5993,
      "step": 1132
    },
    {
      "epoch": 7.487814952498967,
      "grad_norm": 26.126510620117188,
      "learning_rate": 0.0009341584158415842,
      "loss": 3.9419,
      "step": 1133
    },
    {
      "epoch": 7.4944237918215615,
      "grad_norm": 2.2549519538879395,
      "learning_rate": 0.0009336633663366337,
      "loss": 1.2108,
      "step": 1134
    },
    {
      "epoch": 7.501032631144155,
      "grad_norm": 20.91841697692871,
      "learning_rate": 0.0009331683168316832,
      "loss": 2.0198,
      "step": 1135
    },
    {
      "epoch": 7.50764147046675,
      "grad_norm": 21.7864990234375,
      "learning_rate": 0.0009326732673267328,
      "loss": 4.0562,
      "step": 1136
    },
    {
      "epoch": 7.514250309789343,
      "grad_norm": 5.6734089851379395,
      "learning_rate": 0.0009321782178217823,
      "loss": 2.2136,
      "step": 1137
    },
    {
      "epoch": 7.520859149111937,
      "grad_norm": 15.10505485534668,
      "learning_rate": 0.0009316831683168317,
      "loss": 5.7576,
      "step": 1138
    },
    {
      "epoch": 7.527467988434531,
      "grad_norm": 30.380319595336914,
      "learning_rate": 0.0009311881188118812,
      "loss": 3.5682,
      "step": 1139
    },
    {
      "epoch": 7.534076827757125,
      "grad_norm": 21.34581184387207,
      "learning_rate": 0.0009306930693069308,
      "loss": 2.8341,
      "step": 1140
    },
    {
      "epoch": 7.540685667079719,
      "grad_norm": 32.66477584838867,
      "learning_rate": 0.0009301980198019803,
      "loss": 3.8049,
      "step": 1141
    },
    {
      "epoch": 7.547294506402313,
      "grad_norm": 6.878883361816406,
      "learning_rate": 0.0009297029702970298,
      "loss": 0.667,
      "step": 1142
    },
    {
      "epoch": 7.553903345724907,
      "grad_norm": 6.381672382354736,
      "learning_rate": 0.0009292079207920792,
      "loss": 2.6576,
      "step": 1143
    },
    {
      "epoch": 7.560512185047501,
      "grad_norm": 18.292221069335938,
      "learning_rate": 0.0009287128712871288,
      "loss": 2.2433,
      "step": 1144
    },
    {
      "epoch": 7.5671210243700955,
      "grad_norm": 2.0138087272644043,
      "learning_rate": 0.0009282178217821783,
      "loss": 0.7193,
      "step": 1145
    },
    {
      "epoch": 7.573729863692689,
      "grad_norm": 2.5444960594177246,
      "learning_rate": 0.0009277227722772278,
      "loss": 4.7848,
      "step": 1146
    },
    {
      "epoch": 7.580338703015283,
      "grad_norm": 4.3202433586120605,
      "learning_rate": 0.0009272277227722773,
      "loss": 1.4974,
      "step": 1147
    },
    {
      "epoch": 7.586947542337877,
      "grad_norm": 26.988740921020508,
      "learning_rate": 0.0009267326732673268,
      "loss": 4.129,
      "step": 1148
    },
    {
      "epoch": 7.593556381660471,
      "grad_norm": 4.274287700653076,
      "learning_rate": 0.0009262376237623763,
      "loss": 1.3672,
      "step": 1149
    },
    {
      "epoch": 7.600165220983065,
      "grad_norm": 2.3689210414886475,
      "learning_rate": 0.0009257425742574258,
      "loss": 2.8716,
      "step": 1150
    },
    {
      "epoch": 7.606774060305659,
      "grad_norm": 17.15382957458496,
      "learning_rate": 0.0009252475247524753,
      "loss": 2.5166,
      "step": 1151
    },
    {
      "epoch": 7.613382899628252,
      "grad_norm": 4.3493571281433105,
      "learning_rate": 0.0009247524752475249,
      "loss": 3.5699,
      "step": 1152
    },
    {
      "epoch": 7.619991738950847,
      "grad_norm": 11.66142463684082,
      "learning_rate": 0.0009242574257425743,
      "loss": 3.3399,
      "step": 1153
    },
    {
      "epoch": 7.6266005782734405,
      "grad_norm": 14.854637145996094,
      "learning_rate": 0.0009237623762376238,
      "loss": 2.7269,
      "step": 1154
    },
    {
      "epoch": 7.633209417596035,
      "grad_norm": 20.715803146362305,
      "learning_rate": 0.0009232673267326733,
      "loss": 3.122,
      "step": 1155
    },
    {
      "epoch": 7.639818256918629,
      "grad_norm": 4.31148099899292,
      "learning_rate": 0.0009227722772277229,
      "loss": 4.4039,
      "step": 1156
    },
    {
      "epoch": 7.646427096241222,
      "grad_norm": 11.17652702331543,
      "learning_rate": 0.0009222772277227724,
      "loss": 3.2117,
      "step": 1157
    },
    {
      "epoch": 7.653035935563817,
      "grad_norm": 2.354100227355957,
      "learning_rate": 0.0009217821782178218,
      "loss": 2.5781,
      "step": 1158
    },
    {
      "epoch": 7.65964477488641,
      "grad_norm": 21.084718704223633,
      "learning_rate": 0.0009212871287128713,
      "loss": 3.1757,
      "step": 1159
    },
    {
      "epoch": 7.666253614209005,
      "grad_norm": 21.65345001220703,
      "learning_rate": 0.0009207920792079209,
      "loss": 3.7928,
      "step": 1160
    },
    {
      "epoch": 7.672862453531598,
      "grad_norm": 5.286661148071289,
      "learning_rate": 0.0009202970297029704,
      "loss": 2.6521,
      "step": 1161
    },
    {
      "epoch": 7.679471292854193,
      "grad_norm": 10.520648956298828,
      "learning_rate": 0.0009198019801980199,
      "loss": 1.7052,
      "step": 1162
    },
    {
      "epoch": 7.686080132176786,
      "grad_norm": 9.28662109375,
      "learning_rate": 0.0009193069306930693,
      "loss": 2.4792,
      "step": 1163
    },
    {
      "epoch": 7.692688971499381,
      "grad_norm": 30.711002349853516,
      "learning_rate": 0.0009188118811881188,
      "loss": 3.921,
      "step": 1164
    },
    {
      "epoch": 7.6992978108219745,
      "grad_norm": 0.9753094911575317,
      "learning_rate": 0.0009183168316831684,
      "loss": 0.785,
      "step": 1165
    },
    {
      "epoch": 7.705906650144568,
      "grad_norm": 15.65135669708252,
      "learning_rate": 0.0009178217821782179,
      "loss": 2.6783,
      "step": 1166
    },
    {
      "epoch": 7.712515489467163,
      "grad_norm": 10.740575790405273,
      "learning_rate": 0.0009173267326732674,
      "loss": 2.3391,
      "step": 1167
    },
    {
      "epoch": 7.719124328789756,
      "grad_norm": 8.368128776550293,
      "learning_rate": 0.0009168316831683168,
      "loss": 2.34,
      "step": 1168
    },
    {
      "epoch": 7.725733168112351,
      "grad_norm": 3.9289095401763916,
      "learning_rate": 0.0009163366336633664,
      "loss": 0.9211,
      "step": 1169
    },
    {
      "epoch": 7.732342007434944,
      "grad_norm": 22.516721725463867,
      "learning_rate": 0.0009158415841584159,
      "loss": 5.3226,
      "step": 1170
    },
    {
      "epoch": 7.738950846757538,
      "grad_norm": 3.6420538425445557,
      "learning_rate": 0.0009153465346534654,
      "loss": 3.8536,
      "step": 1171
    },
    {
      "epoch": 7.745559686080132,
      "grad_norm": 26.192405700683594,
      "learning_rate": 0.000914851485148515,
      "loss": 2.7424,
      "step": 1172
    },
    {
      "epoch": 7.752168525402726,
      "grad_norm": 51.284976959228516,
      "learning_rate": 0.0009143564356435644,
      "loss": 5.9322,
      "step": 1173
    },
    {
      "epoch": 7.75877736472532,
      "grad_norm": 46.7095947265625,
      "learning_rate": 0.0009138613861386139,
      "loss": 6.2199,
      "step": 1174
    },
    {
      "epoch": 7.765386204047914,
      "grad_norm": 26.8964900970459,
      "learning_rate": 0.0009133663366336634,
      "loss": 4.3824,
      "step": 1175
    },
    {
      "epoch": 7.771995043370508,
      "grad_norm": 7.249937534332275,
      "learning_rate": 0.0009128712871287129,
      "loss": 1.7958,
      "step": 1176
    },
    {
      "epoch": 7.778603882693102,
      "grad_norm": 9.760139465332031,
      "learning_rate": 0.0009123762376237625,
      "loss": 8.3565,
      "step": 1177
    },
    {
      "epoch": 7.785212722015696,
      "grad_norm": 14.356807708740234,
      "learning_rate": 0.0009118811881188119,
      "loss": 2.9613,
      "step": 1178
    },
    {
      "epoch": 7.79182156133829,
      "grad_norm": 13.55296802520752,
      "learning_rate": 0.0009113861386138614,
      "loss": 7.872,
      "step": 1179
    },
    {
      "epoch": 7.798430400660884,
      "grad_norm": 10.651131629943848,
      "learning_rate": 0.0009108910891089109,
      "loss": 1.6333,
      "step": 1180
    },
    {
      "epoch": 7.805039239983478,
      "grad_norm": 14.697077751159668,
      "learning_rate": 0.0009103960396039605,
      "loss": 3.7416,
      "step": 1181
    },
    {
      "epoch": 7.811648079306072,
      "grad_norm": 5.974489212036133,
      "learning_rate": 0.00090990099009901,
      "loss": 1.8273,
      "step": 1182
    },
    {
      "epoch": 7.8182569186286655,
      "grad_norm": 9.51723861694336,
      "learning_rate": 0.0009094059405940594,
      "loss": 1.1716,
      "step": 1183
    },
    {
      "epoch": 7.82486575795126,
      "grad_norm": 5.91031551361084,
      "learning_rate": 0.0009089108910891089,
      "loss": 2.4729,
      "step": 1184
    },
    {
      "epoch": 7.8314745972738535,
      "grad_norm": 25.77920150756836,
      "learning_rate": 0.0009084158415841585,
      "loss": 2.1687,
      "step": 1185
    },
    {
      "epoch": 7.838083436596448,
      "grad_norm": 21.57114028930664,
      "learning_rate": 0.000907920792079208,
      "loss": 3.7204,
      "step": 1186
    },
    {
      "epoch": 7.844692275919042,
      "grad_norm": 15.586792945861816,
      "learning_rate": 0.0009074257425742575,
      "loss": 1.1474,
      "step": 1187
    },
    {
      "epoch": 7.851301115241636,
      "grad_norm": 5.951668739318848,
      "learning_rate": 0.0009069306930693069,
      "loss": 1.2158,
      "step": 1188
    },
    {
      "epoch": 7.85790995456423,
      "grad_norm": 1.6548608541488647,
      "learning_rate": 0.0009064356435643565,
      "loss": 0.7218,
      "step": 1189
    },
    {
      "epoch": 7.864518793886823,
      "grad_norm": 8.874982833862305,
      "learning_rate": 0.000905940594059406,
      "loss": 2.3339,
      "step": 1190
    },
    {
      "epoch": 7.871127633209418,
      "grad_norm": 7.900473117828369,
      "learning_rate": 0.0009054455445544555,
      "loss": 2.4309,
      "step": 1191
    },
    {
      "epoch": 7.877736472532011,
      "grad_norm": 8.162046432495117,
      "learning_rate": 0.000904950495049505,
      "loss": 1.4634,
      "step": 1192
    },
    {
      "epoch": 7.884345311854606,
      "grad_norm": 6.465466499328613,
      "learning_rate": 0.0009044554455445545,
      "loss": 1.8708,
      "step": 1193
    },
    {
      "epoch": 7.8909541511771994,
      "grad_norm": 17.557647705078125,
      "learning_rate": 0.000903960396039604,
      "loss": 2.1272,
      "step": 1194
    },
    {
      "epoch": 7.897562990499793,
      "grad_norm": 26.99766731262207,
      "learning_rate": 0.0009034653465346535,
      "loss": 2.2904,
      "step": 1195
    },
    {
      "epoch": 7.9041718298223875,
      "grad_norm": 10.942673683166504,
      "learning_rate": 0.000902970297029703,
      "loss": 2.0216,
      "step": 1196
    },
    {
      "epoch": 7.910780669144981,
      "grad_norm": 1.5930991172790527,
      "learning_rate": 0.0009024752475247526,
      "loss": 1.4912,
      "step": 1197
    },
    {
      "epoch": 7.917389508467576,
      "grad_norm": 10.687105178833008,
      "learning_rate": 0.000901980198019802,
      "loss": 1.9796,
      "step": 1198
    },
    {
      "epoch": 7.923998347790169,
      "grad_norm": 35.39516067504883,
      "learning_rate": 0.0009014851485148515,
      "loss": 8.692,
      "step": 1199
    },
    {
      "epoch": 7.930607187112764,
      "grad_norm": 6.386179447174072,
      "learning_rate": 0.000900990099009901,
      "loss": 2.3147,
      "step": 1200
    },
    {
      "epoch": 7.937216026435357,
      "grad_norm": 13.70851993560791,
      "learning_rate": 0.0009004950495049506,
      "loss": 1.7014,
      "step": 1201
    },
    {
      "epoch": 7.943824865757951,
      "grad_norm": 5.624391078948975,
      "learning_rate": 0.0009000000000000001,
      "loss": 2.1638,
      "step": 1202
    },
    {
      "epoch": 7.950433705080545,
      "grad_norm": 14.838948249816895,
      "learning_rate": 0.0008995049504950495,
      "loss": 2.137,
      "step": 1203
    },
    {
      "epoch": 7.957042544403139,
      "grad_norm": 13.442492485046387,
      "learning_rate": 0.000899009900990099,
      "loss": 1.0742,
      "step": 1204
    },
    {
      "epoch": 7.9636513837257334,
      "grad_norm": 10.444559097290039,
      "learning_rate": 0.0008985148514851486,
      "loss": 2.1125,
      "step": 1205
    },
    {
      "epoch": 7.970260223048327,
      "grad_norm": 18.73930549621582,
      "learning_rate": 0.0008980198019801981,
      "loss": 3.6351,
      "step": 1206
    },
    {
      "epoch": 7.9768690623709215,
      "grad_norm": 21.296892166137695,
      "learning_rate": 0.0008975247524752476,
      "loss": 1.9615,
      "step": 1207
    },
    {
      "epoch": 7.983477901693515,
      "grad_norm": 3.4278101921081543,
      "learning_rate": 0.000897029702970297,
      "loss": 2.6004,
      "step": 1208
    },
    {
      "epoch": 7.990086741016109,
      "grad_norm": 19.384294509887695,
      "learning_rate": 0.0008965346534653466,
      "loss": 2.7422,
      "step": 1209
    },
    {
      "epoch": 7.996695580338703,
      "grad_norm": 29.878700256347656,
      "learning_rate": 0.0008960396039603961,
      "loss": 4.5518,
      "step": 1210
    },
    {
      "epoch": 7.996695580338703,
      "eval_validation_error_bar": 0.038507877623701045,
      "eval_validation_loss": 4.991768836975098,
      "eval_validation_pearsonr": 0.6460576821015799,
      "eval_validation_rmse": 2.234226703643799,
      "eval_validation_runtime": 28.5472,
      "eval_validation_samples_per_second": 7.111,
      "eval_validation_spearman": 0.7171445990561721,
      "eval_validation_steps_per_second": 7.111,
      "step": 1210
    },
    {
      "epoch": 7.996695580338703,
      "eval_test_error_bar": 0.03690966809519962,
      "eval_test_loss": 5.8644585609436035,
      "eval_test_pearsonr": 0.6098015557911117,
      "eval_test_rmse": 2.4216644763946533,
      "eval_test_runtime": 45.7815,
      "eval_test_samples_per_second": 7.121,
      "eval_test_spearman": 0.62741190211108,
      "eval_test_steps_per_second": 7.121,
      "step": 1210
    },
    {
      "epoch": 8.003304419661298,
      "grad_norm": 6.208991527557373,
      "learning_rate": 0.0008955445544554456,
      "loss": 1.583,
      "step": 1211
    },
    {
      "epoch": 8.009913258983891,
      "grad_norm": 2.726572036743164,
      "learning_rate": 0.0008950495049504951,
      "loss": 0.8081,
      "step": 1212
    },
    {
      "epoch": 8.016522098306485,
      "grad_norm": 4.5657548904418945,
      "learning_rate": 0.0008945544554455445,
      "loss": 0.8424,
      "step": 1213
    },
    {
      "epoch": 8.023130937629078,
      "grad_norm": 2.8707401752471924,
      "learning_rate": 0.0008940594059405941,
      "loss": 1.2362,
      "step": 1214
    },
    {
      "epoch": 8.029739776951672,
      "grad_norm": 4.263603210449219,
      "learning_rate": 0.0008935643564356436,
      "loss": 5.5227,
      "step": 1215
    },
    {
      "epoch": 8.036348616274267,
      "grad_norm": 14.599458694458008,
      "learning_rate": 0.0008930693069306931,
      "loss": 1.8426,
      "step": 1216
    },
    {
      "epoch": 8.042957455596861,
      "grad_norm": 10.471782684326172,
      "learning_rate": 0.0008925742574257427,
      "loss": 0.4597,
      "step": 1217
    },
    {
      "epoch": 8.049566294919455,
      "grad_norm": 12.657054901123047,
      "learning_rate": 0.0008920792079207921,
      "loss": 1.889,
      "step": 1218
    },
    {
      "epoch": 8.056175134242048,
      "grad_norm": 2.265068531036377,
      "learning_rate": 0.0008915841584158416,
      "loss": 1.3939,
      "step": 1219
    },
    {
      "epoch": 8.062783973564642,
      "grad_norm": 11.561443328857422,
      "learning_rate": 0.0008910891089108911,
      "loss": 2.0687,
      "step": 1220
    },
    {
      "epoch": 8.069392812887237,
      "grad_norm": 4.627461910247803,
      "learning_rate": 0.0008905940594059406,
      "loss": 1.6909,
      "step": 1221
    },
    {
      "epoch": 8.07600165220983,
      "grad_norm": 15.752243995666504,
      "learning_rate": 0.0008900990099009902,
      "loss": 3.3896,
      "step": 1222
    },
    {
      "epoch": 8.082610491532424,
      "grad_norm": 18.31694221496582,
      "learning_rate": 0.0008896039603960396,
      "loss": 2.0318,
      "step": 1223
    },
    {
      "epoch": 8.089219330855018,
      "grad_norm": 13.392425537109375,
      "learning_rate": 0.0008891089108910891,
      "loss": 1.296,
      "step": 1224
    },
    {
      "epoch": 8.095828170177613,
      "grad_norm": 19.010812759399414,
      "learning_rate": 0.0008886138613861386,
      "loss": 2.3982,
      "step": 1225
    },
    {
      "epoch": 8.102437009500207,
      "grad_norm": 5.846898078918457,
      "learning_rate": 0.0008881188118811882,
      "loss": 2.712,
      "step": 1226
    },
    {
      "epoch": 8.1090458488228,
      "grad_norm": 2.9169726371765137,
      "learning_rate": 0.0008876237623762377,
      "loss": 3.4685,
      "step": 1227
    },
    {
      "epoch": 8.115654688145394,
      "grad_norm": 8.05644416809082,
      "learning_rate": 0.0008871287128712871,
      "loss": 2.2445,
      "step": 1228
    },
    {
      "epoch": 8.122263527467988,
      "grad_norm": 13.21006965637207,
      "learning_rate": 0.0008866336633663366,
      "loss": 1.8441,
      "step": 1229
    },
    {
      "epoch": 8.128872366790583,
      "grad_norm": 9.397642135620117,
      "learning_rate": 0.0008861386138613862,
      "loss": 2.232,
      "step": 1230
    },
    {
      "epoch": 8.135481206113177,
      "grad_norm": 4.361274719238281,
      "learning_rate": 0.0008856435643564357,
      "loss": 2.1249,
      "step": 1231
    },
    {
      "epoch": 8.14209004543577,
      "grad_norm": 5.241922378540039,
      "learning_rate": 0.0008851485148514852,
      "loss": 1.3638,
      "step": 1232
    },
    {
      "epoch": 8.148698884758364,
      "grad_norm": 7.386017322540283,
      "learning_rate": 0.0008846534653465346,
      "loss": 1.1295,
      "step": 1233
    },
    {
      "epoch": 8.155307724080957,
      "grad_norm": 4.110328197479248,
      "learning_rate": 0.0008841584158415842,
      "loss": 1.2121,
      "step": 1234
    },
    {
      "epoch": 8.161916563403553,
      "grad_norm": 34.173492431640625,
      "learning_rate": 0.0008836633663366337,
      "loss": 3.938,
      "step": 1235
    },
    {
      "epoch": 8.168525402726146,
      "grad_norm": 6.688130855560303,
      "learning_rate": 0.0008831683168316832,
      "loss": 4.8593,
      "step": 1236
    },
    {
      "epoch": 8.17513424204874,
      "grad_norm": 1.9387640953063965,
      "learning_rate": 0.0008826732673267327,
      "loss": 2.6342,
      "step": 1237
    },
    {
      "epoch": 8.181743081371334,
      "grad_norm": 2.294252634048462,
      "learning_rate": 0.0008821782178217822,
      "loss": 1.872,
      "step": 1238
    },
    {
      "epoch": 8.188351920693927,
      "grad_norm": 7.765743255615234,
      "learning_rate": 0.0008816831683168317,
      "loss": 1.3696,
      "step": 1239
    },
    {
      "epoch": 8.194960760016523,
      "grad_norm": 4.280890464782715,
      "learning_rate": 0.0008811881188118812,
      "loss": 3.7418,
      "step": 1240
    },
    {
      "epoch": 8.201569599339116,
      "grad_norm": 5.968766212463379,
      "learning_rate": 0.0008806930693069307,
      "loss": 1.079,
      "step": 1241
    },
    {
      "epoch": 8.20817843866171,
      "grad_norm": 24.714353561401367,
      "learning_rate": 0.0008801980198019803,
      "loss": 2.5453,
      "step": 1242
    },
    {
      "epoch": 8.214787277984303,
      "grad_norm": 32.181915283203125,
      "learning_rate": 0.0008797029702970297,
      "loss": 3.3351,
      "step": 1243
    },
    {
      "epoch": 8.221396117306899,
      "grad_norm": 24.392797470092773,
      "learning_rate": 0.0008792079207920792,
      "loss": 2.2413,
      "step": 1244
    },
    {
      "epoch": 8.228004956629492,
      "grad_norm": 7.646967887878418,
      "learning_rate": 0.0008787128712871287,
      "loss": 2.0126,
      "step": 1245
    },
    {
      "epoch": 8.234613795952086,
      "grad_norm": 3.5103540420532227,
      "learning_rate": 0.0008782178217821783,
      "loss": 4.445,
      "step": 1246
    },
    {
      "epoch": 8.24122263527468,
      "grad_norm": 13.193443298339844,
      "learning_rate": 0.0008777227722772278,
      "loss": 1.7682,
      "step": 1247
    },
    {
      "epoch": 8.247831474597273,
      "grad_norm": 18.04486083984375,
      "learning_rate": 0.0008772277227722772,
      "loss": 2.6916,
      "step": 1248
    },
    {
      "epoch": 8.254440313919869,
      "grad_norm": 6.948596000671387,
      "learning_rate": 0.0008767326732673267,
      "loss": 1.6495,
      "step": 1249
    },
    {
      "epoch": 8.261049153242462,
      "grad_norm": 8.53995418548584,
      "learning_rate": 0.0008762376237623763,
      "loss": 3.3182,
      "step": 1250
    },
    {
      "epoch": 8.267657992565056,
      "grad_norm": 8.225464820861816,
      "learning_rate": 0.0008757425742574258,
      "loss": 4.0248,
      "step": 1251
    },
    {
      "epoch": 8.27426683188765,
      "grad_norm": 18.43722152709961,
      "learning_rate": 0.0008752475247524753,
      "loss": 3.9338,
      "step": 1252
    },
    {
      "epoch": 8.280875671210243,
      "grad_norm": 8.327340126037598,
      "learning_rate": 0.0008747524752475247,
      "loss": 3.4446,
      "step": 1253
    },
    {
      "epoch": 8.287484510532838,
      "grad_norm": 11.700328826904297,
      "learning_rate": 0.0008742574257425743,
      "loss": 3.7544,
      "step": 1254
    },
    {
      "epoch": 8.294093349855432,
      "grad_norm": 12.275969505310059,
      "learning_rate": 0.0008737623762376238,
      "loss": 2.7904,
      "step": 1255
    },
    {
      "epoch": 8.300702189178025,
      "grad_norm": 5.26766300201416,
      "learning_rate": 0.0008732673267326733,
      "loss": 1.4609,
      "step": 1256
    },
    {
      "epoch": 8.307311028500619,
      "grad_norm": 7.677497386932373,
      "learning_rate": 0.0008727722772277228,
      "loss": 3.6559,
      "step": 1257
    },
    {
      "epoch": 8.313919867823213,
      "grad_norm": 2.5795342922210693,
      "learning_rate": 0.0008722772277227722,
      "loss": 1.5999,
      "step": 1258
    },
    {
      "epoch": 8.320528707145808,
      "grad_norm": 5.05858039855957,
      "learning_rate": 0.0008717821782178218,
      "loss": 1.7557,
      "step": 1259
    },
    {
      "epoch": 8.327137546468402,
      "grad_norm": 8.741292953491211,
      "learning_rate": 0.0008712871287128713,
      "loss": 1.0842,
      "step": 1260
    },
    {
      "epoch": 8.333746385790995,
      "grad_norm": 14.440132141113281,
      "learning_rate": 0.0008707920792079208,
      "loss": 3.7913,
      "step": 1261
    },
    {
      "epoch": 8.340355225113589,
      "grad_norm": 15.272621154785156,
      "learning_rate": 0.0008702970297029704,
      "loss": 2.3866,
      "step": 1262
    },
    {
      "epoch": 8.346964064436184,
      "grad_norm": 12.133127212524414,
      "learning_rate": 0.0008698019801980198,
      "loss": 3.0431,
      "step": 1263
    },
    {
      "epoch": 8.353572903758778,
      "grad_norm": 20.847938537597656,
      "learning_rate": 0.0008693069306930693,
      "loss": 5.1103,
      "step": 1264
    },
    {
      "epoch": 8.360181743081371,
      "grad_norm": 15.560566902160645,
      "learning_rate": 0.0008688118811881188,
      "loss": 1.6457,
      "step": 1265
    },
    {
      "epoch": 8.366790582403965,
      "grad_norm": 19.100141525268555,
      "learning_rate": 0.0008683168316831684,
      "loss": 4.5022,
      "step": 1266
    },
    {
      "epoch": 8.373399421726559,
      "grad_norm": 26.56343650817871,
      "learning_rate": 0.0008678217821782179,
      "loss": 3.7356,
      "step": 1267
    },
    {
      "epoch": 8.380008261049154,
      "grad_norm": 2.8514046669006348,
      "learning_rate": 0.0008673267326732673,
      "loss": 3.7362,
      "step": 1268
    },
    {
      "epoch": 8.386617100371748,
      "grad_norm": 8.148209571838379,
      "learning_rate": 0.0008668316831683168,
      "loss": 1.1895,
      "step": 1269
    },
    {
      "epoch": 8.393225939694341,
      "grad_norm": 10.365650177001953,
      "learning_rate": 0.0008663366336633663,
      "loss": 1.7009,
      "step": 1270
    },
    {
      "epoch": 8.399834779016935,
      "grad_norm": 4.220599174499512,
      "learning_rate": 0.0008658415841584159,
      "loss": 2.2901,
      "step": 1271
    },
    {
      "epoch": 8.406443618339528,
      "grad_norm": 6.971029281616211,
      "learning_rate": 0.0008653465346534654,
      "loss": 0.9766,
      "step": 1272
    },
    {
      "epoch": 8.413052457662124,
      "grad_norm": 5.105071067810059,
      "learning_rate": 0.0008648514851485148,
      "loss": 2.0644,
      "step": 1273
    },
    {
      "epoch": 8.419661296984717,
      "grad_norm": 7.130478858947754,
      "learning_rate": 0.0008643564356435643,
      "loss": 0.9708,
      "step": 1274
    },
    {
      "epoch": 8.426270136307311,
      "grad_norm": 5.516958713531494,
      "learning_rate": 0.0008638613861386139,
      "loss": 2.1728,
      "step": 1275
    },
    {
      "epoch": 8.432878975629905,
      "grad_norm": 2.663665294647217,
      "learning_rate": 0.0008633663366336634,
      "loss": 1.8681,
      "step": 1276
    },
    {
      "epoch": 8.439487814952498,
      "grad_norm": 1.9534492492675781,
      "learning_rate": 0.0008628712871287129,
      "loss": 2.7045,
      "step": 1277
    },
    {
      "epoch": 8.446096654275093,
      "grad_norm": 1.703476071357727,
      "learning_rate": 0.0008623762376237623,
      "loss": 1.766,
      "step": 1278
    },
    {
      "epoch": 8.452705493597687,
      "grad_norm": 11.062798500061035,
      "learning_rate": 0.0008618811881188119,
      "loss": 0.9464,
      "step": 1279
    },
    {
      "epoch": 8.45931433292028,
      "grad_norm": 1.9691554307937622,
      "learning_rate": 0.0008613861386138614,
      "loss": 0.3684,
      "step": 1280
    },
    {
      "epoch": 8.465923172242874,
      "grad_norm": 17.129072189331055,
      "learning_rate": 0.0008608910891089109,
      "loss": 1.5906,
      "step": 1281
    },
    {
      "epoch": 8.47253201156547,
      "grad_norm": 10.91138744354248,
      "learning_rate": 0.0008603960396039604,
      "loss": 1.4907,
      "step": 1282
    },
    {
      "epoch": 8.479140850888063,
      "grad_norm": 8.269841194152832,
      "learning_rate": 0.0008599009900990099,
      "loss": 1.911,
      "step": 1283
    },
    {
      "epoch": 8.485749690210657,
      "grad_norm": 1.7544792890548706,
      "learning_rate": 0.0008594059405940594,
      "loss": 1.5663,
      "step": 1284
    },
    {
      "epoch": 8.49235852953325,
      "grad_norm": 5.852181434631348,
      "learning_rate": 0.0008589108910891089,
      "loss": 1.9422,
      "step": 1285
    },
    {
      "epoch": 8.498967368855844,
      "grad_norm": 18.361602783203125,
      "learning_rate": 0.0008584158415841584,
      "loss": 3.649,
      "step": 1286
    },
    {
      "epoch": 8.50557620817844,
      "grad_norm": 16.286420822143555,
      "learning_rate": 0.000857920792079208,
      "loss": 3.1861,
      "step": 1287
    },
    {
      "epoch": 8.512185047501033,
      "grad_norm": 5.773100852966309,
      "learning_rate": 0.0008574257425742574,
      "loss": 2.7312,
      "step": 1288
    },
    {
      "epoch": 8.518793886823627,
      "grad_norm": 3.7603726387023926,
      "learning_rate": 0.0008569306930693069,
      "loss": 2.0374,
      "step": 1289
    },
    {
      "epoch": 8.52540272614622,
      "grad_norm": 14.73166561126709,
      "learning_rate": 0.0008564356435643564,
      "loss": 2.2907,
      "step": 1290
    },
    {
      "epoch": 8.532011565468814,
      "grad_norm": 3.958524703979492,
      "learning_rate": 0.000855940594059406,
      "loss": 1.2885,
      "step": 1291
    },
    {
      "epoch": 8.53862040479141,
      "grad_norm": 6.628711223602295,
      "learning_rate": 0.0008554455445544555,
      "loss": 1.2316,
      "step": 1292
    },
    {
      "epoch": 8.545229244114003,
      "grad_norm": 9.674818992614746,
      "learning_rate": 0.0008549504950495049,
      "loss": 1.5414,
      "step": 1293
    },
    {
      "epoch": 8.551838083436596,
      "grad_norm": 16.98689079284668,
      "learning_rate": 0.0008544554455445544,
      "loss": 3.262,
      "step": 1294
    },
    {
      "epoch": 8.55844692275919,
      "grad_norm": 48.02684020996094,
      "learning_rate": 0.000853960396039604,
      "loss": 4.146,
      "step": 1295
    },
    {
      "epoch": 8.565055762081784,
      "grad_norm": 29.421110153198242,
      "learning_rate": 0.0008534653465346535,
      "loss": 3.0351,
      "step": 1296
    },
    {
      "epoch": 8.571664601404379,
      "grad_norm": 29.8265380859375,
      "learning_rate": 0.000852970297029703,
      "loss": 2.9648,
      "step": 1297
    },
    {
      "epoch": 8.578273440726973,
      "grad_norm": 35.504127502441406,
      "learning_rate": 0.0008524752475247524,
      "loss": 5.7037,
      "step": 1298
    },
    {
      "epoch": 8.584882280049566,
      "grad_norm": 16.736129760742188,
      "learning_rate": 0.000851980198019802,
      "loss": 1.6362,
      "step": 1299
    },
    {
      "epoch": 8.59149111937216,
      "grad_norm": 24.207965850830078,
      "learning_rate": 0.0008514851485148515,
      "loss": 5.192,
      "step": 1300
    },
    {
      "epoch": 8.598099958694753,
      "grad_norm": 10.52847671508789,
      "learning_rate": 0.000850990099009901,
      "loss": 0.9381,
      "step": 1301
    },
    {
      "epoch": 8.604708798017349,
      "grad_norm": 10.185251235961914,
      "learning_rate": 0.0008504950495049505,
      "loss": 1.1825,
      "step": 1302
    },
    {
      "epoch": 8.611317637339942,
      "grad_norm": 1.4376715421676636,
      "learning_rate": 0.00085,
      "loss": 0.9394,
      "step": 1303
    },
    {
      "epoch": 8.617926476662536,
      "grad_norm": 5.622876167297363,
      "learning_rate": 0.0008495049504950495,
      "loss": 2.0961,
      "step": 1304
    },
    {
      "epoch": 8.62453531598513,
      "grad_norm": 23.03361701965332,
      "learning_rate": 0.000849009900990099,
      "loss": 2.638,
      "step": 1305
    },
    {
      "epoch": 8.631144155307725,
      "grad_norm": 5.572436332702637,
      "learning_rate": 0.0008485148514851485,
      "loss": 3.0594,
      "step": 1306
    },
    {
      "epoch": 8.637752994630318,
      "grad_norm": 21.142711639404297,
      "learning_rate": 0.0008480198019801981,
      "loss": 3.3693,
      "step": 1307
    },
    {
      "epoch": 8.644361833952912,
      "grad_norm": 7.695724964141846,
      "learning_rate": 0.0008475247524752475,
      "loss": 1.5949,
      "step": 1308
    },
    {
      "epoch": 8.650970673275506,
      "grad_norm": 8.657766342163086,
      "learning_rate": 0.000847029702970297,
      "loss": 2.2316,
      "step": 1309
    },
    {
      "epoch": 8.6575795125981,
      "grad_norm": 18.303550720214844,
      "learning_rate": 0.0008465346534653465,
      "loss": 4.9028,
      "step": 1310
    },
    {
      "epoch": 8.664188351920695,
      "grad_norm": 15.853800773620605,
      "learning_rate": 0.000846039603960396,
      "loss": 2.1434,
      "step": 1311
    },
    {
      "epoch": 8.670797191243288,
      "grad_norm": 12.098089218139648,
      "learning_rate": 0.0008455445544554456,
      "loss": 3.1255,
      "step": 1312
    },
    {
      "epoch": 8.677406030565882,
      "grad_norm": 7.715877056121826,
      "learning_rate": 0.000845049504950495,
      "loss": 3.2099,
      "step": 1313
    },
    {
      "epoch": 8.684014869888475,
      "grad_norm": 34.51881408691406,
      "learning_rate": 0.0008445544554455445,
      "loss": 3.5203,
      "step": 1314
    },
    {
      "epoch": 8.69062370921107,
      "grad_norm": 15.92184066772461,
      "learning_rate": 0.000844059405940594,
      "loss": 1.7939,
      "step": 1315
    },
    {
      "epoch": 8.697232548533664,
      "grad_norm": 21.6960391998291,
      "learning_rate": 0.0008435643564356436,
      "loss": 2.7155,
      "step": 1316
    },
    {
      "epoch": 8.703841387856258,
      "grad_norm": 11.618569374084473,
      "learning_rate": 0.0008430693069306931,
      "loss": 2.1332,
      "step": 1317
    },
    {
      "epoch": 8.710450227178852,
      "grad_norm": 10.303328514099121,
      "learning_rate": 0.0008425742574257425,
      "loss": 1.3134,
      "step": 1318
    },
    {
      "epoch": 8.717059066501445,
      "grad_norm": 17.581785202026367,
      "learning_rate": 0.000842079207920792,
      "loss": 2.4006,
      "step": 1319
    },
    {
      "epoch": 8.72366790582404,
      "grad_norm": 6.055255889892578,
      "learning_rate": 0.0008415841584158416,
      "loss": 1.1104,
      "step": 1320
    },
    {
      "epoch": 8.730276745146634,
      "grad_norm": 36.820396423339844,
      "learning_rate": 0.0008410891089108911,
      "loss": 3.9154,
      "step": 1321
    },
    {
      "epoch": 8.736885584469228,
      "grad_norm": 48.78833770751953,
      "learning_rate": 0.0008405940594059406,
      "loss": 5.0937,
      "step": 1322
    },
    {
      "epoch": 8.743494423791821,
      "grad_norm": 9.547187805175781,
      "learning_rate": 0.00084009900990099,
      "loss": 1.4011,
      "step": 1323
    },
    {
      "epoch": 8.750103263114415,
      "grad_norm": 8.504777908325195,
      "learning_rate": 0.0008396039603960396,
      "loss": 1.9975,
      "step": 1324
    },
    {
      "epoch": 8.75671210243701,
      "grad_norm": 10.151472091674805,
      "learning_rate": 0.0008391089108910891,
      "loss": 1.5249,
      "step": 1325
    },
    {
      "epoch": 8.763320941759604,
      "grad_norm": 7.9321417808532715,
      "learning_rate": 0.0008386138613861386,
      "loss": 1.2369,
      "step": 1326
    },
    {
      "epoch": 8.769929781082197,
      "grad_norm": 9.886310577392578,
      "learning_rate": 0.0008381188118811881,
      "loss": 0.8791,
      "step": 1327
    },
    {
      "epoch": 8.776538620404791,
      "grad_norm": 2.124459743499756,
      "learning_rate": 0.0008376237623762376,
      "loss": 2.4899,
      "step": 1328
    },
    {
      "epoch": 8.783147459727385,
      "grad_norm": 21.807205200195312,
      "learning_rate": 0.0008371287128712871,
      "loss": 1.6299,
      "step": 1329
    },
    {
      "epoch": 8.78975629904998,
      "grad_norm": 27.1817569732666,
      "learning_rate": 0.0008366336633663366,
      "loss": 3.5034,
      "step": 1330
    },
    {
      "epoch": 8.796365138372574,
      "grad_norm": 28.962322235107422,
      "learning_rate": 0.0008361386138613861,
      "loss": 1.746,
      "step": 1331
    },
    {
      "epoch": 8.802973977695167,
      "grad_norm": 19.36481285095215,
      "learning_rate": 0.0008356435643564357,
      "loss": 2.305,
      "step": 1332
    },
    {
      "epoch": 8.80958281701776,
      "grad_norm": 16.77242088317871,
      "learning_rate": 0.0008351485148514851,
      "loss": 1.7262,
      "step": 1333
    },
    {
      "epoch": 8.816191656340354,
      "grad_norm": 12.76132583618164,
      "learning_rate": 0.0008346534653465346,
      "loss": 3.0028,
      "step": 1334
    },
    {
      "epoch": 8.82280049566295,
      "grad_norm": 28.55536460876465,
      "learning_rate": 0.0008341584158415841,
      "loss": 2.4657,
      "step": 1335
    },
    {
      "epoch": 8.829409334985543,
      "grad_norm": 9.228638648986816,
      "learning_rate": 0.0008336633663366337,
      "loss": 2.9797,
      "step": 1336
    },
    {
      "epoch": 8.836018174308137,
      "grad_norm": 32.14927291870117,
      "learning_rate": 0.0008331683168316832,
      "loss": 3.9632,
      "step": 1337
    },
    {
      "epoch": 8.84262701363073,
      "grad_norm": 27.48375129699707,
      "learning_rate": 0.0008326732673267326,
      "loss": 2.802,
      "step": 1338
    },
    {
      "epoch": 8.849235852953324,
      "grad_norm": 23.467151641845703,
      "learning_rate": 0.0008321782178217821,
      "loss": 3.1888,
      "step": 1339
    },
    {
      "epoch": 8.85584469227592,
      "grad_norm": 16.9580020904541,
      "learning_rate": 0.0008316831683168317,
      "loss": 2.1834,
      "step": 1340
    },
    {
      "epoch": 8.862453531598513,
      "grad_norm": 13.419290542602539,
      "learning_rate": 0.0008311881188118812,
      "loss": 2.1779,
      "step": 1341
    },
    {
      "epoch": 8.869062370921107,
      "grad_norm": 13.469734191894531,
      "learning_rate": 0.0008306930693069307,
      "loss": 1.2271,
      "step": 1342
    },
    {
      "epoch": 8.8756712102437,
      "grad_norm": 18.229896545410156,
      "learning_rate": 0.0008301980198019801,
      "loss": 1.9909,
      "step": 1343
    },
    {
      "epoch": 8.882280049566296,
      "grad_norm": 21.434179306030273,
      "learning_rate": 0.0008297029702970297,
      "loss": 2.5882,
      "step": 1344
    },
    {
      "epoch": 8.88888888888889,
      "grad_norm": 8.578368186950684,
      "learning_rate": 0.0008292079207920792,
      "loss": 1.2843,
      "step": 1345
    },
    {
      "epoch": 8.895497728211483,
      "grad_norm": 8.807422637939453,
      "learning_rate": 0.0008287128712871287,
      "loss": 2.0039,
      "step": 1346
    },
    {
      "epoch": 8.902106567534076,
      "grad_norm": 10.801820755004883,
      "learning_rate": 0.0008282178217821782,
      "loss": 2.9533,
      "step": 1347
    },
    {
      "epoch": 8.90871540685667,
      "grad_norm": 22.002683639526367,
      "learning_rate": 0.0008277227722772277,
      "loss": 3.7147,
      "step": 1348
    },
    {
      "epoch": 8.915324246179265,
      "grad_norm": 30.424266815185547,
      "learning_rate": 0.0008272277227722772,
      "loss": 2.593,
      "step": 1349
    },
    {
      "epoch": 8.921933085501859,
      "grad_norm": 10.82185173034668,
      "learning_rate": 0.0008267326732673267,
      "loss": 2.7481,
      "step": 1350
    },
    {
      "epoch": 8.928541924824453,
      "grad_norm": 18.211645126342773,
      "learning_rate": 0.0008262376237623762,
      "loss": 1.4476,
      "step": 1351
    },
    {
      "epoch": 8.935150764147046,
      "grad_norm": 6.843142032623291,
      "learning_rate": 0.0008257425742574258,
      "loss": 1.4917,
      "step": 1352
    },
    {
      "epoch": 8.94175960346964,
      "grad_norm": 6.43137788772583,
      "learning_rate": 0.0008252475247524752,
      "loss": 2.4196,
      "step": 1353
    },
    {
      "epoch": 8.948368442792235,
      "grad_norm": 33.4769287109375,
      "learning_rate": 0.0008247524752475247,
      "loss": 3.1856,
      "step": 1354
    },
    {
      "epoch": 8.954977282114829,
      "grad_norm": 55.35019302368164,
      "learning_rate": 0.0008242574257425742,
      "loss": 6.8521,
      "step": 1355
    },
    {
      "epoch": 8.961586121437422,
      "grad_norm": 25.10325050354004,
      "learning_rate": 0.0008237623762376238,
      "loss": 1.7893,
      "step": 1356
    },
    {
      "epoch": 8.968194960760016,
      "grad_norm": 14.915887832641602,
      "learning_rate": 0.0008232673267326733,
      "loss": 1.8534,
      "step": 1357
    },
    {
      "epoch": 8.974803800082611,
      "grad_norm": 0.9733213186264038,
      "learning_rate": 0.0008227722772277227,
      "loss": 0.6785,
      "step": 1358
    },
    {
      "epoch": 8.981412639405205,
      "grad_norm": 5.005252838134766,
      "learning_rate": 0.0008222772277227722,
      "loss": 3.2673,
      "step": 1359
    },
    {
      "epoch": 8.988021478727799,
      "grad_norm": 2.343968629837036,
      "learning_rate": 0.0008217821782178218,
      "loss": 2.7021,
      "step": 1360
    },
    {
      "epoch": 8.994630318050392,
      "grad_norm": 19.814434051513672,
      "learning_rate": 0.0008212871287128713,
      "loss": 2.1155,
      "step": 1361
    },
    {
      "epoch": 8.994630318050392,
      "eval_validation_error_bar": 0.036049249712485625,
      "eval_validation_loss": 4.117702484130859,
      "eval_validation_pearsonr": 0.7050802382374179,
      "eval_validation_rmse": 2.029212236404419,
      "eval_validation_runtime": 28.5089,
      "eval_validation_samples_per_second": 7.121,
      "eval_validation_spearman": 0.7405106871354534,
      "eval_validation_steps_per_second": 7.121,
      "step": 1361
    },
    {
      "epoch": 8.994630318050392,
      "eval_test_error_bar": 0.037489889972960724,
      "eval_test_loss": 5.993548393249512,
      "eval_test_pearsonr": 0.6069843809379123,
      "eval_test_rmse": 2.4481725692749023,
      "eval_test_runtime": 45.7449,
      "eval_test_samples_per_second": 7.126,
      "eval_test_spearman": 0.6186282186095061,
      "eval_test_steps_per_second": 7.126,
      "step": 1361
    },
    {
      "epoch": 9.001239157372986,
      "grad_norm": 15.481131553649902,
      "learning_rate": 0.0008207920792079208,
      "loss": 1.3036,
      "step": 1362
    },
    {
      "epoch": 9.007847996695581,
      "grad_norm": 11.118115425109863,
      "learning_rate": 0.0008202970297029702,
      "loss": 0.5732,
      "step": 1363
    },
    {
      "epoch": 9.014456836018175,
      "grad_norm": 6.34958553314209,
      "learning_rate": 0.0008198019801980197,
      "loss": 2.7755,
      "step": 1364
    },
    {
      "epoch": 9.021065675340768,
      "grad_norm": 31.155841827392578,
      "learning_rate": 0.0008193069306930693,
      "loss": 3.2181,
      "step": 1365
    },
    {
      "epoch": 9.027674514663362,
      "grad_norm": 47.488739013671875,
      "learning_rate": 0.0008188118811881188,
      "loss": 4.2017,
      "step": 1366
    },
    {
      "epoch": 9.034283353985956,
      "grad_norm": 40.906341552734375,
      "learning_rate": 0.0008183168316831683,
      "loss": 3.7636,
      "step": 1367
    },
    {
      "epoch": 9.04089219330855,
      "grad_norm": 24.9556827545166,
      "learning_rate": 0.0008178217821782177,
      "loss": 3.5044,
      "step": 1368
    },
    {
      "epoch": 9.047501032631144,
      "grad_norm": 19.203948974609375,
      "learning_rate": 0.0008173267326732673,
      "loss": 1.7892,
      "step": 1369
    },
    {
      "epoch": 9.054109871953738,
      "grad_norm": 7.74018669128418,
      "learning_rate": 0.0008168316831683168,
      "loss": 1.4692,
      "step": 1370
    },
    {
      "epoch": 9.060718711276332,
      "grad_norm": 10.309477806091309,
      "learning_rate": 0.0008163366336633663,
      "loss": 1.2619,
      "step": 1371
    },
    {
      "epoch": 9.067327550598925,
      "grad_norm": 20.68752098083496,
      "learning_rate": 0.0008158415841584159,
      "loss": 1.7562,
      "step": 1372
    },
    {
      "epoch": 9.07393638992152,
      "grad_norm": 28.97593116760254,
      "learning_rate": 0.0008153465346534653,
      "loss": 3.9494,
      "step": 1373
    },
    {
      "epoch": 9.080545229244114,
      "grad_norm": 19.224918365478516,
      "learning_rate": 0.0008148514851485148,
      "loss": 4.838,
      "step": 1374
    },
    {
      "epoch": 9.087154068566708,
      "grad_norm": 9.344245910644531,
      "learning_rate": 0.0008143564356435643,
      "loss": 1.6684,
      "step": 1375
    },
    {
      "epoch": 9.093762907889301,
      "grad_norm": 4.467669486999512,
      "learning_rate": 0.0008138613861386138,
      "loss": 1.4897,
      "step": 1376
    },
    {
      "epoch": 9.100371747211897,
      "grad_norm": 17.955158233642578,
      "learning_rate": 0.0008133663366336634,
      "loss": 3.6761,
      "step": 1377
    },
    {
      "epoch": 9.10698058653449,
      "grad_norm": 18.264629364013672,
      "learning_rate": 0.0008128712871287128,
      "loss": 3.7587,
      "step": 1378
    },
    {
      "epoch": 9.113589425857084,
      "grad_norm": 13.166974067687988,
      "learning_rate": 0.0008123762376237624,
      "loss": 0.7458,
      "step": 1379
    },
    {
      "epoch": 9.120198265179678,
      "grad_norm": 8.682183265686035,
      "learning_rate": 0.000811881188118812,
      "loss": 1.4839,
      "step": 1380
    },
    {
      "epoch": 9.126807104502271,
      "grad_norm": 12.977030754089355,
      "learning_rate": 0.0008113861386138615,
      "loss": 1.9657,
      "step": 1381
    },
    {
      "epoch": 9.133415943824867,
      "grad_norm": 20.12005615234375,
      "learning_rate": 0.000810891089108911,
      "loss": 3.2769,
      "step": 1382
    },
    {
      "epoch": 9.14002478314746,
      "grad_norm": 1.4167169332504272,
      "learning_rate": 0.0008103960396039604,
      "loss": 3.6615,
      "step": 1383
    },
    {
      "epoch": 9.146633622470054,
      "grad_norm": 22.3277645111084,
      "learning_rate": 0.00080990099009901,
      "loss": 2.0619,
      "step": 1384
    },
    {
      "epoch": 9.153242461792647,
      "grad_norm": 10.856255531311035,
      "learning_rate": 0.0008094059405940595,
      "loss": 0.9464,
      "step": 1385
    },
    {
      "epoch": 9.159851301115241,
      "grad_norm": 8.187748908996582,
      "learning_rate": 0.000808910891089109,
      "loss": 2.7367,
      "step": 1386
    },
    {
      "epoch": 9.166460140437836,
      "grad_norm": 1.5975404977798462,
      "learning_rate": 0.0008084158415841585,
      "loss": 1.2629,
      "step": 1387
    },
    {
      "epoch": 9.17306897976043,
      "grad_norm": 9.856574058532715,
      "learning_rate": 0.0008079207920792079,
      "loss": 3.194,
      "step": 1388
    },
    {
      "epoch": 9.179677819083023,
      "grad_norm": 4.636284351348877,
      "learning_rate": 0.0008074257425742575,
      "loss": 3.8446,
      "step": 1389
    },
    {
      "epoch": 9.186286658405617,
      "grad_norm": 25.778099060058594,
      "learning_rate": 0.000806930693069307,
      "loss": 1.8838,
      "step": 1390
    },
    {
      "epoch": 9.19289549772821,
      "grad_norm": 29.93349838256836,
      "learning_rate": 0.0008064356435643565,
      "loss": 6.1658,
      "step": 1391
    },
    {
      "epoch": 9.199504337050806,
      "grad_norm": 7.082529067993164,
      "learning_rate": 0.000805940594059406,
      "loss": 2.5653,
      "step": 1392
    },
    {
      "epoch": 9.2061131763734,
      "grad_norm": 18.145044326782227,
      "learning_rate": 0.0008054455445544555,
      "loss": 2.2214,
      "step": 1393
    },
    {
      "epoch": 9.212722015695993,
      "grad_norm": 14.092097282409668,
      "learning_rate": 0.000804950495049505,
      "loss": 1.067,
      "step": 1394
    },
    {
      "epoch": 9.219330855018587,
      "grad_norm": 7.918251037597656,
      "learning_rate": 0.0008044554455445545,
      "loss": 2.7446,
      "step": 1395
    },
    {
      "epoch": 9.225939694341182,
      "grad_norm": 22.035629272460938,
      "learning_rate": 0.000803960396039604,
      "loss": 2.9662,
      "step": 1396
    },
    {
      "epoch": 9.232548533663776,
      "grad_norm": 14.300031661987305,
      "learning_rate": 0.0008034653465346536,
      "loss": 2.2444,
      "step": 1397
    },
    {
      "epoch": 9.23915737298637,
      "grad_norm": 11.608436584472656,
      "learning_rate": 0.000802970297029703,
      "loss": 2.3114,
      "step": 1398
    },
    {
      "epoch": 9.245766212308963,
      "grad_norm": 9.13817024230957,
      "learning_rate": 0.0008024752475247525,
      "loss": 1.3514,
      "step": 1399
    },
    {
      "epoch": 9.252375051631557,
      "grad_norm": 1.323444128036499,
      "learning_rate": 0.000801980198019802,
      "loss": 1.6197,
      "step": 1400
    },
    {
      "epoch": 9.258983890954152,
      "grad_norm": 8.990121841430664,
      "learning_rate": 0.0008014851485148516,
      "loss": 1.8515,
      "step": 1401
    },
    {
      "epoch": 9.265592730276746,
      "grad_norm": 3.417876958847046,
      "learning_rate": 0.0008009900990099011,
      "loss": 2.6104,
      "step": 1402
    },
    {
      "epoch": 9.27220156959934,
      "grad_norm": 7.963024616241455,
      "learning_rate": 0.0008004950495049505,
      "loss": 3.3415,
      "step": 1403
    },
    {
      "epoch": 9.278810408921933,
      "grad_norm": 5.2077131271362305,
      "learning_rate": 0.0008,
      "loss": 2.6207,
      "step": 1404
    },
    {
      "epoch": 9.285419248244526,
      "grad_norm": 19.688323974609375,
      "learning_rate": 0.0007995049504950496,
      "loss": 1.2033,
      "step": 1405
    },
    {
      "epoch": 9.292028087567122,
      "grad_norm": 35.15301513671875,
      "learning_rate": 0.0007990099009900991,
      "loss": 4.2572,
      "step": 1406
    },
    {
      "epoch": 9.298636926889715,
      "grad_norm": 28.843265533447266,
      "learning_rate": 0.0007985148514851486,
      "loss": 1.5113,
      "step": 1407
    },
    {
      "epoch": 9.305245766212309,
      "grad_norm": 61.15287780761719,
      "learning_rate": 0.000798019801980198,
      "loss": 9.6391,
      "step": 1408
    },
    {
      "epoch": 9.311854605534903,
      "grad_norm": 8.676337242126465,
      "learning_rate": 0.0007975247524752476,
      "loss": 2.1686,
      "step": 1409
    },
    {
      "epoch": 9.318463444857496,
      "grad_norm": 12.618478775024414,
      "learning_rate": 0.0007970297029702971,
      "loss": 1.3526,
      "step": 1410
    },
    {
      "epoch": 9.325072284180091,
      "grad_norm": 11.83022689819336,
      "learning_rate": 0.0007965346534653466,
      "loss": 1.5688,
      "step": 1411
    },
    {
      "epoch": 9.331681123502685,
      "grad_norm": 27.4998779296875,
      "learning_rate": 0.0007960396039603961,
      "loss": 5.2743,
      "step": 1412
    },
    {
      "epoch": 9.338289962825279,
      "grad_norm": 30.165353775024414,
      "learning_rate": 0.0007955445544554456,
      "loss": 3.2439,
      "step": 1413
    },
    {
      "epoch": 9.344898802147872,
      "grad_norm": 27.66294288635254,
      "learning_rate": 0.0007950495049504951,
      "loss": 3.1183,
      "step": 1414
    },
    {
      "epoch": 9.351507641470466,
      "grad_norm": 4.6500372886657715,
      "learning_rate": 0.0007945544554455446,
      "loss": 2.711,
      "step": 1415
    },
    {
      "epoch": 9.358116480793061,
      "grad_norm": 9.325827598571777,
      "learning_rate": 0.0007940594059405941,
      "loss": 1.1357,
      "step": 1416
    },
    {
      "epoch": 9.364725320115655,
      "grad_norm": 20.699796676635742,
      "learning_rate": 0.0007935643564356437,
      "loss": 2.2075,
      "step": 1417
    },
    {
      "epoch": 9.371334159438248,
      "grad_norm": 11.107644081115723,
      "learning_rate": 0.0007930693069306931,
      "loss": 2.1864,
      "step": 1418
    },
    {
      "epoch": 9.377942998760842,
      "grad_norm": 1.9368749856948853,
      "learning_rate": 0.0007925742574257426,
      "loss": 1.734,
      "step": 1419
    },
    {
      "epoch": 9.384551838083437,
      "grad_norm": 25.432666778564453,
      "learning_rate": 0.0007920792079207921,
      "loss": 2.1443,
      "step": 1420
    },
    {
      "epoch": 9.391160677406031,
      "grad_norm": 17.330673217773438,
      "learning_rate": 0.0007915841584158417,
      "loss": 1.9556,
      "step": 1421
    },
    {
      "epoch": 9.397769516728625,
      "grad_norm": 7.742949485778809,
      "learning_rate": 0.0007910891089108912,
      "loss": 1.0585,
      "step": 1422
    },
    {
      "epoch": 9.404378356051218,
      "grad_norm": 5.100226402282715,
      "learning_rate": 0.0007905940594059406,
      "loss": 1.0012,
      "step": 1423
    },
    {
      "epoch": 9.410987195373812,
      "grad_norm": 9.406042098999023,
      "learning_rate": 0.0007900990099009901,
      "loss": 2.2274,
      "step": 1424
    },
    {
      "epoch": 9.417596034696407,
      "grad_norm": 8.360526084899902,
      "learning_rate": 0.0007896039603960397,
      "loss": 2.5363,
      "step": 1425
    },
    {
      "epoch": 9.424204874019,
      "grad_norm": 15.923236846923828,
      "learning_rate": 0.0007891089108910892,
      "loss": 2.3259,
      "step": 1426
    },
    {
      "epoch": 9.430813713341594,
      "grad_norm": 15.188470840454102,
      "learning_rate": 0.0007886138613861387,
      "loss": 3.5182,
      "step": 1427
    },
    {
      "epoch": 9.437422552664188,
      "grad_norm": 7.370889663696289,
      "learning_rate": 0.0007881188118811881,
      "loss": 1.8474,
      "step": 1428
    },
    {
      "epoch": 9.444031391986782,
      "grad_norm": 27.725021362304688,
      "learning_rate": 0.0007876237623762377,
      "loss": 3.1653,
      "step": 1429
    },
    {
      "epoch": 9.450640231309377,
      "grad_norm": 17.8743896484375,
      "learning_rate": 0.0007871287128712872,
      "loss": 4.2184,
      "step": 1430
    },
    {
      "epoch": 9.45724907063197,
      "grad_norm": 24.2329044342041,
      "learning_rate": 0.0007866336633663367,
      "loss": 3.0052,
      "step": 1431
    },
    {
      "epoch": 9.463857909954564,
      "grad_norm": 2.5915687084198,
      "learning_rate": 0.0007861386138613862,
      "loss": 2.1661,
      "step": 1432
    },
    {
      "epoch": 9.470466749277158,
      "grad_norm": 4.901941776275635,
      "learning_rate": 0.0007856435643564356,
      "loss": 0.9408,
      "step": 1433
    },
    {
      "epoch": 9.477075588599753,
      "grad_norm": 11.642596244812012,
      "learning_rate": 0.0007851485148514852,
      "loss": 1.6456,
      "step": 1434
    },
    {
      "epoch": 9.483684427922347,
      "grad_norm": 2.3563199043273926,
      "learning_rate": 0.0007846534653465347,
      "loss": 1.8889,
      "step": 1435
    },
    {
      "epoch": 9.49029326724494,
      "grad_norm": 12.803070068359375,
      "learning_rate": 0.0007841584158415842,
      "loss": 2.5461,
      "step": 1436
    },
    {
      "epoch": 9.496902106567534,
      "grad_norm": 4.027229309082031,
      "learning_rate": 0.0007836633663366338,
      "loss": 0.3824,
      "step": 1437
    },
    {
      "epoch": 9.503510945890127,
      "grad_norm": 27.593902587890625,
      "learning_rate": 0.0007831683168316832,
      "loss": 4.5951,
      "step": 1438
    },
    {
      "epoch": 9.510119785212723,
      "grad_norm": 13.455245971679688,
      "learning_rate": 0.0007826732673267327,
      "loss": 1.6336,
      "step": 1439
    },
    {
      "epoch": 9.516728624535316,
      "grad_norm": 35.83656311035156,
      "learning_rate": 0.0007821782178217822,
      "loss": 6.3216,
      "step": 1440
    },
    {
      "epoch": 9.52333746385791,
      "grad_norm": 3.8961305618286133,
      "learning_rate": 0.0007816831683168317,
      "loss": 3.1908,
      "step": 1441
    },
    {
      "epoch": 9.529946303180504,
      "grad_norm": 2.725353717803955,
      "learning_rate": 0.0007811881188118813,
      "loss": 1.3151,
      "step": 1442
    },
    {
      "epoch": 9.536555142503097,
      "grad_norm": 3.0151560306549072,
      "learning_rate": 0.0007806930693069307,
      "loss": 2.4593,
      "step": 1443
    },
    {
      "epoch": 9.543163981825693,
      "grad_norm": 3.6728198528289795,
      "learning_rate": 0.0007801980198019802,
      "loss": 1.9414,
      "step": 1444
    },
    {
      "epoch": 9.549772821148286,
      "grad_norm": 25.751781463623047,
      "learning_rate": 0.0007797029702970297,
      "loss": 2.578,
      "step": 1445
    },
    {
      "epoch": 9.55638166047088,
      "grad_norm": 10.26613998413086,
      "learning_rate": 0.0007792079207920793,
      "loss": 2.3739,
      "step": 1446
    },
    {
      "epoch": 9.562990499793473,
      "grad_norm": 18.710037231445312,
      "learning_rate": 0.0007787128712871288,
      "loss": 1.8281,
      "step": 1447
    },
    {
      "epoch": 9.569599339116067,
      "grad_norm": 3.238534688949585,
      "learning_rate": 0.0007782178217821782,
      "loss": 1.7705,
      "step": 1448
    },
    {
      "epoch": 9.576208178438662,
      "grad_norm": 3.6407294273376465,
      "learning_rate": 0.0007777227722772277,
      "loss": 2.5774,
      "step": 1449
    },
    {
      "epoch": 9.582817017761256,
      "grad_norm": 12.282689094543457,
      "learning_rate": 0.0007772277227722773,
      "loss": 2.5365,
      "step": 1450
    },
    {
      "epoch": 9.58942585708385,
      "grad_norm": 16.63513946533203,
      "learning_rate": 0.0007767326732673268,
      "loss": 3.303,
      "step": 1451
    },
    {
      "epoch": 9.596034696406443,
      "grad_norm": 11.123305320739746,
      "learning_rate": 0.0007762376237623763,
      "loss": 0.8417,
      "step": 1452
    },
    {
      "epoch": 9.602643535729037,
      "grad_norm": 6.855027675628662,
      "learning_rate": 0.0007757425742574257,
      "loss": 3.98,
      "step": 1453
    },
    {
      "epoch": 9.609252375051632,
      "grad_norm": 14.536750793457031,
      "learning_rate": 0.0007752475247524753,
      "loss": 2.7193,
      "step": 1454
    },
    {
      "epoch": 9.615861214374226,
      "grad_norm": 21.09046745300293,
      "learning_rate": 0.0007747524752475248,
      "loss": 1.0771,
      "step": 1455
    },
    {
      "epoch": 9.62247005369682,
      "grad_norm": 13.345052719116211,
      "learning_rate": 0.0007742574257425743,
      "loss": 2.3997,
      "step": 1456
    },
    {
      "epoch": 9.629078893019413,
      "grad_norm": 19.323837280273438,
      "learning_rate": 0.0007737623762376238,
      "loss": 1.5495,
      "step": 1457
    },
    {
      "epoch": 9.635687732342008,
      "grad_norm": 6.027174472808838,
      "learning_rate": 0.0007732673267326733,
      "loss": 1.8543,
      "step": 1458
    },
    {
      "epoch": 9.642296571664602,
      "grad_norm": 34.981143951416016,
      "learning_rate": 0.0007727722772277228,
      "loss": 3.1364,
      "step": 1459
    },
    {
      "epoch": 9.648905410987195,
      "grad_norm": 30.338228225708008,
      "learning_rate": 0.0007722772277227723,
      "loss": 1.9441,
      "step": 1460
    },
    {
      "epoch": 9.655514250309789,
      "grad_norm": 15.736319541931152,
      "learning_rate": 0.0007717821782178218,
      "loss": 2.1805,
      "step": 1461
    },
    {
      "epoch": 9.662123089632383,
      "grad_norm": 31.520618438720703,
      "learning_rate": 0.0007712871287128714,
      "loss": 3.4096,
      "step": 1462
    },
    {
      "epoch": 9.668731928954978,
      "grad_norm": 25.431180953979492,
      "learning_rate": 0.0007707920792079208,
      "loss": 1.877,
      "step": 1463
    },
    {
      "epoch": 9.675340768277572,
      "grad_norm": 1.5119476318359375,
      "learning_rate": 0.0007702970297029703,
      "loss": 1.8126,
      "step": 1464
    },
    {
      "epoch": 9.681949607600165,
      "grad_norm": 11.857477188110352,
      "learning_rate": 0.0007698019801980198,
      "loss": 1.347,
      "step": 1465
    },
    {
      "epoch": 9.688558446922759,
      "grad_norm": 15.479321479797363,
      "learning_rate": 0.0007693069306930694,
      "loss": 3.2515,
      "step": 1466
    },
    {
      "epoch": 9.695167286245352,
      "grad_norm": 16.230697631835938,
      "learning_rate": 0.0007688118811881189,
      "loss": 1.4276,
      "step": 1467
    },
    {
      "epoch": 9.701776125567948,
      "grad_norm": 28.658206939697266,
      "learning_rate": 0.0007683168316831683,
      "loss": 2.0987,
      "step": 1468
    },
    {
      "epoch": 9.708384964890541,
      "grad_norm": 19.859920501708984,
      "learning_rate": 0.0007678217821782178,
      "loss": 2.2485,
      "step": 1469
    },
    {
      "epoch": 9.714993804213135,
      "grad_norm": 7.788412570953369,
      "learning_rate": 0.0007673267326732674,
      "loss": 2.4495,
      "step": 1470
    },
    {
      "epoch": 9.721602643535729,
      "grad_norm": 5.66983699798584,
      "learning_rate": 0.0007668316831683169,
      "loss": 1.5764,
      "step": 1471
    },
    {
      "epoch": 9.728211482858324,
      "grad_norm": 12.269771575927734,
      "learning_rate": 0.0007663366336633664,
      "loss": 1.8458,
      "step": 1472
    },
    {
      "epoch": 9.734820322180918,
      "grad_norm": 19.737525939941406,
      "learning_rate": 0.0007658415841584158,
      "loss": 2.0607,
      "step": 1473
    },
    {
      "epoch": 9.741429161503511,
      "grad_norm": 25.699949264526367,
      "learning_rate": 0.0007653465346534654,
      "loss": 1.6254,
      "step": 1474
    },
    {
      "epoch": 9.748038000826105,
      "grad_norm": 3.3330495357513428,
      "learning_rate": 0.0007648514851485149,
      "loss": 0.8495,
      "step": 1475
    },
    {
      "epoch": 9.754646840148698,
      "grad_norm": 11.092809677124023,
      "learning_rate": 0.0007643564356435644,
      "loss": 1.8113,
      "step": 1476
    },
    {
      "epoch": 9.761255679471294,
      "grad_norm": 20.55202865600586,
      "learning_rate": 0.0007638613861386139,
      "loss": 2.6797,
      "step": 1477
    },
    {
      "epoch": 9.767864518793887,
      "grad_norm": 18.195459365844727,
      "learning_rate": 0.0007633663366336634,
      "loss": 0.976,
      "step": 1478
    },
    {
      "epoch": 9.77447335811648,
      "grad_norm": 23.106611251831055,
      "learning_rate": 0.0007628712871287129,
      "loss": 2.325,
      "step": 1479
    },
    {
      "epoch": 9.781082197439074,
      "grad_norm": 42.87214279174805,
      "learning_rate": 0.0007623762376237624,
      "loss": 4.5825,
      "step": 1480
    },
    {
      "epoch": 9.787691036761668,
      "grad_norm": 7.987798690795898,
      "learning_rate": 0.0007618811881188119,
      "loss": 3.7001,
      "step": 1481
    },
    {
      "epoch": 9.794299876084263,
      "grad_norm": 4.883697509765625,
      "learning_rate": 0.0007613861386138615,
      "loss": 1.3131,
      "step": 1482
    },
    {
      "epoch": 9.800908715406857,
      "grad_norm": 11.389262199401855,
      "learning_rate": 0.0007608910891089109,
      "loss": 1.1963,
      "step": 1483
    },
    {
      "epoch": 9.80751755472945,
      "grad_norm": 13.538362503051758,
      "learning_rate": 0.0007603960396039604,
      "loss": 1.8631,
      "step": 1484
    },
    {
      "epoch": 9.814126394052044,
      "grad_norm": 2.3556013107299805,
      "learning_rate": 0.0007599009900990099,
      "loss": 2.0683,
      "step": 1485
    },
    {
      "epoch": 9.820735233374638,
      "grad_norm": 1.5636039972305298,
      "learning_rate": 0.0007594059405940595,
      "loss": 1.0104,
      "step": 1486
    },
    {
      "epoch": 9.827344072697233,
      "grad_norm": 8.147729873657227,
      "learning_rate": 0.000758910891089109,
      "loss": 0.837,
      "step": 1487
    },
    {
      "epoch": 9.833952912019827,
      "grad_norm": 11.596951484680176,
      "learning_rate": 0.0007584158415841584,
      "loss": 1.8982,
      "step": 1488
    },
    {
      "epoch": 9.84056175134242,
      "grad_norm": 1.4901213645935059,
      "learning_rate": 0.0007579207920792079,
      "loss": 1.2042,
      "step": 1489
    },
    {
      "epoch": 9.847170590665014,
      "grad_norm": 3.266483783721924,
      "learning_rate": 0.0007574257425742574,
      "loss": 0.9887,
      "step": 1490
    },
    {
      "epoch": 9.853779429987608,
      "grad_norm": 5.946475982666016,
      "learning_rate": 0.000756930693069307,
      "loss": 1.4397,
      "step": 1491
    },
    {
      "epoch": 9.860388269310203,
      "grad_norm": 13.607275009155273,
      "learning_rate": 0.0007564356435643565,
      "loss": 2.8501,
      "step": 1492
    },
    {
      "epoch": 9.866997108632797,
      "grad_norm": 11.253809928894043,
      "learning_rate": 0.0007559405940594059,
      "loss": 1.0409,
      "step": 1493
    },
    {
      "epoch": 9.87360594795539,
      "grad_norm": 1.7627105712890625,
      "learning_rate": 0.0007554455445544554,
      "loss": 2.5255,
      "step": 1494
    },
    {
      "epoch": 9.880214787277984,
      "grad_norm": 2.1736319065093994,
      "learning_rate": 0.000754950495049505,
      "loss": 1.8775,
      "step": 1495
    },
    {
      "epoch": 9.886823626600577,
      "grad_norm": 2.5935115814208984,
      "learning_rate": 0.0007544554455445545,
      "loss": 1.5984,
      "step": 1496
    },
    {
      "epoch": 9.893432465923173,
      "grad_norm": 10.495429992675781,
      "learning_rate": 0.000753960396039604,
      "loss": 0.919,
      "step": 1497
    },
    {
      "epoch": 9.900041305245766,
      "grad_norm": 8.251891136169434,
      "learning_rate": 0.0007534653465346534,
      "loss": 1.1553,
      "step": 1498
    },
    {
      "epoch": 9.90665014456836,
      "grad_norm": 15.463247299194336,
      "learning_rate": 0.000752970297029703,
      "loss": 0.9802,
      "step": 1499
    },
    {
      "epoch": 9.913258983890954,
      "grad_norm": 3.1822609901428223,
      "learning_rate": 0.0007524752475247525,
      "loss": 1.4425,
      "step": 1500
    },
    {
      "epoch": 9.919867823213549,
      "grad_norm": 16.041889190673828,
      "learning_rate": 0.000751980198019802,
      "loss": 1.0926,
      "step": 1501
    },
    {
      "epoch": 9.926476662536142,
      "grad_norm": 5.1042704582214355,
      "learning_rate": 0.0007514851485148515,
      "loss": 1.4064,
      "step": 1502
    },
    {
      "epoch": 9.933085501858736,
      "grad_norm": 5.78680944442749,
      "learning_rate": 0.000750990099009901,
      "loss": 0.8342,
      "step": 1503
    },
    {
      "epoch": 9.93969434118133,
      "grad_norm": 22.31165885925293,
      "learning_rate": 0.0007504950495049505,
      "loss": 5.2217,
      "step": 1504
    },
    {
      "epoch": 9.946303180503923,
      "grad_norm": 0.984966516494751,
      "learning_rate": 0.00075,
      "loss": 0.7063,
      "step": 1505
    },
    {
      "epoch": 9.952912019826519,
      "grad_norm": 8.730978012084961,
      "learning_rate": 0.0007495049504950495,
      "loss": 2.6221,
      "step": 1506
    },
    {
      "epoch": 9.959520859149112,
      "grad_norm": 1.2801392078399658,
      "learning_rate": 0.0007490099009900991,
      "loss": 1.1266,
      "step": 1507
    },
    {
      "epoch": 9.966129698471706,
      "grad_norm": 13.70744800567627,
      "learning_rate": 0.0007485148514851485,
      "loss": 1.3175,
      "step": 1508
    },
    {
      "epoch": 9.9727385377943,
      "grad_norm": 4.330085277557373,
      "learning_rate": 0.000748019801980198,
      "loss": 2.2308,
      "step": 1509
    },
    {
      "epoch": 9.979347377116893,
      "grad_norm": 13.272536277770996,
      "learning_rate": 0.0007475247524752475,
      "loss": 2.4412,
      "step": 1510
    },
    {
      "epoch": 9.985956216439488,
      "grad_norm": 5.7348761558532715,
      "learning_rate": 0.0007470297029702971,
      "loss": 1.7989,
      "step": 1511
    },
    {
      "epoch": 9.992565055762082,
      "grad_norm": 5.150696277618408,
      "learning_rate": 0.0007465346534653466,
      "loss": 0.9957,
      "step": 1512
    },
    {
      "epoch": 9.999173895084676,
      "grad_norm": 26.69778823852539,
      "learning_rate": 0.000746039603960396,
      "loss": 3.7833,
      "step": 1513
    },
    {
      "epoch": 9.999173895084676,
      "eval_validation_error_bar": 0.040542418964891365,
      "eval_validation_loss": 4.339807510375977,
      "eval_validation_pearsonr": 0.6657154020179078,
      "eval_validation_rmse": 2.0832204818725586,
      "eval_validation_runtime": 28.5291,
      "eval_validation_samples_per_second": 7.116,
      "eval_validation_spearman": 0.6969229665786825,
      "eval_validation_steps_per_second": 7.116,
      "step": 1513
    },
    {
      "epoch": 9.999173895084676,
      "eval_test_error_bar": 0.03707605946528018,
      "eval_test_loss": 5.890840530395508,
      "eval_test_pearsonr": 0.6100366036640549,
      "eval_test_rmse": 2.42710542678833,
      "eval_test_runtime": 45.7617,
      "eval_test_samples_per_second": 7.124,
      "eval_test_spearman": 0.6249106157920825,
      "eval_test_steps_per_second": 7.124,
      "step": 1513
    },
    {
      "epoch": 10.00578273440727,
      "grad_norm": 15.13452434539795,
      "learning_rate": 0.0007455445544554455,
      "loss": 1.6761,
      "step": 1514
    },
    {
      "epoch": 10.012391573729865,
      "grad_norm": 3.109196424484253,
      "learning_rate": 0.0007450495049504951,
      "loss": 2.0295,
      "step": 1515
    },
    {
      "epoch": 10.019000413052458,
      "grad_norm": 5.683027267456055,
      "learning_rate": 0.0007445544554455446,
      "loss": 1.1494,
      "step": 1516
    },
    {
      "epoch": 10.025609252375052,
      "grad_norm": 2.0563697814941406,
      "learning_rate": 0.0007440594059405941,
      "loss": 1.6381,
      "step": 1517
    },
    {
      "epoch": 10.032218091697645,
      "grad_norm": 11.842557907104492,
      "learning_rate": 0.0007435643564356435,
      "loss": 2.7911,
      "step": 1518
    },
    {
      "epoch": 10.038826931020239,
      "grad_norm": 1.3211723566055298,
      "learning_rate": 0.0007430693069306931,
      "loss": 1.9988,
      "step": 1519
    },
    {
      "epoch": 10.045435770342834,
      "grad_norm": 2.5747640132904053,
      "learning_rate": 0.0007425742574257426,
      "loss": 2.3993,
      "step": 1520
    },
    {
      "epoch": 10.052044609665428,
      "grad_norm": 16.965530395507812,
      "learning_rate": 0.0007420792079207921,
      "loss": 2.2222,
      "step": 1521
    },
    {
      "epoch": 10.058653448988021,
      "grad_norm": 8.606541633605957,
      "learning_rate": 0.0007415841584158416,
      "loss": 1.0881,
      "step": 1522
    },
    {
      "epoch": 10.065262288310615,
      "grad_norm": 25.95334815979004,
      "learning_rate": 0.000741089108910891,
      "loss": 1.2548,
      "step": 1523
    },
    {
      "epoch": 10.071871127633209,
      "grad_norm": 24.518001556396484,
      "learning_rate": 0.0007405940594059406,
      "loss": 1.9767,
      "step": 1524
    },
    {
      "epoch": 10.078479966955804,
      "grad_norm": 8.784863471984863,
      "learning_rate": 0.0007400990099009901,
      "loss": 0.6299,
      "step": 1525
    },
    {
      "epoch": 10.085088806278398,
      "grad_norm": 22.93144416809082,
      "learning_rate": 0.0007396039603960396,
      "loss": 3.5223,
      "step": 1526
    },
    {
      "epoch": 10.091697645600991,
      "grad_norm": 6.234158515930176,
      "learning_rate": 0.0007391089108910892,
      "loss": 1.417,
      "step": 1527
    },
    {
      "epoch": 10.098306484923585,
      "grad_norm": 2.907309055328369,
      "learning_rate": 0.0007386138613861386,
      "loss": 3.9028,
      "step": 1528
    },
    {
      "epoch": 10.104915324246178,
      "grad_norm": 2.3441808223724365,
      "learning_rate": 0.0007381188118811881,
      "loss": 1.0372,
      "step": 1529
    },
    {
      "epoch": 10.111524163568774,
      "grad_norm": 20.50887680053711,
      "learning_rate": 0.0007376237623762376,
      "loss": 2.3694,
      "step": 1530
    },
    {
      "epoch": 10.118133002891367,
      "grad_norm": 8.392984390258789,
      "learning_rate": 0.0007371287128712872,
      "loss": 4.4363,
      "step": 1531
    },
    {
      "epoch": 10.124741842213961,
      "grad_norm": 6.597374439239502,
      "learning_rate": 0.0007366336633663367,
      "loss": 2.6355,
      "step": 1532
    },
    {
      "epoch": 10.131350681536555,
      "grad_norm": 3.399514675140381,
      "learning_rate": 0.0007361386138613861,
      "loss": 3.2606,
      "step": 1533
    },
    {
      "epoch": 10.13795952085915,
      "grad_norm": 7.889833927154541,
      "learning_rate": 0.0007356435643564356,
      "loss": 3.2102,
      "step": 1534
    },
    {
      "epoch": 10.144568360181744,
      "grad_norm": 10.130472183227539,
      "learning_rate": 0.0007351485148514852,
      "loss": 1.3447,
      "step": 1535
    },
    {
      "epoch": 10.151177199504337,
      "grad_norm": 13.316484451293945,
      "learning_rate": 0.0007346534653465347,
      "loss": 1.9515,
      "step": 1536
    },
    {
      "epoch": 10.15778603882693,
      "grad_norm": 15.408084869384766,
      "learning_rate": 0.0007341584158415842,
      "loss": 1.7036,
      "step": 1537
    },
    {
      "epoch": 10.164394878149524,
      "grad_norm": 19.714183807373047,
      "learning_rate": 0.0007336633663366336,
      "loss": 2.3679,
      "step": 1538
    },
    {
      "epoch": 10.17100371747212,
      "grad_norm": 5.532100200653076,
      "learning_rate": 0.0007331683168316831,
      "loss": 1.3762,
      "step": 1539
    },
    {
      "epoch": 10.177612556794713,
      "grad_norm": 7.007350921630859,
      "learning_rate": 0.0007326732673267327,
      "loss": 0.7262,
      "step": 1540
    },
    {
      "epoch": 10.184221396117307,
      "grad_norm": 2.7253472805023193,
      "learning_rate": 0.0007321782178217822,
      "loss": 1.4656,
      "step": 1541
    },
    {
      "epoch": 10.1908302354399,
      "grad_norm": 24.766510009765625,
      "learning_rate": 0.0007316831683168317,
      "loss": 1.7343,
      "step": 1542
    },
    {
      "epoch": 10.197439074762494,
      "grad_norm": 26.96978759765625,
      "learning_rate": 0.0007311881188118811,
      "loss": 3.428,
      "step": 1543
    },
    {
      "epoch": 10.20404791408509,
      "grad_norm": 12.205283164978027,
      "learning_rate": 0.0007306930693069307,
      "loss": 4.5884,
      "step": 1544
    },
    {
      "epoch": 10.210656753407683,
      "grad_norm": 18.69894790649414,
      "learning_rate": 0.0007301980198019802,
      "loss": 1.6776,
      "step": 1545
    },
    {
      "epoch": 10.217265592730277,
      "grad_norm": 4.843259334564209,
      "learning_rate": 0.0007297029702970297,
      "loss": 2.6233,
      "step": 1546
    },
    {
      "epoch": 10.22387443205287,
      "grad_norm": 13.634584426879883,
      "learning_rate": 0.0007292079207920792,
      "loss": 2.2833,
      "step": 1547
    },
    {
      "epoch": 10.230483271375464,
      "grad_norm": 2.9880759716033936,
      "learning_rate": 0.0007287128712871287,
      "loss": 2.052,
      "step": 1548
    },
    {
      "epoch": 10.23709211069806,
      "grad_norm": 9.47634506225586,
      "learning_rate": 0.0007282178217821782,
      "loss": 0.6116,
      "step": 1549
    },
    {
      "epoch": 10.243700950020653,
      "grad_norm": 5.579597473144531,
      "learning_rate": 0.0007277227722772277,
      "loss": 2.2137,
      "step": 1550
    },
    {
      "epoch": 10.250309789343246,
      "grad_norm": 15.590414047241211,
      "learning_rate": 0.0007272277227722772,
      "loss": 2.2406,
      "step": 1551
    },
    {
      "epoch": 10.25691862866584,
      "grad_norm": 10.867822647094727,
      "learning_rate": 0.0007267326732673268,
      "loss": 1.0732,
      "step": 1552
    },
    {
      "epoch": 10.263527467988435,
      "grad_norm": 13.59554386138916,
      "learning_rate": 0.0007262376237623762,
      "loss": 1.8251,
      "step": 1553
    },
    {
      "epoch": 10.270136307311029,
      "grad_norm": 7.9821648597717285,
      "learning_rate": 0.0007257425742574257,
      "loss": 1.6861,
      "step": 1554
    },
    {
      "epoch": 10.276745146633623,
      "grad_norm": 5.6566314697265625,
      "learning_rate": 0.0007252475247524752,
      "loss": 1.4294,
      "step": 1555
    },
    {
      "epoch": 10.283353985956216,
      "grad_norm": 13.642061233520508,
      "learning_rate": 0.0007247524752475248,
      "loss": 2.1731,
      "step": 1556
    },
    {
      "epoch": 10.28996282527881,
      "grad_norm": 12.857304573059082,
      "learning_rate": 0.0007242574257425743,
      "loss": 2.1185,
      "step": 1557
    },
    {
      "epoch": 10.296571664601405,
      "grad_norm": 16.147666931152344,
      "learning_rate": 0.0007237623762376237,
      "loss": 1.215,
      "step": 1558
    },
    {
      "epoch": 10.303180503923999,
      "grad_norm": 5.6324462890625,
      "learning_rate": 0.0007232673267326732,
      "loss": 1.2995,
      "step": 1559
    },
    {
      "epoch": 10.309789343246592,
      "grad_norm": 8.881711959838867,
      "learning_rate": 0.0007227722772277228,
      "loss": 1.2157,
      "step": 1560
    },
    {
      "epoch": 10.316398182569186,
      "grad_norm": 8.824740409851074,
      "learning_rate": 0.0007222772277227723,
      "loss": 1.8148,
      "step": 1561
    },
    {
      "epoch": 10.32300702189178,
      "grad_norm": 11.76038932800293,
      "learning_rate": 0.0007217821782178218,
      "loss": 1.5049,
      "step": 1562
    },
    {
      "epoch": 10.329615861214375,
      "grad_norm": 26.47049331665039,
      "learning_rate": 0.0007212871287128712,
      "loss": 1.9575,
      "step": 1563
    },
    {
      "epoch": 10.336224700536969,
      "grad_norm": 21.826953887939453,
      "learning_rate": 0.0007207920792079208,
      "loss": 1.872,
      "step": 1564
    },
    {
      "epoch": 10.342833539859562,
      "grad_norm": 22.66981315612793,
      "learning_rate": 0.0007202970297029703,
      "loss": 2.8159,
      "step": 1565
    },
    {
      "epoch": 10.349442379182156,
      "grad_norm": 1.7790521383285522,
      "learning_rate": 0.0007198019801980198,
      "loss": 2.8417,
      "step": 1566
    },
    {
      "epoch": 10.35605121850475,
      "grad_norm": 10.735426902770996,
      "learning_rate": 0.0007193069306930693,
      "loss": 1.533,
      "step": 1567
    },
    {
      "epoch": 10.362660057827345,
      "grad_norm": 7.3846635818481445,
      "learning_rate": 0.0007188118811881188,
      "loss": 1.88,
      "step": 1568
    },
    {
      "epoch": 10.369268897149938,
      "grad_norm": 15.69562816619873,
      "learning_rate": 0.0007183168316831683,
      "loss": 1.4638,
      "step": 1569
    },
    {
      "epoch": 10.375877736472532,
      "grad_norm": 6.014191150665283,
      "learning_rate": 0.0007178217821782178,
      "loss": 1.5168,
      "step": 1570
    },
    {
      "epoch": 10.382486575795125,
      "grad_norm": 7.577126502990723,
      "learning_rate": 0.0007173267326732673,
      "loss": 1.612,
      "step": 1571
    },
    {
      "epoch": 10.389095415117719,
      "grad_norm": 4.804172992706299,
      "learning_rate": 0.0007168316831683169,
      "loss": 1.2331,
      "step": 1572
    },
    {
      "epoch": 10.395704254440314,
      "grad_norm": 4.530461311340332,
      "learning_rate": 0.0007163366336633663,
      "loss": 4.8943,
      "step": 1573
    },
    {
      "epoch": 10.402313093762908,
      "grad_norm": 11.186676025390625,
      "learning_rate": 0.0007158415841584158,
      "loss": 3.1344,
      "step": 1574
    },
    {
      "epoch": 10.408921933085502,
      "grad_norm": 2.3852460384368896,
      "learning_rate": 0.0007153465346534653,
      "loss": 0.3376,
      "step": 1575
    },
    {
      "epoch": 10.415530772408095,
      "grad_norm": 5.365614414215088,
      "learning_rate": 0.0007148514851485149,
      "loss": 1.4805,
      "step": 1576
    },
    {
      "epoch": 10.42213961173069,
      "grad_norm": 11.766447067260742,
      "learning_rate": 0.0007143564356435644,
      "loss": 2.3643,
      "step": 1577
    },
    {
      "epoch": 10.428748451053284,
      "grad_norm": 7.19704008102417,
      "learning_rate": 0.0007138613861386138,
      "loss": 1.7818,
      "step": 1578
    },
    {
      "epoch": 10.435357290375878,
      "grad_norm": 6.462158679962158,
      "learning_rate": 0.0007133663366336633,
      "loss": 2.7204,
      "step": 1579
    },
    {
      "epoch": 10.441966129698471,
      "grad_norm": 19.10457992553711,
      "learning_rate": 0.0007128712871287129,
      "loss": 1.2,
      "step": 1580
    },
    {
      "epoch": 10.448574969021065,
      "grad_norm": 9.15799331665039,
      "learning_rate": 0.0007123762376237624,
      "loss": 1.3656,
      "step": 1581
    },
    {
      "epoch": 10.45518380834366,
      "grad_norm": 10.951021194458008,
      "learning_rate": 0.0007118811881188119,
      "loss": 1.9282,
      "step": 1582
    },
    {
      "epoch": 10.461792647666254,
      "grad_norm": 6.7963786125183105,
      "learning_rate": 0.0007113861386138613,
      "loss": 1.0231,
      "step": 1583
    },
    {
      "epoch": 10.468401486988848,
      "grad_norm": 21.087839126586914,
      "learning_rate": 0.0007108910891089109,
      "loss": 1.7094,
      "step": 1584
    },
    {
      "epoch": 10.475010326311441,
      "grad_norm": 25.801326751708984,
      "learning_rate": 0.0007103960396039604,
      "loss": 3.702,
      "step": 1585
    },
    {
      "epoch": 10.481619165634035,
      "grad_norm": 29.807937622070312,
      "learning_rate": 0.0007099009900990099,
      "loss": 1.9547,
      "step": 1586
    },
    {
      "epoch": 10.48822800495663,
      "grad_norm": 30.089340209960938,
      "learning_rate": 0.0007094059405940594,
      "loss": 3.3278,
      "step": 1587
    },
    {
      "epoch": 10.494836844279224,
      "grad_norm": 22.77960968017578,
      "learning_rate": 0.0007089108910891088,
      "loss": 1.8915,
      "step": 1588
    },
    {
      "epoch": 10.501445683601817,
      "grad_norm": 6.621363162994385,
      "learning_rate": 0.0007084158415841584,
      "loss": 0.8685,
      "step": 1589
    },
    {
      "epoch": 10.508054522924411,
      "grad_norm": 4.197847843170166,
      "learning_rate": 0.0007079207920792079,
      "loss": 1.4708,
      "step": 1590
    },
    {
      "epoch": 10.514663362247006,
      "grad_norm": 12.188940048217773,
      "learning_rate": 0.0007074257425742574,
      "loss": 1.0431,
      "step": 1591
    },
    {
      "epoch": 10.5212722015696,
      "grad_norm": 44.134639739990234,
      "learning_rate": 0.000706930693069307,
      "loss": 5.2865,
      "step": 1592
    },
    {
      "epoch": 10.527881040892193,
      "grad_norm": 50.86959457397461,
      "learning_rate": 0.0007064356435643564,
      "loss": 3.3871,
      "step": 1593
    },
    {
      "epoch": 10.534489880214787,
      "grad_norm": 54.25599670410156,
      "learning_rate": 0.0007059405940594059,
      "loss": 4.1364,
      "step": 1594
    },
    {
      "epoch": 10.54109871953738,
      "grad_norm": 35.658180236816406,
      "learning_rate": 0.0007054455445544554,
      "loss": 4.3093,
      "step": 1595
    },
    {
      "epoch": 10.547707558859976,
      "grad_norm": 48.50088882446289,
      "learning_rate": 0.000704950495049505,
      "loss": 4.0332,
      "step": 1596
    },
    {
      "epoch": 10.55431639818257,
      "grad_norm": 8.478470802307129,
      "learning_rate": 0.0007044554455445545,
      "loss": 2.9136,
      "step": 1597
    },
    {
      "epoch": 10.560925237505163,
      "grad_norm": 7.5236945152282715,
      "learning_rate": 0.0007039603960396039,
      "loss": 1.2341,
      "step": 1598
    },
    {
      "epoch": 10.567534076827757,
      "grad_norm": 17.491985321044922,
      "learning_rate": 0.0007034653465346534,
      "loss": 3.121,
      "step": 1599
    },
    {
      "epoch": 10.57414291615035,
      "grad_norm": 10.406397819519043,
      "learning_rate": 0.0007029702970297029,
      "loss": 2.1996,
      "step": 1600
    },
    {
      "epoch": 10.580751755472946,
      "grad_norm": 39.31465148925781,
      "learning_rate": 0.0007024752475247525,
      "loss": 3.9235,
      "step": 1601
    },
    {
      "epoch": 10.58736059479554,
      "grad_norm": 34.168704986572266,
      "learning_rate": 0.000701980198019802,
      "loss": 3.562,
      "step": 1602
    },
    {
      "epoch": 10.593969434118133,
      "grad_norm": 1.7649435997009277,
      "learning_rate": 0.0007014851485148514,
      "loss": 4.0735,
      "step": 1603
    },
    {
      "epoch": 10.600578273440727,
      "grad_norm": 20.242168426513672,
      "learning_rate": 0.0007009900990099009,
      "loss": 4.078,
      "step": 1604
    },
    {
      "epoch": 10.60718711276332,
      "grad_norm": 5.4427876472473145,
      "learning_rate": 0.0007004950495049505,
      "loss": 0.9468,
      "step": 1605
    },
    {
      "epoch": 10.613795952085916,
      "grad_norm": 5.228337287902832,
      "learning_rate": 0.0007,
      "loss": 3.798,
      "step": 1606
    },
    {
      "epoch": 10.62040479140851,
      "grad_norm": 7.427547454833984,
      "learning_rate": 0.0006995049504950495,
      "loss": 1.8593,
      "step": 1607
    },
    {
      "epoch": 10.627013630731103,
      "grad_norm": 11.622196197509766,
      "learning_rate": 0.0006990099009900989,
      "loss": 0.9576,
      "step": 1608
    },
    {
      "epoch": 10.633622470053696,
      "grad_norm": 9.856786727905273,
      "learning_rate": 0.0006985148514851485,
      "loss": 1.4018,
      "step": 1609
    },
    {
      "epoch": 10.64023130937629,
      "grad_norm": 14.73979663848877,
      "learning_rate": 0.000698019801980198,
      "loss": 1.6776,
      "step": 1610
    },
    {
      "epoch": 10.646840148698885,
      "grad_norm": 7.174704551696777,
      "learning_rate": 0.0006975247524752475,
      "loss": 0.7877,
      "step": 1611
    },
    {
      "epoch": 10.653448988021479,
      "grad_norm": 13.748085975646973,
      "learning_rate": 0.000697029702970297,
      "loss": 0.7666,
      "step": 1612
    },
    {
      "epoch": 10.660057827344072,
      "grad_norm": 8.504312515258789,
      "learning_rate": 0.0006965346534653465,
      "loss": 1.2245,
      "step": 1613
    },
    {
      "epoch": 10.666666666666666,
      "grad_norm": 3.630861759185791,
      "learning_rate": 0.000696039603960396,
      "loss": 1.1868,
      "step": 1614
    },
    {
      "epoch": 10.673275505989261,
      "grad_norm": 2.9081897735595703,
      "learning_rate": 0.0006955445544554455,
      "loss": 1.3757,
      "step": 1615
    },
    {
      "epoch": 10.679884345311855,
      "grad_norm": 6.978986740112305,
      "learning_rate": 0.000695049504950495,
      "loss": 0.703,
      "step": 1616
    },
    {
      "epoch": 10.686493184634449,
      "grad_norm": 13.164384841918945,
      "learning_rate": 0.0006945544554455446,
      "loss": 2.5967,
      "step": 1617
    },
    {
      "epoch": 10.693102023957042,
      "grad_norm": 1.3133724927902222,
      "learning_rate": 0.000694059405940594,
      "loss": 0.798,
      "step": 1618
    },
    {
      "epoch": 10.699710863279636,
      "grad_norm": 15.41946792602539,
      "learning_rate": 0.0006935643564356435,
      "loss": 1.7318,
      "step": 1619
    },
    {
      "epoch": 10.706319702602231,
      "grad_norm": 19.407926559448242,
      "learning_rate": 0.000693069306930693,
      "loss": 2.704,
      "step": 1620
    },
    {
      "epoch": 10.712928541924825,
      "grad_norm": 10.711281776428223,
      "learning_rate": 0.0006925742574257426,
      "loss": 2.3444,
      "step": 1621
    },
    {
      "epoch": 10.719537381247418,
      "grad_norm": 2.853470802307129,
      "learning_rate": 0.0006920792079207921,
      "loss": 1.6688,
      "step": 1622
    },
    {
      "epoch": 10.726146220570012,
      "grad_norm": 3.162365436553955,
      "learning_rate": 0.0006915841584158415,
      "loss": 2.2441,
      "step": 1623
    },
    {
      "epoch": 10.732755059892606,
      "grad_norm": 7.923062324523926,
      "learning_rate": 0.000691089108910891,
      "loss": 0.991,
      "step": 1624
    },
    {
      "epoch": 10.739363899215201,
      "grad_norm": 3.688093900680542,
      "learning_rate": 0.0006905940594059406,
      "loss": 2.2231,
      "step": 1625
    },
    {
      "epoch": 10.745972738537795,
      "grad_norm": 3.529127359390259,
      "learning_rate": 0.0006900990099009901,
      "loss": 1.6887,
      "step": 1626
    },
    {
      "epoch": 10.752581577860388,
      "grad_norm": 9.415641784667969,
      "learning_rate": 0.0006896039603960396,
      "loss": 1.1531,
      "step": 1627
    },
    {
      "epoch": 10.759190417182982,
      "grad_norm": 4.8592209815979,
      "learning_rate": 0.000689108910891089,
      "loss": 2.5706,
      "step": 1628
    },
    {
      "epoch": 10.765799256505577,
      "grad_norm": 17.55013656616211,
      "learning_rate": 0.0006886138613861386,
      "loss": 2.3027,
      "step": 1629
    },
    {
      "epoch": 10.77240809582817,
      "grad_norm": 3.023108720779419,
      "learning_rate": 0.0006881188118811881,
      "loss": 2.2322,
      "step": 1630
    },
    {
      "epoch": 10.779016935150764,
      "grad_norm": 16.420618057250977,
      "learning_rate": 0.0006876237623762376,
      "loss": 2.4378,
      "step": 1631
    },
    {
      "epoch": 10.785625774473358,
      "grad_norm": 3.5936849117279053,
      "learning_rate": 0.0006871287128712872,
      "loss": 1.6632,
      "step": 1632
    },
    {
      "epoch": 10.792234613795952,
      "grad_norm": 2.4497056007385254,
      "learning_rate": 0.0006866336633663367,
      "loss": 1.5529,
      "step": 1633
    },
    {
      "epoch": 10.798843453118547,
      "grad_norm": 1.5644097328186035,
      "learning_rate": 0.0006861386138613862,
      "loss": 1.1541,
      "step": 1634
    },
    {
      "epoch": 10.80545229244114,
      "grad_norm": 34.786312103271484,
      "learning_rate": 0.0006856435643564357,
      "loss": 3.6745,
      "step": 1635
    },
    {
      "epoch": 10.812061131763734,
      "grad_norm": 31.481061935424805,
      "learning_rate": 0.0006851485148514852,
      "loss": 2.2969,
      "step": 1636
    },
    {
      "epoch": 10.818669971086328,
      "grad_norm": 38.313880920410156,
      "learning_rate": 0.0006846534653465348,
      "loss": 3.2681,
      "step": 1637
    },
    {
      "epoch": 10.825278810408921,
      "grad_norm": 25.394163131713867,
      "learning_rate": 0.0006841584158415842,
      "loss": 2.6427,
      "step": 1638
    },
    {
      "epoch": 10.831887649731517,
      "grad_norm": 13.925151824951172,
      "learning_rate": 0.0006836633663366337,
      "loss": 1.0944,
      "step": 1639
    },
    {
      "epoch": 10.83849648905411,
      "grad_norm": 5.484980583190918,
      "learning_rate": 0.0006831683168316832,
      "loss": 3.324,
      "step": 1640
    },
    {
      "epoch": 10.845105328376704,
      "grad_norm": 2.1920158863067627,
      "learning_rate": 0.0006826732673267328,
      "loss": 1.905,
      "step": 1641
    },
    {
      "epoch": 10.851714167699297,
      "grad_norm": 14.100828170776367,
      "learning_rate": 0.0006821782178217823,
      "loss": 2.6151,
      "step": 1642
    },
    {
      "epoch": 10.858323007021891,
      "grad_norm": 19.94968032836914,
      "learning_rate": 0.0006816831683168317,
      "loss": 1.0011,
      "step": 1643
    },
    {
      "epoch": 10.864931846344486,
      "grad_norm": 9.303386688232422,
      "learning_rate": 0.0006811881188118812,
      "loss": 1.2627,
      "step": 1644
    },
    {
      "epoch": 10.87154068566708,
      "grad_norm": 8.406792640686035,
      "learning_rate": 0.0006806930693069308,
      "loss": 2.2547,
      "step": 1645
    },
    {
      "epoch": 10.878149524989674,
      "grad_norm": 12.584375381469727,
      "learning_rate": 0.0006801980198019803,
      "loss": 1.8436,
      "step": 1646
    },
    {
      "epoch": 10.884758364312267,
      "grad_norm": 30.206958770751953,
      "learning_rate": 0.0006797029702970298,
      "loss": 5.5236,
      "step": 1647
    },
    {
      "epoch": 10.89136720363486,
      "grad_norm": 6.778384685516357,
      "learning_rate": 0.0006792079207920792,
      "loss": 2.1802,
      "step": 1648
    },
    {
      "epoch": 10.897976042957456,
      "grad_norm": 10.56336498260498,
      "learning_rate": 0.0006787128712871288,
      "loss": 2.7198,
      "step": 1649
    },
    {
      "epoch": 10.90458488228005,
      "grad_norm": 12.28691291809082,
      "learning_rate": 0.0006782178217821783,
      "loss": 1.539,
      "step": 1650
    },
    {
      "epoch": 10.911193721602643,
      "grad_norm": 2.5142316818237305,
      "learning_rate": 0.0006777227722772278,
      "loss": 3.6975,
      "step": 1651
    },
    {
      "epoch": 10.917802560925237,
      "grad_norm": 11.745988845825195,
      "learning_rate": 0.0006772277227722773,
      "loss": 0.6126,
      "step": 1652
    },
    {
      "epoch": 10.924411400247832,
      "grad_norm": 18.924877166748047,
      "learning_rate": 0.0006767326732673267,
      "loss": 2.4066,
      "step": 1653
    },
    {
      "epoch": 10.931020239570426,
      "grad_norm": 24.54170799255371,
      "learning_rate": 0.0006762376237623763,
      "loss": 2.3339,
      "step": 1654
    },
    {
      "epoch": 10.93762907889302,
      "grad_norm": 7.825098037719727,
      "learning_rate": 0.0006757425742574258,
      "loss": 1.4786,
      "step": 1655
    },
    {
      "epoch": 10.944237918215613,
      "grad_norm": 7.106819152832031,
      "learning_rate": 0.0006752475247524753,
      "loss": 1.5434,
      "step": 1656
    },
    {
      "epoch": 10.950846757538207,
      "grad_norm": 4.821408748626709,
      "learning_rate": 0.0006747524752475249,
      "loss": 1.4474,
      "step": 1657
    },
    {
      "epoch": 10.957455596860802,
      "grad_norm": 16.838762283325195,
      "learning_rate": 0.0006742574257425743,
      "loss": 2.2166,
      "step": 1658
    },
    {
      "epoch": 10.964064436183396,
      "grad_norm": 18.175857543945312,
      "learning_rate": 0.0006737623762376238,
      "loss": 3.0684,
      "step": 1659
    },
    {
      "epoch": 10.97067327550599,
      "grad_norm": 34.597354888916016,
      "learning_rate": 0.0006732673267326733,
      "loss": 3.4212,
      "step": 1660
    },
    {
      "epoch": 10.977282114828583,
      "grad_norm": 18.786235809326172,
      "learning_rate": 0.0006727722772277228,
      "loss": 2.3389,
      "step": 1661
    },
    {
      "epoch": 10.983890954151176,
      "grad_norm": 21.34021759033203,
      "learning_rate": 0.0006722772277227724,
      "loss": 2.1857,
      "step": 1662
    },
    {
      "epoch": 10.990499793473772,
      "grad_norm": 2.7056987285614014,
      "learning_rate": 0.0006717821782178218,
      "loss": 0.9099,
      "step": 1663
    },
    {
      "epoch": 10.997108632796365,
      "grad_norm": 2.7732460498809814,
      "learning_rate": 0.0006712871287128713,
      "loss": 1.0769,
      "step": 1664
    },
    {
      "epoch": 10.997108632796365,
      "eval_validation_error_bar": 0.03974696841570026,
      "eval_validation_loss": 4.322849750518799,
      "eval_validation_pearsonr": 0.6619925950102629,
      "eval_validation_rmse": 2.079146385192871,
      "eval_validation_runtime": 28.5492,
      "eval_validation_samples_per_second": 7.111,
      "eval_validation_spearman": 0.7049304260536832,
      "eval_validation_steps_per_second": 7.111,
      "step": 1664
    },
    {
      "epoch": 10.997108632796365,
      "eval_test_error_bar": 0.041667107395128064,
      "eval_test_loss": 6.645835876464844,
      "eval_test_pearsonr": 0.5359539233967188,
      "eval_test_rmse": 2.5779519081115723,
      "eval_test_runtime": 45.7715,
      "eval_test_samples_per_second": 7.122,
      "eval_test_spearman": 0.5495426315964765,
      "eval_test_steps_per_second": 7.122,
      "step": 1664
    },
    {
      "epoch": 11.003717472118959,
      "grad_norm": 17.964786529541016,
      "learning_rate": 0.0006707920792079208,
      "loss": 1.7279,
      "step": 1665
    },
    {
      "epoch": 11.010326311441553,
      "grad_norm": 13.918216705322266,
      "learning_rate": 0.0006702970297029704,
      "loss": 0.7966,
      "step": 1666
    },
    {
      "epoch": 11.016935150764146,
      "grad_norm": 8.519726753234863,
      "learning_rate": 0.0006698019801980199,
      "loss": 0.9235,
      "step": 1667
    },
    {
      "epoch": 11.023543990086742,
      "grad_norm": 6.034689903259277,
      "learning_rate": 0.0006693069306930693,
      "loss": 1.9873,
      "step": 1668
    },
    {
      "epoch": 11.030152829409335,
      "grad_norm": 9.414724349975586,
      "learning_rate": 0.0006688118811881188,
      "loss": 0.9827,
      "step": 1669
    },
    {
      "epoch": 11.036761668731929,
      "grad_norm": 6.011530876159668,
      "learning_rate": 0.0006683168316831684,
      "loss": 3.0897,
      "step": 1670
    },
    {
      "epoch": 11.043370508054522,
      "grad_norm": 18.1776180267334,
      "learning_rate": 0.0006678217821782179,
      "loss": 4.9816,
      "step": 1671
    },
    {
      "epoch": 11.049979347377118,
      "grad_norm": 12.360079765319824,
      "learning_rate": 0.0006673267326732674,
      "loss": 1.8099,
      "step": 1672
    },
    {
      "epoch": 11.056588186699711,
      "grad_norm": 37.12476348876953,
      "learning_rate": 0.0006668316831683168,
      "loss": 3.2573,
      "step": 1673
    },
    {
      "epoch": 11.063197026022305,
      "grad_norm": 32.1170654296875,
      "learning_rate": 0.0006663366336633664,
      "loss": 2.6918,
      "step": 1674
    },
    {
      "epoch": 11.069805865344899,
      "grad_norm": 17.191865921020508,
      "learning_rate": 0.0006658415841584159,
      "loss": 3.1281,
      "step": 1675
    },
    {
      "epoch": 11.076414704667492,
      "grad_norm": 10.27529525756836,
      "learning_rate": 0.0006653465346534654,
      "loss": 0.9817,
      "step": 1676
    },
    {
      "epoch": 11.083023543990087,
      "grad_norm": 21.861783981323242,
      "learning_rate": 0.0006648514851485149,
      "loss": 1.4748,
      "step": 1677
    },
    {
      "epoch": 11.089632383312681,
      "grad_norm": 8.250202178955078,
      "learning_rate": 0.0006643564356435644,
      "loss": 1.3175,
      "step": 1678
    },
    {
      "epoch": 11.096241222635275,
      "grad_norm": 2.3335742950439453,
      "learning_rate": 0.0006638613861386139,
      "loss": 1.391,
      "step": 1679
    },
    {
      "epoch": 11.102850061957868,
      "grad_norm": 24.59646987915039,
      "learning_rate": 0.0006633663366336634,
      "loss": 3.707,
      "step": 1680
    },
    {
      "epoch": 11.109458901280462,
      "grad_norm": 3.015190601348877,
      "learning_rate": 0.0006628712871287129,
      "loss": 0.6636,
      "step": 1681
    },
    {
      "epoch": 11.116067740603057,
      "grad_norm": 10.149739265441895,
      "learning_rate": 0.0006623762376237625,
      "loss": 0.8105,
      "step": 1682
    },
    {
      "epoch": 11.12267657992565,
      "grad_norm": 4.5410895347595215,
      "learning_rate": 0.0006618811881188119,
      "loss": 1.3915,
      "step": 1683
    },
    {
      "epoch": 11.129285419248244,
      "grad_norm": 14.58065128326416,
      "learning_rate": 0.0006613861386138614,
      "loss": 1.2292,
      "step": 1684
    },
    {
      "epoch": 11.135894258570838,
      "grad_norm": 15.036478996276855,
      "learning_rate": 0.0006608910891089109,
      "loss": 1.8339,
      "step": 1685
    },
    {
      "epoch": 11.142503097893432,
      "grad_norm": 13.632838249206543,
      "learning_rate": 0.0006603960396039605,
      "loss": 2.2521,
      "step": 1686
    },
    {
      "epoch": 11.149111937216027,
      "grad_norm": 7.398731708526611,
      "learning_rate": 0.00065990099009901,
      "loss": 0.8162,
      "step": 1687
    },
    {
      "epoch": 11.15572077653862,
      "grad_norm": 5.618923187255859,
      "learning_rate": 0.0006594059405940594,
      "loss": 1.3736,
      "step": 1688
    },
    {
      "epoch": 11.162329615861214,
      "grad_norm": 9.942071914672852,
      "learning_rate": 0.0006589108910891089,
      "loss": 2.5511,
      "step": 1689
    },
    {
      "epoch": 11.168938455183808,
      "grad_norm": 13.244848251342773,
      "learning_rate": 0.0006584158415841585,
      "loss": 3.0391,
      "step": 1690
    },
    {
      "epoch": 11.175547294506403,
      "grad_norm": 17.822586059570312,
      "learning_rate": 0.000657920792079208,
      "loss": 1.9415,
      "step": 1691
    },
    {
      "epoch": 11.182156133828997,
      "grad_norm": 9.760724067687988,
      "learning_rate": 0.0006574257425742575,
      "loss": 2.0439,
      "step": 1692
    },
    {
      "epoch": 11.18876497315159,
      "grad_norm": 3.2390944957733154,
      "learning_rate": 0.0006569306930693069,
      "loss": 1.1205,
      "step": 1693
    },
    {
      "epoch": 11.195373812474184,
      "grad_norm": 6.430410861968994,
      "learning_rate": 0.0006564356435643565,
      "loss": 1.6344,
      "step": 1694
    },
    {
      "epoch": 11.201982651796778,
      "grad_norm": 7.347653865814209,
      "learning_rate": 0.000655940594059406,
      "loss": 0.7457,
      "step": 1695
    },
    {
      "epoch": 11.208591491119373,
      "grad_norm": 1.5152883529663086,
      "learning_rate": 0.0006554455445544555,
      "loss": 1.5658,
      "step": 1696
    },
    {
      "epoch": 11.215200330441967,
      "grad_norm": 4.724211692810059,
      "learning_rate": 0.000654950495049505,
      "loss": 1.2837,
      "step": 1697
    },
    {
      "epoch": 11.22180916976456,
      "grad_norm": 21.562368392944336,
      "learning_rate": 0.0006544554455445545,
      "loss": 1.8947,
      "step": 1698
    },
    {
      "epoch": 11.228418009087154,
      "grad_norm": 3.567006826400757,
      "learning_rate": 0.000653960396039604,
      "loss": 1.2829,
      "step": 1699
    },
    {
      "epoch": 11.235026848409747,
      "grad_norm": 9.070393562316895,
      "learning_rate": 0.0006534653465346535,
      "loss": 0.7418,
      "step": 1700
    },
    {
      "epoch": 11.241635687732343,
      "grad_norm": 4.230207443237305,
      "learning_rate": 0.000652970297029703,
      "loss": 4.7914,
      "step": 1701
    },
    {
      "epoch": 11.248244527054936,
      "grad_norm": 16.880285263061523,
      "learning_rate": 0.0006524752475247526,
      "loss": 1.1055,
      "step": 1702
    },
    {
      "epoch": 11.25485336637753,
      "grad_norm": 6.479663372039795,
      "learning_rate": 0.000651980198019802,
      "loss": 1.4365,
      "step": 1703
    },
    {
      "epoch": 11.261462205700123,
      "grad_norm": 5.824268341064453,
      "learning_rate": 0.0006514851485148515,
      "loss": 2.3786,
      "step": 1704
    },
    {
      "epoch": 11.268071045022717,
      "grad_norm": 35.51007843017578,
      "learning_rate": 0.000650990099009901,
      "loss": 2.6441,
      "step": 1705
    },
    {
      "epoch": 11.274679884345312,
      "grad_norm": 26.841386795043945,
      "learning_rate": 0.0006504950495049506,
      "loss": 1.7152,
      "step": 1706
    },
    {
      "epoch": 11.281288723667906,
      "grad_norm": 32.945884704589844,
      "learning_rate": 0.0006500000000000001,
      "loss": 1.9009,
      "step": 1707
    },
    {
      "epoch": 11.2878975629905,
      "grad_norm": 20.90907096862793,
      "learning_rate": 0.0006495049504950495,
      "loss": 2.1875,
      "step": 1708
    },
    {
      "epoch": 11.294506402313093,
      "grad_norm": 25.161033630371094,
      "learning_rate": 0.000649009900990099,
      "loss": 2.6095,
      "step": 1709
    },
    {
      "epoch": 11.301115241635689,
      "grad_norm": 5.020782947540283,
      "learning_rate": 0.0006485148514851485,
      "loss": 1.5855,
      "step": 1710
    },
    {
      "epoch": 11.307724080958282,
      "grad_norm": 13.359058380126953,
      "learning_rate": 0.0006480198019801981,
      "loss": 3.5861,
      "step": 1711
    },
    {
      "epoch": 11.314332920280876,
      "grad_norm": 23.9241886138916,
      "learning_rate": 0.0006475247524752476,
      "loss": 2.9357,
      "step": 1712
    },
    {
      "epoch": 11.32094175960347,
      "grad_norm": 36.550472259521484,
      "learning_rate": 0.000647029702970297,
      "loss": 3.5513,
      "step": 1713
    },
    {
      "epoch": 11.327550598926063,
      "grad_norm": 23.386384963989258,
      "learning_rate": 0.0006465346534653465,
      "loss": 1.6368,
      "step": 1714
    },
    {
      "epoch": 11.334159438248658,
      "grad_norm": 12.294744491577148,
      "learning_rate": 0.0006460396039603961,
      "loss": 0.8124,
      "step": 1715
    },
    {
      "epoch": 11.340768277571252,
      "grad_norm": 6.5052361488342285,
      "learning_rate": 0.0006455445544554456,
      "loss": 1.6667,
      "step": 1716
    },
    {
      "epoch": 11.347377116893846,
      "grad_norm": 8.651323318481445,
      "learning_rate": 0.0006450495049504951,
      "loss": 2.0678,
      "step": 1717
    },
    {
      "epoch": 11.35398595621644,
      "grad_norm": 14.92980670928955,
      "learning_rate": 0.0006445544554455445,
      "loss": 1.1953,
      "step": 1718
    },
    {
      "epoch": 11.360594795539033,
      "grad_norm": 39.88839340209961,
      "learning_rate": 0.0006440594059405941,
      "loss": 4.062,
      "step": 1719
    },
    {
      "epoch": 11.367203634861628,
      "grad_norm": 47.097713470458984,
      "learning_rate": 0.0006435643564356436,
      "loss": 4.2858,
      "step": 1720
    },
    {
      "epoch": 11.373812474184222,
      "grad_norm": 19.62429428100586,
      "learning_rate": 0.0006430693069306931,
      "loss": 1.7945,
      "step": 1721
    },
    {
      "epoch": 11.380421313506815,
      "grad_norm": 35.94872283935547,
      "learning_rate": 0.0006425742574257426,
      "loss": 2.6891,
      "step": 1722
    },
    {
      "epoch": 11.387030152829409,
      "grad_norm": 34.65157699584961,
      "learning_rate": 0.0006420792079207921,
      "loss": 2.452,
      "step": 1723
    },
    {
      "epoch": 11.393638992152002,
      "grad_norm": 6.322772979736328,
      "learning_rate": 0.0006415841584158416,
      "loss": 2.646,
      "step": 1724
    },
    {
      "epoch": 11.400247831474598,
      "grad_norm": 7.308193206787109,
      "learning_rate": 0.0006410891089108911,
      "loss": 1.7535,
      "step": 1725
    },
    {
      "epoch": 11.406856670797191,
      "grad_norm": 8.141335487365723,
      "learning_rate": 0.0006405940594059406,
      "loss": 1.7596,
      "step": 1726
    },
    {
      "epoch": 11.413465510119785,
      "grad_norm": 41.91923522949219,
      "learning_rate": 0.0006400990099009902,
      "loss": 4.2817,
      "step": 1727
    },
    {
      "epoch": 11.420074349442379,
      "grad_norm": 16.627582550048828,
      "learning_rate": 0.0006396039603960396,
      "loss": 2.3483,
      "step": 1728
    },
    {
      "epoch": 11.426683188764972,
      "grad_norm": 36.246795654296875,
      "learning_rate": 0.0006391089108910891,
      "loss": 3.4061,
      "step": 1729
    },
    {
      "epoch": 11.433292028087568,
      "grad_norm": 28.375991821289062,
      "learning_rate": 0.0006386138613861386,
      "loss": 2.7111,
      "step": 1730
    },
    {
      "epoch": 11.439900867410161,
      "grad_norm": 16.320106506347656,
      "learning_rate": 0.0006381188118811882,
      "loss": 1.8908,
      "step": 1731
    },
    {
      "epoch": 11.446509706732755,
      "grad_norm": 11.052532196044922,
      "learning_rate": 0.0006376237623762377,
      "loss": 1.3108,
      "step": 1732
    },
    {
      "epoch": 11.453118546055348,
      "grad_norm": 1.3719850778579712,
      "learning_rate": 0.0006371287128712871,
      "loss": 0.934,
      "step": 1733
    },
    {
      "epoch": 11.459727385377944,
      "grad_norm": 1.3763352632522583,
      "learning_rate": 0.0006366336633663366,
      "loss": 0.6789,
      "step": 1734
    },
    {
      "epoch": 11.466336224700537,
      "grad_norm": 28.290515899658203,
      "learning_rate": 0.0006361386138613862,
      "loss": 1.4149,
      "step": 1735
    },
    {
      "epoch": 11.472945064023131,
      "grad_norm": 34.34722137451172,
      "learning_rate": 0.0006356435643564357,
      "loss": 1.9693,
      "step": 1736
    },
    {
      "epoch": 11.479553903345725,
      "grad_norm": 28.311031341552734,
      "learning_rate": 0.0006351485148514852,
      "loss": 1.3243,
      "step": 1737
    },
    {
      "epoch": 11.486162742668318,
      "grad_norm": 18.476266860961914,
      "learning_rate": 0.0006346534653465346,
      "loss": 1.1846,
      "step": 1738
    },
    {
      "epoch": 11.492771581990914,
      "grad_norm": 12.85179615020752,
      "learning_rate": 0.0006341584158415842,
      "loss": 1.2904,
      "step": 1739
    },
    {
      "epoch": 11.499380421313507,
      "grad_norm": 3.9419350624084473,
      "learning_rate": 0.0006336633663366337,
      "loss": 1.5372,
      "step": 1740
    },
    {
      "epoch": 11.5059892606361,
      "grad_norm": 7.439996719360352,
      "learning_rate": 0.0006331683168316832,
      "loss": 1.044,
      "step": 1741
    },
    {
      "epoch": 11.512598099958694,
      "grad_norm": 13.268950462341309,
      "learning_rate": 0.0006326732673267327,
      "loss": 1.6862,
      "step": 1742
    },
    {
      "epoch": 11.519206939281288,
      "grad_norm": 30.066680908203125,
      "learning_rate": 0.0006321782178217822,
      "loss": 2.4637,
      "step": 1743
    },
    {
      "epoch": 11.525815778603883,
      "grad_norm": 25.82246208190918,
      "learning_rate": 0.0006316831683168317,
      "loss": 3.1852,
      "step": 1744
    },
    {
      "epoch": 11.532424617926477,
      "grad_norm": 13.857699394226074,
      "learning_rate": 0.0006311881188118812,
      "loss": 1.6612,
      "step": 1745
    },
    {
      "epoch": 11.53903345724907,
      "grad_norm": 5.8835272789001465,
      "learning_rate": 0.0006306930693069307,
      "loss": 1.4209,
      "step": 1746
    },
    {
      "epoch": 11.545642296571664,
      "grad_norm": 17.49514389038086,
      "learning_rate": 0.0006301980198019803,
      "loss": 1.3947,
      "step": 1747
    },
    {
      "epoch": 11.55225113589426,
      "grad_norm": 19.643268585205078,
      "learning_rate": 0.0006297029702970297,
      "loss": 3.0953,
      "step": 1748
    },
    {
      "epoch": 11.558859975216853,
      "grad_norm": 22.600664138793945,
      "learning_rate": 0.0006292079207920792,
      "loss": 1.3136,
      "step": 1749
    },
    {
      "epoch": 11.565468814539447,
      "grad_norm": 27.039691925048828,
      "learning_rate": 0.0006287128712871287,
      "loss": 2.26,
      "step": 1750
    },
    {
      "epoch": 11.57207765386204,
      "grad_norm": 22.534826278686523,
      "learning_rate": 0.0006282178217821783,
      "loss": 1.9418,
      "step": 1751
    },
    {
      "epoch": 11.578686493184634,
      "grad_norm": 3.610168933868408,
      "learning_rate": 0.0006277227722772278,
      "loss": 2.6797,
      "step": 1752
    },
    {
      "epoch": 11.58529533250723,
      "grad_norm": 2.4227607250213623,
      "learning_rate": 0.0006272277227722772,
      "loss": 0.8074,
      "step": 1753
    },
    {
      "epoch": 11.591904171829823,
      "grad_norm": 40.2660026550293,
      "learning_rate": 0.0006267326732673267,
      "loss": 2.8156,
      "step": 1754
    },
    {
      "epoch": 11.598513011152416,
      "grad_norm": 22.4910945892334,
      "learning_rate": 0.0006262376237623763,
      "loss": 2.5315,
      "step": 1755
    },
    {
      "epoch": 11.60512185047501,
      "grad_norm": 8.870923042297363,
      "learning_rate": 0.0006257425742574258,
      "loss": 0.4702,
      "step": 1756
    },
    {
      "epoch": 11.611730689797604,
      "grad_norm": 9.058921813964844,
      "learning_rate": 0.0006252475247524753,
      "loss": 1.8968,
      "step": 1757
    },
    {
      "epoch": 11.618339529120199,
      "grad_norm": 13.977298736572266,
      "learning_rate": 0.0006247524752475247,
      "loss": 1.4061,
      "step": 1758
    },
    {
      "epoch": 11.624948368442793,
      "grad_norm": 10.762150764465332,
      "learning_rate": 0.0006242574257425742,
      "loss": 1.1068,
      "step": 1759
    },
    {
      "epoch": 11.631557207765386,
      "grad_norm": 4.286403179168701,
      "learning_rate": 0.0006237623762376238,
      "loss": 1.0547,
      "step": 1760
    },
    {
      "epoch": 11.63816604708798,
      "grad_norm": 16.48691177368164,
      "learning_rate": 0.0006232673267326733,
      "loss": 1.5157,
      "step": 1761
    },
    {
      "epoch": 11.644774886410573,
      "grad_norm": 10.914156913757324,
      "learning_rate": 0.0006227722772277228,
      "loss": 1.3838,
      "step": 1762
    },
    {
      "epoch": 11.651383725733169,
      "grad_norm": 19.111536026000977,
      "learning_rate": 0.0006222772277227722,
      "loss": 0.9783,
      "step": 1763
    },
    {
      "epoch": 11.657992565055762,
      "grad_norm": 8.077604293823242,
      "learning_rate": 0.0006217821782178218,
      "loss": 2.3493,
      "step": 1764
    },
    {
      "epoch": 11.664601404378356,
      "grad_norm": 8.836478233337402,
      "learning_rate": 0.0006212871287128713,
      "loss": 0.6316,
      "step": 1765
    },
    {
      "epoch": 11.67121024370095,
      "grad_norm": 2.95112681388855,
      "learning_rate": 0.0006207920792079208,
      "loss": 1.9317,
      "step": 1766
    },
    {
      "epoch": 11.677819083023543,
      "grad_norm": 8.948736190795898,
      "learning_rate": 0.0006202970297029703,
      "loss": 2.0696,
      "step": 1767
    },
    {
      "epoch": 11.684427922346138,
      "grad_norm": 9.098037719726562,
      "learning_rate": 0.0006198019801980198,
      "loss": 1.7681,
      "step": 1768
    },
    {
      "epoch": 11.691036761668732,
      "grad_norm": 9.668343544006348,
      "learning_rate": 0.0006193069306930693,
      "loss": 1.3911,
      "step": 1769
    },
    {
      "epoch": 11.697645600991326,
      "grad_norm": 7.156837463378906,
      "learning_rate": 0.0006188118811881188,
      "loss": 0.2697,
      "step": 1770
    },
    {
      "epoch": 11.70425444031392,
      "grad_norm": 12.833897590637207,
      "learning_rate": 0.0006183168316831683,
      "loss": 1.594,
      "step": 1771
    },
    {
      "epoch": 11.710863279636515,
      "grad_norm": 22.416845321655273,
      "learning_rate": 0.0006178217821782179,
      "loss": 2.1045,
      "step": 1772
    },
    {
      "epoch": 11.717472118959108,
      "grad_norm": 21.86298370361328,
      "learning_rate": 0.0006173267326732673,
      "loss": 2.1269,
      "step": 1773
    },
    {
      "epoch": 11.724080958281702,
      "grad_norm": 2.047623872756958,
      "learning_rate": 0.0006168316831683168,
      "loss": 2.667,
      "step": 1774
    },
    {
      "epoch": 11.730689797604295,
      "grad_norm": 17.70072364807129,
      "learning_rate": 0.0006163366336633663,
      "loss": 2.5457,
      "step": 1775
    },
    {
      "epoch": 11.737298636926889,
      "grad_norm": 24.69384765625,
      "learning_rate": 0.0006158415841584159,
      "loss": 2.1777,
      "step": 1776
    },
    {
      "epoch": 11.743907476249484,
      "grad_norm": 7.4291791915893555,
      "learning_rate": 0.0006153465346534654,
      "loss": 0.7905,
      "step": 1777
    },
    {
      "epoch": 11.750516315572078,
      "grad_norm": 13.881636619567871,
      "learning_rate": 0.0006148514851485148,
      "loss": 1.8169,
      "step": 1778
    },
    {
      "epoch": 11.757125154894672,
      "grad_norm": 18.03363037109375,
      "learning_rate": 0.0006143564356435643,
      "loss": 1.1571,
      "step": 1779
    },
    {
      "epoch": 11.763733994217265,
      "grad_norm": 8.662032127380371,
      "learning_rate": 0.0006138613861386139,
      "loss": 2.0791,
      "step": 1780
    },
    {
      "epoch": 11.770342833539859,
      "grad_norm": 31.897186279296875,
      "learning_rate": 0.0006133663366336634,
      "loss": 2.8064,
      "step": 1781
    },
    {
      "epoch": 11.776951672862454,
      "grad_norm": 21.55478286743164,
      "learning_rate": 0.0006128712871287129,
      "loss": 1.5427,
      "step": 1782
    },
    {
      "epoch": 11.783560512185048,
      "grad_norm": 24.341646194458008,
      "learning_rate": 0.0006123762376237623,
      "loss": 2.4119,
      "step": 1783
    },
    {
      "epoch": 11.790169351507641,
      "grad_norm": 7.330545902252197,
      "learning_rate": 0.0006118811881188119,
      "loss": 1.9812,
      "step": 1784
    },
    {
      "epoch": 11.796778190830235,
      "grad_norm": 6.3654656410217285,
      "learning_rate": 0.0006113861386138614,
      "loss": 1.9023,
      "step": 1785
    },
    {
      "epoch": 11.80338703015283,
      "grad_norm": 9.10777473449707,
      "learning_rate": 0.0006108910891089109,
      "loss": 0.9415,
      "step": 1786
    },
    {
      "epoch": 11.809995869475424,
      "grad_norm": 6.632053375244141,
      "learning_rate": 0.0006103960396039604,
      "loss": 1.3659,
      "step": 1787
    },
    {
      "epoch": 11.816604708798017,
      "grad_norm": 9.232616424560547,
      "learning_rate": 0.0006099009900990099,
      "loss": 0.8938,
      "step": 1788
    },
    {
      "epoch": 11.823213548120611,
      "grad_norm": 19.794635772705078,
      "learning_rate": 0.0006094059405940594,
      "loss": 3.9341,
      "step": 1789
    },
    {
      "epoch": 11.829822387443205,
      "grad_norm": 17.04947280883789,
      "learning_rate": 0.0006089108910891089,
      "loss": 3.826,
      "step": 1790
    },
    {
      "epoch": 11.8364312267658,
      "grad_norm": 17.155925750732422,
      "learning_rate": 0.0006084158415841584,
      "loss": 0.7801,
      "step": 1791
    },
    {
      "epoch": 11.843040066088394,
      "grad_norm": 9.550296783447266,
      "learning_rate": 0.000607920792079208,
      "loss": 1.4281,
      "step": 1792
    },
    {
      "epoch": 11.849648905410987,
      "grad_norm": 9.019991874694824,
      "learning_rate": 0.0006074257425742574,
      "loss": 1.467,
      "step": 1793
    },
    {
      "epoch": 11.85625774473358,
      "grad_norm": 9.112932205200195,
      "learning_rate": 0.0006069306930693069,
      "loss": 3.1666,
      "step": 1794
    },
    {
      "epoch": 11.862866584056174,
      "grad_norm": 6.450294494628906,
      "learning_rate": 0.0006064356435643564,
      "loss": 1.7876,
      "step": 1795
    },
    {
      "epoch": 11.86947542337877,
      "grad_norm": 10.09946346282959,
      "learning_rate": 0.000605940594059406,
      "loss": 0.7021,
      "step": 1796
    },
    {
      "epoch": 11.876084262701363,
      "grad_norm": 2.19754695892334,
      "learning_rate": 0.0006054455445544555,
      "loss": 1.2312,
      "step": 1797
    },
    {
      "epoch": 11.882693102023957,
      "grad_norm": 6.577397346496582,
      "learning_rate": 0.0006049504950495049,
      "loss": 1.6685,
      "step": 1798
    },
    {
      "epoch": 11.88930194134655,
      "grad_norm": 17.08086395263672,
      "learning_rate": 0.0006044554455445544,
      "loss": 2.8005,
      "step": 1799
    },
    {
      "epoch": 11.895910780669144,
      "grad_norm": 11.771778106689453,
      "learning_rate": 0.000603960396039604,
      "loss": 0.7955,
      "step": 1800
    },
    {
      "epoch": 11.90251961999174,
      "grad_norm": 17.788742065429688,
      "learning_rate": 0.0006034653465346535,
      "loss": 1.9644,
      "step": 1801
    },
    {
      "epoch": 11.909128459314333,
      "grad_norm": 4.89302396774292,
      "learning_rate": 0.000602970297029703,
      "loss": 2.4153,
      "step": 1802
    },
    {
      "epoch": 11.915737298636927,
      "grad_norm": 3.7579517364501953,
      "learning_rate": 0.0006024752475247524,
      "loss": 1.839,
      "step": 1803
    },
    {
      "epoch": 11.92234613795952,
      "grad_norm": 19.418119430541992,
      "learning_rate": 0.000601980198019802,
      "loss": 2.4029,
      "step": 1804
    },
    {
      "epoch": 11.928954977282114,
      "grad_norm": 19.951505661010742,
      "learning_rate": 0.0006014851485148515,
      "loss": 3.0692,
      "step": 1805
    },
    {
      "epoch": 11.93556381660471,
      "grad_norm": 11.363659858703613,
      "learning_rate": 0.000600990099009901,
      "loss": 3.7666,
      "step": 1806
    },
    {
      "epoch": 11.942172655927303,
      "grad_norm": 14.238852500915527,
      "learning_rate": 0.0006004950495049505,
      "loss": 1.0544,
      "step": 1807
    },
    {
      "epoch": 11.948781495249897,
      "grad_norm": 18.890682220458984,
      "learning_rate": 0.0006,
      "loss": 1.1074,
      "step": 1808
    },
    {
      "epoch": 11.95539033457249,
      "grad_norm": 11.44566822052002,
      "learning_rate": 0.0005995049504950495,
      "loss": 1.9456,
      "step": 1809
    },
    {
      "epoch": 11.961999173895085,
      "grad_norm": 3.03297758102417,
      "learning_rate": 0.000599009900990099,
      "loss": 0.7645,
      "step": 1810
    },
    {
      "epoch": 11.968608013217679,
      "grad_norm": 16.18581771850586,
      "learning_rate": 0.0005985148514851485,
      "loss": 4.8761,
      "step": 1811
    },
    {
      "epoch": 11.975216852540273,
      "grad_norm": 11.04481315612793,
      "learning_rate": 0.000598019801980198,
      "loss": 2.0694,
      "step": 1812
    },
    {
      "epoch": 11.981825691862866,
      "grad_norm": 25.19761085510254,
      "learning_rate": 0.0005975247524752475,
      "loss": 4.204,
      "step": 1813
    },
    {
      "epoch": 11.98843453118546,
      "grad_norm": 13.063855171203613,
      "learning_rate": 0.000597029702970297,
      "loss": 2.2613,
      "step": 1814
    },
    {
      "epoch": 11.995043370508055,
      "grad_norm": 1.3297255039215088,
      "learning_rate": 0.0005965346534653465,
      "loss": 1.4356,
      "step": 1815
    },
    {
      "epoch": 11.995043370508055,
      "eval_validation_error_bar": 0.03718118323803957,
      "eval_validation_loss": 4.185102462768555,
      "eval_validation_pearsonr": 0.6833592152837935,
      "eval_validation_rmse": 2.0457522869110107,
      "eval_validation_runtime": 28.5535,
      "eval_validation_samples_per_second": 7.109,
      "eval_validation_spearman": 0.7298909769073219,
      "eval_validation_steps_per_second": 7.109,
      "step": 1815
    },
    {
      "epoch": 11.995043370508055,
      "eval_test_error_bar": 0.03518510898140023,
      "eval_test_loss": 5.491991996765137,
      "eval_test_pearsonr": 0.6407928586520308,
      "eval_test_rmse": 2.3434998989105225,
      "eval_test_runtime": 45.7661,
      "eval_test_samples_per_second": 7.123,
      "eval_test_spearman": 0.6525513622891586,
      "eval_test_steps_per_second": 7.123,
      "step": 1815
    },
    {
      "epoch": 12.001652209830649,
      "grad_norm": 6.966808319091797,
      "learning_rate": 0.000596039603960396,
      "loss": 2.9031,
      "step": 1816
    },
    {
      "epoch": 12.008261049153242,
      "grad_norm": 9.48255443572998,
      "learning_rate": 0.0005955445544554456,
      "loss": 0.8647,
      "step": 1817
    },
    {
      "epoch": 12.014869888475836,
      "grad_norm": 7.444290637969971,
      "learning_rate": 0.000595049504950495,
      "loss": 0.6732,
      "step": 1818
    },
    {
      "epoch": 12.02147872779843,
      "grad_norm": 9.243494987487793,
      "learning_rate": 0.0005945544554455445,
      "loss": 0.9575,
      "step": 1819
    },
    {
      "epoch": 12.028087567121025,
      "grad_norm": 10.267885208129883,
      "learning_rate": 0.000594059405940594,
      "loss": 1.0232,
      "step": 1820
    },
    {
      "epoch": 12.034696406443619,
      "grad_norm": 12.739816665649414,
      "learning_rate": 0.0005935643564356436,
      "loss": 0.843,
      "step": 1821
    },
    {
      "epoch": 12.041305245766212,
      "grad_norm": 31.317169189453125,
      "learning_rate": 0.0005930693069306931,
      "loss": 1.4052,
      "step": 1822
    },
    {
      "epoch": 12.047914085088806,
      "grad_norm": 14.885051727294922,
      "learning_rate": 0.0005925742574257425,
      "loss": 1.3135,
      "step": 1823
    },
    {
      "epoch": 12.0545229244114,
      "grad_norm": 32.37578201293945,
      "learning_rate": 0.000592079207920792,
      "loss": 2.0638,
      "step": 1824
    },
    {
      "epoch": 12.061131763733995,
      "grad_norm": 1.8173203468322754,
      "learning_rate": 0.0005915841584158416,
      "loss": 1.5943,
      "step": 1825
    },
    {
      "epoch": 12.067740603056588,
      "grad_norm": 5.703922748565674,
      "learning_rate": 0.0005910891089108911,
      "loss": 1.2352,
      "step": 1826
    },
    {
      "epoch": 12.074349442379182,
      "grad_norm": 4.628486633300781,
      "learning_rate": 0.0005905940594059406,
      "loss": 0.9511,
      "step": 1827
    },
    {
      "epoch": 12.080958281701776,
      "grad_norm": 2.5902891159057617,
      "learning_rate": 0.00059009900990099,
      "loss": 0.6581,
      "step": 1828
    },
    {
      "epoch": 12.087567121024371,
      "grad_norm": 3.466156244277954,
      "learning_rate": 0.0005896039603960396,
      "loss": 2.0852,
      "step": 1829
    },
    {
      "epoch": 12.094175960346965,
      "grad_norm": 28.523395538330078,
      "learning_rate": 0.0005891089108910891,
      "loss": 2.3556,
      "step": 1830
    },
    {
      "epoch": 12.100784799669558,
      "grad_norm": 24.617746353149414,
      "learning_rate": 0.0005886138613861386,
      "loss": 1.8626,
      "step": 1831
    },
    {
      "epoch": 12.107393638992152,
      "grad_norm": 14.104182243347168,
      "learning_rate": 0.0005881188118811881,
      "loss": 2.9125,
      "step": 1832
    },
    {
      "epoch": 12.114002478314745,
      "grad_norm": 4.467866897583008,
      "learning_rate": 0.0005876237623762376,
      "loss": 0.7261,
      "step": 1833
    },
    {
      "epoch": 12.12061131763734,
      "grad_norm": 17.95000457763672,
      "learning_rate": 0.0005871287128712871,
      "loss": 1.3451,
      "step": 1834
    },
    {
      "epoch": 12.127220156959934,
      "grad_norm": 26.592939376831055,
      "learning_rate": 0.0005866336633663366,
      "loss": 1.7961,
      "step": 1835
    },
    {
      "epoch": 12.133828996282528,
      "grad_norm": 34.0721321105957,
      "learning_rate": 0.0005861386138613861,
      "loss": 2.6295,
      "step": 1836
    },
    {
      "epoch": 12.140437835605121,
      "grad_norm": 1.2424219846725464,
      "learning_rate": 0.0005856435643564357,
      "loss": 1.2497,
      "step": 1837
    },
    {
      "epoch": 12.147046674927715,
      "grad_norm": 4.2848219871521,
      "learning_rate": 0.0005851485148514851,
      "loss": 2.5643,
      "step": 1838
    },
    {
      "epoch": 12.15365551425031,
      "grad_norm": 4.522171497344971,
      "learning_rate": 0.0005846534653465346,
      "loss": 2.6198,
      "step": 1839
    },
    {
      "epoch": 12.160264353572904,
      "grad_norm": 11.921889305114746,
      "learning_rate": 0.0005841584158415841,
      "loss": 1.5112,
      "step": 1840
    },
    {
      "epoch": 12.166873192895498,
      "grad_norm": 12.44359016418457,
      "learning_rate": 0.0005836633663366337,
      "loss": 1.6161,
      "step": 1841
    },
    {
      "epoch": 12.173482032218091,
      "grad_norm": 15.365095138549805,
      "learning_rate": 0.0005831683168316832,
      "loss": 1.7414,
      "step": 1842
    },
    {
      "epoch": 12.180090871540685,
      "grad_norm": 9.134673118591309,
      "learning_rate": 0.0005826732673267326,
      "loss": 3.0854,
      "step": 1843
    },
    {
      "epoch": 12.18669971086328,
      "grad_norm": 9.09382438659668,
      "learning_rate": 0.0005821782178217821,
      "loss": 0.8141,
      "step": 1844
    },
    {
      "epoch": 12.193308550185874,
      "grad_norm": 15.625860214233398,
      "learning_rate": 0.0005816831683168317,
      "loss": 2.4015,
      "step": 1845
    },
    {
      "epoch": 12.199917389508467,
      "grad_norm": 13.8468599319458,
      "learning_rate": 0.0005811881188118812,
      "loss": 2.1894,
      "step": 1846
    },
    {
      "epoch": 12.206526228831061,
      "grad_norm": 14.353958129882812,
      "learning_rate": 0.0005806930693069307,
      "loss": 1.8654,
      "step": 1847
    },
    {
      "epoch": 12.213135068153656,
      "grad_norm": 7.473824977874756,
      "learning_rate": 0.0005801980198019801,
      "loss": 0.934,
      "step": 1848
    },
    {
      "epoch": 12.21974390747625,
      "grad_norm": 1.7590718269348145,
      "learning_rate": 0.0005797029702970297,
      "loss": 2.4851,
      "step": 1849
    },
    {
      "epoch": 12.226352746798844,
      "grad_norm": 3.064727544784546,
      "learning_rate": 0.0005792079207920792,
      "loss": 1.4247,
      "step": 1850
    },
    {
      "epoch": 12.232961586121437,
      "grad_norm": 10.96498966217041,
      "learning_rate": 0.0005787128712871287,
      "loss": 0.5451,
      "step": 1851
    },
    {
      "epoch": 12.23957042544403,
      "grad_norm": 13.908866882324219,
      "learning_rate": 0.0005782178217821782,
      "loss": 0.9897,
      "step": 1852
    },
    {
      "epoch": 12.246179264766626,
      "grad_norm": 2.147862672805786,
      "learning_rate": 0.0005777227722772277,
      "loss": 2.5417,
      "step": 1853
    },
    {
      "epoch": 12.25278810408922,
      "grad_norm": 1.9739413261413574,
      "learning_rate": 0.0005772277227722772,
      "loss": 2.7302,
      "step": 1854
    },
    {
      "epoch": 12.259396943411813,
      "grad_norm": 2.3657631874084473,
      "learning_rate": 0.0005767326732673267,
      "loss": 1.4395,
      "step": 1855
    },
    {
      "epoch": 12.266005782734407,
      "grad_norm": 2.1370983123779297,
      "learning_rate": 0.0005762376237623762,
      "loss": 0.6824,
      "step": 1856
    },
    {
      "epoch": 12.272614622057,
      "grad_norm": 14.130247116088867,
      "learning_rate": 0.0005757425742574258,
      "loss": 1.208,
      "step": 1857
    },
    {
      "epoch": 12.279223461379596,
      "grad_norm": 11.319354057312012,
      "learning_rate": 0.0005752475247524752,
      "loss": 1.8786,
      "step": 1858
    },
    {
      "epoch": 12.28583230070219,
      "grad_norm": 3.3523151874542236,
      "learning_rate": 0.0005747524752475247,
      "loss": 0.9318,
      "step": 1859
    },
    {
      "epoch": 12.292441140024783,
      "grad_norm": 10.215815544128418,
      "learning_rate": 0.0005742574257425742,
      "loss": 0.8146,
      "step": 1860
    },
    {
      "epoch": 12.299049979347377,
      "grad_norm": 13.635552406311035,
      "learning_rate": 0.0005737623762376238,
      "loss": 2.4846,
      "step": 1861
    },
    {
      "epoch": 12.30565881866997,
      "grad_norm": 8.47772216796875,
      "learning_rate": 0.0005732673267326733,
      "loss": 1.2951,
      "step": 1862
    },
    {
      "epoch": 12.312267657992566,
      "grad_norm": 13.788554191589355,
      "learning_rate": 0.0005727722772277227,
      "loss": 2.4023,
      "step": 1863
    },
    {
      "epoch": 12.31887649731516,
      "grad_norm": 4.679986953735352,
      "learning_rate": 0.0005722772277227722,
      "loss": 3.3438,
      "step": 1864
    },
    {
      "epoch": 12.325485336637753,
      "grad_norm": 2.4103620052337646,
      "learning_rate": 0.0005717821782178217,
      "loss": 2.3419,
      "step": 1865
    },
    {
      "epoch": 12.332094175960346,
      "grad_norm": 17.311424255371094,
      "learning_rate": 0.0005712871287128713,
      "loss": 1.916,
      "step": 1866
    },
    {
      "epoch": 12.338703015282942,
      "grad_norm": 2.93611216545105,
      "learning_rate": 0.0005707920792079208,
      "loss": 2.4907,
      "step": 1867
    },
    {
      "epoch": 12.345311854605535,
      "grad_norm": 5.072943687438965,
      "learning_rate": 0.0005702970297029702,
      "loss": 1.2188,
      "step": 1868
    },
    {
      "epoch": 12.351920693928129,
      "grad_norm": 1.4921469688415527,
      "learning_rate": 0.0005698019801980197,
      "loss": 1.2009,
      "step": 1869
    },
    {
      "epoch": 12.358529533250723,
      "grad_norm": 2.7622342109680176,
      "learning_rate": 0.0005693069306930693,
      "loss": 0.7511,
      "step": 1870
    },
    {
      "epoch": 12.365138372573316,
      "grad_norm": 5.0944695472717285,
      "learning_rate": 0.0005688118811881188,
      "loss": 2.3342,
      "step": 1871
    },
    {
      "epoch": 12.371747211895912,
      "grad_norm": 18.564577102661133,
      "learning_rate": 0.0005683168316831683,
      "loss": 2.5155,
      "step": 1872
    },
    {
      "epoch": 12.378356051218505,
      "grad_norm": 48.80722427368164,
      "learning_rate": 0.0005678217821782177,
      "loss": 3.6297,
      "step": 1873
    },
    {
      "epoch": 12.384964890541099,
      "grad_norm": 56.50494384765625,
      "learning_rate": 0.0005673267326732673,
      "loss": 5.5955,
      "step": 1874
    },
    {
      "epoch": 12.391573729863692,
      "grad_norm": 51.295196533203125,
      "learning_rate": 0.0005668316831683168,
      "loss": 2.9656,
      "step": 1875
    },
    {
      "epoch": 12.398182569186286,
      "grad_norm": 37.680023193359375,
      "learning_rate": 0.0005663366336633663,
      "loss": 3.1326,
      "step": 1876
    },
    {
      "epoch": 12.404791408508881,
      "grad_norm": 24.336851119995117,
      "learning_rate": 0.0005658415841584158,
      "loss": 2.698,
      "step": 1877
    },
    {
      "epoch": 12.411400247831475,
      "grad_norm": 13.432190895080566,
      "learning_rate": 0.0005653465346534653,
      "loss": 2.4718,
      "step": 1878
    },
    {
      "epoch": 12.418009087154068,
      "grad_norm": 8.641135215759277,
      "learning_rate": 0.0005648514851485148,
      "loss": 2.8379,
      "step": 1879
    },
    {
      "epoch": 12.424617926476662,
      "grad_norm": 29.92340850830078,
      "learning_rate": 0.0005643564356435643,
      "loss": 1.8152,
      "step": 1880
    },
    {
      "epoch": 12.431226765799256,
      "grad_norm": 22.019742965698242,
      "learning_rate": 0.0005638613861386138,
      "loss": 1.3959,
      "step": 1881
    },
    {
      "epoch": 12.437835605121851,
      "grad_norm": 37.94477844238281,
      "learning_rate": 0.0005633663366336634,
      "loss": 5.7194,
      "step": 1882
    },
    {
      "epoch": 12.444444444444445,
      "grad_norm": 10.607845306396484,
      "learning_rate": 0.0005628712871287128,
      "loss": 2.797,
      "step": 1883
    },
    {
      "epoch": 12.451053283767038,
      "grad_norm": 8.671988487243652,
      "learning_rate": 0.0005623762376237624,
      "loss": 0.9455,
      "step": 1884
    },
    {
      "epoch": 12.457662123089632,
      "grad_norm": 9.779193878173828,
      "learning_rate": 0.000561881188118812,
      "loss": 0.9286,
      "step": 1885
    },
    {
      "epoch": 12.464270962412227,
      "grad_norm": 7.471151351928711,
      "learning_rate": 0.0005613861386138615,
      "loss": 1.1019,
      "step": 1886
    },
    {
      "epoch": 12.47087980173482,
      "grad_norm": 16.05757713317871,
      "learning_rate": 0.000560891089108911,
      "loss": 1.0301,
      "step": 1887
    },
    {
      "epoch": 12.477488641057414,
      "grad_norm": 3.442864179611206,
      "learning_rate": 0.0005603960396039604,
      "loss": 1.2281,
      "step": 1888
    },
    {
      "epoch": 12.484097480380008,
      "grad_norm": 6.999918460845947,
      "learning_rate": 0.0005599009900990099,
      "loss": 2.1582,
      "step": 1889
    },
    {
      "epoch": 12.490706319702602,
      "grad_norm": 4.276644706726074,
      "learning_rate": 0.0005594059405940595,
      "loss": 1.754,
      "step": 1890
    },
    {
      "epoch": 12.497315159025197,
      "grad_norm": 14.188165664672852,
      "learning_rate": 0.000558910891089109,
      "loss": 0.8786,
      "step": 1891
    },
    {
      "epoch": 12.50392399834779,
      "grad_norm": 11.541078567504883,
      "learning_rate": 0.0005584158415841585,
      "loss": 0.9196,
      "step": 1892
    },
    {
      "epoch": 12.510532837670384,
      "grad_norm": 15.624798774719238,
      "learning_rate": 0.0005579207920792079,
      "loss": 1.2793,
      "step": 1893
    },
    {
      "epoch": 12.517141676992978,
      "grad_norm": 4.83634090423584,
      "learning_rate": 0.0005574257425742575,
      "loss": 2.3542,
      "step": 1894
    },
    {
      "epoch": 12.523750516315571,
      "grad_norm": 9.328936576843262,
      "learning_rate": 0.000556930693069307,
      "loss": 1.0495,
      "step": 1895
    },
    {
      "epoch": 12.530359355638167,
      "grad_norm": 24.711421966552734,
      "learning_rate": 0.0005564356435643565,
      "loss": 2.5737,
      "step": 1896
    },
    {
      "epoch": 12.53696819496076,
      "grad_norm": 20.07008171081543,
      "learning_rate": 0.000555940594059406,
      "loss": 2.9759,
      "step": 1897
    },
    {
      "epoch": 12.543577034283354,
      "grad_norm": 9.105175971984863,
      "learning_rate": 0.0005554455445544555,
      "loss": 2.1983,
      "step": 1898
    },
    {
      "epoch": 12.550185873605948,
      "grad_norm": 19.41267204284668,
      "learning_rate": 0.000554950495049505,
      "loss": 3.1399,
      "step": 1899
    },
    {
      "epoch": 12.556794712928541,
      "grad_norm": 10.903018951416016,
      "learning_rate": 0.0005544554455445545,
      "loss": 1.5708,
      "step": 1900
    },
    {
      "epoch": 12.563403552251136,
      "grad_norm": 12.02286148071289,
      "learning_rate": 0.000553960396039604,
      "loss": 1.7734,
      "step": 1901
    },
    {
      "epoch": 12.57001239157373,
      "grad_norm": 16.17494010925293,
      "learning_rate": 0.0005534653465346536,
      "loss": 5.0934,
      "step": 1902
    },
    {
      "epoch": 12.576621230896324,
      "grad_norm": 41.37189483642578,
      "learning_rate": 0.000552970297029703,
      "loss": 3.1671,
      "step": 1903
    },
    {
      "epoch": 12.583230070218917,
      "grad_norm": 1.8262578248977661,
      "learning_rate": 0.0005524752475247525,
      "loss": 1.5498,
      "step": 1904
    },
    {
      "epoch": 12.589838909541513,
      "grad_norm": 17.96316146850586,
      "learning_rate": 0.000551980198019802,
      "loss": 1.315,
      "step": 1905
    },
    {
      "epoch": 12.596447748864106,
      "grad_norm": 8.477090835571289,
      "learning_rate": 0.0005514851485148516,
      "loss": 1.4453,
      "step": 1906
    },
    {
      "epoch": 12.6030565881867,
      "grad_norm": 14.169769287109375,
      "learning_rate": 0.0005509900990099011,
      "loss": 1.6526,
      "step": 1907
    },
    {
      "epoch": 12.609665427509293,
      "grad_norm": 10.153192520141602,
      "learning_rate": 0.0005504950495049505,
      "loss": 2.7234,
      "step": 1908
    },
    {
      "epoch": 12.616274266831887,
      "grad_norm": 22.776897430419922,
      "learning_rate": 0.00055,
      "loss": 1.8126,
      "step": 1909
    },
    {
      "epoch": 12.622883106154482,
      "grad_norm": 10.657743453979492,
      "learning_rate": 0.0005495049504950496,
      "loss": 1.2103,
      "step": 1910
    },
    {
      "epoch": 12.629491945477076,
      "grad_norm": 7.6501240730285645,
      "learning_rate": 0.0005490099009900991,
      "loss": 1.6433,
      "step": 1911
    },
    {
      "epoch": 12.63610078479967,
      "grad_norm": 10.921784400939941,
      "learning_rate": 0.0005485148514851486,
      "loss": 3.2927,
      "step": 1912
    },
    {
      "epoch": 12.642709624122263,
      "grad_norm": 12.399557113647461,
      "learning_rate": 0.000548019801980198,
      "loss": 1.9638,
      "step": 1913
    },
    {
      "epoch": 12.649318463444857,
      "grad_norm": 2.967864513397217,
      "learning_rate": 0.0005475247524752476,
      "loss": 0.853,
      "step": 1914
    },
    {
      "epoch": 12.655927302767452,
      "grad_norm": 9.519655227661133,
      "learning_rate": 0.0005470297029702971,
      "loss": 1.8589,
      "step": 1915
    },
    {
      "epoch": 12.662536142090046,
      "grad_norm": 23.00409698486328,
      "learning_rate": 0.0005465346534653466,
      "loss": 2.3983,
      "step": 1916
    },
    {
      "epoch": 12.66914498141264,
      "grad_norm": 15.961538314819336,
      "learning_rate": 0.0005460396039603961,
      "loss": 1.3639,
      "step": 1917
    },
    {
      "epoch": 12.675753820735233,
      "grad_norm": 14.348014831542969,
      "learning_rate": 0.0005455445544554456,
      "loss": 1.6441,
      "step": 1918
    },
    {
      "epoch": 12.682362660057827,
      "grad_norm": 3.0214931964874268,
      "learning_rate": 0.0005450495049504951,
      "loss": 0.5222,
      "step": 1919
    },
    {
      "epoch": 12.688971499380422,
      "grad_norm": 20.80198860168457,
      "learning_rate": 0.0005445544554455446,
      "loss": 0.9436,
      "step": 1920
    },
    {
      "epoch": 12.695580338703015,
      "grad_norm": 3.4978647232055664,
      "learning_rate": 0.0005440594059405941,
      "loss": 0.6263,
      "step": 1921
    },
    {
      "epoch": 12.702189178025609,
      "grad_norm": 22.63205909729004,
      "learning_rate": 0.0005435643564356437,
      "loss": 1.8313,
      "step": 1922
    },
    {
      "epoch": 12.708798017348203,
      "grad_norm": 6.474946975708008,
      "learning_rate": 0.0005430693069306931,
      "loss": 1.6095,
      "step": 1923
    },
    {
      "epoch": 12.715406856670796,
      "grad_norm": 2.4754140377044678,
      "learning_rate": 0.0005425742574257426,
      "loss": 1.2807,
      "step": 1924
    },
    {
      "epoch": 12.722015695993392,
      "grad_norm": 9.581216812133789,
      "learning_rate": 0.0005420792079207921,
      "loss": 1.0163,
      "step": 1925
    },
    {
      "epoch": 12.728624535315985,
      "grad_norm": 14.308342933654785,
      "learning_rate": 0.0005415841584158417,
      "loss": 1.4511,
      "step": 1926
    },
    {
      "epoch": 12.735233374638579,
      "grad_norm": 6.983640670776367,
      "learning_rate": 0.0005410891089108912,
      "loss": 0.9918,
      "step": 1927
    },
    {
      "epoch": 12.741842213961172,
      "grad_norm": 2.5991601943969727,
      "learning_rate": 0.0005405940594059406,
      "loss": 2.82,
      "step": 1928
    },
    {
      "epoch": 12.748451053283768,
      "grad_norm": 4.992497444152832,
      "learning_rate": 0.0005400990099009901,
      "loss": 3.4482,
      "step": 1929
    },
    {
      "epoch": 12.755059892606361,
      "grad_norm": 11.51600170135498,
      "learning_rate": 0.0005396039603960396,
      "loss": 2.9183,
      "step": 1930
    },
    {
      "epoch": 12.761668731928955,
      "grad_norm": 20.67010498046875,
      "learning_rate": 0.0005391089108910892,
      "loss": 1.606,
      "step": 1931
    },
    {
      "epoch": 12.768277571251549,
      "grad_norm": 31.60459327697754,
      "learning_rate": 0.0005386138613861387,
      "loss": 3.2807,
      "step": 1932
    },
    {
      "epoch": 12.774886410574142,
      "grad_norm": 32.89438247680664,
      "learning_rate": 0.0005381188118811881,
      "loss": 2.7327,
      "step": 1933
    },
    {
      "epoch": 12.781495249896738,
      "grad_norm": 20.015626907348633,
      "learning_rate": 0.0005376237623762376,
      "loss": 2.1094,
      "step": 1934
    },
    {
      "epoch": 12.788104089219331,
      "grad_norm": 7.085136413574219,
      "learning_rate": 0.0005371287128712872,
      "loss": 1.3218,
      "step": 1935
    },
    {
      "epoch": 12.794712928541925,
      "grad_norm": 2.114845037460327,
      "learning_rate": 0.0005366336633663367,
      "loss": 0.7399,
      "step": 1936
    },
    {
      "epoch": 12.801321767864518,
      "grad_norm": 13.754060745239258,
      "learning_rate": 0.0005361386138613862,
      "loss": 1.4255,
      "step": 1937
    },
    {
      "epoch": 12.807930607187112,
      "grad_norm": 10.435355186462402,
      "learning_rate": 0.0005356435643564356,
      "loss": 0.89,
      "step": 1938
    },
    {
      "epoch": 12.814539446509707,
      "grad_norm": 10.526201248168945,
      "learning_rate": 0.0005351485148514852,
      "loss": 1.1937,
      "step": 1939
    },
    {
      "epoch": 12.821148285832301,
      "grad_norm": 5.75796365737915,
      "learning_rate": 0.0005346534653465347,
      "loss": 1.8511,
      "step": 1940
    },
    {
      "epoch": 12.827757125154895,
      "grad_norm": 4.548215389251709,
      "learning_rate": 0.0005341584158415842,
      "loss": 2.6322,
      "step": 1941
    },
    {
      "epoch": 12.834365964477488,
      "grad_norm": 5.785668849945068,
      "learning_rate": 0.0005336633663366337,
      "loss": 0.5792,
      "step": 1942
    },
    {
      "epoch": 12.840974803800083,
      "grad_norm": 22.195772171020508,
      "learning_rate": 0.0005331683168316832,
      "loss": 1.7354,
      "step": 1943
    },
    {
      "epoch": 12.847583643122677,
      "grad_norm": 18.747591018676758,
      "learning_rate": 0.0005326732673267327,
      "loss": 1.1839,
      "step": 1944
    },
    {
      "epoch": 12.85419248244527,
      "grad_norm": 16.390823364257812,
      "learning_rate": 0.0005321782178217822,
      "loss": 1.0611,
      "step": 1945
    },
    {
      "epoch": 12.860801321767864,
      "grad_norm": 11.667104721069336,
      "learning_rate": 0.0005316831683168317,
      "loss": 1.8474,
      "step": 1946
    },
    {
      "epoch": 12.867410161090458,
      "grad_norm": 8.061994552612305,
      "learning_rate": 0.0005311881188118813,
      "loss": 3.7787,
      "step": 1947
    },
    {
      "epoch": 12.874019000413053,
      "grad_norm": 25.03322410583496,
      "learning_rate": 0.0005306930693069307,
      "loss": 1.9363,
      "step": 1948
    },
    {
      "epoch": 12.880627839735647,
      "grad_norm": 4.010353088378906,
      "learning_rate": 0.0005301980198019802,
      "loss": 1.2762,
      "step": 1949
    },
    {
      "epoch": 12.88723667905824,
      "grad_norm": 7.675115585327148,
      "learning_rate": 0.0005297029702970297,
      "loss": 2.0523,
      "step": 1950
    },
    {
      "epoch": 12.893845518380834,
      "grad_norm": 15.40196704864502,
      "learning_rate": 0.0005292079207920793,
      "loss": 1.2421,
      "step": 1951
    },
    {
      "epoch": 12.900454357703428,
      "grad_norm": 2.2951297760009766,
      "learning_rate": 0.0005287128712871288,
      "loss": 2.9381,
      "step": 1952
    },
    {
      "epoch": 12.907063197026023,
      "grad_norm": 10.326678276062012,
      "learning_rate": 0.0005282178217821782,
      "loss": 0.9985,
      "step": 1953
    },
    {
      "epoch": 12.913672036348617,
      "grad_norm": 5.483420372009277,
      "learning_rate": 0.0005277227722772277,
      "loss": 0.9797,
      "step": 1954
    },
    {
      "epoch": 12.92028087567121,
      "grad_norm": 5.429724216461182,
      "learning_rate": 0.0005272277227722773,
      "loss": 0.4182,
      "step": 1955
    },
    {
      "epoch": 12.926889714993804,
      "grad_norm": 18.74530792236328,
      "learning_rate": 0.0005267326732673268,
      "loss": 1.8352,
      "step": 1956
    },
    {
      "epoch": 12.933498554316397,
      "grad_norm": 30.131589889526367,
      "learning_rate": 0.0005262376237623763,
      "loss": 2.1133,
      "step": 1957
    },
    {
      "epoch": 12.940107393638993,
      "grad_norm": 1.7433677911758423,
      "learning_rate": 0.0005257425742574257,
      "loss": 1.0793,
      "step": 1958
    },
    {
      "epoch": 12.946716232961586,
      "grad_norm": 4.187807559967041,
      "learning_rate": 0.0005252475247524753,
      "loss": 0.6851,
      "step": 1959
    },
    {
      "epoch": 12.95332507228418,
      "grad_norm": 1.1165859699249268,
      "learning_rate": 0.0005247524752475248,
      "loss": 0.394,
      "step": 1960
    },
    {
      "epoch": 12.959933911606774,
      "grad_norm": 7.272568702697754,
      "learning_rate": 0.0005242574257425743,
      "loss": 0.7684,
      "step": 1961
    },
    {
      "epoch": 12.966542750929367,
      "grad_norm": 7.302259922027588,
      "learning_rate": 0.0005237623762376238,
      "loss": 0.8068,
      "step": 1962
    },
    {
      "epoch": 12.973151590251963,
      "grad_norm": 9.887219429016113,
      "learning_rate": 0.0005232673267326733,
      "loss": 3.7814,
      "step": 1963
    },
    {
      "epoch": 12.979760429574556,
      "grad_norm": 8.249428749084473,
      "learning_rate": 0.0005227722772277228,
      "loss": 0.8899,
      "step": 1964
    },
    {
      "epoch": 12.98636926889715,
      "grad_norm": 7.867940425872803,
      "learning_rate": 0.0005222772277227723,
      "loss": 1.2438,
      "step": 1965
    },
    {
      "epoch": 12.992978108219743,
      "grad_norm": 15.199664115905762,
      "learning_rate": 0.0005217821782178218,
      "loss": 3.9803,
      "step": 1966
    },
    {
      "epoch": 12.999586947542339,
      "grad_norm": 11.089152336120605,
      "learning_rate": 0.0005212871287128714,
      "loss": 0.9943,
      "step": 1967
    },
    {
      "epoch": 12.999586947542339,
      "eval_validation_error_bar": 0.03638495365784554,
      "eval_validation_loss": 5.184619426727295,
      "eval_validation_pearsonr": 0.6824185831414322,
      "eval_validation_rmse": 2.2769758701324463,
      "eval_validation_runtime": 28.5332,
      "eval_validation_samples_per_second": 7.115,
      "eval_validation_spearman": 0.7373848802210811,
      "eval_validation_steps_per_second": 7.115,
      "step": 1967
    },
    {
      "epoch": 12.999586947542339,
      "eval_test_error_bar": 0.03398468603777201,
      "eval_test_loss": 5.903283596038818,
      "eval_test_pearsonr": 0.6458595683365829,
      "eval_test_rmse": 2.4296674728393555,
      "eval_test_runtime": 45.7374,
      "eval_test_samples_per_second": 7.128,
      "eval_test_spearman": 0.6692694384172552,
      "eval_test_steps_per_second": 7.128,
      "step": 1967
    },
    {
      "epoch": 13.006195786864932,
      "grad_norm": 4.3559675216674805,
      "learning_rate": 0.0005207920792079208,
      "loss": 4.9691,
      "step": 1968
    },
    {
      "epoch": 13.012804626187526,
      "grad_norm": 12.068745613098145,
      "learning_rate": 0.0005202970297029703,
      "loss": 1.096,
      "step": 1969
    },
    {
      "epoch": 13.01941346551012,
      "grad_norm": 2.7086102962493896,
      "learning_rate": 0.0005198019801980198,
      "loss": 1.8994,
      "step": 1970
    },
    {
      "epoch": 13.026022304832713,
      "grad_norm": 1.7267873287200928,
      "learning_rate": 0.0005193069306930694,
      "loss": 0.7696,
      "step": 1971
    },
    {
      "epoch": 13.032631144155308,
      "grad_norm": 3.656582832336426,
      "learning_rate": 0.0005188118811881189,
      "loss": 1.0799,
      "step": 1972
    },
    {
      "epoch": 13.039239983477902,
      "grad_norm": 14.491729736328125,
      "learning_rate": 0.0005183168316831683,
      "loss": 2.0696,
      "step": 1973
    },
    {
      "epoch": 13.045848822800496,
      "grad_norm": 25.09470558166504,
      "learning_rate": 0.0005178217821782178,
      "loss": 4.5987,
      "step": 1974
    },
    {
      "epoch": 13.05245766212309,
      "grad_norm": 19.822904586791992,
      "learning_rate": 0.0005173267326732674,
      "loss": 1.3755,
      "step": 1975
    },
    {
      "epoch": 13.059066501445683,
      "grad_norm": 26.346881866455078,
      "learning_rate": 0.0005168316831683169,
      "loss": 2.1641,
      "step": 1976
    },
    {
      "epoch": 13.065675340768278,
      "grad_norm": 25.65656852722168,
      "learning_rate": 0.0005163366336633664,
      "loss": 1.6098,
      "step": 1977
    },
    {
      "epoch": 13.072284180090872,
      "grad_norm": 18.23579978942871,
      "learning_rate": 0.0005158415841584158,
      "loss": 1.2587,
      "step": 1978
    },
    {
      "epoch": 13.078893019413465,
      "grad_norm": 7.146745204925537,
      "learning_rate": 0.0005153465346534653,
      "loss": 0.8961,
      "step": 1979
    },
    {
      "epoch": 13.085501858736059,
      "grad_norm": 3.7399508953094482,
      "learning_rate": 0.0005148514851485149,
      "loss": 1.0453,
      "step": 1980
    },
    {
      "epoch": 13.092110698058653,
      "grad_norm": 31.19049072265625,
      "learning_rate": 0.0005143564356435644,
      "loss": 2.9736,
      "step": 1981
    },
    {
      "epoch": 13.098719537381248,
      "grad_norm": 36.87384033203125,
      "learning_rate": 0.0005138613861386139,
      "loss": 2.9761,
      "step": 1982
    },
    {
      "epoch": 13.105328376703842,
      "grad_norm": 3.9349160194396973,
      "learning_rate": 0.0005133663366336633,
      "loss": 2.6654,
      "step": 1983
    },
    {
      "epoch": 13.111937216026435,
      "grad_norm": 15.437263488769531,
      "learning_rate": 0.0005128712871287129,
      "loss": 1.5285,
      "step": 1984
    },
    {
      "epoch": 13.118546055349029,
      "grad_norm": 14.281198501586914,
      "learning_rate": 0.0005123762376237624,
      "loss": 3.1772,
      "step": 1985
    },
    {
      "epoch": 13.125154894671624,
      "grad_norm": 2.8203682899475098,
      "learning_rate": 0.0005118811881188119,
      "loss": 2.9913,
      "step": 1986
    },
    {
      "epoch": 13.131763733994218,
      "grad_norm": 1.3776817321777344,
      "learning_rate": 0.0005113861386138615,
      "loss": 1.4107,
      "step": 1987
    },
    {
      "epoch": 13.138372573316811,
      "grad_norm": 5.879432678222656,
      "learning_rate": 0.0005108910891089109,
      "loss": 1.1424,
      "step": 1988
    },
    {
      "epoch": 13.144981412639405,
      "grad_norm": 12.895209312438965,
      "learning_rate": 0.0005103960396039604,
      "loss": 0.8811,
      "step": 1989
    },
    {
      "epoch": 13.151590251961998,
      "grad_norm": 15.85107135772705,
      "learning_rate": 0.0005099009900990099,
      "loss": 2.9812,
      "step": 1990
    },
    {
      "epoch": 13.158199091284594,
      "grad_norm": 3.526110887527466,
      "learning_rate": 0.0005094059405940594,
      "loss": 2.5209,
      "step": 1991
    },
    {
      "epoch": 13.164807930607187,
      "grad_norm": 14.715782165527344,
      "learning_rate": 0.000508910891089109,
      "loss": 0.7806,
      "step": 1992
    },
    {
      "epoch": 13.171416769929781,
      "grad_norm": 2.6653079986572266,
      "learning_rate": 0.0005084158415841584,
      "loss": 0.9827,
      "step": 1993
    },
    {
      "epoch": 13.178025609252375,
      "grad_norm": 2.9512367248535156,
      "learning_rate": 0.0005079207920792079,
      "loss": 0.7282,
      "step": 1994
    },
    {
      "epoch": 13.184634448574968,
      "grad_norm": 14.02962589263916,
      "learning_rate": 0.0005074257425742574,
      "loss": 1.346,
      "step": 1995
    },
    {
      "epoch": 13.191243287897564,
      "grad_norm": 3.5674328804016113,
      "learning_rate": 0.000506930693069307,
      "loss": 1.7912,
      "step": 1996
    },
    {
      "epoch": 13.197852127220157,
      "grad_norm": 11.425383567810059,
      "learning_rate": 0.0005064356435643565,
      "loss": 0.7964,
      "step": 1997
    },
    {
      "epoch": 13.20446096654275,
      "grad_norm": 4.17103385925293,
      "learning_rate": 0.0005059405940594059,
      "loss": 2.058,
      "step": 1998
    },
    {
      "epoch": 13.211069805865344,
      "grad_norm": 20.113065719604492,
      "learning_rate": 0.0005054455445544554,
      "loss": 2.572,
      "step": 1999
    },
    {
      "epoch": 13.217678645187938,
      "grad_norm": 2.382171392440796,
      "learning_rate": 0.000504950495049505,
      "loss": 0.5831,
      "step": 2000
    },
    {
      "epoch": 13.224287484510533,
      "grad_norm": 7.889638423919678,
      "learning_rate": 0.0005044554455445545,
      "loss": 1.2595,
      "step": 2001
    },
    {
      "epoch": 13.230896323833127,
      "grad_norm": 3.812415599822998,
      "learning_rate": 0.000503960396039604,
      "loss": 1.2289,
      "step": 2002
    },
    {
      "epoch": 13.23750516315572,
      "grad_norm": 13.439577102661133,
      "learning_rate": 0.0005034653465346534,
      "loss": 1.0568,
      "step": 2003
    },
    {
      "epoch": 13.244114002478314,
      "grad_norm": 8.927070617675781,
      "learning_rate": 0.000502970297029703,
      "loss": 1.2146,
      "step": 2004
    },
    {
      "epoch": 13.25072284180091,
      "grad_norm": 13.19253921508789,
      "learning_rate": 0.0005024752475247525,
      "loss": 2.0308,
      "step": 2005
    },
    {
      "epoch": 13.257331681123503,
      "grad_norm": 8.065754890441895,
      "learning_rate": 0.000501980198019802,
      "loss": 1.377,
      "step": 2006
    },
    {
      "epoch": 13.263940520446097,
      "grad_norm": 0.999008297920227,
      "learning_rate": 0.0005014851485148515,
      "loss": 0.577,
      "step": 2007
    },
    {
      "epoch": 13.27054935976869,
      "grad_norm": 1.3440253734588623,
      "learning_rate": 0.000500990099009901,
      "loss": 1.368,
      "step": 2008
    },
    {
      "epoch": 13.277158199091284,
      "grad_norm": 17.620872497558594,
      "learning_rate": 0.0005004950495049505,
      "loss": 3.8095,
      "step": 2009
    },
    {
      "epoch": 13.28376703841388,
      "grad_norm": 28.367177963256836,
      "learning_rate": 0.0005,
      "loss": 1.8147,
      "step": 2010
    },
    {
      "epoch": 13.290375877736473,
      "grad_norm": 27.055253982543945,
      "learning_rate": 0.0004995049504950495,
      "loss": 2.1474,
      "step": 2011
    },
    {
      "epoch": 13.296984717059066,
      "grad_norm": 18.586894989013672,
      "learning_rate": 0.0004990099009900991,
      "loss": 1.0403,
      "step": 2012
    },
    {
      "epoch": 13.30359355638166,
      "grad_norm": 6.143836975097656,
      "learning_rate": 0.0004985148514851485,
      "loss": 1.1263,
      "step": 2013
    },
    {
      "epoch": 13.310202395704254,
      "grad_norm": 5.356714725494385,
      "learning_rate": 0.000498019801980198,
      "loss": 1.7355,
      "step": 2014
    },
    {
      "epoch": 13.316811235026849,
      "grad_norm": 24.65137481689453,
      "learning_rate": 0.0004975247524752475,
      "loss": 3.276,
      "step": 2015
    },
    {
      "epoch": 13.323420074349443,
      "grad_norm": 13.360881805419922,
      "learning_rate": 0.0004970297029702971,
      "loss": 1.7713,
      "step": 2016
    },
    {
      "epoch": 13.330028913672036,
      "grad_norm": 17.046710968017578,
      "learning_rate": 0.0004965346534653466,
      "loss": 1.378,
      "step": 2017
    },
    {
      "epoch": 13.33663775299463,
      "grad_norm": 6.839305877685547,
      "learning_rate": 0.000496039603960396,
      "loss": 1.3365,
      "step": 2018
    },
    {
      "epoch": 13.343246592317223,
      "grad_norm": 5.240439414978027,
      "learning_rate": 0.0004955445544554455,
      "loss": 0.7668,
      "step": 2019
    },
    {
      "epoch": 13.349855431639819,
      "grad_norm": 7.104846477508545,
      "learning_rate": 0.0004950495049504951,
      "loss": 1.4223,
      "step": 2020
    },
    {
      "epoch": 13.356464270962412,
      "grad_norm": 25.42690658569336,
      "learning_rate": 0.0004945544554455446,
      "loss": 3.2836,
      "step": 2021
    },
    {
      "epoch": 13.363073110285006,
      "grad_norm": 29.254810333251953,
      "learning_rate": 0.0004940594059405941,
      "loss": 2.3526,
      "step": 2022
    },
    {
      "epoch": 13.3696819496076,
      "grad_norm": 26.247316360473633,
      "learning_rate": 0.0004935643564356435,
      "loss": 1.5511,
      "step": 2023
    },
    {
      "epoch": 13.376290788930195,
      "grad_norm": 57.38860321044922,
      "learning_rate": 0.000493069306930693,
      "loss": 4.9717,
      "step": 2024
    },
    {
      "epoch": 13.382899628252789,
      "grad_norm": 42.58155822753906,
      "learning_rate": 0.0004925742574257426,
      "loss": 3.1966,
      "step": 2025
    },
    {
      "epoch": 13.389508467575382,
      "grad_norm": 25.165163040161133,
      "learning_rate": 0.0004920792079207921,
      "loss": 2.7973,
      "step": 2026
    },
    {
      "epoch": 13.396117306897976,
      "grad_norm": 29.3380184173584,
      "learning_rate": 0.0004915841584158416,
      "loss": 2.8701,
      "step": 2027
    },
    {
      "epoch": 13.40272614622057,
      "grad_norm": 7.015172958374023,
      "learning_rate": 0.000491089108910891,
      "loss": 0.7596,
      "step": 2028
    },
    {
      "epoch": 13.409334985543165,
      "grad_norm": 7.738642692565918,
      "learning_rate": 0.0004905940594059406,
      "loss": 1.7169,
      "step": 2029
    },
    {
      "epoch": 13.415943824865758,
      "grad_norm": 19.491619110107422,
      "learning_rate": 0.0004900990099009901,
      "loss": 1.1116,
      "step": 2030
    },
    {
      "epoch": 13.422552664188352,
      "grad_norm": 15.017683982849121,
      "learning_rate": 0.0004896039603960396,
      "loss": 1.9216,
      "step": 2031
    },
    {
      "epoch": 13.429161503510946,
      "grad_norm": 14.327396392822266,
      "learning_rate": 0.0004891089108910892,
      "loss": 1.7345,
      "step": 2032
    },
    {
      "epoch": 13.435770342833539,
      "grad_norm": 22.073162078857422,
      "learning_rate": 0.0004886138613861386,
      "loss": 2.6245,
      "step": 2033
    },
    {
      "epoch": 13.442379182156134,
      "grad_norm": 19.955154418945312,
      "learning_rate": 0.0004881188118811881,
      "loss": 2.7899,
      "step": 2034
    },
    {
      "epoch": 13.448988021478728,
      "grad_norm": 4.494882106781006,
      "learning_rate": 0.0004876237623762376,
      "loss": 1.3409,
      "step": 2035
    },
    {
      "epoch": 13.455596860801322,
      "grad_norm": 6.957244396209717,
      "learning_rate": 0.00048712871287128715,
      "loss": 0.6539,
      "step": 2036
    },
    {
      "epoch": 13.462205700123915,
      "grad_norm": 4.743139743804932,
      "learning_rate": 0.0004866336633663366,
      "loss": 1.5176,
      "step": 2037
    },
    {
      "epoch": 13.468814539446509,
      "grad_norm": 34.07493591308594,
      "learning_rate": 0.00048613861386138615,
      "loss": 2.8327,
      "step": 2038
    },
    {
      "epoch": 13.475423378769104,
      "grad_norm": 31.006412506103516,
      "learning_rate": 0.0004856435643564356,
      "loss": 1.779,
      "step": 2039
    },
    {
      "epoch": 13.482032218091698,
      "grad_norm": 37.01805114746094,
      "learning_rate": 0.00048514851485148515,
      "loss": 2.3058,
      "step": 2040
    },
    {
      "epoch": 13.488641057414291,
      "grad_norm": 25.184688568115234,
      "learning_rate": 0.00048465346534653467,
      "loss": 1.3053,
      "step": 2041
    },
    {
      "epoch": 13.495249896736885,
      "grad_norm": 9.317033767700195,
      "learning_rate": 0.00048415841584158414,
      "loss": 0.8044,
      "step": 2042
    },
    {
      "epoch": 13.501858736059479,
      "grad_norm": 7.4688401222229,
      "learning_rate": 0.00048366336633663367,
      "loss": 1.5694,
      "step": 2043
    },
    {
      "epoch": 13.508467575382074,
      "grad_norm": 6.14442253112793,
      "learning_rate": 0.00048316831683168314,
      "loss": 1.2053,
      "step": 2044
    },
    {
      "epoch": 13.515076414704668,
      "grad_norm": 22.00837516784668,
      "learning_rate": 0.00048267326732673267,
      "loss": 2.2917,
      "step": 2045
    },
    {
      "epoch": 13.521685254027261,
      "grad_norm": 9.830109596252441,
      "learning_rate": 0.0004821782178217822,
      "loss": 2.2698,
      "step": 2046
    },
    {
      "epoch": 13.528294093349855,
      "grad_norm": 2.94199275970459,
      "learning_rate": 0.00048168316831683167,
      "loss": 0.9969,
      "step": 2047
    },
    {
      "epoch": 13.53490293267245,
      "grad_norm": 11.480566024780273,
      "learning_rate": 0.0004811881188118812,
      "loss": 1.5577,
      "step": 2048
    },
    {
      "epoch": 13.541511771995044,
      "grad_norm": 4.203981876373291,
      "learning_rate": 0.00048069306930693066,
      "loss": 1.4612,
      "step": 2049
    },
    {
      "epoch": 13.548120611317637,
      "grad_norm": 15.163990020751953,
      "learning_rate": 0.0004801980198019802,
      "loss": 1.3869,
      "step": 2050
    },
    {
      "epoch": 13.554729450640231,
      "grad_norm": 9.290759086608887,
      "learning_rate": 0.0004797029702970297,
      "loss": 1.4978,
      "step": 2051
    },
    {
      "epoch": 13.561338289962825,
      "grad_norm": 8.49170207977295,
      "learning_rate": 0.0004792079207920792,
      "loss": 1.0105,
      "step": 2052
    },
    {
      "epoch": 13.56794712928542,
      "grad_norm": 8.24251937866211,
      "learning_rate": 0.0004787128712871287,
      "loss": 0.7426,
      "step": 2053
    },
    {
      "epoch": 13.574555968608014,
      "grad_norm": 2.247182846069336,
      "learning_rate": 0.0004782178217821782,
      "loss": 1.1048,
      "step": 2054
    },
    {
      "epoch": 13.581164807930607,
      "grad_norm": 7.834209442138672,
      "learning_rate": 0.0004777227722772277,
      "loss": 1.0156,
      "step": 2055
    },
    {
      "epoch": 13.5877736472532,
      "grad_norm": 10.35494327545166,
      "learning_rate": 0.00047722772277227724,
      "loss": 3.0774,
      "step": 2056
    },
    {
      "epoch": 13.594382486575796,
      "grad_norm": 17.641986846923828,
      "learning_rate": 0.0004767326732673267,
      "loss": 1.1566,
      "step": 2057
    },
    {
      "epoch": 13.60099132589839,
      "grad_norm": 14.102741241455078,
      "learning_rate": 0.00047623762376237624,
      "loss": 1.9386,
      "step": 2058
    },
    {
      "epoch": 13.607600165220983,
      "grad_norm": 21.78459358215332,
      "learning_rate": 0.0004757425742574257,
      "loss": 1.5296,
      "step": 2059
    },
    {
      "epoch": 13.614209004543577,
      "grad_norm": 16.980207443237305,
      "learning_rate": 0.00047524752475247524,
      "loss": 0.9435,
      "step": 2060
    },
    {
      "epoch": 13.62081784386617,
      "grad_norm": 27.222379684448242,
      "learning_rate": 0.00047475247524752476,
      "loss": 3.7335,
      "step": 2061
    },
    {
      "epoch": 13.627426683188766,
      "grad_norm": 23.94563102722168,
      "learning_rate": 0.00047425742574257423,
      "loss": 1.5569,
      "step": 2062
    },
    {
      "epoch": 13.63403552251136,
      "grad_norm": 13.429788589477539,
      "learning_rate": 0.00047376237623762376,
      "loss": 0.7922,
      "step": 2063
    },
    {
      "epoch": 13.640644361833953,
      "grad_norm": 16.329240798950195,
      "learning_rate": 0.00047326732673267323,
      "loss": 2.8494,
      "step": 2064
    },
    {
      "epoch": 13.647253201156547,
      "grad_norm": 20.629901885986328,
      "learning_rate": 0.00047277227722772276,
      "loss": 4.9925,
      "step": 2065
    },
    {
      "epoch": 13.65386204047914,
      "grad_norm": 7.122309684753418,
      "learning_rate": 0.0004722772277227723,
      "loss": 1.1852,
      "step": 2066
    },
    {
      "epoch": 13.660470879801736,
      "grad_norm": 13.99242115020752,
      "learning_rate": 0.00047178217821782176,
      "loss": 1.0517,
      "step": 2067
    },
    {
      "epoch": 13.66707971912433,
      "grad_norm": 16.901508331298828,
      "learning_rate": 0.0004712871287128713,
      "loss": 0.8155,
      "step": 2068
    },
    {
      "epoch": 13.673688558446923,
      "grad_norm": 18.05730628967285,
      "learning_rate": 0.00047079207920792075,
      "loss": 2.0536,
      "step": 2069
    },
    {
      "epoch": 13.680297397769516,
      "grad_norm": 4.1821818351745605,
      "learning_rate": 0.0004702970297029703,
      "loss": 2.3581,
      "step": 2070
    },
    {
      "epoch": 13.68690623709211,
      "grad_norm": 10.546517372131348,
      "learning_rate": 0.0004698019801980198,
      "loss": 2.8952,
      "step": 2071
    },
    {
      "epoch": 13.693515076414705,
      "grad_norm": 17.387165069580078,
      "learning_rate": 0.0004693069306930693,
      "loss": 1.3485,
      "step": 2072
    },
    {
      "epoch": 13.700123915737299,
      "grad_norm": 13.466193199157715,
      "learning_rate": 0.0004688118811881188,
      "loss": 0.6754,
      "step": 2073
    },
    {
      "epoch": 13.706732755059893,
      "grad_norm": 1.954131007194519,
      "learning_rate": 0.00046831683168316833,
      "loss": 1.7059,
      "step": 2074
    },
    {
      "epoch": 13.713341594382486,
      "grad_norm": 4.166245937347412,
      "learning_rate": 0.00046782178217821786,
      "loss": 1.4768,
      "step": 2075
    },
    {
      "epoch": 13.71995043370508,
      "grad_norm": 2.702280282974243,
      "learning_rate": 0.0004673267326732674,
      "loss": 1.0989,
      "step": 2076
    },
    {
      "epoch": 13.726559273027675,
      "grad_norm": 9.978666305541992,
      "learning_rate": 0.00046683168316831686,
      "loss": 0.9754,
      "step": 2077
    },
    {
      "epoch": 13.733168112350269,
      "grad_norm": 14.75952434539795,
      "learning_rate": 0.0004663366336633664,
      "loss": 2.9863,
      "step": 2078
    },
    {
      "epoch": 13.739776951672862,
      "grad_norm": 11.912568092346191,
      "learning_rate": 0.00046584158415841585,
      "loss": 1.2336,
      "step": 2079
    },
    {
      "epoch": 13.746385790995456,
      "grad_norm": 37.77455139160156,
      "learning_rate": 0.0004653465346534654,
      "loss": 3.1894,
      "step": 2080
    },
    {
      "epoch": 13.75299463031805,
      "grad_norm": 16.67315673828125,
      "learning_rate": 0.0004648514851485149,
      "loss": 0.8332,
      "step": 2081
    },
    {
      "epoch": 13.759603469640645,
      "grad_norm": 17.407787322998047,
      "learning_rate": 0.0004643564356435644,
      "loss": 3.6705,
      "step": 2082
    },
    {
      "epoch": 13.766212308963238,
      "grad_norm": 7.260221481323242,
      "learning_rate": 0.0004638613861386139,
      "loss": 0.9319,
      "step": 2083
    },
    {
      "epoch": 13.772821148285832,
      "grad_norm": 31.254680633544922,
      "learning_rate": 0.0004633663366336634,
      "loss": 3.4339,
      "step": 2084
    },
    {
      "epoch": 13.779429987608426,
      "grad_norm": 8.203667640686035,
      "learning_rate": 0.0004628712871287129,
      "loss": 1.0932,
      "step": 2085
    },
    {
      "epoch": 13.786038826931021,
      "grad_norm": 9.864105224609375,
      "learning_rate": 0.00046237623762376243,
      "loss": 2.5993,
      "step": 2086
    },
    {
      "epoch": 13.792647666253615,
      "grad_norm": 25.606979370117188,
      "learning_rate": 0.0004618811881188119,
      "loss": 1.9605,
      "step": 2087
    },
    {
      "epoch": 13.799256505576208,
      "grad_norm": 30.60686492919922,
      "learning_rate": 0.00046138613861386143,
      "loss": 2.9204,
      "step": 2088
    },
    {
      "epoch": 13.805865344898802,
      "grad_norm": 6.347346305847168,
      "learning_rate": 0.0004608910891089109,
      "loss": 2.0311,
      "step": 2089
    },
    {
      "epoch": 13.812474184221395,
      "grad_norm": 22.901308059692383,
      "learning_rate": 0.0004603960396039604,
      "loss": 2.283,
      "step": 2090
    },
    {
      "epoch": 13.81908302354399,
      "grad_norm": 17.989511489868164,
      "learning_rate": 0.00045990099009900995,
      "loss": 1.0372,
      "step": 2091
    },
    {
      "epoch": 13.825691862866584,
      "grad_norm": 22.33806800842285,
      "learning_rate": 0.0004594059405940594,
      "loss": 1.2378,
      "step": 2092
    },
    {
      "epoch": 13.832300702189178,
      "grad_norm": 4.4926958084106445,
      "learning_rate": 0.00045891089108910895,
      "loss": 1.15,
      "step": 2093
    },
    {
      "epoch": 13.838909541511772,
      "grad_norm": 8.39628791809082,
      "learning_rate": 0.0004584158415841584,
      "loss": 0.9336,
      "step": 2094
    },
    {
      "epoch": 13.845518380834365,
      "grad_norm": 16.413013458251953,
      "learning_rate": 0.00045792079207920795,
      "loss": 1.9369,
      "step": 2095
    },
    {
      "epoch": 13.85212722015696,
      "grad_norm": 1.8876938819885254,
      "learning_rate": 0.0004574257425742575,
      "loss": 0.9907,
      "step": 2096
    },
    {
      "epoch": 13.858736059479554,
      "grad_norm": 8.255386352539062,
      "learning_rate": 0.00045693069306930695,
      "loss": 0.7977,
      "step": 2097
    },
    {
      "epoch": 13.865344898802148,
      "grad_norm": 0.9331794381141663,
      "learning_rate": 0.00045643564356435647,
      "loss": 0.6206,
      "step": 2098
    },
    {
      "epoch": 13.871953738124741,
      "grad_norm": 13.52892780303955,
      "learning_rate": 0.00045594059405940594,
      "loss": 1.6365,
      "step": 2099
    },
    {
      "epoch": 13.878562577447337,
      "grad_norm": 4.120652198791504,
      "learning_rate": 0.00045544554455445547,
      "loss": 0.7237,
      "step": 2100
    },
    {
      "epoch": 13.88517141676993,
      "grad_norm": 3.3633663654327393,
      "learning_rate": 0.000454950495049505,
      "loss": 1.9917,
      "step": 2101
    },
    {
      "epoch": 13.891780256092524,
      "grad_norm": 18.542043685913086,
      "learning_rate": 0.00045445544554455447,
      "loss": 1.6718,
      "step": 2102
    },
    {
      "epoch": 13.898389095415117,
      "grad_norm": 16.835969924926758,
      "learning_rate": 0.000453960396039604,
      "loss": 1.7118,
      "step": 2103
    },
    {
      "epoch": 13.904997934737711,
      "grad_norm": 16.280853271484375,
      "learning_rate": 0.00045346534653465347,
      "loss": 1.5869,
      "step": 2104
    },
    {
      "epoch": 13.911606774060306,
      "grad_norm": 7.653775691986084,
      "learning_rate": 0.000452970297029703,
      "loss": 0.9641,
      "step": 2105
    },
    {
      "epoch": 13.9182156133829,
      "grad_norm": 1.592343807220459,
      "learning_rate": 0.0004524752475247525,
      "loss": 0.9875,
      "step": 2106
    },
    {
      "epoch": 13.924824452705494,
      "grad_norm": 11.959986686706543,
      "learning_rate": 0.000451980198019802,
      "loss": 0.6423,
      "step": 2107
    },
    {
      "epoch": 13.931433292028087,
      "grad_norm": 1.5693528652191162,
      "learning_rate": 0.0004514851485148515,
      "loss": 1.509,
      "step": 2108
    },
    {
      "epoch": 13.93804213135068,
      "grad_norm": 2.7643439769744873,
      "learning_rate": 0.000450990099009901,
      "loss": 0.6362,
      "step": 2109
    },
    {
      "epoch": 13.944650970673276,
      "grad_norm": 20.7794132232666,
      "learning_rate": 0.0004504950495049505,
      "loss": 1.1884,
      "step": 2110
    },
    {
      "epoch": 13.95125980999587,
      "grad_norm": 20.023475646972656,
      "learning_rate": 0.00045000000000000004,
      "loss": 0.9596,
      "step": 2111
    },
    {
      "epoch": 13.957868649318463,
      "grad_norm": 15.721635818481445,
      "learning_rate": 0.0004495049504950495,
      "loss": 1.5421,
      "step": 2112
    },
    {
      "epoch": 13.964477488641057,
      "grad_norm": 2.319641590118408,
      "learning_rate": 0.00044900990099009904,
      "loss": 1.4864,
      "step": 2113
    },
    {
      "epoch": 13.97108632796365,
      "grad_norm": 9.684669494628906,
      "learning_rate": 0.0004485148514851485,
      "loss": 0.6804,
      "step": 2114
    },
    {
      "epoch": 13.977695167286246,
      "grad_norm": 5.040513038635254,
      "learning_rate": 0.00044801980198019804,
      "loss": 0.9863,
      "step": 2115
    },
    {
      "epoch": 13.98430400660884,
      "grad_norm": 20.426721572875977,
      "learning_rate": 0.00044752475247524756,
      "loss": 1.4948,
      "step": 2116
    },
    {
      "epoch": 13.990912845931433,
      "grad_norm": 25.629791259765625,
      "learning_rate": 0.00044702970297029704,
      "loss": 1.7463,
      "step": 2117
    },
    {
      "epoch": 13.997521685254027,
      "grad_norm": 20.23701286315918,
      "learning_rate": 0.00044653465346534656,
      "loss": 1.3655,
      "step": 2118
    },
    {
      "epoch": 13.997521685254027,
      "eval_validation_error_bar": 0.03676689764923726,
      "eval_validation_loss": 4.328092098236084,
      "eval_validation_pearsonr": 0.6873021559595901,
      "eval_validation_rmse": 2.080406665802002,
      "eval_validation_runtime": 28.4939,
      "eval_validation_samples_per_second": 7.124,
      "eval_validation_spearman": 0.733804332236559,
      "eval_validation_steps_per_second": 7.124,
      "step": 2118
    },
    {
      "epoch": 13.997521685254027,
      "eval_test_error_bar": 0.03179623392711289,
      "eval_test_loss": 5.056883335113525,
      "eval_test_pearsonr": 0.6796178497139148,
      "eval_test_rmse": 2.248751401901245,
      "eval_test_runtime": 45.9023,
      "eval_test_samples_per_second": 7.102,
      "eval_test_spearman": 0.6982906289528776,
      "eval_test_steps_per_second": 7.102,
      "step": 2118
    },
    {
      "epoch": 14.004130524576622,
      "grad_norm": 21.44469451904297,
      "learning_rate": 0.00044603960396039603,
      "loss": 1.8364,
      "step": 2119
    },
    {
      "epoch": 14.010739363899216,
      "grad_norm": 9.840133666992188,
      "learning_rate": 0.00044554455445544556,
      "loss": 1.9718,
      "step": 2120
    },
    {
      "epoch": 14.01734820322181,
      "grad_norm": 14.222371101379395,
      "learning_rate": 0.0004450495049504951,
      "loss": 1.0739,
      "step": 2121
    },
    {
      "epoch": 14.023957042544403,
      "grad_norm": 23.07244873046875,
      "learning_rate": 0.00044455445544554456,
      "loss": 0.9829,
      "step": 2122
    },
    {
      "epoch": 14.030565881866996,
      "grad_norm": 17.43992805480957,
      "learning_rate": 0.0004440594059405941,
      "loss": 0.731,
      "step": 2123
    },
    {
      "epoch": 14.037174721189592,
      "grad_norm": 16.52290153503418,
      "learning_rate": 0.00044356435643564356,
      "loss": 0.7447,
      "step": 2124
    },
    {
      "epoch": 14.043783560512185,
      "grad_norm": 15.108985900878906,
      "learning_rate": 0.0004430693069306931,
      "loss": 0.8106,
      "step": 2125
    },
    {
      "epoch": 14.050392399834779,
      "grad_norm": 15.092973709106445,
      "learning_rate": 0.0004425742574257426,
      "loss": 1.7277,
      "step": 2126
    },
    {
      "epoch": 14.057001239157373,
      "grad_norm": 2.47662091255188,
      "learning_rate": 0.0004420792079207921,
      "loss": 0.629,
      "step": 2127
    },
    {
      "epoch": 14.063610078479966,
      "grad_norm": 14.254166603088379,
      "learning_rate": 0.0004415841584158416,
      "loss": 0.7573,
      "step": 2128
    },
    {
      "epoch": 14.070218917802562,
      "grad_norm": 3.631631851196289,
      "learning_rate": 0.0004410891089108911,
      "loss": 4.9292,
      "step": 2129
    },
    {
      "epoch": 14.076827757125155,
      "grad_norm": 6.661168098449707,
      "learning_rate": 0.0004405940594059406,
      "loss": 0.9886,
      "step": 2130
    },
    {
      "epoch": 14.083436596447749,
      "grad_norm": 11.720724105834961,
      "learning_rate": 0.00044009900990099013,
      "loss": 2.6904,
      "step": 2131
    },
    {
      "epoch": 14.090045435770342,
      "grad_norm": 9.13514518737793,
      "learning_rate": 0.0004396039603960396,
      "loss": 1.2445,
      "step": 2132
    },
    {
      "epoch": 14.096654275092936,
      "grad_norm": 10.31227970123291,
      "learning_rate": 0.00043910891089108913,
      "loss": 1.1156,
      "step": 2133
    },
    {
      "epoch": 14.103263114415531,
      "grad_norm": 8.375572204589844,
      "learning_rate": 0.0004386138613861386,
      "loss": 0.4103,
      "step": 2134
    },
    {
      "epoch": 14.109871953738125,
      "grad_norm": 16.95706558227539,
      "learning_rate": 0.00043811881188118813,
      "loss": 2.0074,
      "step": 2135
    },
    {
      "epoch": 14.116480793060719,
      "grad_norm": 3.5090696811676025,
      "learning_rate": 0.00043762376237623765,
      "loss": 1.4014,
      "step": 2136
    },
    {
      "epoch": 14.123089632383312,
      "grad_norm": 2.2974417209625244,
      "learning_rate": 0.0004371287128712871,
      "loss": 1.151,
      "step": 2137
    },
    {
      "epoch": 14.129698471705908,
      "grad_norm": 19.10246467590332,
      "learning_rate": 0.00043663366336633665,
      "loss": 1.1375,
      "step": 2138
    },
    {
      "epoch": 14.136307311028501,
      "grad_norm": 17.313247680664062,
      "learning_rate": 0.0004361386138613861,
      "loss": 0.8901,
      "step": 2139
    },
    {
      "epoch": 14.142916150351095,
      "grad_norm": 1.9875819683074951,
      "learning_rate": 0.00043564356435643565,
      "loss": 0.4638,
      "step": 2140
    },
    {
      "epoch": 14.149524989673688,
      "grad_norm": 10.209156036376953,
      "learning_rate": 0.0004351485148514852,
      "loss": 1.0868,
      "step": 2141
    },
    {
      "epoch": 14.156133828996282,
      "grad_norm": 7.593759059906006,
      "learning_rate": 0.00043465346534653465,
      "loss": 0.9785,
      "step": 2142
    },
    {
      "epoch": 14.162742668318877,
      "grad_norm": 15.339385032653809,
      "learning_rate": 0.0004341584158415842,
      "loss": 3.0341,
      "step": 2143
    },
    {
      "epoch": 14.169351507641471,
      "grad_norm": 6.118446350097656,
      "learning_rate": 0.00043366336633663365,
      "loss": 1.5609,
      "step": 2144
    },
    {
      "epoch": 14.175960346964064,
      "grad_norm": 9.08639144897461,
      "learning_rate": 0.0004331683168316832,
      "loss": 1.3921,
      "step": 2145
    },
    {
      "epoch": 14.182569186286658,
      "grad_norm": 8.663758277893066,
      "learning_rate": 0.0004326732673267327,
      "loss": 1.3328,
      "step": 2146
    },
    {
      "epoch": 14.189178025609252,
      "grad_norm": 4.045756816864014,
      "learning_rate": 0.00043217821782178217,
      "loss": 1.3956,
      "step": 2147
    },
    {
      "epoch": 14.195786864931847,
      "grad_norm": 12.777113914489746,
      "learning_rate": 0.0004316831683168317,
      "loss": 0.8277,
      "step": 2148
    },
    {
      "epoch": 14.20239570425444,
      "grad_norm": 1.4000543355941772,
      "learning_rate": 0.00043118811881188117,
      "loss": 0.8748,
      "step": 2149
    },
    {
      "epoch": 14.209004543577034,
      "grad_norm": 6.060405731201172,
      "learning_rate": 0.0004306930693069307,
      "loss": 1.3759,
      "step": 2150
    },
    {
      "epoch": 14.215613382899628,
      "grad_norm": 3.727163314819336,
      "learning_rate": 0.0004301980198019802,
      "loss": 0.7202,
      "step": 2151
    },
    {
      "epoch": 14.222222222222221,
      "grad_norm": 12.5964937210083,
      "learning_rate": 0.0004297029702970297,
      "loss": 1.0502,
      "step": 2152
    },
    {
      "epoch": 14.228831061544817,
      "grad_norm": 6.47731351852417,
      "learning_rate": 0.0004292079207920792,
      "loss": 1.4626,
      "step": 2153
    },
    {
      "epoch": 14.23543990086741,
      "grad_norm": 14.03933334350586,
      "learning_rate": 0.0004287128712871287,
      "loss": 3.2475,
      "step": 2154
    },
    {
      "epoch": 14.242048740190004,
      "grad_norm": 4.482199668884277,
      "learning_rate": 0.0004282178217821782,
      "loss": 0.9346,
      "step": 2155
    },
    {
      "epoch": 14.248657579512598,
      "grad_norm": 16.16534423828125,
      "learning_rate": 0.00042772277227722774,
      "loss": 2.1786,
      "step": 2156
    },
    {
      "epoch": 14.255266418835191,
      "grad_norm": 16.75577163696289,
      "learning_rate": 0.0004272277227722772,
      "loss": 0.9451,
      "step": 2157
    },
    {
      "epoch": 14.261875258157787,
      "grad_norm": 7.089616775512695,
      "learning_rate": 0.00042673267326732674,
      "loss": 1.1873,
      "step": 2158
    },
    {
      "epoch": 14.26848409748038,
      "grad_norm": 11.575760841369629,
      "learning_rate": 0.0004262376237623762,
      "loss": 0.5156,
      "step": 2159
    },
    {
      "epoch": 14.275092936802974,
      "grad_norm": 11.378744125366211,
      "learning_rate": 0.00042574257425742574,
      "loss": 2.1356,
      "step": 2160
    },
    {
      "epoch": 14.281701776125567,
      "grad_norm": 20.503358840942383,
      "learning_rate": 0.00042524752475247527,
      "loss": 2.7127,
      "step": 2161
    },
    {
      "epoch": 14.288310615448163,
      "grad_norm": 5.67852783203125,
      "learning_rate": 0.00042475247524752474,
      "loss": 3.5197,
      "step": 2162
    },
    {
      "epoch": 14.294919454770756,
      "grad_norm": 1.971883773803711,
      "learning_rate": 0.00042425742574257427,
      "loss": 0.565,
      "step": 2163
    },
    {
      "epoch": 14.30152829409335,
      "grad_norm": 2.348620891571045,
      "learning_rate": 0.00042376237623762374,
      "loss": 1.9071,
      "step": 2164
    },
    {
      "epoch": 14.308137133415944,
      "grad_norm": 23.55881690979004,
      "learning_rate": 0.00042326732673267326,
      "loss": 1.7325,
      "step": 2165
    },
    {
      "epoch": 14.314745972738537,
      "grad_norm": 9.826783180236816,
      "learning_rate": 0.0004227722772277228,
      "loss": 1.1771,
      "step": 2166
    },
    {
      "epoch": 14.321354812061132,
      "grad_norm": 12.363664627075195,
      "learning_rate": 0.00042227722772277226,
      "loss": 0.7047,
      "step": 2167
    },
    {
      "epoch": 14.327963651383726,
      "grad_norm": 14.792625427246094,
      "learning_rate": 0.0004217821782178218,
      "loss": 0.6138,
      "step": 2168
    },
    {
      "epoch": 14.33457249070632,
      "grad_norm": 4.084346294403076,
      "learning_rate": 0.00042128712871287126,
      "loss": 0.9818,
      "step": 2169
    },
    {
      "epoch": 14.341181330028913,
      "grad_norm": 3.9830989837646484,
      "learning_rate": 0.0004207920792079208,
      "loss": 1.1259,
      "step": 2170
    },
    {
      "epoch": 14.347790169351507,
      "grad_norm": 3.7136285305023193,
      "learning_rate": 0.0004202970297029703,
      "loss": 1.0959,
      "step": 2171
    },
    {
      "epoch": 14.354399008674102,
      "grad_norm": 10.480086326599121,
      "learning_rate": 0.0004198019801980198,
      "loss": 1.3484,
      "step": 2172
    },
    {
      "epoch": 14.361007847996696,
      "grad_norm": 19.334014892578125,
      "learning_rate": 0.0004193069306930693,
      "loss": 1.3561,
      "step": 2173
    },
    {
      "epoch": 14.36761668731929,
      "grad_norm": 25.4866943359375,
      "learning_rate": 0.0004188118811881188,
      "loss": 1.2788,
      "step": 2174
    },
    {
      "epoch": 14.374225526641883,
      "grad_norm": 6.424367427825928,
      "learning_rate": 0.0004183168316831683,
      "loss": 1.6763,
      "step": 2175
    },
    {
      "epoch": 14.380834365964478,
      "grad_norm": 12.32412052154541,
      "learning_rate": 0.00041782178217821784,
      "loss": 3.0108,
      "step": 2176
    },
    {
      "epoch": 14.387443205287072,
      "grad_norm": 11.253999710083008,
      "learning_rate": 0.0004173267326732673,
      "loss": 2.1086,
      "step": 2177
    },
    {
      "epoch": 14.394052044609666,
      "grad_norm": 6.4742913246154785,
      "learning_rate": 0.00041683168316831683,
      "loss": 2.2678,
      "step": 2178
    },
    {
      "epoch": 14.40066088393226,
      "grad_norm": 2.276371479034424,
      "learning_rate": 0.0004163366336633663,
      "loss": 1.693,
      "step": 2179
    },
    {
      "epoch": 14.407269723254853,
      "grad_norm": 30.00308609008789,
      "learning_rate": 0.00041584158415841583,
      "loss": 1.5689,
      "step": 2180
    },
    {
      "epoch": 14.413878562577448,
      "grad_norm": 6.070219993591309,
      "learning_rate": 0.00041534653465346536,
      "loss": 1.8433,
      "step": 2181
    },
    {
      "epoch": 14.420487401900042,
      "grad_norm": 23.82154655456543,
      "learning_rate": 0.00041485148514851483,
      "loss": 2.7056,
      "step": 2182
    },
    {
      "epoch": 14.427096241222635,
      "grad_norm": 10.130586624145508,
      "learning_rate": 0.00041435643564356436,
      "loss": 0.9678,
      "step": 2183
    },
    {
      "epoch": 14.433705080545229,
      "grad_norm": 13.461957931518555,
      "learning_rate": 0.00041386138613861383,
      "loss": 1.8185,
      "step": 2184
    },
    {
      "epoch": 14.440313919867823,
      "grad_norm": 2.7042734622955322,
      "learning_rate": 0.00041336633663366335,
      "loss": 0.7355,
      "step": 2185
    },
    {
      "epoch": 14.446922759190418,
      "grad_norm": 1.2457927465438843,
      "learning_rate": 0.0004128712871287129,
      "loss": 1.0473,
      "step": 2186
    },
    {
      "epoch": 14.453531598513012,
      "grad_norm": 12.22086238861084,
      "learning_rate": 0.00041237623762376235,
      "loss": 2.0514,
      "step": 2187
    },
    {
      "epoch": 14.460140437835605,
      "grad_norm": 31.552499771118164,
      "learning_rate": 0.0004118811881188119,
      "loss": 1.6803,
      "step": 2188
    },
    {
      "epoch": 14.466749277158199,
      "grad_norm": 25.379499435424805,
      "learning_rate": 0.00041138613861386135,
      "loss": 1.2411,
      "step": 2189
    },
    {
      "epoch": 14.473358116480792,
      "grad_norm": 21.880537033081055,
      "learning_rate": 0.0004108910891089109,
      "loss": 1.2815,
      "step": 2190
    },
    {
      "epoch": 14.479966955803388,
      "grad_norm": 10.207140922546387,
      "learning_rate": 0.0004103960396039604,
      "loss": 0.9529,
      "step": 2191
    },
    {
      "epoch": 14.486575795125981,
      "grad_norm": 17.467927932739258,
      "learning_rate": 0.0004099009900990099,
      "loss": 0.9949,
      "step": 2192
    },
    {
      "epoch": 14.493184634448575,
      "grad_norm": 8.980571746826172,
      "learning_rate": 0.0004094059405940594,
      "loss": 1.0702,
      "step": 2193
    },
    {
      "epoch": 14.499793473771168,
      "grad_norm": 5.932035446166992,
      "learning_rate": 0.0004089108910891089,
      "loss": 0.9766,
      "step": 2194
    },
    {
      "epoch": 14.506402313093762,
      "grad_norm": 27.32144546508789,
      "learning_rate": 0.0004084158415841584,
      "loss": 1.7424,
      "step": 2195
    },
    {
      "epoch": 14.513011152416357,
      "grad_norm": 24.708515167236328,
      "learning_rate": 0.0004079207920792079,
      "loss": 2.3844,
      "step": 2196
    },
    {
      "epoch": 14.519619991738951,
      "grad_norm": 16.341211318969727,
      "learning_rate": 0.0004074257425742574,
      "loss": 0.7099,
      "step": 2197
    },
    {
      "epoch": 14.526228831061545,
      "grad_norm": 13.919317245483398,
      "learning_rate": 0.0004069306930693069,
      "loss": 1.4082,
      "step": 2198
    },
    {
      "epoch": 14.532837670384138,
      "grad_norm": 8.73542308807373,
      "learning_rate": 0.0004064356435643564,
      "loss": 0.4466,
      "step": 2199
    },
    {
      "epoch": 14.539446509706734,
      "grad_norm": 13.501774787902832,
      "learning_rate": 0.000405940594059406,
      "loss": 2.7819,
      "step": 2200
    },
    {
      "epoch": 14.546055349029327,
      "grad_norm": 9.263821601867676,
      "learning_rate": 0.0004054455445544555,
      "loss": 1.502,
      "step": 2201
    },
    {
      "epoch": 14.55266418835192,
      "grad_norm": 13.225980758666992,
      "learning_rate": 0.000404950495049505,
      "loss": 1.3333,
      "step": 2202
    },
    {
      "epoch": 14.559273027674514,
      "grad_norm": 3.262972354888916,
      "learning_rate": 0.0004044554455445545,
      "loss": 0.9431,
      "step": 2203
    },
    {
      "epoch": 14.565881866997108,
      "grad_norm": 18.447952270507812,
      "learning_rate": 0.00040396039603960397,
      "loss": 1.6888,
      "step": 2204
    },
    {
      "epoch": 14.572490706319703,
      "grad_norm": 15.240479469299316,
      "learning_rate": 0.0004034653465346535,
      "loss": 1.902,
      "step": 2205
    },
    {
      "epoch": 14.579099545642297,
      "grad_norm": 16.152751922607422,
      "learning_rate": 0.000402970297029703,
      "loss": 1.8168,
      "step": 2206
    },
    {
      "epoch": 14.58570838496489,
      "grad_norm": 6.786593437194824,
      "learning_rate": 0.0004024752475247525,
      "loss": 1.3499,
      "step": 2207
    },
    {
      "epoch": 14.592317224287484,
      "grad_norm": 2.666794538497925,
      "learning_rate": 0.000401980198019802,
      "loss": 1.5037,
      "step": 2208
    },
    {
      "epoch": 14.598926063610078,
      "grad_norm": 1.5496861934661865,
      "learning_rate": 0.0004014851485148515,
      "loss": 0.7817,
      "step": 2209
    },
    {
      "epoch": 14.605534902932673,
      "grad_norm": 8.445682525634766,
      "learning_rate": 0.000400990099009901,
      "loss": 1.6805,
      "step": 2210
    },
    {
      "epoch": 14.612143742255267,
      "grad_norm": 1.8957929611206055,
      "learning_rate": 0.00040049504950495055,
      "loss": 1.259,
      "step": 2211
    },
    {
      "epoch": 14.61875258157786,
      "grad_norm": 11.137459754943848,
      "learning_rate": 0.0004,
      "loss": 0.7997,
      "step": 2212
    },
    {
      "epoch": 14.625361420900454,
      "grad_norm": 2.192298173904419,
      "learning_rate": 0.00039950495049504955,
      "loss": 1.1709,
      "step": 2213
    },
    {
      "epoch": 14.63197026022305,
      "grad_norm": 18.904346466064453,
      "learning_rate": 0.000399009900990099,
      "loss": 1.1,
      "step": 2214
    },
    {
      "epoch": 14.638579099545643,
      "grad_norm": 1.653367280960083,
      "learning_rate": 0.00039851485148514854,
      "loss": 0.4999,
      "step": 2215
    },
    {
      "epoch": 14.645187938868236,
      "grad_norm": 6.0658159255981445,
      "learning_rate": 0.00039801980198019807,
      "loss": 3.0907,
      "step": 2216
    },
    {
      "epoch": 14.65179677819083,
      "grad_norm": 1.843577265739441,
      "learning_rate": 0.00039752475247524754,
      "loss": 3.0902,
      "step": 2217
    },
    {
      "epoch": 14.658405617513424,
      "grad_norm": 6.770829677581787,
      "learning_rate": 0.00039702970297029707,
      "loss": 1.3583,
      "step": 2218
    },
    {
      "epoch": 14.665014456836019,
      "grad_norm": 23.322294235229492,
      "learning_rate": 0.00039653465346534654,
      "loss": 1.1525,
      "step": 2219
    },
    {
      "epoch": 14.671623296158613,
      "grad_norm": 13.560111045837402,
      "learning_rate": 0.00039603960396039607,
      "loss": 1.6206,
      "step": 2220
    },
    {
      "epoch": 14.678232135481206,
      "grad_norm": 3.4859020709991455,
      "learning_rate": 0.0003955445544554456,
      "loss": 1.1312,
      "step": 2221
    },
    {
      "epoch": 14.6848409748038,
      "grad_norm": 8.688129425048828,
      "learning_rate": 0.00039504950495049506,
      "loss": 0.9957,
      "step": 2222
    },
    {
      "epoch": 14.691449814126393,
      "grad_norm": 4.124814987182617,
      "learning_rate": 0.0003945544554455446,
      "loss": 0.7475,
      "step": 2223
    },
    {
      "epoch": 14.698058653448989,
      "grad_norm": 1.6700881719589233,
      "learning_rate": 0.00039405940594059406,
      "loss": 0.2893,
      "step": 2224
    },
    {
      "epoch": 14.704667492771582,
      "grad_norm": 4.341763973236084,
      "learning_rate": 0.0003935643564356436,
      "loss": 1.2956,
      "step": 2225
    },
    {
      "epoch": 14.711276332094176,
      "grad_norm": 22.536014556884766,
      "learning_rate": 0.0003930693069306931,
      "loss": 1.1314,
      "step": 2226
    },
    {
      "epoch": 14.71788517141677,
      "grad_norm": 2.3414688110351562,
      "learning_rate": 0.0003925742574257426,
      "loss": 0.6366,
      "step": 2227
    },
    {
      "epoch": 14.724494010739363,
      "grad_norm": 1.262499451637268,
      "learning_rate": 0.0003920792079207921,
      "loss": 1.1446,
      "step": 2228
    },
    {
      "epoch": 14.731102850061959,
      "grad_norm": 6.405233860015869,
      "learning_rate": 0.0003915841584158416,
      "loss": 2.4619,
      "step": 2229
    },
    {
      "epoch": 14.737711689384552,
      "grad_norm": 14.400041580200195,
      "learning_rate": 0.0003910891089108911,
      "loss": 0.9277,
      "step": 2230
    },
    {
      "epoch": 14.744320528707146,
      "grad_norm": 22.44487953186035,
      "learning_rate": 0.00039059405940594064,
      "loss": 2.7558,
      "step": 2231
    },
    {
      "epoch": 14.75092936802974,
      "grad_norm": 23.0714111328125,
      "learning_rate": 0.0003900990099009901,
      "loss": 2.4946,
      "step": 2232
    },
    {
      "epoch": 14.757538207352333,
      "grad_norm": 19.84248924255371,
      "learning_rate": 0.00038960396039603964,
      "loss": 1.6692,
      "step": 2233
    },
    {
      "epoch": 14.764147046674928,
      "grad_norm": 15.120071411132812,
      "learning_rate": 0.0003891089108910891,
      "loss": 2.8275,
      "step": 2234
    },
    {
      "epoch": 14.770755885997522,
      "grad_norm": 11.57789134979248,
      "learning_rate": 0.00038861386138613863,
      "loss": 2.7234,
      "step": 2235
    },
    {
      "epoch": 14.777364725320115,
      "grad_norm": 5.513012886047363,
      "learning_rate": 0.00038811881188118816,
      "loss": 0.7179,
      "step": 2236
    },
    {
      "epoch": 14.783973564642709,
      "grad_norm": 5.526133060455322,
      "learning_rate": 0.00038762376237623763,
      "loss": 1.4594,
      "step": 2237
    },
    {
      "epoch": 14.790582403965303,
      "grad_norm": 14.826197624206543,
      "learning_rate": 0.00038712871287128716,
      "loss": 2.242,
      "step": 2238
    },
    {
      "epoch": 14.797191243287898,
      "grad_norm": 38.54133987426758,
      "learning_rate": 0.00038663366336633663,
      "loss": 4.1742,
      "step": 2239
    },
    {
      "epoch": 14.803800082610492,
      "grad_norm": 29.17828369140625,
      "learning_rate": 0.00038613861386138616,
      "loss": 1.4738,
      "step": 2240
    },
    {
      "epoch": 14.810408921933085,
      "grad_norm": 28.063385009765625,
      "learning_rate": 0.0003856435643564357,
      "loss": 2.2846,
      "step": 2241
    },
    {
      "epoch": 14.817017761255679,
      "grad_norm": 9.526713371276855,
      "learning_rate": 0.00038514851485148515,
      "loss": 1.891,
      "step": 2242
    },
    {
      "epoch": 14.823626600578274,
      "grad_norm": 27.83039665222168,
      "learning_rate": 0.0003846534653465347,
      "loss": 1.9122,
      "step": 2243
    },
    {
      "epoch": 14.830235439900868,
      "grad_norm": 16.37256622314453,
      "learning_rate": 0.00038415841584158415,
      "loss": 1.8801,
      "step": 2244
    },
    {
      "epoch": 14.836844279223461,
      "grad_norm": 3.3486595153808594,
      "learning_rate": 0.0003836633663366337,
      "loss": 1.891,
      "step": 2245
    },
    {
      "epoch": 14.843453118546055,
      "grad_norm": 6.979223728179932,
      "learning_rate": 0.0003831683168316832,
      "loss": 1.0143,
      "step": 2246
    },
    {
      "epoch": 14.850061957868649,
      "grad_norm": 27.3643798828125,
      "learning_rate": 0.0003826732673267327,
      "loss": 1.8305,
      "step": 2247
    },
    {
      "epoch": 14.856670797191244,
      "grad_norm": 40.4991340637207,
      "learning_rate": 0.0003821782178217822,
      "loss": 5.4151,
      "step": 2248
    },
    {
      "epoch": 14.863279636513838,
      "grad_norm": 39.41187286376953,
      "learning_rate": 0.0003816831683168317,
      "loss": 2.5471,
      "step": 2249
    },
    {
      "epoch": 14.869888475836431,
      "grad_norm": 46.250892639160156,
      "learning_rate": 0.0003811881188118812,
      "loss": 3.9329,
      "step": 2250
    },
    {
      "epoch": 14.876497315159025,
      "grad_norm": 30.11549949645996,
      "learning_rate": 0.00038069306930693073,
      "loss": 2.3459,
      "step": 2251
    },
    {
      "epoch": 14.88310615448162,
      "grad_norm": 26.394981384277344,
      "learning_rate": 0.0003801980198019802,
      "loss": 1.6918,
      "step": 2252
    },
    {
      "epoch": 14.889714993804214,
      "grad_norm": 24.478559494018555,
      "learning_rate": 0.0003797029702970297,
      "loss": 2.3097,
      "step": 2253
    },
    {
      "epoch": 14.896323833126807,
      "grad_norm": 7.274405002593994,
      "learning_rate": 0.0003792079207920792,
      "loss": 2.0112,
      "step": 2254
    },
    {
      "epoch": 14.902932672449401,
      "grad_norm": 2.0668468475341797,
      "learning_rate": 0.0003787128712871287,
      "loss": 0.7499,
      "step": 2255
    },
    {
      "epoch": 14.909541511771994,
      "grad_norm": 11.551627159118652,
      "learning_rate": 0.00037821782178217825,
      "loss": 2.3937,
      "step": 2256
    },
    {
      "epoch": 14.91615035109459,
      "grad_norm": 22.98375129699707,
      "learning_rate": 0.0003777227722772277,
      "loss": 1.1114,
      "step": 2257
    },
    {
      "epoch": 14.922759190417183,
      "grad_norm": 22.939062118530273,
      "learning_rate": 0.00037722772277227725,
      "loss": 1.2962,
      "step": 2258
    },
    {
      "epoch": 14.929368029739777,
      "grad_norm": 5.967092037200928,
      "learning_rate": 0.0003767326732673267,
      "loss": 0.8967,
      "step": 2259
    },
    {
      "epoch": 14.93597686906237,
      "grad_norm": 38.60254669189453,
      "learning_rate": 0.00037623762376237625,
      "loss": 2.5825,
      "step": 2260
    },
    {
      "epoch": 14.942585708384964,
      "grad_norm": 13.660749435424805,
      "learning_rate": 0.00037574257425742577,
      "loss": 0.7745,
      "step": 2261
    },
    {
      "epoch": 14.94919454770756,
      "grad_norm": 12.221692085266113,
      "learning_rate": 0.00037524752475247524,
      "loss": 1.9416,
      "step": 2262
    },
    {
      "epoch": 14.955803387030153,
      "grad_norm": 12.159974098205566,
      "learning_rate": 0.00037475247524752477,
      "loss": 0.4796,
      "step": 2263
    },
    {
      "epoch": 14.962412226352747,
      "grad_norm": 5.0668110847473145,
      "learning_rate": 0.00037425742574257424,
      "loss": 1.0605,
      "step": 2264
    },
    {
      "epoch": 14.96902106567534,
      "grad_norm": 19.13247299194336,
      "learning_rate": 0.00037376237623762377,
      "loss": 0.7029,
      "step": 2265
    },
    {
      "epoch": 14.975629904997934,
      "grad_norm": 28.742752075195312,
      "learning_rate": 0.0003732673267326733,
      "loss": 1.3329,
      "step": 2266
    },
    {
      "epoch": 14.98223874432053,
      "grad_norm": 20.20973014831543,
      "learning_rate": 0.00037277227722772277,
      "loss": 1.6422,
      "step": 2267
    },
    {
      "epoch": 14.988847583643123,
      "grad_norm": 21.67402458190918,
      "learning_rate": 0.0003722772277227723,
      "loss": 1.9449,
      "step": 2268
    },
    {
      "epoch": 14.995456422965717,
      "grad_norm": 8.194058418273926,
      "learning_rate": 0.00037178217821782177,
      "loss": 0.8434,
      "step": 2269
    },
    {
      "epoch": 14.995456422965717,
      "eval_validation_error_bar": 0.039210689586493076,
      "eval_validation_loss": 4.725356101989746,
      "eval_validation_pearsonr": 0.6788052034353068,
      "eval_validation_rmse": 2.17378830909729,
      "eval_validation_runtime": 28.5079,
      "eval_validation_samples_per_second": 7.121,
      "eval_validation_spearman": 0.7102546263119176,
      "eval_validation_steps_per_second": 7.121,
      "step": 2269
    },
    {
      "epoch": 14.995456422965717,
      "eval_test_error_bar": 0.033089076182934406,
      "eval_test_loss": 5.615512371063232,
      "eval_test_pearsonr": 0.6626165920522264,
      "eval_test_rmse": 2.3697073459625244,
      "eval_test_runtime": 45.7036,
      "eval_test_samples_per_second": 7.133,
      "eval_test_spearman": 0.6813626546721381,
      "eval_test_steps_per_second": 7.133,
      "step": 2269
    },
    {
      "epoch": 15.00206526228831,
      "grad_norm": 4.870028018951416,
      "learning_rate": 0.0003712871287128713,
      "loss": 0.8785,
      "step": 2270
    },
    {
      "epoch": 15.008674101610904,
      "grad_norm": 3.34378719329834,
      "learning_rate": 0.0003707920792079208,
      "loss": 0.5297,
      "step": 2271
    },
    {
      "epoch": 15.0152829409335,
      "grad_norm": 2.2614431381225586,
      "learning_rate": 0.0003702970297029703,
      "loss": 1.1274,
      "step": 2272
    },
    {
      "epoch": 15.021891780256093,
      "grad_norm": 8.158247947692871,
      "learning_rate": 0.0003698019801980198,
      "loss": 3.2003,
      "step": 2273
    },
    {
      "epoch": 15.028500619578686,
      "grad_norm": 19.762929916381836,
      "learning_rate": 0.0003693069306930693,
      "loss": 1.4413,
      "step": 2274
    },
    {
      "epoch": 15.03510945890128,
      "grad_norm": 1.0170366764068604,
      "learning_rate": 0.0003688118811881188,
      "loss": 0.7967,
      "step": 2275
    },
    {
      "epoch": 15.041718298223875,
      "grad_norm": 17.81706428527832,
      "learning_rate": 0.00036831683168316834,
      "loss": 2.279,
      "step": 2276
    },
    {
      "epoch": 15.048327137546469,
      "grad_norm": 14.132133483886719,
      "learning_rate": 0.0003678217821782178,
      "loss": 0.9723,
      "step": 2277
    },
    {
      "epoch": 15.054935976869062,
      "grad_norm": 12.416116714477539,
      "learning_rate": 0.00036732673267326734,
      "loss": 1.628,
      "step": 2278
    },
    {
      "epoch": 15.061544816191656,
      "grad_norm": 7.110641002655029,
      "learning_rate": 0.0003668316831683168,
      "loss": 0.7392,
      "step": 2279
    },
    {
      "epoch": 15.06815365551425,
      "grad_norm": 2.3639755249023438,
      "learning_rate": 0.00036633663366336634,
      "loss": 0.7231,
      "step": 2280
    },
    {
      "epoch": 15.074762494836845,
      "grad_norm": 6.432998180389404,
      "learning_rate": 0.00036584158415841586,
      "loss": 0.543,
      "step": 2281
    },
    {
      "epoch": 15.081371334159439,
      "grad_norm": 12.771540641784668,
      "learning_rate": 0.00036534653465346533,
      "loss": 0.9776,
      "step": 2282
    },
    {
      "epoch": 15.087980173482032,
      "grad_norm": 20.931821823120117,
      "learning_rate": 0.00036485148514851486,
      "loss": 0.6856,
      "step": 2283
    },
    {
      "epoch": 15.094589012804626,
      "grad_norm": 16.142900466918945,
      "learning_rate": 0.00036435643564356433,
      "loss": 0.9881,
      "step": 2284
    },
    {
      "epoch": 15.10119785212722,
      "grad_norm": 12.62866497039795,
      "learning_rate": 0.00036386138613861386,
      "loss": 1.3845,
      "step": 2285
    },
    {
      "epoch": 15.107806691449815,
      "grad_norm": 5.786801338195801,
      "learning_rate": 0.0003633663366336634,
      "loss": 0.6108,
      "step": 2286
    },
    {
      "epoch": 15.114415530772408,
      "grad_norm": 6.405241966247559,
      "learning_rate": 0.00036287128712871286,
      "loss": 1.3955,
      "step": 2287
    },
    {
      "epoch": 15.121024370095002,
      "grad_norm": 3.838834762573242,
      "learning_rate": 0.0003623762376237624,
      "loss": 1.9096,
      "step": 2288
    },
    {
      "epoch": 15.127633209417596,
      "grad_norm": 4.6663923263549805,
      "learning_rate": 0.00036188118811881186,
      "loss": 1.0867,
      "step": 2289
    },
    {
      "epoch": 15.13424204874019,
      "grad_norm": 23.818836212158203,
      "learning_rate": 0.0003613861386138614,
      "loss": 1.0381,
      "step": 2290
    },
    {
      "epoch": 15.140850888062785,
      "grad_norm": 46.868160247802734,
      "learning_rate": 0.0003608910891089109,
      "loss": 3.8332,
      "step": 2291
    },
    {
      "epoch": 15.147459727385378,
      "grad_norm": 28.26932716369629,
      "learning_rate": 0.0003603960396039604,
      "loss": 1.6599,
      "step": 2292
    },
    {
      "epoch": 15.154068566707972,
      "grad_norm": 35.48039627075195,
      "learning_rate": 0.0003599009900990099,
      "loss": 1.4089,
      "step": 2293
    },
    {
      "epoch": 15.160677406030565,
      "grad_norm": 31.638866424560547,
      "learning_rate": 0.0003594059405940594,
      "loss": 1.9474,
      "step": 2294
    },
    {
      "epoch": 15.16728624535316,
      "grad_norm": 38.84280014038086,
      "learning_rate": 0.0003589108910891089,
      "loss": 2.1904,
      "step": 2295
    },
    {
      "epoch": 15.173895084675754,
      "grad_norm": 25.506000518798828,
      "learning_rate": 0.00035841584158415843,
      "loss": 1.4093,
      "step": 2296
    },
    {
      "epoch": 15.180503923998348,
      "grad_norm": 29.34642791748047,
      "learning_rate": 0.0003579207920792079,
      "loss": 2.2101,
      "step": 2297
    },
    {
      "epoch": 15.187112763320942,
      "grad_norm": 27.124614715576172,
      "learning_rate": 0.00035742574257425743,
      "loss": 1.571,
      "step": 2298
    },
    {
      "epoch": 15.193721602643535,
      "grad_norm": 5.931814670562744,
      "learning_rate": 0.0003569306930693069,
      "loss": 0.687,
      "step": 2299
    },
    {
      "epoch": 15.20033044196613,
      "grad_norm": 17.173322677612305,
      "learning_rate": 0.0003564356435643564,
      "loss": 1.1185,
      "step": 2300
    },
    {
      "epoch": 15.206939281288724,
      "grad_norm": 26.77123260498047,
      "learning_rate": 0.00035594059405940595,
      "loss": 3.0195,
      "step": 2301
    },
    {
      "epoch": 15.213548120611318,
      "grad_norm": 15.915090560913086,
      "learning_rate": 0.0003554455445544554,
      "loss": 0.7165,
      "step": 2302
    },
    {
      "epoch": 15.220156959933911,
      "grad_norm": 36.241573333740234,
      "learning_rate": 0.00035495049504950495,
      "loss": 2.9093,
      "step": 2303
    },
    {
      "epoch": 15.226765799256505,
      "grad_norm": 20.00556182861328,
      "learning_rate": 0.0003544554455445544,
      "loss": 1.6711,
      "step": 2304
    },
    {
      "epoch": 15.2333746385791,
      "grad_norm": 28.453083038330078,
      "learning_rate": 0.00035396039603960395,
      "loss": 1.7444,
      "step": 2305
    },
    {
      "epoch": 15.239983477901694,
      "grad_norm": 15.668740272521973,
      "learning_rate": 0.0003534653465346535,
      "loss": 2.1565,
      "step": 2306
    },
    {
      "epoch": 15.246592317224287,
      "grad_norm": 10.987854957580566,
      "learning_rate": 0.00035297029702970295,
      "loss": 1.169,
      "step": 2307
    },
    {
      "epoch": 15.253201156546881,
      "grad_norm": 4.214659690856934,
      "learning_rate": 0.0003524752475247525,
      "loss": 1.3434,
      "step": 2308
    },
    {
      "epoch": 15.259809995869475,
      "grad_norm": 17.709562301635742,
      "learning_rate": 0.00035198019801980195,
      "loss": 0.945,
      "step": 2309
    },
    {
      "epoch": 15.26641883519207,
      "grad_norm": 10.119217872619629,
      "learning_rate": 0.00035148514851485147,
      "loss": 1.2029,
      "step": 2310
    },
    {
      "epoch": 15.273027674514664,
      "grad_norm": 1.6170295476913452,
      "learning_rate": 0.000350990099009901,
      "loss": 2.1712,
      "step": 2311
    },
    {
      "epoch": 15.279636513837257,
      "grad_norm": 9.3853759765625,
      "learning_rate": 0.00035049504950495047,
      "loss": 1.4015,
      "step": 2312
    },
    {
      "epoch": 15.28624535315985,
      "grad_norm": 19.487905502319336,
      "learning_rate": 0.00035,
      "loss": 1.3355,
      "step": 2313
    },
    {
      "epoch": 15.292854192482444,
      "grad_norm": 24.230560302734375,
      "learning_rate": 0.00034950495049504947,
      "loss": 1.8436,
      "step": 2314
    },
    {
      "epoch": 15.29946303180504,
      "grad_norm": 4.532491207122803,
      "learning_rate": 0.000349009900990099,
      "loss": 1.2978,
      "step": 2315
    },
    {
      "epoch": 15.306071871127633,
      "grad_norm": 25.72246551513672,
      "learning_rate": 0.0003485148514851485,
      "loss": 3.9316,
      "step": 2316
    },
    {
      "epoch": 15.312680710450227,
      "grad_norm": 19.55926513671875,
      "learning_rate": 0.000348019801980198,
      "loss": 3.2546,
      "step": 2317
    },
    {
      "epoch": 15.31928954977282,
      "grad_norm": 8.782838821411133,
      "learning_rate": 0.0003475247524752475,
      "loss": 0.6881,
      "step": 2318
    },
    {
      "epoch": 15.325898389095416,
      "grad_norm": 8.504412651062012,
      "learning_rate": 0.000347029702970297,
      "loss": 1.0682,
      "step": 2319
    },
    {
      "epoch": 15.33250722841801,
      "grad_norm": 18.496585845947266,
      "learning_rate": 0.0003465346534653465,
      "loss": 2.2312,
      "step": 2320
    },
    {
      "epoch": 15.339116067740603,
      "grad_norm": 25.506210327148438,
      "learning_rate": 0.00034603960396039604,
      "loss": 2.5044,
      "step": 2321
    },
    {
      "epoch": 15.345724907063197,
      "grad_norm": 6.801049709320068,
      "learning_rate": 0.0003455445544554455,
      "loss": 1.6358,
      "step": 2322
    },
    {
      "epoch": 15.35233374638579,
      "grad_norm": 22.575820922851562,
      "learning_rate": 0.00034504950495049504,
      "loss": 1.4791,
      "step": 2323
    },
    {
      "epoch": 15.358942585708386,
      "grad_norm": 12.453451156616211,
      "learning_rate": 0.0003445544554455445,
      "loss": 0.8142,
      "step": 2324
    },
    {
      "epoch": 15.36555142503098,
      "grad_norm": 6.316161632537842,
      "learning_rate": 0.00034405940594059404,
      "loss": 0.929,
      "step": 2325
    },
    {
      "epoch": 15.372160264353573,
      "grad_norm": 3.618051052093506,
      "learning_rate": 0.0003435643564356436,
      "loss": 2.4781,
      "step": 2326
    },
    {
      "epoch": 15.378769103676166,
      "grad_norm": 1.869511365890503,
      "learning_rate": 0.0003430693069306931,
      "loss": 1.3988,
      "step": 2327
    },
    {
      "epoch": 15.38537794299876,
      "grad_norm": 4.036249160766602,
      "learning_rate": 0.0003425742574257426,
      "loss": 0.8665,
      "step": 2328
    },
    {
      "epoch": 15.391986782321355,
      "grad_norm": 6.5641093254089355,
      "learning_rate": 0.0003420792079207921,
      "loss": 0.8653,
      "step": 2329
    },
    {
      "epoch": 15.398595621643949,
      "grad_norm": 8.698809623718262,
      "learning_rate": 0.0003415841584158416,
      "loss": 0.9785,
      "step": 2330
    },
    {
      "epoch": 15.405204460966543,
      "grad_norm": 7.206671237945557,
      "learning_rate": 0.00034108910891089114,
      "loss": 1.4467,
      "step": 2331
    },
    {
      "epoch": 15.411813300289136,
      "grad_norm": 3.763202667236328,
      "learning_rate": 0.0003405940594059406,
      "loss": 0.7007,
      "step": 2332
    },
    {
      "epoch": 15.418422139611732,
      "grad_norm": 15.987698554992676,
      "learning_rate": 0.00034009900990099014,
      "loss": 0.7093,
      "step": 2333
    },
    {
      "epoch": 15.425030978934325,
      "grad_norm": 2.3034658432006836,
      "learning_rate": 0.0003396039603960396,
      "loss": 0.8242,
      "step": 2334
    },
    {
      "epoch": 15.431639818256919,
      "grad_norm": 12.815827369689941,
      "learning_rate": 0.00033910891089108914,
      "loss": 2.3041,
      "step": 2335
    },
    {
      "epoch": 15.438248657579512,
      "grad_norm": 31.60710906982422,
      "learning_rate": 0.00033861386138613867,
      "loss": 4.6152,
      "step": 2336
    },
    {
      "epoch": 15.444857496902106,
      "grad_norm": 3.336207866668701,
      "learning_rate": 0.00033811881188118814,
      "loss": 2.1453,
      "step": 2337
    },
    {
      "epoch": 15.451466336224701,
      "grad_norm": 12.669180870056152,
      "learning_rate": 0.00033762376237623766,
      "loss": 1.2628,
      "step": 2338
    },
    {
      "epoch": 15.458075175547295,
      "grad_norm": 11.987018585205078,
      "learning_rate": 0.00033712871287128714,
      "loss": 1.0721,
      "step": 2339
    },
    {
      "epoch": 15.464684014869889,
      "grad_norm": 10.744996070861816,
      "learning_rate": 0.00033663366336633666,
      "loss": 0.6665,
      "step": 2340
    },
    {
      "epoch": 15.471292854192482,
      "grad_norm": 4.0804548263549805,
      "learning_rate": 0.0003361386138613862,
      "loss": 0.6145,
      "step": 2341
    },
    {
      "epoch": 15.477901693515076,
      "grad_norm": 15.99972152709961,
      "learning_rate": 0.00033564356435643566,
      "loss": 1.4781,
      "step": 2342
    },
    {
      "epoch": 15.484510532837671,
      "grad_norm": 4.619326591491699,
      "learning_rate": 0.0003351485148514852,
      "loss": 1.4906,
      "step": 2343
    },
    {
      "epoch": 15.491119372160265,
      "grad_norm": 7.158880233764648,
      "learning_rate": 0.00033465346534653466,
      "loss": 3.467,
      "step": 2344
    },
    {
      "epoch": 15.497728211482858,
      "grad_norm": 3.1120996475219727,
      "learning_rate": 0.0003341584158415842,
      "loss": 0.5095,
      "step": 2345
    },
    {
      "epoch": 15.504337050805452,
      "grad_norm": 1.7797679901123047,
      "learning_rate": 0.0003336633663366337,
      "loss": 1.1649,
      "step": 2346
    },
    {
      "epoch": 15.510945890128045,
      "grad_norm": 5.10479736328125,
      "learning_rate": 0.0003331683168316832,
      "loss": 0.4956,
      "step": 2347
    },
    {
      "epoch": 15.51755472945064,
      "grad_norm": 4.369948387145996,
      "learning_rate": 0.0003326732673267327,
      "loss": 0.663,
      "step": 2348
    },
    {
      "epoch": 15.524163568773234,
      "grad_norm": 1.8879332542419434,
      "learning_rate": 0.0003321782178217822,
      "loss": 1.9545,
      "step": 2349
    },
    {
      "epoch": 15.530772408095828,
      "grad_norm": 9.026124954223633,
      "learning_rate": 0.0003316831683168317,
      "loss": 1.0781,
      "step": 2350
    },
    {
      "epoch": 15.537381247418422,
      "grad_norm": 8.454410552978516,
      "learning_rate": 0.00033118811881188123,
      "loss": 0.8607,
      "step": 2351
    },
    {
      "epoch": 15.543990086741015,
      "grad_norm": 1.940503478050232,
      "learning_rate": 0.0003306930693069307,
      "loss": 2.5476,
      "step": 2352
    },
    {
      "epoch": 15.55059892606361,
      "grad_norm": 2.2604732513427734,
      "learning_rate": 0.00033019801980198023,
      "loss": 2.2895,
      "step": 2353
    },
    {
      "epoch": 15.557207765386204,
      "grad_norm": 20.17556381225586,
      "learning_rate": 0.0003297029702970297,
      "loss": 0.9368,
      "step": 2354
    },
    {
      "epoch": 15.563816604708798,
      "grad_norm": 1.9510071277618408,
      "learning_rate": 0.00032920792079207923,
      "loss": 1.6817,
      "step": 2355
    },
    {
      "epoch": 15.570425444031391,
      "grad_norm": 5.216599464416504,
      "learning_rate": 0.00032871287128712876,
      "loss": 2.2184,
      "step": 2356
    },
    {
      "epoch": 15.577034283353987,
      "grad_norm": 18.530376434326172,
      "learning_rate": 0.00032821782178217823,
      "loss": 1.0347,
      "step": 2357
    },
    {
      "epoch": 15.58364312267658,
      "grad_norm": 19.042146682739258,
      "learning_rate": 0.00032772277227722775,
      "loss": 1.6819,
      "step": 2358
    },
    {
      "epoch": 15.590251961999174,
      "grad_norm": 16.807228088378906,
      "learning_rate": 0.0003272277227722772,
      "loss": 0.8636,
      "step": 2359
    },
    {
      "epoch": 15.596860801321768,
      "grad_norm": 20.41583251953125,
      "learning_rate": 0.00032673267326732675,
      "loss": 1.4308,
      "step": 2360
    },
    {
      "epoch": 15.603469640644361,
      "grad_norm": 5.81039571762085,
      "learning_rate": 0.0003262376237623763,
      "loss": 0.8755,
      "step": 2361
    },
    {
      "epoch": 15.610078479966957,
      "grad_norm": 10.610556602478027,
      "learning_rate": 0.00032574257425742575,
      "loss": 4.3715,
      "step": 2362
    },
    {
      "epoch": 15.61668731928955,
      "grad_norm": 5.983381271362305,
      "learning_rate": 0.0003252475247524753,
      "loss": 0.6767,
      "step": 2363
    },
    {
      "epoch": 15.623296158612144,
      "grad_norm": 3.242981195449829,
      "learning_rate": 0.00032475247524752475,
      "loss": 0.6156,
      "step": 2364
    },
    {
      "epoch": 15.629904997934737,
      "grad_norm": 1.1736226081848145,
      "learning_rate": 0.0003242574257425743,
      "loss": 0.6454,
      "step": 2365
    },
    {
      "epoch": 15.636513837257331,
      "grad_norm": 13.499968528747559,
      "learning_rate": 0.0003237623762376238,
      "loss": 1.2506,
      "step": 2366
    },
    {
      "epoch": 15.643122676579926,
      "grad_norm": 24.256282806396484,
      "learning_rate": 0.00032326732673267327,
      "loss": 2.2591,
      "step": 2367
    },
    {
      "epoch": 15.64973151590252,
      "grad_norm": 4.327503204345703,
      "learning_rate": 0.0003227722772277228,
      "loss": 0.5956,
      "step": 2368
    },
    {
      "epoch": 15.656340355225113,
      "grad_norm": 1.6795674562454224,
      "learning_rate": 0.00032227722772277227,
      "loss": 2.0862,
      "step": 2369
    },
    {
      "epoch": 15.662949194547707,
      "grad_norm": 3.498904228210449,
      "learning_rate": 0.0003217821782178218,
      "loss": 0.8255,
      "step": 2370
    },
    {
      "epoch": 15.669558033870302,
      "grad_norm": 15.94437313079834,
      "learning_rate": 0.0003212871287128713,
      "loss": 1.467,
      "step": 2371
    },
    {
      "epoch": 15.676166873192896,
      "grad_norm": 1.3701074123382568,
      "learning_rate": 0.0003207920792079208,
      "loss": 1.0706,
      "step": 2372
    },
    {
      "epoch": 15.68277571251549,
      "grad_norm": 10.50931453704834,
      "learning_rate": 0.0003202970297029703,
      "loss": 2.0308,
      "step": 2373
    },
    {
      "epoch": 15.689384551838083,
      "grad_norm": 4.286776065826416,
      "learning_rate": 0.0003198019801980198,
      "loss": 0.4425,
      "step": 2374
    },
    {
      "epoch": 15.695993391160677,
      "grad_norm": 8.919193267822266,
      "learning_rate": 0.0003193069306930693,
      "loss": 1.0361,
      "step": 2375
    },
    {
      "epoch": 15.702602230483272,
      "grad_norm": 1.551429271697998,
      "learning_rate": 0.00031881188118811885,
      "loss": 0.5546,
      "step": 2376
    },
    {
      "epoch": 15.709211069805866,
      "grad_norm": 6.437125205993652,
      "learning_rate": 0.0003183168316831683,
      "loss": 2.3543,
      "step": 2377
    },
    {
      "epoch": 15.71581990912846,
      "grad_norm": 8.10629653930664,
      "learning_rate": 0.00031782178217821784,
      "loss": 1.2814,
      "step": 2378
    },
    {
      "epoch": 15.722428748451053,
      "grad_norm": 4.121908187866211,
      "learning_rate": 0.0003173267326732673,
      "loss": 1.8598,
      "step": 2379
    },
    {
      "epoch": 15.729037587773647,
      "grad_norm": 9.424857139587402,
      "learning_rate": 0.00031683168316831684,
      "loss": 0.9683,
      "step": 2380
    },
    {
      "epoch": 15.735646427096242,
      "grad_norm": 13.236377716064453,
      "learning_rate": 0.00031633663366336637,
      "loss": 1.1327,
      "step": 2381
    },
    {
      "epoch": 15.742255266418836,
      "grad_norm": 8.38673210144043,
      "learning_rate": 0.00031584158415841584,
      "loss": 1.1084,
      "step": 2382
    },
    {
      "epoch": 15.74886410574143,
      "grad_norm": 6.886220455169678,
      "learning_rate": 0.00031534653465346537,
      "loss": 1.5464,
      "step": 2383
    },
    {
      "epoch": 15.755472945064023,
      "grad_norm": 6.623248100280762,
      "learning_rate": 0.00031485148514851484,
      "loss": 2.5969,
      "step": 2384
    },
    {
      "epoch": 15.762081784386616,
      "grad_norm": 3.0076558589935303,
      "learning_rate": 0.00031435643564356436,
      "loss": 1.2725,
      "step": 2385
    },
    {
      "epoch": 15.768690623709212,
      "grad_norm": 10.07568359375,
      "learning_rate": 0.0003138613861386139,
      "loss": 1.019,
      "step": 2386
    },
    {
      "epoch": 15.775299463031805,
      "grad_norm": 2.322014808654785,
      "learning_rate": 0.00031336633663366336,
      "loss": 1.1223,
      "step": 2387
    },
    {
      "epoch": 15.781908302354399,
      "grad_norm": 17.159902572631836,
      "learning_rate": 0.0003128712871287129,
      "loss": 1.4121,
      "step": 2388
    },
    {
      "epoch": 15.788517141676992,
      "grad_norm": 11.983150482177734,
      "learning_rate": 0.00031237623762376236,
      "loss": 0.5665,
      "step": 2389
    },
    {
      "epoch": 15.795125980999586,
      "grad_norm": 6.154947280883789,
      "learning_rate": 0.0003118811881188119,
      "loss": 1.9543,
      "step": 2390
    },
    {
      "epoch": 15.801734820322181,
      "grad_norm": 4.625430107116699,
      "learning_rate": 0.0003113861386138614,
      "loss": 1.7558,
      "step": 2391
    },
    {
      "epoch": 15.808343659644775,
      "grad_norm": 12.331817626953125,
      "learning_rate": 0.0003108910891089109,
      "loss": 1.2588,
      "step": 2392
    },
    {
      "epoch": 15.814952498967369,
      "grad_norm": 5.325423240661621,
      "learning_rate": 0.0003103960396039604,
      "loss": 1.3204,
      "step": 2393
    },
    {
      "epoch": 15.821561338289962,
      "grad_norm": 3.7757043838500977,
      "learning_rate": 0.0003099009900990099,
      "loss": 1.5963,
      "step": 2394
    },
    {
      "epoch": 15.828170177612558,
      "grad_norm": 6.161892414093018,
      "learning_rate": 0.0003094059405940594,
      "loss": 0.7958,
      "step": 2395
    },
    {
      "epoch": 15.834779016935151,
      "grad_norm": 12.344525337219238,
      "learning_rate": 0.00030891089108910894,
      "loss": 3.1888,
      "step": 2396
    },
    {
      "epoch": 15.841387856257745,
      "grad_norm": 11.603442192077637,
      "learning_rate": 0.0003084158415841584,
      "loss": 1.3577,
      "step": 2397
    },
    {
      "epoch": 15.847996695580338,
      "grad_norm": 7.340849876403809,
      "learning_rate": 0.00030792079207920793,
      "loss": 1.4573,
      "step": 2398
    },
    {
      "epoch": 15.854605534902932,
      "grad_norm": 4.358137130737305,
      "learning_rate": 0.0003074257425742574,
      "loss": 3.341,
      "step": 2399
    },
    {
      "epoch": 15.861214374225527,
      "grad_norm": 6.065522193908691,
      "learning_rate": 0.00030693069306930693,
      "loss": 1.1345,
      "step": 2400
    },
    {
      "epoch": 15.867823213548121,
      "grad_norm": 2.9814047813415527,
      "learning_rate": 0.00030643564356435646,
      "loss": 1.9427,
      "step": 2401
    },
    {
      "epoch": 15.874432052870715,
      "grad_norm": 1.1334524154663086,
      "learning_rate": 0.00030594059405940593,
      "loss": 1.3346,
      "step": 2402
    },
    {
      "epoch": 15.881040892193308,
      "grad_norm": 1.6496144533157349,
      "learning_rate": 0.00030544554455445546,
      "loss": 0.9587,
      "step": 2403
    },
    {
      "epoch": 15.887649731515902,
      "grad_norm": 4.665916919708252,
      "learning_rate": 0.00030495049504950493,
      "loss": 0.8589,
      "step": 2404
    },
    {
      "epoch": 15.894258570838497,
      "grad_norm": 3.6640257835388184,
      "learning_rate": 0.00030445544554455445,
      "loss": 1.3453,
      "step": 2405
    },
    {
      "epoch": 15.90086741016109,
      "grad_norm": 1.2265048027038574,
      "learning_rate": 0.000303960396039604,
      "loss": 0.56,
      "step": 2406
    },
    {
      "epoch": 15.907476249483684,
      "grad_norm": 2.858872652053833,
      "learning_rate": 0.00030346534653465345,
      "loss": 0.9283,
      "step": 2407
    },
    {
      "epoch": 15.914085088806278,
      "grad_norm": 3.851215362548828,
      "learning_rate": 0.000302970297029703,
      "loss": 0.5097,
      "step": 2408
    },
    {
      "epoch": 15.920693928128873,
      "grad_norm": 12.312153816223145,
      "learning_rate": 0.00030247524752475245,
      "loss": 1.3008,
      "step": 2409
    },
    {
      "epoch": 15.927302767451467,
      "grad_norm": 8.652262687683105,
      "learning_rate": 0.000301980198019802,
      "loss": 0.6866,
      "step": 2410
    },
    {
      "epoch": 15.93391160677406,
      "grad_norm": 2.949733018875122,
      "learning_rate": 0.0003014851485148515,
      "loss": 1.7852,
      "step": 2411
    },
    {
      "epoch": 15.940520446096654,
      "grad_norm": 10.864083290100098,
      "learning_rate": 0.000300990099009901,
      "loss": 2.525,
      "step": 2412
    },
    {
      "epoch": 15.947129285419248,
      "grad_norm": 6.045307636260986,
      "learning_rate": 0.0003004950495049505,
      "loss": 0.6413,
      "step": 2413
    },
    {
      "epoch": 15.953738124741843,
      "grad_norm": 1.3811988830566406,
      "learning_rate": 0.0003,
      "loss": 1.761,
      "step": 2414
    },
    {
      "epoch": 15.960346964064437,
      "grad_norm": 3.3697922229766846,
      "learning_rate": 0.0002995049504950495,
      "loss": 1.1071,
      "step": 2415
    },
    {
      "epoch": 15.96695580338703,
      "grad_norm": 3.629638433456421,
      "learning_rate": 0.000299009900990099,
      "loss": 1.0709,
      "step": 2416
    },
    {
      "epoch": 15.973564642709624,
      "grad_norm": 4.966370105743408,
      "learning_rate": 0.0002985148514851485,
      "loss": 0.5881,
      "step": 2417
    },
    {
      "epoch": 15.980173482032217,
      "grad_norm": 1.0668611526489258,
      "learning_rate": 0.000298019801980198,
      "loss": 0.5215,
      "step": 2418
    },
    {
      "epoch": 15.986782321354813,
      "grad_norm": 3.4462625980377197,
      "learning_rate": 0.0002975247524752475,
      "loss": 1.342,
      "step": 2419
    },
    {
      "epoch": 15.993391160677406,
      "grad_norm": 2.431471586227417,
      "learning_rate": 0.000297029702970297,
      "loss": 0.7972,
      "step": 2420
    },
    {
      "epoch": 16.0,
      "grad_norm": 7.361089706420898,
      "learning_rate": 0.00029653465346534655,
      "loss": 0.3471,
      "step": 2421
    },
    {
      "epoch": 16.0,
      "eval_validation_error_bar": 0.03569104553034731,
      "eval_validation_loss": 3.883239984512329,
      "eval_validation_pearsonr": 0.7128466606918328,
      "eval_validation_rmse": 1.970593810081482,
      "eval_validation_runtime": 28.5573,
      "eval_validation_samples_per_second": 7.109,
      "eval_validation_spearman": 0.7438244154384365,
      "eval_validation_steps_per_second": 7.109,
      "step": 2421
    },
    {
      "epoch": 16.0,
      "eval_test_error_bar": 0.03332452749976358,
      "eval_test_loss": 5.319401264190674,
      "eval_test_pearsonr": 0.6611399621197717,
      "eval_test_rmse": 2.306382656097412,
      "eval_test_runtime": 45.7881,
      "eval_test_samples_per_second": 7.12,
      "eval_test_spearman": 0.6782134896493139,
      "eval_test_steps_per_second": 7.12,
      "step": 2421
    },
    {
      "epoch": 16.006608839322595,
      "grad_norm": 9.298163414001465,
      "learning_rate": 0.000296039603960396,
      "loss": 1.9365,
      "step": 2422
    },
    {
      "epoch": 16.013217678645187,
      "grad_norm": 10.201752662658691,
      "learning_rate": 0.00029554455445544555,
      "loss": 2.2823,
      "step": 2423
    },
    {
      "epoch": 16.019826517967783,
      "grad_norm": 1.456993579864502,
      "learning_rate": 0.000295049504950495,
      "loss": 0.5778,
      "step": 2424
    },
    {
      "epoch": 16.026435357290374,
      "grad_norm": 6.336151123046875,
      "learning_rate": 0.00029455445544554455,
      "loss": 1.2053,
      "step": 2425
    },
    {
      "epoch": 16.03304419661297,
      "grad_norm": 4.7576212882995605,
      "learning_rate": 0.00029405940594059407,
      "loss": 1.4965,
      "step": 2426
    },
    {
      "epoch": 16.039653035935565,
      "grad_norm": 19.80862045288086,
      "learning_rate": 0.00029356435643564354,
      "loss": 1.4665,
      "step": 2427
    },
    {
      "epoch": 16.046261875258157,
      "grad_norm": 13.878962516784668,
      "learning_rate": 0.00029306930693069307,
      "loss": 0.7195,
      "step": 2428
    },
    {
      "epoch": 16.052870714580752,
      "grad_norm": 33.17961120605469,
      "learning_rate": 0.00029257425742574254,
      "loss": 2.0808,
      "step": 2429
    },
    {
      "epoch": 16.059479553903344,
      "grad_norm": 19.3549747467041,
      "learning_rate": 0.00029207920792079207,
      "loss": 1.3702,
      "step": 2430
    },
    {
      "epoch": 16.06608839322594,
      "grad_norm": 7.322460651397705,
      "learning_rate": 0.0002915841584158416,
      "loss": 0.5056,
      "step": 2431
    },
    {
      "epoch": 16.072697232548535,
      "grad_norm": 11.612974166870117,
      "learning_rate": 0.00029108910891089107,
      "loss": 0.8311,
      "step": 2432
    },
    {
      "epoch": 16.079306071871127,
      "grad_norm": 15.78592300415039,
      "learning_rate": 0.0002905940594059406,
      "loss": 1.9522,
      "step": 2433
    },
    {
      "epoch": 16.085914911193722,
      "grad_norm": 6.413180351257324,
      "learning_rate": 0.00029009900990099006,
      "loss": 0.5359,
      "step": 2434
    },
    {
      "epoch": 16.092523750516314,
      "grad_norm": 14.529219627380371,
      "learning_rate": 0.0002896039603960396,
      "loss": 1.7957,
      "step": 2435
    },
    {
      "epoch": 16.09913258983891,
      "grad_norm": 21.243345260620117,
      "learning_rate": 0.0002891089108910891,
      "loss": 1.2492,
      "step": 2436
    },
    {
      "epoch": 16.105741429161505,
      "grad_norm": 18.000411987304688,
      "learning_rate": 0.0002886138613861386,
      "loss": 0.8603,
      "step": 2437
    },
    {
      "epoch": 16.112350268484096,
      "grad_norm": 36.8470344543457,
      "learning_rate": 0.0002881188118811881,
      "loss": 2.1124,
      "step": 2438
    },
    {
      "epoch": 16.118959107806692,
      "grad_norm": 25.075376510620117,
      "learning_rate": 0.0002876237623762376,
      "loss": 2.2194,
      "step": 2439
    },
    {
      "epoch": 16.125567947129284,
      "grad_norm": 11.399089813232422,
      "learning_rate": 0.0002871287128712871,
      "loss": 0.3112,
      "step": 2440
    },
    {
      "epoch": 16.13217678645188,
      "grad_norm": 3.5397636890411377,
      "learning_rate": 0.00028663366336633664,
      "loss": 1.7773,
      "step": 2441
    },
    {
      "epoch": 16.138785625774474,
      "grad_norm": 2.2567811012268066,
      "learning_rate": 0.0002861386138613861,
      "loss": 2.0045,
      "step": 2442
    },
    {
      "epoch": 16.145394465097066,
      "grad_norm": 16.18624496459961,
      "learning_rate": 0.00028564356435643564,
      "loss": 1.7044,
      "step": 2443
    },
    {
      "epoch": 16.15200330441966,
      "grad_norm": 37.10255432128906,
      "learning_rate": 0.0002851485148514851,
      "loss": 3.2798,
      "step": 2444
    },
    {
      "epoch": 16.158612143742257,
      "grad_norm": 14.645445823669434,
      "learning_rate": 0.00028465346534653464,
      "loss": 0.7705,
      "step": 2445
    },
    {
      "epoch": 16.16522098306485,
      "grad_norm": 9.59262466430664,
      "learning_rate": 0.00028415841584158416,
      "loss": 0.6939,
      "step": 2446
    },
    {
      "epoch": 16.171829822387444,
      "grad_norm": 2.8325459957122803,
      "learning_rate": 0.00028366336633663363,
      "loss": 1.0615,
      "step": 2447
    },
    {
      "epoch": 16.178438661710036,
      "grad_norm": 8.473731994628906,
      "learning_rate": 0.00028316831683168316,
      "loss": 0.8838,
      "step": 2448
    },
    {
      "epoch": 16.18504750103263,
      "grad_norm": 1.130969524383545,
      "learning_rate": 0.00028267326732673263,
      "loss": 0.5525,
      "step": 2449
    },
    {
      "epoch": 16.191656340355227,
      "grad_norm": 3.5029776096343994,
      "learning_rate": 0.00028217821782178216,
      "loss": 1.2272,
      "step": 2450
    },
    {
      "epoch": 16.19826517967782,
      "grad_norm": 20.332731246948242,
      "learning_rate": 0.0002816831683168317,
      "loss": 1.2976,
      "step": 2451
    },
    {
      "epoch": 16.204874019000414,
      "grad_norm": 9.602639198303223,
      "learning_rate": 0.0002811881188118812,
      "loss": 1.8666,
      "step": 2452
    },
    {
      "epoch": 16.211482858323006,
      "grad_norm": 4.90816068649292,
      "learning_rate": 0.00028069306930693074,
      "loss": 0.5714,
      "step": 2453
    },
    {
      "epoch": 16.2180916976456,
      "grad_norm": 12.678507804870605,
      "learning_rate": 0.0002801980198019802,
      "loss": 1.5184,
      "step": 2454
    },
    {
      "epoch": 16.224700536968196,
      "grad_norm": 10.588314056396484,
      "learning_rate": 0.00027970297029702973,
      "loss": 0.7859,
      "step": 2455
    },
    {
      "epoch": 16.23130937629079,
      "grad_norm": 4.812167644500732,
      "learning_rate": 0.00027920792079207926,
      "loss": 1.3916,
      "step": 2456
    },
    {
      "epoch": 16.237918215613384,
      "grad_norm": 9.233129501342773,
      "learning_rate": 0.00027871287128712873,
      "loss": 0.5229,
      "step": 2457
    },
    {
      "epoch": 16.244527054935975,
      "grad_norm": 6.329776287078857,
      "learning_rate": 0.00027821782178217826,
      "loss": 2.4428,
      "step": 2458
    },
    {
      "epoch": 16.25113589425857,
      "grad_norm": 1.4734020233154297,
      "learning_rate": 0.00027772277227722773,
      "loss": 0.4726,
      "step": 2459
    },
    {
      "epoch": 16.257744733581166,
      "grad_norm": 15.923526763916016,
      "learning_rate": 0.00027722772277227726,
      "loss": 0.7092,
      "step": 2460
    },
    {
      "epoch": 16.264353572903758,
      "grad_norm": 5.265707969665527,
      "learning_rate": 0.0002767326732673268,
      "loss": 0.8746,
      "step": 2461
    },
    {
      "epoch": 16.270962412226353,
      "grad_norm": 9.180636405944824,
      "learning_rate": 0.00027623762376237626,
      "loss": 1.1529,
      "step": 2462
    },
    {
      "epoch": 16.277571251548945,
      "grad_norm": 5.533848285675049,
      "learning_rate": 0.0002757425742574258,
      "loss": 0.809,
      "step": 2463
    },
    {
      "epoch": 16.28418009087154,
      "grad_norm": 2.0605411529541016,
      "learning_rate": 0.00027524752475247525,
      "loss": 1.0312,
      "step": 2464
    },
    {
      "epoch": 16.290788930194136,
      "grad_norm": 6.492859363555908,
      "learning_rate": 0.0002747524752475248,
      "loss": 2.1582,
      "step": 2465
    },
    {
      "epoch": 16.297397769516728,
      "grad_norm": 3.020263671875,
      "learning_rate": 0.0002742574257425743,
      "loss": 1.0007,
      "step": 2466
    },
    {
      "epoch": 16.304006608839323,
      "grad_norm": 19.27847671508789,
      "learning_rate": 0.0002737623762376238,
      "loss": 3.3895,
      "step": 2467
    },
    {
      "epoch": 16.310615448161915,
      "grad_norm": 12.754088401794434,
      "learning_rate": 0.0002732673267326733,
      "loss": 2.3484,
      "step": 2468
    },
    {
      "epoch": 16.31722428748451,
      "grad_norm": 11.343659400939941,
      "learning_rate": 0.0002727722772277228,
      "loss": 1.431,
      "step": 2469
    },
    {
      "epoch": 16.323833126807106,
      "grad_norm": 4.084343910217285,
      "learning_rate": 0.0002722772277227723,
      "loss": 1.2079,
      "step": 2470
    },
    {
      "epoch": 16.330441966129698,
      "grad_norm": 14.799668312072754,
      "learning_rate": 0.00027178217821782183,
      "loss": 1.2485,
      "step": 2471
    },
    {
      "epoch": 16.337050805452293,
      "grad_norm": 5.823192119598389,
      "learning_rate": 0.0002712871287128713,
      "loss": 2.435,
      "step": 2472
    },
    {
      "epoch": 16.343659644774885,
      "grad_norm": 13.49541187286377,
      "learning_rate": 0.0002707920792079208,
      "loss": 2.0084,
      "step": 2473
    },
    {
      "epoch": 16.35026848409748,
      "grad_norm": 2.8296921253204346,
      "learning_rate": 0.0002702970297029703,
      "loss": 1.633,
      "step": 2474
    },
    {
      "epoch": 16.356877323420075,
      "grad_norm": 6.7085347175598145,
      "learning_rate": 0.0002698019801980198,
      "loss": 0.8178,
      "step": 2475
    },
    {
      "epoch": 16.363486162742667,
      "grad_norm": 15.692901611328125,
      "learning_rate": 0.00026930693069306935,
      "loss": 0.8737,
      "step": 2476
    },
    {
      "epoch": 16.370095002065263,
      "grad_norm": 16.932893753051758,
      "learning_rate": 0.0002688118811881188,
      "loss": 0.6869,
      "step": 2477
    },
    {
      "epoch": 16.376703841387855,
      "grad_norm": 22.077268600463867,
      "learning_rate": 0.00026831683168316835,
      "loss": 1.8312,
      "step": 2478
    },
    {
      "epoch": 16.38331268071045,
      "grad_norm": 14.381365776062012,
      "learning_rate": 0.0002678217821782178,
      "loss": 0.7862,
      "step": 2479
    },
    {
      "epoch": 16.389921520033045,
      "grad_norm": 11.500038146972656,
      "learning_rate": 0.00026732673267326735,
      "loss": 1.3906,
      "step": 2480
    },
    {
      "epoch": 16.396530359355637,
      "grad_norm": 8.884407043457031,
      "learning_rate": 0.0002668316831683169,
      "loss": 0.9186,
      "step": 2481
    },
    {
      "epoch": 16.403139198678232,
      "grad_norm": 1.2439073324203491,
      "learning_rate": 0.00026633663366336635,
      "loss": 0.9447,
      "step": 2482
    },
    {
      "epoch": 16.409748038000828,
      "grad_norm": 6.2672438621521,
      "learning_rate": 0.00026584158415841587,
      "loss": 1.7707,
      "step": 2483
    },
    {
      "epoch": 16.41635687732342,
      "grad_norm": 8.23673152923584,
      "learning_rate": 0.00026534653465346534,
      "loss": 1.4323,
      "step": 2484
    },
    {
      "epoch": 16.422965716646015,
      "grad_norm": 5.573929786682129,
      "learning_rate": 0.00026485148514851487,
      "loss": 1.8397,
      "step": 2485
    },
    {
      "epoch": 16.429574555968607,
      "grad_norm": 11.456242561340332,
      "learning_rate": 0.0002643564356435644,
      "loss": 1.1926,
      "step": 2486
    },
    {
      "epoch": 16.436183395291202,
      "grad_norm": 12.667000770568848,
      "learning_rate": 0.00026386138613861387,
      "loss": 0.4982,
      "step": 2487
    },
    {
      "epoch": 16.442792234613798,
      "grad_norm": 11.798934936523438,
      "learning_rate": 0.0002633663366336634,
      "loss": 1.3014,
      "step": 2488
    },
    {
      "epoch": 16.44940107393639,
      "grad_norm": 6.905542850494385,
      "learning_rate": 0.00026287128712871287,
      "loss": 1.3052,
      "step": 2489
    },
    {
      "epoch": 16.456009913258985,
      "grad_norm": 1.5432454347610474,
      "learning_rate": 0.0002623762376237624,
      "loss": 0.4664,
      "step": 2490
    },
    {
      "epoch": 16.462618752581577,
      "grad_norm": 2.28084397315979,
      "learning_rate": 0.0002618811881188119,
      "loss": 1.5959,
      "step": 2491
    },
    {
      "epoch": 16.469227591904172,
      "grad_norm": 2.9366555213928223,
      "learning_rate": 0.0002613861386138614,
      "loss": 1.5282,
      "step": 2492
    },
    {
      "epoch": 16.475836431226767,
      "grad_norm": 3.7582461833953857,
      "learning_rate": 0.0002608910891089109,
      "loss": 5.6617,
      "step": 2493
    },
    {
      "epoch": 16.48244527054936,
      "grad_norm": 5.03102970123291,
      "learning_rate": 0.0002603960396039604,
      "loss": 1.4532,
      "step": 2494
    },
    {
      "epoch": 16.489054109871955,
      "grad_norm": 2.3712449073791504,
      "learning_rate": 0.0002599009900990099,
      "loss": 0.5032,
      "step": 2495
    },
    {
      "epoch": 16.495662949194546,
      "grad_norm": 2.9533791542053223,
      "learning_rate": 0.00025940594059405944,
      "loss": 1.1131,
      "step": 2496
    },
    {
      "epoch": 16.50227178851714,
      "grad_norm": 1.5261934995651245,
      "learning_rate": 0.0002589108910891089,
      "loss": 0.3683,
      "step": 2497
    },
    {
      "epoch": 16.508880627839737,
      "grad_norm": 2.657360315322876,
      "learning_rate": 0.00025841584158415844,
      "loss": 0.7762,
      "step": 2498
    },
    {
      "epoch": 16.51548946716233,
      "grad_norm": 2.301116466522217,
      "learning_rate": 0.0002579207920792079,
      "loss": 0.916,
      "step": 2499
    },
    {
      "epoch": 16.522098306484924,
      "grad_norm": 2.371628522872925,
      "learning_rate": 0.00025742574257425744,
      "loss": 0.6121,
      "step": 2500
    },
    {
      "epoch": 16.528707145807516,
      "grad_norm": 9.014091491699219,
      "learning_rate": 0.00025693069306930696,
      "loss": 1.1644,
      "step": 2501
    },
    {
      "epoch": 16.53531598513011,
      "grad_norm": 1.7391644716262817,
      "learning_rate": 0.00025643564356435644,
      "loss": 1.2588,
      "step": 2502
    },
    {
      "epoch": 16.541924824452707,
      "grad_norm": 11.475360870361328,
      "learning_rate": 0.00025594059405940596,
      "loss": 1.3475,
      "step": 2503
    },
    {
      "epoch": 16.5485336637753,
      "grad_norm": 5.491511821746826,
      "learning_rate": 0.00025544554455445543,
      "loss": 1.2881,
      "step": 2504
    },
    {
      "epoch": 16.555142503097894,
      "grad_norm": 5.202719211578369,
      "learning_rate": 0.00025495049504950496,
      "loss": 0.9119,
      "step": 2505
    },
    {
      "epoch": 16.561751342420486,
      "grad_norm": 8.85570240020752,
      "learning_rate": 0.0002544554455445545,
      "loss": 0.7565,
      "step": 2506
    },
    {
      "epoch": 16.56836018174308,
      "grad_norm": 2.8536558151245117,
      "learning_rate": 0.00025396039603960396,
      "loss": 1.1554,
      "step": 2507
    },
    {
      "epoch": 16.574969021065677,
      "grad_norm": 6.190672874450684,
      "learning_rate": 0.0002534653465346535,
      "loss": 0.682,
      "step": 2508
    },
    {
      "epoch": 16.58157786038827,
      "grad_norm": 3.229694128036499,
      "learning_rate": 0.00025297029702970296,
      "loss": 0.9011,
      "step": 2509
    },
    {
      "epoch": 16.588186699710864,
      "grad_norm": 12.951083183288574,
      "learning_rate": 0.0002524752475247525,
      "loss": 1.5112,
      "step": 2510
    },
    {
      "epoch": 16.594795539033456,
      "grad_norm": 8.208761215209961,
      "learning_rate": 0.000251980198019802,
      "loss": 0.5948,
      "step": 2511
    },
    {
      "epoch": 16.60140437835605,
      "grad_norm": 11.515442848205566,
      "learning_rate": 0.0002514851485148515,
      "loss": 2.6824,
      "step": 2512
    },
    {
      "epoch": 16.608013217678646,
      "grad_norm": 8.750266075134277,
      "learning_rate": 0.000250990099009901,
      "loss": 1.0661,
      "step": 2513
    },
    {
      "epoch": 16.614622057001238,
      "grad_norm": 6.8770012855529785,
      "learning_rate": 0.0002504950495049505,
      "loss": 1.5853,
      "step": 2514
    },
    {
      "epoch": 16.621230896323834,
      "grad_norm": 9.887038230895996,
      "learning_rate": 0.00025,
      "loss": 0.5456,
      "step": 2515
    },
    {
      "epoch": 16.627839735646425,
      "grad_norm": 8.491121292114258,
      "learning_rate": 0.00024950495049504953,
      "loss": 1.954,
      "step": 2516
    },
    {
      "epoch": 16.63444857496902,
      "grad_norm": 12.060188293457031,
      "learning_rate": 0.000249009900990099,
      "loss": 1.0273,
      "step": 2517
    },
    {
      "epoch": 16.641057414291616,
      "grad_norm": 1.7515767812728882,
      "learning_rate": 0.00024851485148514853,
      "loss": 1.5683,
      "step": 2518
    },
    {
      "epoch": 16.647666253614208,
      "grad_norm": 23.24473762512207,
      "learning_rate": 0.000248019801980198,
      "loss": 0.9286,
      "step": 2519
    },
    {
      "epoch": 16.654275092936803,
      "grad_norm": 11.699479103088379,
      "learning_rate": 0.00024752475247524753,
      "loss": 0.8513,
      "step": 2520
    },
    {
      "epoch": 16.660883932259395,
      "grad_norm": 28.310686111450195,
      "learning_rate": 0.00024702970297029705,
      "loss": 1.5803,
      "step": 2521
    },
    {
      "epoch": 16.66749277158199,
      "grad_norm": 16.458322525024414,
      "learning_rate": 0.0002465346534653465,
      "loss": 1.9003,
      "step": 2522
    },
    {
      "epoch": 16.674101610904586,
      "grad_norm": 8.765401840209961,
      "learning_rate": 0.00024603960396039605,
      "loss": 0.682,
      "step": 2523
    },
    {
      "epoch": 16.680710450227178,
      "grad_norm": 3.596118688583374,
      "learning_rate": 0.0002455445544554455,
      "loss": 1.0217,
      "step": 2524
    },
    {
      "epoch": 16.687319289549773,
      "grad_norm": 3.294776678085327,
      "learning_rate": 0.00024504950495049505,
      "loss": 1.2565,
      "step": 2525
    },
    {
      "epoch": 16.69392812887237,
      "grad_norm": 6.709600925445557,
      "learning_rate": 0.0002445544554455446,
      "loss": 0.806,
      "step": 2526
    },
    {
      "epoch": 16.70053696819496,
      "grad_norm": 12.572683334350586,
      "learning_rate": 0.00024405940594059405,
      "loss": 2.5463,
      "step": 2527
    },
    {
      "epoch": 16.707145807517556,
      "grad_norm": 6.36482572555542,
      "learning_rate": 0.00024356435643564357,
      "loss": 1.4767,
      "step": 2528
    },
    {
      "epoch": 16.713754646840147,
      "grad_norm": 10.68578052520752,
      "learning_rate": 0.00024306930693069307,
      "loss": 2.0673,
      "step": 2529
    },
    {
      "epoch": 16.720363486162743,
      "grad_norm": 16.797775268554688,
      "learning_rate": 0.00024257425742574257,
      "loss": 1.8713,
      "step": 2530
    },
    {
      "epoch": 16.726972325485338,
      "grad_norm": 14.111564636230469,
      "learning_rate": 0.00024207920792079207,
      "loss": 2.4397,
      "step": 2531
    },
    {
      "epoch": 16.73358116480793,
      "grad_norm": 1.1268246173858643,
      "learning_rate": 0.00024158415841584157,
      "loss": 0.95,
      "step": 2532
    },
    {
      "epoch": 16.740190004130525,
      "grad_norm": 2.300579071044922,
      "learning_rate": 0.0002410891089108911,
      "loss": 0.7614,
      "step": 2533
    },
    {
      "epoch": 16.746798843453117,
      "grad_norm": 6.872528076171875,
      "learning_rate": 0.0002405940594059406,
      "loss": 0.848,
      "step": 2534
    },
    {
      "epoch": 16.753407682775713,
      "grad_norm": 9.167513847351074,
      "learning_rate": 0.0002400990099009901,
      "loss": 1.1836,
      "step": 2535
    },
    {
      "epoch": 16.760016522098308,
      "grad_norm": 6.804056644439697,
      "learning_rate": 0.0002396039603960396,
      "loss": 0.5466,
      "step": 2536
    },
    {
      "epoch": 16.7666253614209,
      "grad_norm": 4.902860164642334,
      "learning_rate": 0.0002391089108910891,
      "loss": 0.5722,
      "step": 2537
    },
    {
      "epoch": 16.773234200743495,
      "grad_norm": 3.8114757537841797,
      "learning_rate": 0.00023861386138613862,
      "loss": 0.7982,
      "step": 2538
    },
    {
      "epoch": 16.779843040066087,
      "grad_norm": 3.401721239089966,
      "learning_rate": 0.00023811881188118812,
      "loss": 0.6718,
      "step": 2539
    },
    {
      "epoch": 16.786451879388682,
      "grad_norm": 12.62507152557373,
      "learning_rate": 0.00023762376237623762,
      "loss": 1.5728,
      "step": 2540
    },
    {
      "epoch": 16.793060718711278,
      "grad_norm": 2.1584279537200928,
      "learning_rate": 0.00023712871287128712,
      "loss": 0.8489,
      "step": 2541
    },
    {
      "epoch": 16.79966955803387,
      "grad_norm": 13.07945442199707,
      "learning_rate": 0.00023663366336633662,
      "loss": 0.6794,
      "step": 2542
    },
    {
      "epoch": 16.806278397356465,
      "grad_norm": 7.071303844451904,
      "learning_rate": 0.00023613861386138614,
      "loss": 0.3946,
      "step": 2543
    },
    {
      "epoch": 16.812887236679057,
      "grad_norm": 19.42050552368164,
      "learning_rate": 0.00023564356435643564,
      "loss": 1.1905,
      "step": 2544
    },
    {
      "epoch": 16.819496076001652,
      "grad_norm": 16.355972290039062,
      "learning_rate": 0.00023514851485148514,
      "loss": 1.0743,
      "step": 2545
    },
    {
      "epoch": 16.826104915324247,
      "grad_norm": 16.655668258666992,
      "learning_rate": 0.00023465346534653464,
      "loss": 1.2697,
      "step": 2546
    },
    {
      "epoch": 16.83271375464684,
      "grad_norm": 4.104165554046631,
      "learning_rate": 0.00023415841584158417,
      "loss": 2.063,
      "step": 2547
    },
    {
      "epoch": 16.839322593969435,
      "grad_norm": 10.45448112487793,
      "learning_rate": 0.0002336633663366337,
      "loss": 5.5794,
      "step": 2548
    },
    {
      "epoch": 16.845931433292026,
      "grad_norm": 9.829627990722656,
      "learning_rate": 0.0002331683168316832,
      "loss": 0.8508,
      "step": 2549
    },
    {
      "epoch": 16.852540272614622,
      "grad_norm": 2.802558183670044,
      "learning_rate": 0.0002326732673267327,
      "loss": 0.6442,
      "step": 2550
    },
    {
      "epoch": 16.859149111937217,
      "grad_norm": 2.607218027114868,
      "learning_rate": 0.0002321782178217822,
      "loss": 1.4503,
      "step": 2551
    },
    {
      "epoch": 16.86575795125981,
      "grad_norm": 6.536892414093018,
      "learning_rate": 0.0002316831683168317,
      "loss": 0.6297,
      "step": 2552
    },
    {
      "epoch": 16.872366790582404,
      "grad_norm": 2.3864991664886475,
      "learning_rate": 0.00023118811881188121,
      "loss": 1.1182,
      "step": 2553
    },
    {
      "epoch": 16.878975629904996,
      "grad_norm": 5.073830604553223,
      "learning_rate": 0.00023069306930693071,
      "loss": 1.0517,
      "step": 2554
    },
    {
      "epoch": 16.88558446922759,
      "grad_norm": 6.442938327789307,
      "learning_rate": 0.0002301980198019802,
      "loss": 1.2449,
      "step": 2555
    },
    {
      "epoch": 16.892193308550187,
      "grad_norm": 2.5335936546325684,
      "learning_rate": 0.0002297029702970297,
      "loss": 1.5266,
      "step": 2556
    },
    {
      "epoch": 16.89880214787278,
      "grad_norm": 10.519699096679688,
      "learning_rate": 0.0002292079207920792,
      "loss": 2.0951,
      "step": 2557
    },
    {
      "epoch": 16.905410987195374,
      "grad_norm": 7.960129737854004,
      "learning_rate": 0.00022871287128712874,
      "loss": 1.1263,
      "step": 2558
    },
    {
      "epoch": 16.91201982651797,
      "grad_norm": 17.21173667907715,
      "learning_rate": 0.00022821782178217824,
      "loss": 1.0668,
      "step": 2559
    },
    {
      "epoch": 16.91862866584056,
      "grad_norm": 21.744548797607422,
      "learning_rate": 0.00022772277227722774,
      "loss": 1.3041,
      "step": 2560
    },
    {
      "epoch": 16.925237505163157,
      "grad_norm": 22.342458724975586,
      "learning_rate": 0.00022722772277227723,
      "loss": 1.2844,
      "step": 2561
    },
    {
      "epoch": 16.93184634448575,
      "grad_norm": 2.454719066619873,
      "learning_rate": 0.00022673267326732673,
      "loss": 0.9715,
      "step": 2562
    },
    {
      "epoch": 16.938455183808344,
      "grad_norm": 2.2273759841918945,
      "learning_rate": 0.00022623762376237626,
      "loss": 1.0639,
      "step": 2563
    },
    {
      "epoch": 16.94506402313094,
      "grad_norm": 5.665536403656006,
      "learning_rate": 0.00022574257425742576,
      "loss": 1.041,
      "step": 2564
    },
    {
      "epoch": 16.95167286245353,
      "grad_norm": 5.648093223571777,
      "learning_rate": 0.00022524752475247526,
      "loss": 1.141,
      "step": 2565
    },
    {
      "epoch": 16.958281701776126,
      "grad_norm": 3.9250295162200928,
      "learning_rate": 0.00022475247524752476,
      "loss": 0.8598,
      "step": 2566
    },
    {
      "epoch": 16.96489054109872,
      "grad_norm": 7.255314826965332,
      "learning_rate": 0.00022425742574257426,
      "loss": 1.6317,
      "step": 2567
    },
    {
      "epoch": 16.971499380421314,
      "grad_norm": 1.8470349311828613,
      "learning_rate": 0.00022376237623762378,
      "loss": 0.5022,
      "step": 2568
    },
    {
      "epoch": 16.97810821974391,
      "grad_norm": 4.01727819442749,
      "learning_rate": 0.00022326732673267328,
      "loss": 1.15,
      "step": 2569
    },
    {
      "epoch": 16.9847170590665,
      "grad_norm": 1.6393179893493652,
      "learning_rate": 0.00022277227722772278,
      "loss": 0.5945,
      "step": 2570
    },
    {
      "epoch": 16.991325898389096,
      "grad_norm": 12.663661003112793,
      "learning_rate": 0.00022227722772277228,
      "loss": 1.2846,
      "step": 2571
    },
    {
      "epoch": 16.997934737711688,
      "grad_norm": 4.527017593383789,
      "learning_rate": 0.00022178217821782178,
      "loss": 1.313,
      "step": 2572
    },
    {
      "epoch": 16.997934737711688,
      "eval_validation_error_bar": 0.036759654910217474,
      "eval_validation_loss": 3.9460580348968506,
      "eval_validation_pearsonr": 0.7234245633565047,
      "eval_validation_rmse": 1.98646879196167,
      "eval_validation_runtime": 28.8212,
      "eval_validation_samples_per_second": 7.043,
      "eval_validation_spearman": 0.7338724716713607,
      "eval_validation_steps_per_second": 7.043,
      "step": 2572
    },
    {
      "epoch": 16.997934737711688,
      "eval_test_error_bar": 0.033483554444088154,
      "eval_test_loss": 5.430423736572266,
      "eval_test_pearsonr": 0.6600374201362199,
      "eval_test_rmse": 2.330327033996582,
      "eval_test_runtime": 46.9009,
      "eval_test_samples_per_second": 6.951,
      "eval_test_spearman": 0.6760745027819083,
      "eval_test_steps_per_second": 6.951,
      "step": 2572
    },
    {
      "epoch": 17.004543577034283,
      "grad_norm": 2.3855857849121094,
      "learning_rate": 0.0002212871287128713,
      "loss": 0.4733,
      "step": 2573
    },
    {
      "epoch": 17.01115241635688,
      "grad_norm": 9.54336166381836,
      "learning_rate": 0.0002207920792079208,
      "loss": 1.361,
      "step": 2574
    },
    {
      "epoch": 17.01776125567947,
      "grad_norm": 24.521825790405273,
      "learning_rate": 0.0002202970297029703,
      "loss": 2.3536,
      "step": 2575
    },
    {
      "epoch": 17.024370095002066,
      "grad_norm": 1.1249475479125977,
      "learning_rate": 0.0002198019801980198,
      "loss": 0.5385,
      "step": 2576
    },
    {
      "epoch": 17.030978934324658,
      "grad_norm": 8.250762939453125,
      "learning_rate": 0.0002193069306930693,
      "loss": 0.5122,
      "step": 2577
    },
    {
      "epoch": 17.037587773647253,
      "grad_norm": 11.795316696166992,
      "learning_rate": 0.00021881188118811883,
      "loss": 0.9074,
      "step": 2578
    },
    {
      "epoch": 17.04419661296985,
      "grad_norm": 11.837672233581543,
      "learning_rate": 0.00021831683168316833,
      "loss": 0.4866,
      "step": 2579
    },
    {
      "epoch": 17.05080545229244,
      "grad_norm": 5.907211780548096,
      "learning_rate": 0.00021782178217821783,
      "loss": 1.1819,
      "step": 2580
    },
    {
      "epoch": 17.057414291615036,
      "grad_norm": 5.683156967163086,
      "learning_rate": 0.00021732673267326732,
      "loss": 0.7319,
      "step": 2581
    },
    {
      "epoch": 17.064023130937628,
      "grad_norm": 7.97827672958374,
      "learning_rate": 0.00021683168316831682,
      "loss": 0.8324,
      "step": 2582
    },
    {
      "epoch": 17.070631970260223,
      "grad_norm": 9.951884269714355,
      "learning_rate": 0.00021633663366336635,
      "loss": 0.7061,
      "step": 2583
    },
    {
      "epoch": 17.07724080958282,
      "grad_norm": 7.866006374359131,
      "learning_rate": 0.00021584158415841585,
      "loss": 1.226,
      "step": 2584
    },
    {
      "epoch": 17.08384964890541,
      "grad_norm": 10.90846061706543,
      "learning_rate": 0.00021534653465346535,
      "loss": 1.52,
      "step": 2585
    },
    {
      "epoch": 17.090458488228006,
      "grad_norm": 4.181757926940918,
      "learning_rate": 0.00021485148514851485,
      "loss": 0.6865,
      "step": 2586
    },
    {
      "epoch": 17.097067327550597,
      "grad_norm": 2.8806378841400146,
      "learning_rate": 0.00021435643564356435,
      "loss": 0.4348,
      "step": 2587
    },
    {
      "epoch": 17.103676166873193,
      "grad_norm": 18.932405471801758,
      "learning_rate": 0.00021386138613861387,
      "loss": 1.2826,
      "step": 2588
    },
    {
      "epoch": 17.110285006195788,
      "grad_norm": 11.185441970825195,
      "learning_rate": 0.00021336633663366337,
      "loss": 1.256,
      "step": 2589
    },
    {
      "epoch": 17.11689384551838,
      "grad_norm": 4.897912979125977,
      "learning_rate": 0.00021287128712871287,
      "loss": 1.0897,
      "step": 2590
    },
    {
      "epoch": 17.123502684840975,
      "grad_norm": 4.49355411529541,
      "learning_rate": 0.00021237623762376237,
      "loss": 2.7255,
      "step": 2591
    },
    {
      "epoch": 17.130111524163567,
      "grad_norm": 17.554349899291992,
      "learning_rate": 0.00021188118811881187,
      "loss": 0.6356,
      "step": 2592
    },
    {
      "epoch": 17.136720363486162,
      "grad_norm": 3.9949605464935303,
      "learning_rate": 0.0002113861386138614,
      "loss": 0.9197,
      "step": 2593
    },
    {
      "epoch": 17.143329202808758,
      "grad_norm": 4.174068450927734,
      "learning_rate": 0.0002108910891089109,
      "loss": 3.388,
      "step": 2594
    },
    {
      "epoch": 17.14993804213135,
      "grad_norm": 7.233972549438477,
      "learning_rate": 0.0002103960396039604,
      "loss": 0.7044,
      "step": 2595
    },
    {
      "epoch": 17.156546881453945,
      "grad_norm": 1.241843342781067,
      "learning_rate": 0.0002099009900990099,
      "loss": 1.0126,
      "step": 2596
    },
    {
      "epoch": 17.163155720776537,
      "grad_norm": 9.706116676330566,
      "learning_rate": 0.0002094059405940594,
      "loss": 3.0897,
      "step": 2597
    },
    {
      "epoch": 17.169764560099132,
      "grad_norm": 3.053022623062134,
      "learning_rate": 0.00020891089108910892,
      "loss": 0.7513,
      "step": 2598
    },
    {
      "epoch": 17.176373399421728,
      "grad_norm": 7.064605712890625,
      "learning_rate": 0.00020841584158415842,
      "loss": 1.3952,
      "step": 2599
    },
    {
      "epoch": 17.18298223874432,
      "grad_norm": 2.9030117988586426,
      "learning_rate": 0.00020792079207920792,
      "loss": 0.707,
      "step": 2600
    },
    {
      "epoch": 17.189591078066915,
      "grad_norm": 10.455584526062012,
      "learning_rate": 0.00020742574257425741,
      "loss": 2.1734,
      "step": 2601
    },
    {
      "epoch": 17.19619991738951,
      "grad_norm": 32.328773498535156,
      "learning_rate": 0.00020693069306930691,
      "loss": 3.8562,
      "step": 2602
    },
    {
      "epoch": 17.202808756712102,
      "grad_norm": 3.5405843257904053,
      "learning_rate": 0.00020643564356435644,
      "loss": 0.8396,
      "step": 2603
    },
    {
      "epoch": 17.209417596034697,
      "grad_norm": 5.189692974090576,
      "learning_rate": 0.00020594059405940594,
      "loss": 1.0671,
      "step": 2604
    },
    {
      "epoch": 17.21602643535729,
      "grad_norm": 9.576248168945312,
      "learning_rate": 0.00020544554455445544,
      "loss": 1.0564,
      "step": 2605
    },
    {
      "epoch": 17.222635274679885,
      "grad_norm": 5.886253356933594,
      "learning_rate": 0.00020495049504950494,
      "loss": 0.6382,
      "step": 2606
    },
    {
      "epoch": 17.22924411400248,
      "grad_norm": 11.704907417297363,
      "learning_rate": 0.00020445544554455444,
      "loss": 0.7915,
      "step": 2607
    },
    {
      "epoch": 17.23585295332507,
      "grad_norm": 4.962289333343506,
      "learning_rate": 0.00020396039603960396,
      "loss": 0.8726,
      "step": 2608
    },
    {
      "epoch": 17.242461792647667,
      "grad_norm": 8.119100570678711,
      "learning_rate": 0.00020346534653465346,
      "loss": 2.4679,
      "step": 2609
    },
    {
      "epoch": 17.24907063197026,
      "grad_norm": 4.527313709259033,
      "learning_rate": 0.000202970297029703,
      "loss": 0.6472,
      "step": 2610
    },
    {
      "epoch": 17.255679471292854,
      "grad_norm": 4.200290679931641,
      "learning_rate": 0.0002024752475247525,
      "loss": 0.8722,
      "step": 2611
    },
    {
      "epoch": 17.26228831061545,
      "grad_norm": 6.8470354080200195,
      "learning_rate": 0.00020198019801980199,
      "loss": 0.6277,
      "step": 2612
    },
    {
      "epoch": 17.26889714993804,
      "grad_norm": 7.6300201416015625,
      "learning_rate": 0.0002014851485148515,
      "loss": 1.1371,
      "step": 2613
    },
    {
      "epoch": 17.275505989260637,
      "grad_norm": 18.84584617614746,
      "learning_rate": 0.000200990099009901,
      "loss": 1.5992,
      "step": 2614
    },
    {
      "epoch": 17.28211482858323,
      "grad_norm": 10.538581848144531,
      "learning_rate": 0.0002004950495049505,
      "loss": 0.5813,
      "step": 2615
    },
    {
      "epoch": 17.288723667905824,
      "grad_norm": 1.970387578010559,
      "learning_rate": 0.0002,
      "loss": 0.8897,
      "step": 2616
    },
    {
      "epoch": 17.29533250722842,
      "grad_norm": 3.6933414936065674,
      "learning_rate": 0.0001995049504950495,
      "loss": 1.0674,
      "step": 2617
    },
    {
      "epoch": 17.30194134655101,
      "grad_norm": 17.860013961791992,
      "learning_rate": 0.00019900990099009903,
      "loss": 0.9453,
      "step": 2618
    },
    {
      "epoch": 17.308550185873607,
      "grad_norm": 18.47412109375,
      "learning_rate": 0.00019851485148514853,
      "loss": 0.935,
      "step": 2619
    },
    {
      "epoch": 17.3151590251962,
      "grad_norm": 5.089444637298584,
      "learning_rate": 0.00019801980198019803,
      "loss": 1.0089,
      "step": 2620
    },
    {
      "epoch": 17.321767864518794,
      "grad_norm": 1.3479663133621216,
      "learning_rate": 0.00019752475247524753,
      "loss": 0.8376,
      "step": 2621
    },
    {
      "epoch": 17.32837670384139,
      "grad_norm": 1.834100365638733,
      "learning_rate": 0.00019702970297029703,
      "loss": 0.83,
      "step": 2622
    },
    {
      "epoch": 17.33498554316398,
      "grad_norm": 2.9935147762298584,
      "learning_rate": 0.00019653465346534656,
      "loss": 0.7321,
      "step": 2623
    },
    {
      "epoch": 17.341594382486576,
      "grad_norm": 8.77296257019043,
      "learning_rate": 0.00019603960396039606,
      "loss": 1.5386,
      "step": 2624
    },
    {
      "epoch": 17.348203221809168,
      "grad_norm": 2.9908993244171143,
      "learning_rate": 0.00019554455445544556,
      "loss": 0.6655,
      "step": 2625
    },
    {
      "epoch": 17.354812061131764,
      "grad_norm": 10.835640907287598,
      "learning_rate": 0.00019504950495049505,
      "loss": 0.7718,
      "step": 2626
    },
    {
      "epoch": 17.36142090045436,
      "grad_norm": 8.241430282592773,
      "learning_rate": 0.00019455445544554455,
      "loss": 0.5597,
      "step": 2627
    },
    {
      "epoch": 17.36802973977695,
      "grad_norm": 6.8635573387146,
      "learning_rate": 0.00019405940594059408,
      "loss": 0.7177,
      "step": 2628
    },
    {
      "epoch": 17.374638579099546,
      "grad_norm": 3.2396395206451416,
      "learning_rate": 0.00019356435643564358,
      "loss": 0.3057,
      "step": 2629
    },
    {
      "epoch": 17.381247418422138,
      "grad_norm": 7.681257724761963,
      "learning_rate": 0.00019306930693069308,
      "loss": 1.0981,
      "step": 2630
    },
    {
      "epoch": 17.387856257744733,
      "grad_norm": 1.6755696535110474,
      "learning_rate": 0.00019257425742574258,
      "loss": 0.6558,
      "step": 2631
    },
    {
      "epoch": 17.39446509706733,
      "grad_norm": 25.174930572509766,
      "learning_rate": 0.00019207920792079208,
      "loss": 2.9475,
      "step": 2632
    },
    {
      "epoch": 17.40107393638992,
      "grad_norm": 1.4424867630004883,
      "learning_rate": 0.0001915841584158416,
      "loss": 0.6727,
      "step": 2633
    },
    {
      "epoch": 17.407682775712516,
      "grad_norm": 1.3267855644226074,
      "learning_rate": 0.0001910891089108911,
      "loss": 1.2518,
      "step": 2634
    },
    {
      "epoch": 17.41429161503511,
      "grad_norm": 0.7745242118835449,
      "learning_rate": 0.0001905940594059406,
      "loss": 0.3594,
      "step": 2635
    },
    {
      "epoch": 17.420900454357703,
      "grad_norm": 6.825374603271484,
      "learning_rate": 0.0001900990099009901,
      "loss": 2.5627,
      "step": 2636
    },
    {
      "epoch": 17.4275092936803,
      "grad_norm": 5.888375282287598,
      "learning_rate": 0.0001896039603960396,
      "loss": 0.3881,
      "step": 2637
    },
    {
      "epoch": 17.43411813300289,
      "grad_norm": 6.798860549926758,
      "learning_rate": 0.00018910891089108913,
      "loss": 3.1826,
      "step": 2638
    },
    {
      "epoch": 17.440726972325486,
      "grad_norm": 1.4276371002197266,
      "learning_rate": 0.00018861386138613862,
      "loss": 1.4234,
      "step": 2639
    },
    {
      "epoch": 17.44733581164808,
      "grad_norm": 7.3518548011779785,
      "learning_rate": 0.00018811881188118812,
      "loss": 0.6978,
      "step": 2640
    },
    {
      "epoch": 17.453944650970673,
      "grad_norm": 3.6835670471191406,
      "learning_rate": 0.00018762376237623762,
      "loss": 1.1177,
      "step": 2641
    },
    {
      "epoch": 17.460553490293268,
      "grad_norm": 6.632709980010986,
      "learning_rate": 0.00018712871287128712,
      "loss": 0.8807,
      "step": 2642
    },
    {
      "epoch": 17.46716232961586,
      "grad_norm": 3.4208760261535645,
      "learning_rate": 0.00018663366336633665,
      "loss": 0.7102,
      "step": 2643
    },
    {
      "epoch": 17.473771168938455,
      "grad_norm": 4.545500755310059,
      "learning_rate": 0.00018613861386138615,
      "loss": 0.8456,
      "step": 2644
    },
    {
      "epoch": 17.48038000826105,
      "grad_norm": 10.859253883361816,
      "learning_rate": 0.00018564356435643565,
      "loss": 1.7734,
      "step": 2645
    },
    {
      "epoch": 17.486988847583643,
      "grad_norm": 2.4809811115264893,
      "learning_rate": 0.00018514851485148514,
      "loss": 0.6413,
      "step": 2646
    },
    {
      "epoch": 17.493597686906238,
      "grad_norm": 2.395869255065918,
      "learning_rate": 0.00018465346534653464,
      "loss": 1.1459,
      "step": 2647
    },
    {
      "epoch": 17.50020652622883,
      "grad_norm": 9.857475280761719,
      "learning_rate": 0.00018415841584158417,
      "loss": 0.8235,
      "step": 2648
    },
    {
      "epoch": 17.506815365551425,
      "grad_norm": 1.521009087562561,
      "learning_rate": 0.00018366336633663367,
      "loss": 0.6773,
      "step": 2649
    },
    {
      "epoch": 17.51342420487402,
      "grad_norm": 22.09845733642578,
      "learning_rate": 0.00018316831683168317,
      "loss": 1.0499,
      "step": 2650
    },
    {
      "epoch": 17.520033044196612,
      "grad_norm": 10.232152938842773,
      "learning_rate": 0.00018267326732673267,
      "loss": 1.3118,
      "step": 2651
    },
    {
      "epoch": 17.526641883519208,
      "grad_norm": 2.7091267108917236,
      "learning_rate": 0.00018217821782178217,
      "loss": 0.9655,
      "step": 2652
    },
    {
      "epoch": 17.5332507228418,
      "grad_norm": 22.747751235961914,
      "learning_rate": 0.0001816831683168317,
      "loss": 2.2304,
      "step": 2653
    },
    {
      "epoch": 17.539859562164395,
      "grad_norm": 9.211897850036621,
      "learning_rate": 0.0001811881188118812,
      "loss": 1.1851,
      "step": 2654
    },
    {
      "epoch": 17.54646840148699,
      "grad_norm": 15.977787017822266,
      "learning_rate": 0.0001806930693069307,
      "loss": 0.8061,
      "step": 2655
    },
    {
      "epoch": 17.553077240809582,
      "grad_norm": 1.9804742336273193,
      "learning_rate": 0.0001801980198019802,
      "loss": 0.7315,
      "step": 2656
    },
    {
      "epoch": 17.559686080132177,
      "grad_norm": 0.8853079080581665,
      "learning_rate": 0.0001797029702970297,
      "loss": 0.6239,
      "step": 2657
    },
    {
      "epoch": 17.56629491945477,
      "grad_norm": 3.1642346382141113,
      "learning_rate": 0.00017920792079207922,
      "loss": 2.197,
      "step": 2658
    },
    {
      "epoch": 17.572903758777365,
      "grad_norm": 2.5280234813690186,
      "learning_rate": 0.00017871287128712871,
      "loss": 0.7105,
      "step": 2659
    },
    {
      "epoch": 17.57951259809996,
      "grad_norm": 4.6258544921875,
      "learning_rate": 0.0001782178217821782,
      "loss": 2.8704,
      "step": 2660
    },
    {
      "epoch": 17.586121437422552,
      "grad_norm": 6.603541851043701,
      "learning_rate": 0.0001777227722772277,
      "loss": 2.0208,
      "step": 2661
    },
    {
      "epoch": 17.592730276745147,
      "grad_norm": 6.537559986114502,
      "learning_rate": 0.0001772277227722772,
      "loss": 0.7451,
      "step": 2662
    },
    {
      "epoch": 17.59933911606774,
      "grad_norm": 2.078390121459961,
      "learning_rate": 0.00017673267326732674,
      "loss": 1.137,
      "step": 2663
    },
    {
      "epoch": 17.605947955390334,
      "grad_norm": 28.623180389404297,
      "learning_rate": 0.00017623762376237624,
      "loss": 2.4719,
      "step": 2664
    },
    {
      "epoch": 17.61255679471293,
      "grad_norm": 16.620861053466797,
      "learning_rate": 0.00017574257425742574,
      "loss": 2.9267,
      "step": 2665
    },
    {
      "epoch": 17.61916563403552,
      "grad_norm": 2.8851490020751953,
      "learning_rate": 0.00017524752475247524,
      "loss": 1.4478,
      "step": 2666
    },
    {
      "epoch": 17.625774473358117,
      "grad_norm": 5.066531658172607,
      "learning_rate": 0.00017475247524752473,
      "loss": 2.4995,
      "step": 2667
    },
    {
      "epoch": 17.63238331268071,
      "grad_norm": 5.069362163543701,
      "learning_rate": 0.00017425742574257426,
      "loss": 0.29,
      "step": 2668
    },
    {
      "epoch": 17.638992152003304,
      "grad_norm": 11.917778968811035,
      "learning_rate": 0.00017376237623762376,
      "loss": 1.4366,
      "step": 2669
    },
    {
      "epoch": 17.6456009913259,
      "grad_norm": 9.345516204833984,
      "learning_rate": 0.00017326732673267326,
      "loss": 0.4398,
      "step": 2670
    },
    {
      "epoch": 17.65220983064849,
      "grad_norm": 5.723199367523193,
      "learning_rate": 0.00017277227722772276,
      "loss": 0.8631,
      "step": 2671
    },
    {
      "epoch": 17.658818669971087,
      "grad_norm": 7.414927959442139,
      "learning_rate": 0.00017227722772277226,
      "loss": 2.0069,
      "step": 2672
    },
    {
      "epoch": 17.66542750929368,
      "grad_norm": 6.966030597686768,
      "learning_rate": 0.0001717821782178218,
      "loss": 0.542,
      "step": 2673
    },
    {
      "epoch": 17.672036348616274,
      "grad_norm": 10.873476028442383,
      "learning_rate": 0.0001712871287128713,
      "loss": 0.7638,
      "step": 2674
    },
    {
      "epoch": 17.67864518793887,
      "grad_norm": 0.8713357448577881,
      "learning_rate": 0.0001707920792079208,
      "loss": 0.3877,
      "step": 2675
    },
    {
      "epoch": 17.68525402726146,
      "grad_norm": 11.6705961227417,
      "learning_rate": 0.0001702970297029703,
      "loss": 0.9345,
      "step": 2676
    },
    {
      "epoch": 17.691862866584056,
      "grad_norm": 1.4565505981445312,
      "learning_rate": 0.0001698019801980198,
      "loss": 0.5727,
      "step": 2677
    },
    {
      "epoch": 17.698471705906652,
      "grad_norm": 14.908378601074219,
      "learning_rate": 0.00016930693069306933,
      "loss": 0.6088,
      "step": 2678
    },
    {
      "epoch": 17.705080545229244,
      "grad_norm": 6.331033706665039,
      "learning_rate": 0.00016881188118811883,
      "loss": 0.4183,
      "step": 2679
    },
    {
      "epoch": 17.71168938455184,
      "grad_norm": 13.553028106689453,
      "learning_rate": 0.00016831683168316833,
      "loss": 0.8583,
      "step": 2680
    },
    {
      "epoch": 17.71829822387443,
      "grad_norm": 1.418928623199463,
      "learning_rate": 0.00016782178217821783,
      "loss": 1.1095,
      "step": 2681
    },
    {
      "epoch": 17.724907063197026,
      "grad_norm": 4.8653740882873535,
      "learning_rate": 0.00016732673267326733,
      "loss": 3.1258,
      "step": 2682
    },
    {
      "epoch": 17.73151590251962,
      "grad_norm": 4.985296726226807,
      "learning_rate": 0.00016683168316831686,
      "loss": 1.1185,
      "step": 2683
    },
    {
      "epoch": 17.738124741842213,
      "grad_norm": 5.2670183181762695,
      "learning_rate": 0.00016633663366336635,
      "loss": 0.5554,
      "step": 2684
    },
    {
      "epoch": 17.74473358116481,
      "grad_norm": 18.429824829101562,
      "learning_rate": 0.00016584158415841585,
      "loss": 1.0338,
      "step": 2685
    },
    {
      "epoch": 17.7513424204874,
      "grad_norm": 5.516468524932861,
      "learning_rate": 0.00016534653465346535,
      "loss": 0.6479,
      "step": 2686
    },
    {
      "epoch": 17.757951259809996,
      "grad_norm": 0.8249009847640991,
      "learning_rate": 0.00016485148514851485,
      "loss": 0.55,
      "step": 2687
    },
    {
      "epoch": 17.76456009913259,
      "grad_norm": 22.949338912963867,
      "learning_rate": 0.00016435643564356438,
      "loss": 1.0706,
      "step": 2688
    },
    {
      "epoch": 17.771168938455183,
      "grad_norm": 11.047609329223633,
      "learning_rate": 0.00016386138613861388,
      "loss": 0.713,
      "step": 2689
    },
    {
      "epoch": 17.77777777777778,
      "grad_norm": 12.542872428894043,
      "learning_rate": 0.00016336633663366338,
      "loss": 1.0822,
      "step": 2690
    },
    {
      "epoch": 17.78438661710037,
      "grad_norm": 11.067777633666992,
      "learning_rate": 0.00016287128712871287,
      "loss": 0.9129,
      "step": 2691
    },
    {
      "epoch": 17.790995456422966,
      "grad_norm": 6.248368263244629,
      "learning_rate": 0.00016237623762376237,
      "loss": 0.9663,
      "step": 2692
    },
    {
      "epoch": 17.79760429574556,
      "grad_norm": 5.115170955657959,
      "learning_rate": 0.0001618811881188119,
      "loss": 2.3163,
      "step": 2693
    },
    {
      "epoch": 17.804213135068153,
      "grad_norm": 0.7416458129882812,
      "learning_rate": 0.0001613861386138614,
      "loss": 0.7427,
      "step": 2694
    },
    {
      "epoch": 17.81082197439075,
      "grad_norm": 6.321577548980713,
      "learning_rate": 0.0001608910891089109,
      "loss": 1.0915,
      "step": 2695
    },
    {
      "epoch": 17.81743081371334,
      "grad_norm": 7.284835338592529,
      "learning_rate": 0.0001603960396039604,
      "loss": 0.9153,
      "step": 2696
    },
    {
      "epoch": 17.824039653035936,
      "grad_norm": 3.3427107334136963,
      "learning_rate": 0.0001599009900990099,
      "loss": 0.4425,
      "step": 2697
    },
    {
      "epoch": 17.83064849235853,
      "grad_norm": 5.43231725692749,
      "learning_rate": 0.00015940594059405942,
      "loss": 1.5713,
      "step": 2698
    },
    {
      "epoch": 17.837257331681123,
      "grad_norm": 7.246674537658691,
      "learning_rate": 0.00015891089108910892,
      "loss": 1.3936,
      "step": 2699
    },
    {
      "epoch": 17.843866171003718,
      "grad_norm": 17.729969024658203,
      "learning_rate": 0.00015841584158415842,
      "loss": 0.7239,
      "step": 2700
    },
    {
      "epoch": 17.85047501032631,
      "grad_norm": 9.722740173339844,
      "learning_rate": 0.00015792079207920792,
      "loss": 1.1094,
      "step": 2701
    },
    {
      "epoch": 17.857083849648905,
      "grad_norm": 2.1497244834899902,
      "learning_rate": 0.00015742574257425742,
      "loss": 1.817,
      "step": 2702
    },
    {
      "epoch": 17.8636926889715,
      "grad_norm": 2.2389187812805176,
      "learning_rate": 0.00015693069306930695,
      "loss": 0.739,
      "step": 2703
    },
    {
      "epoch": 17.870301528294092,
      "grad_norm": 18.784038543701172,
      "learning_rate": 0.00015643564356435644,
      "loss": 1.3013,
      "step": 2704
    },
    {
      "epoch": 17.876910367616688,
      "grad_norm": 3.075495719909668,
      "learning_rate": 0.00015594059405940594,
      "loss": 0.8262,
      "step": 2705
    },
    {
      "epoch": 17.88351920693928,
      "grad_norm": 4.680947303771973,
      "learning_rate": 0.00015544554455445544,
      "loss": 1.0359,
      "step": 2706
    },
    {
      "epoch": 17.890128046261875,
      "grad_norm": 1.5200872421264648,
      "learning_rate": 0.00015495049504950494,
      "loss": 2.5446,
      "step": 2707
    },
    {
      "epoch": 17.89673688558447,
      "grad_norm": 10.122881889343262,
      "learning_rate": 0.00015445544554455447,
      "loss": 0.6791,
      "step": 2708
    },
    {
      "epoch": 17.903345724907062,
      "grad_norm": 2.74184250831604,
      "learning_rate": 0.00015396039603960397,
      "loss": 2.1126,
      "step": 2709
    },
    {
      "epoch": 17.909954564229658,
      "grad_norm": 2.7796013355255127,
      "learning_rate": 0.00015346534653465347,
      "loss": 1.0595,
      "step": 2710
    },
    {
      "epoch": 17.916563403552253,
      "grad_norm": 7.403759002685547,
      "learning_rate": 0.00015297029702970297,
      "loss": 0.5171,
      "step": 2711
    },
    {
      "epoch": 17.923172242874845,
      "grad_norm": 2.697216749191284,
      "learning_rate": 0.00015247524752475246,
      "loss": 1.1795,
      "step": 2712
    },
    {
      "epoch": 17.92978108219744,
      "grad_norm": 5.9904584884643555,
      "learning_rate": 0.000151980198019802,
      "loss": 0.4915,
      "step": 2713
    },
    {
      "epoch": 17.936389921520032,
      "grad_norm": 10.830872535705566,
      "learning_rate": 0.0001514851485148515,
      "loss": 2.4195,
      "step": 2714
    },
    {
      "epoch": 17.942998760842627,
      "grad_norm": 10.884592056274414,
      "learning_rate": 0.000150990099009901,
      "loss": 1.3595,
      "step": 2715
    },
    {
      "epoch": 17.949607600165223,
      "grad_norm": 4.400218486785889,
      "learning_rate": 0.0001504950495049505,
      "loss": 0.4961,
      "step": 2716
    },
    {
      "epoch": 17.956216439487815,
      "grad_norm": 2.1352121829986572,
      "learning_rate": 0.00015,
      "loss": 1.4065,
      "step": 2717
    },
    {
      "epoch": 17.96282527881041,
      "grad_norm": 5.341940402984619,
      "learning_rate": 0.0001495049504950495,
      "loss": 2.0096,
      "step": 2718
    },
    {
      "epoch": 17.969434118133,
      "grad_norm": 7.131203651428223,
      "learning_rate": 0.000149009900990099,
      "loss": 1.1449,
      "step": 2719
    },
    {
      "epoch": 17.976042957455597,
      "grad_norm": 5.4186882972717285,
      "learning_rate": 0.0001485148514851485,
      "loss": 1.745,
      "step": 2720
    },
    {
      "epoch": 17.982651796778192,
      "grad_norm": 6.174856662750244,
      "learning_rate": 0.000148019801980198,
      "loss": 0.8035,
      "step": 2721
    },
    {
      "epoch": 17.989260636100784,
      "grad_norm": 4.131686210632324,
      "learning_rate": 0.0001475247524752475,
      "loss": 0.9148,
      "step": 2722
    },
    {
      "epoch": 17.99586947542338,
      "grad_norm": 7.753267765045166,
      "learning_rate": 0.00014702970297029704,
      "loss": 1.5765,
      "step": 2723
    },
    {
      "epoch": 17.99586947542338,
      "eval_validation_error_bar": 0.035749979801239125,
      "eval_validation_loss": 3.7772903442382812,
      "eval_validation_pearsonr": 0.7171453414004552,
      "eval_validation_rmse": 1.9435251951217651,
      "eval_validation_runtime": 28.5922,
      "eval_validation_samples_per_second": 7.1,
      "eval_validation_spearman": 0.7432807344744407,
      "eval_validation_steps_per_second": 7.1,
      "step": 2723
    },
    {
      "epoch": 17.99586947542338,
      "eval_test_error_bar": 0.035145789162953425,
      "eval_test_loss": 5.586056709289551,
      "eval_test_pearsonr": 0.6360975878632421,
      "eval_test_rmse": 2.3634839057922363,
      "eval_test_runtime": 45.8923,
      "eval_test_samples_per_second": 7.104,
      "eval_test_spearman": 0.6531086661300872,
      "eval_test_steps_per_second": 7.104,
      "step": 2723
    },
    {
      "epoch": 18.00247831474597,
      "grad_norm": 14.912629127502441,
      "learning_rate": 0.00014653465346534653,
      "loss": 1.0761,
      "step": 2724
    },
    {
      "epoch": 18.009087154068567,
      "grad_norm": 1.7824256420135498,
      "learning_rate": 0.00014603960396039603,
      "loss": 1.2323,
      "step": 2725
    },
    {
      "epoch": 18.015695993391162,
      "grad_norm": 4.40239143371582,
      "learning_rate": 0.00014554455445544553,
      "loss": 0.5375,
      "step": 2726
    },
    {
      "epoch": 18.022304832713754,
      "grad_norm": 4.6248579025268555,
      "learning_rate": 0.00014504950495049503,
      "loss": 0.885,
      "step": 2727
    },
    {
      "epoch": 18.02891367203635,
      "grad_norm": 8.036933898925781,
      "learning_rate": 0.00014455445544554456,
      "loss": 0.5897,
      "step": 2728
    },
    {
      "epoch": 18.03552251135894,
      "grad_norm": 12.529101371765137,
      "learning_rate": 0.00014405940594059406,
      "loss": 1.5934,
      "step": 2729
    },
    {
      "epoch": 18.042131350681537,
      "grad_norm": 7.085362434387207,
      "learning_rate": 0.00014356435643564356,
      "loss": 1.2479,
      "step": 2730
    },
    {
      "epoch": 18.048740190004132,
      "grad_norm": 1.5641266107559204,
      "learning_rate": 0.00014306930693069306,
      "loss": 1.3985,
      "step": 2731
    },
    {
      "epoch": 18.055349029326724,
      "grad_norm": 13.137307167053223,
      "learning_rate": 0.00014257425742574255,
      "loss": 0.7865,
      "step": 2732
    },
    {
      "epoch": 18.06195786864932,
      "grad_norm": 8.619301795959473,
      "learning_rate": 0.00014207920792079208,
      "loss": 1.1288,
      "step": 2733
    },
    {
      "epoch": 18.06856670797191,
      "grad_norm": 2.8499979972839355,
      "learning_rate": 0.00014158415841584158,
      "loss": 0.8369,
      "step": 2734
    },
    {
      "epoch": 18.075175547294506,
      "grad_norm": 2.251753091812134,
      "learning_rate": 0.00014108910891089108,
      "loss": 0.5508,
      "step": 2735
    },
    {
      "epoch": 18.0817843866171,
      "grad_norm": 7.766125679016113,
      "learning_rate": 0.0001405940594059406,
      "loss": 0.6988,
      "step": 2736
    },
    {
      "epoch": 18.088393225939694,
      "grad_norm": 15.901214599609375,
      "learning_rate": 0.0001400990099009901,
      "loss": 1.8997,
      "step": 2737
    },
    {
      "epoch": 18.09500206526229,
      "grad_norm": 17.301408767700195,
      "learning_rate": 0.00013960396039603963,
      "loss": 0.9953,
      "step": 2738
    },
    {
      "epoch": 18.10161090458488,
      "grad_norm": 7.766331195831299,
      "learning_rate": 0.00013910891089108913,
      "loss": 1.8459,
      "step": 2739
    },
    {
      "epoch": 18.108219743907476,
      "grad_norm": 13.93484115600586,
      "learning_rate": 0.00013861386138613863,
      "loss": 0.858,
      "step": 2740
    },
    {
      "epoch": 18.11482858323007,
      "grad_norm": 7.366004943847656,
      "learning_rate": 0.00013811881188118813,
      "loss": 0.4942,
      "step": 2741
    },
    {
      "epoch": 18.121437422552663,
      "grad_norm": 10.629107475280762,
      "learning_rate": 0.00013762376237623763,
      "loss": 0.8706,
      "step": 2742
    },
    {
      "epoch": 18.12804626187526,
      "grad_norm": 7.790359020233154,
      "learning_rate": 0.00013712871287128715,
      "loss": 1.275,
      "step": 2743
    },
    {
      "epoch": 18.13465510119785,
      "grad_norm": 11.738812446594238,
      "learning_rate": 0.00013663366336633665,
      "loss": 0.3627,
      "step": 2744
    },
    {
      "epoch": 18.141263940520446,
      "grad_norm": 9.35411548614502,
      "learning_rate": 0.00013613861386138615,
      "loss": 2.5097,
      "step": 2745
    },
    {
      "epoch": 18.14787277984304,
      "grad_norm": 5.748443603515625,
      "learning_rate": 0.00013564356435643565,
      "loss": 2.1556,
      "step": 2746
    },
    {
      "epoch": 18.154481619165633,
      "grad_norm": 29.79709243774414,
      "learning_rate": 0.00013514851485148515,
      "loss": 1.7735,
      "step": 2747
    },
    {
      "epoch": 18.16109045848823,
      "grad_norm": 21.004152297973633,
      "learning_rate": 0.00013465346534653468,
      "loss": 2.2588,
      "step": 2748
    },
    {
      "epoch": 18.16769929781082,
      "grad_norm": 19.45534324645996,
      "learning_rate": 0.00013415841584158417,
      "loss": 1.4359,
      "step": 2749
    },
    {
      "epoch": 18.174308137133416,
      "grad_norm": 17.48627471923828,
      "learning_rate": 0.00013366336633663367,
      "loss": 0.7606,
      "step": 2750
    },
    {
      "epoch": 18.18091697645601,
      "grad_norm": 13.417765617370605,
      "learning_rate": 0.00013316831683168317,
      "loss": 0.6975,
      "step": 2751
    },
    {
      "epoch": 18.187525815778603,
      "grad_norm": 3.08297061920166,
      "learning_rate": 0.00013267326732673267,
      "loss": 2.2944,
      "step": 2752
    },
    {
      "epoch": 18.194134655101198,
      "grad_norm": 14.428339958190918,
      "learning_rate": 0.0001321782178217822,
      "loss": 0.9188,
      "step": 2753
    },
    {
      "epoch": 18.200743494423794,
      "grad_norm": 25.335052490234375,
      "learning_rate": 0.0001316831683168317,
      "loss": 1.6848,
      "step": 2754
    },
    {
      "epoch": 18.207352333746385,
      "grad_norm": 13.129591941833496,
      "learning_rate": 0.0001311881188118812,
      "loss": 0.7307,
      "step": 2755
    },
    {
      "epoch": 18.21396117306898,
      "grad_norm": 9.591277122497559,
      "learning_rate": 0.0001306930693069307,
      "loss": 0.6137,
      "step": 2756
    },
    {
      "epoch": 18.220570012391573,
      "grad_norm": 10.090550422668457,
      "learning_rate": 0.0001301980198019802,
      "loss": 1.6433,
      "step": 2757
    },
    {
      "epoch": 18.227178851714168,
      "grad_norm": 13.672924995422363,
      "learning_rate": 0.00012970297029702972,
      "loss": 2.0364,
      "step": 2758
    },
    {
      "epoch": 18.233787691036763,
      "grad_norm": 11.80873966217041,
      "learning_rate": 0.00012920792079207922,
      "loss": 0.7162,
      "step": 2759
    },
    {
      "epoch": 18.240396530359355,
      "grad_norm": 1.269966721534729,
      "learning_rate": 0.00012871287128712872,
      "loss": 1.4347,
      "step": 2760
    },
    {
      "epoch": 18.24700536968195,
      "grad_norm": 1.0698567628860474,
      "learning_rate": 0.00012821782178217822,
      "loss": 0.3852,
      "step": 2761
    },
    {
      "epoch": 18.253614209004542,
      "grad_norm": 20.928239822387695,
      "learning_rate": 0.00012772277227722772,
      "loss": 0.9119,
      "step": 2762
    },
    {
      "epoch": 18.260223048327138,
      "grad_norm": 25.906877517700195,
      "learning_rate": 0.00012722772277227724,
      "loss": 2.7397,
      "step": 2763
    },
    {
      "epoch": 18.266831887649733,
      "grad_norm": 13.085970878601074,
      "learning_rate": 0.00012673267326732674,
      "loss": 0.725,
      "step": 2764
    },
    {
      "epoch": 18.273440726972325,
      "grad_norm": 6.5415849685668945,
      "learning_rate": 0.00012623762376237624,
      "loss": 0.9431,
      "step": 2765
    },
    {
      "epoch": 18.28004956629492,
      "grad_norm": 7.70836067199707,
      "learning_rate": 0.00012574257425742574,
      "loss": 0.8491,
      "step": 2766
    },
    {
      "epoch": 18.286658405617512,
      "grad_norm": 1.4289416074752808,
      "learning_rate": 0.00012524752475247524,
      "loss": 0.8315,
      "step": 2767
    },
    {
      "epoch": 18.293267244940107,
      "grad_norm": 6.874687194824219,
      "learning_rate": 0.00012475247524752477,
      "loss": 0.6286,
      "step": 2768
    },
    {
      "epoch": 18.299876084262703,
      "grad_norm": 2.343676805496216,
      "learning_rate": 0.00012425742574257426,
      "loss": 1.4351,
      "step": 2769
    },
    {
      "epoch": 18.306484923585295,
      "grad_norm": 14.225994110107422,
      "learning_rate": 0.00012376237623762376,
      "loss": 2.2808,
      "step": 2770
    },
    {
      "epoch": 18.31309376290789,
      "grad_norm": 8.193578720092773,
      "learning_rate": 0.00012326732673267326,
      "loss": 0.3692,
      "step": 2771
    },
    {
      "epoch": 18.319702602230482,
      "grad_norm": 16.05797004699707,
      "learning_rate": 0.00012277227722772276,
      "loss": 1.3492,
      "step": 2772
    },
    {
      "epoch": 18.326311441553077,
      "grad_norm": 7.729948997497559,
      "learning_rate": 0.0001222772277227723,
      "loss": 0.8222,
      "step": 2773
    },
    {
      "epoch": 18.332920280875673,
      "grad_norm": 13.974233627319336,
      "learning_rate": 0.00012178217821782179,
      "loss": 0.8148,
      "step": 2774
    },
    {
      "epoch": 18.339529120198264,
      "grad_norm": 3.47937273979187,
      "learning_rate": 0.00012128712871287129,
      "loss": 1.567,
      "step": 2775
    },
    {
      "epoch": 18.34613795952086,
      "grad_norm": 5.728778839111328,
      "learning_rate": 0.00012079207920792079,
      "loss": 0.6496,
      "step": 2776
    },
    {
      "epoch": 18.35274679884345,
      "grad_norm": 5.427596569061279,
      "learning_rate": 0.0001202970297029703,
      "loss": 0.7411,
      "step": 2777
    },
    {
      "epoch": 18.359355638166047,
      "grad_norm": 8.663191795349121,
      "learning_rate": 0.0001198019801980198,
      "loss": 0.3672,
      "step": 2778
    },
    {
      "epoch": 18.365964477488642,
      "grad_norm": 0.9801846742630005,
      "learning_rate": 0.00011930693069306931,
      "loss": 0.6516,
      "step": 2779
    },
    {
      "epoch": 18.372573316811234,
      "grad_norm": 9.224470138549805,
      "learning_rate": 0.00011881188118811881,
      "loss": 0.6339,
      "step": 2780
    },
    {
      "epoch": 18.37918215613383,
      "grad_norm": 3.272150993347168,
      "learning_rate": 0.00011831683168316831,
      "loss": 0.5985,
      "step": 2781
    },
    {
      "epoch": 18.38579099545642,
      "grad_norm": 1.7513900995254517,
      "learning_rate": 0.00011782178217821782,
      "loss": 1.2974,
      "step": 2782
    },
    {
      "epoch": 18.392399834779017,
      "grad_norm": 1.5200109481811523,
      "learning_rate": 0.00011732673267326732,
      "loss": 1.0951,
      "step": 2783
    },
    {
      "epoch": 18.399008674101612,
      "grad_norm": 11.427145004272461,
      "learning_rate": 0.00011683168316831685,
      "loss": 0.6264,
      "step": 2784
    },
    {
      "epoch": 18.405617513424204,
      "grad_norm": 8.668213844299316,
      "learning_rate": 0.00011633663366336635,
      "loss": 1.9186,
      "step": 2785
    },
    {
      "epoch": 18.4122263527468,
      "grad_norm": 5.778853893280029,
      "learning_rate": 0.00011584158415841584,
      "loss": 2.1193,
      "step": 2786
    },
    {
      "epoch": 18.41883519206939,
      "grad_norm": 4.200103759765625,
      "learning_rate": 0.00011534653465346536,
      "loss": 1.8921,
      "step": 2787
    },
    {
      "epoch": 18.425444031391986,
      "grad_norm": 4.410191535949707,
      "learning_rate": 0.00011485148514851486,
      "loss": 0.4845,
      "step": 2788
    },
    {
      "epoch": 18.432052870714582,
      "grad_norm": 4.124378681182861,
      "learning_rate": 0.00011435643564356437,
      "loss": 0.388,
      "step": 2789
    },
    {
      "epoch": 18.438661710037174,
      "grad_norm": 8.05137825012207,
      "learning_rate": 0.00011386138613861387,
      "loss": 0.6083,
      "step": 2790
    },
    {
      "epoch": 18.44527054935977,
      "grad_norm": 5.570196628570557,
      "learning_rate": 0.00011336633663366337,
      "loss": 0.7152,
      "step": 2791
    },
    {
      "epoch": 18.451879388682364,
      "grad_norm": 7.102170944213867,
      "learning_rate": 0.00011287128712871288,
      "loss": 1.8823,
      "step": 2792
    },
    {
      "epoch": 18.458488228004956,
      "grad_norm": 3.967454195022583,
      "learning_rate": 0.00011237623762376238,
      "loss": 1.0382,
      "step": 2793
    },
    {
      "epoch": 18.46509706732755,
      "grad_norm": 12.293856620788574,
      "learning_rate": 0.00011188118811881189,
      "loss": 0.6811,
      "step": 2794
    },
    {
      "epoch": 18.471705906650143,
      "grad_norm": 13.710827827453613,
      "learning_rate": 0.00011138613861386139,
      "loss": 2.137,
      "step": 2795
    },
    {
      "epoch": 18.47831474597274,
      "grad_norm": 6.0496673583984375,
      "learning_rate": 0.00011089108910891089,
      "loss": 0.6339,
      "step": 2796
    },
    {
      "epoch": 18.484923585295334,
      "grad_norm": 15.919970512390137,
      "learning_rate": 0.0001103960396039604,
      "loss": 2.8643,
      "step": 2797
    },
    {
      "epoch": 18.491532424617926,
      "grad_norm": 3.2609434127807617,
      "learning_rate": 0.0001099009900990099,
      "loss": 0.2946,
      "step": 2798
    },
    {
      "epoch": 18.49814126394052,
      "grad_norm": 1.8474897146224976,
      "learning_rate": 0.00010940594059405941,
      "loss": 0.55,
      "step": 2799
    },
    {
      "epoch": 18.504750103263113,
      "grad_norm": 4.585716247558594,
      "learning_rate": 0.00010891089108910891,
      "loss": 2.1976,
      "step": 2800
    },
    {
      "epoch": 18.51135894258571,
      "grad_norm": 5.762361526489258,
      "learning_rate": 0.00010841584158415841,
      "loss": 1.1038,
      "step": 2801
    },
    {
      "epoch": 18.517967781908304,
      "grad_norm": 9.864856719970703,
      "learning_rate": 0.00010792079207920792,
      "loss": 0.4066,
      "step": 2802
    },
    {
      "epoch": 18.524576621230896,
      "grad_norm": 2.418400764465332,
      "learning_rate": 0.00010742574257425742,
      "loss": 1.866,
      "step": 2803
    },
    {
      "epoch": 18.53118546055349,
      "grad_norm": 7.534714221954346,
      "learning_rate": 0.00010693069306930694,
      "loss": 1.2668,
      "step": 2804
    },
    {
      "epoch": 18.537794299876083,
      "grad_norm": 6.018867492675781,
      "learning_rate": 0.00010643564356435644,
      "loss": 0.1981,
      "step": 2805
    },
    {
      "epoch": 18.54440313919868,
      "grad_norm": 1.5304632186889648,
      "learning_rate": 0.00010594059405940593,
      "loss": 0.7936,
      "step": 2806
    },
    {
      "epoch": 18.551011978521274,
      "grad_norm": 9.999067306518555,
      "learning_rate": 0.00010544554455445545,
      "loss": 1.0257,
      "step": 2807
    },
    {
      "epoch": 18.557620817843866,
      "grad_norm": 6.225728988647461,
      "learning_rate": 0.00010495049504950495,
      "loss": 0.5538,
      "step": 2808
    },
    {
      "epoch": 18.56422965716646,
      "grad_norm": 3.5555336475372314,
      "learning_rate": 0.00010445544554455446,
      "loss": 0.8066,
      "step": 2809
    },
    {
      "epoch": 18.570838496489053,
      "grad_norm": 13.228854179382324,
      "learning_rate": 0.00010396039603960396,
      "loss": 0.2637,
      "step": 2810
    },
    {
      "epoch": 18.577447335811648,
      "grad_norm": 10.401023864746094,
      "learning_rate": 0.00010346534653465346,
      "loss": 0.5725,
      "step": 2811
    },
    {
      "epoch": 18.584056175134243,
      "grad_norm": 2.5588767528533936,
      "learning_rate": 0.00010297029702970297,
      "loss": 0.856,
      "step": 2812
    },
    {
      "epoch": 18.590665014456835,
      "grad_norm": 6.617391109466553,
      "learning_rate": 0.00010247524752475247,
      "loss": 1.0026,
      "step": 2813
    },
    {
      "epoch": 18.59727385377943,
      "grad_norm": 33.56556701660156,
      "learning_rate": 0.00010198019801980198,
      "loss": 3.817,
      "step": 2814
    },
    {
      "epoch": 18.603882693102022,
      "grad_norm": 19.59937858581543,
      "learning_rate": 0.0001014851485148515,
      "loss": 1.252,
      "step": 2815
    },
    {
      "epoch": 18.610491532424618,
      "grad_norm": 14.971447944641113,
      "learning_rate": 0.00010099009900990099,
      "loss": 1.5198,
      "step": 2816
    },
    {
      "epoch": 18.617100371747213,
      "grad_norm": 1.1898274421691895,
      "learning_rate": 0.0001004950495049505,
      "loss": 1.4594,
      "step": 2817
    },
    {
      "epoch": 18.623709211069805,
      "grad_norm": 11.983990669250488,
      "learning_rate": 0.0001,
      "loss": 0.5774,
      "step": 2818
    },
    {
      "epoch": 18.6303180503924,
      "grad_norm": 10.795882225036621,
      "learning_rate": 9.950495049504952e-05,
      "loss": 1.0135,
      "step": 2819
    },
    {
      "epoch": 18.636926889714992,
      "grad_norm": 21.518056869506836,
      "learning_rate": 9.900990099009902e-05,
      "loss": 2.8188,
      "step": 2820
    },
    {
      "epoch": 18.643535729037588,
      "grad_norm": 6.279518127441406,
      "learning_rate": 9.851485148514852e-05,
      "loss": 0.6251,
      "step": 2821
    },
    {
      "epoch": 18.650144568360183,
      "grad_norm": 8.524181365966797,
      "learning_rate": 9.801980198019803e-05,
      "loss": 0.6627,
      "step": 2822
    },
    {
      "epoch": 18.656753407682775,
      "grad_norm": 2.502415657043457,
      "learning_rate": 9.752475247524753e-05,
      "loss": 0.6633,
      "step": 2823
    },
    {
      "epoch": 18.66336224700537,
      "grad_norm": 27.96247100830078,
      "learning_rate": 9.702970297029704e-05,
      "loss": 1.3906,
      "step": 2824
    },
    {
      "epoch": 18.669971086327962,
      "grad_norm": 17.296789169311523,
      "learning_rate": 9.653465346534654e-05,
      "loss": 0.7609,
      "step": 2825
    },
    {
      "epoch": 18.676579925650557,
      "grad_norm": 16.986356735229492,
      "learning_rate": 9.603960396039604e-05,
      "loss": 1.5862,
      "step": 2826
    },
    {
      "epoch": 18.683188764973153,
      "grad_norm": 14.044599533081055,
      "learning_rate": 9.554455445544555e-05,
      "loss": 0.8977,
      "step": 2827
    },
    {
      "epoch": 18.689797604295745,
      "grad_norm": 19.521284103393555,
      "learning_rate": 9.504950495049505e-05,
      "loss": 1.6835,
      "step": 2828
    },
    {
      "epoch": 18.69640644361834,
      "grad_norm": 9.514252662658691,
      "learning_rate": 9.455445544554456e-05,
      "loss": 0.821,
      "step": 2829
    },
    {
      "epoch": 18.70301528294093,
      "grad_norm": 18.817935943603516,
      "learning_rate": 9.405940594059406e-05,
      "loss": 1.1904,
      "step": 2830
    },
    {
      "epoch": 18.709624122263527,
      "grad_norm": 25.701509475708008,
      "learning_rate": 9.356435643564356e-05,
      "loss": 1.8823,
      "step": 2831
    },
    {
      "epoch": 18.716232961586122,
      "grad_norm": 10.792491912841797,
      "learning_rate": 9.306930693069307e-05,
      "loss": 0.9276,
      "step": 2832
    },
    {
      "epoch": 18.722841800908714,
      "grad_norm": 4.082231521606445,
      "learning_rate": 9.257425742574257e-05,
      "loss": 1.6291,
      "step": 2833
    },
    {
      "epoch": 18.72945064023131,
      "grad_norm": 14.709829330444336,
      "learning_rate": 9.207920792079209e-05,
      "loss": 0.9541,
      "step": 2834
    },
    {
      "epoch": 18.736059479553905,
      "grad_norm": 3.0978715419769287,
      "learning_rate": 9.158415841584158e-05,
      "loss": 0.8963,
      "step": 2835
    },
    {
      "epoch": 18.742668318876497,
      "grad_norm": 3.6097896099090576,
      "learning_rate": 9.108910891089108e-05,
      "loss": 0.9695,
      "step": 2836
    },
    {
      "epoch": 18.749277158199092,
      "grad_norm": 3.216161012649536,
      "learning_rate": 9.05940594059406e-05,
      "loss": 1.0929,
      "step": 2837
    },
    {
      "epoch": 18.755885997521684,
      "grad_norm": 5.200669288635254,
      "learning_rate": 9.00990099009901e-05,
      "loss": 0.4499,
      "step": 2838
    },
    {
      "epoch": 18.76249483684428,
      "grad_norm": 2.4410221576690674,
      "learning_rate": 8.960396039603961e-05,
      "loss": 1.0201,
      "step": 2839
    },
    {
      "epoch": 18.769103676166875,
      "grad_norm": 8.599186897277832,
      "learning_rate": 8.91089108910891e-05,
      "loss": 2.2833,
      "step": 2840
    },
    {
      "epoch": 18.775712515489467,
      "grad_norm": 20.705896377563477,
      "learning_rate": 8.86138613861386e-05,
      "loss": 1.0966,
      "step": 2841
    },
    {
      "epoch": 18.782321354812062,
      "grad_norm": 6.019442558288574,
      "learning_rate": 8.811881188118812e-05,
      "loss": 1.2043,
      "step": 2842
    },
    {
      "epoch": 18.788930194134654,
      "grad_norm": 25.211320877075195,
      "learning_rate": 8.762376237623762e-05,
      "loss": 1.9612,
      "step": 2843
    },
    {
      "epoch": 18.79553903345725,
      "grad_norm": 16.822063446044922,
      "learning_rate": 8.712871287128713e-05,
      "loss": 2.7085,
      "step": 2844
    },
    {
      "epoch": 18.802147872779845,
      "grad_norm": 2.6123437881469727,
      "learning_rate": 8.663366336633663e-05,
      "loss": 0.5327,
      "step": 2845
    },
    {
      "epoch": 18.808756712102436,
      "grad_norm": 1.9783093929290771,
      "learning_rate": 8.613861386138613e-05,
      "loss": 0.219,
      "step": 2846
    },
    {
      "epoch": 18.81536555142503,
      "grad_norm": 6.489029884338379,
      "learning_rate": 8.564356435643565e-05,
      "loss": 2.6291,
      "step": 2847
    },
    {
      "epoch": 18.821974390747624,
      "grad_norm": 10.166106224060059,
      "learning_rate": 8.514851485148515e-05,
      "loss": 1.1466,
      "step": 2848
    },
    {
      "epoch": 18.82858323007022,
      "grad_norm": 5.719285011291504,
      "learning_rate": 8.465346534653467e-05,
      "loss": 0.8431,
      "step": 2849
    },
    {
      "epoch": 18.835192069392814,
      "grad_norm": 1.2752878665924072,
      "learning_rate": 8.415841584158417e-05,
      "loss": 2.8084,
      "step": 2850
    },
    {
      "epoch": 18.841800908715406,
      "grad_norm": 6.253787994384766,
      "learning_rate": 8.366336633663366e-05,
      "loss": 0.3326,
      "step": 2851
    },
    {
      "epoch": 18.848409748038,
      "grad_norm": 2.5828473567962646,
      "learning_rate": 8.316831683168318e-05,
      "loss": 0.5347,
      "step": 2852
    },
    {
      "epoch": 18.855018587360593,
      "grad_norm": 15.987470626831055,
      "learning_rate": 8.267326732673268e-05,
      "loss": 1.4842,
      "step": 2853
    },
    {
      "epoch": 18.86162742668319,
      "grad_norm": 5.677209854125977,
      "learning_rate": 8.217821782178219e-05,
      "loss": 0.4358,
      "step": 2854
    },
    {
      "epoch": 18.868236266005784,
      "grad_norm": 1.0447845458984375,
      "learning_rate": 8.168316831683169e-05,
      "loss": 0.4239,
      "step": 2855
    },
    {
      "epoch": 18.874845105328376,
      "grad_norm": 13.101861000061035,
      "learning_rate": 8.118811881188119e-05,
      "loss": 0.8501,
      "step": 2856
    },
    {
      "epoch": 18.88145394465097,
      "grad_norm": 5.5906219482421875,
      "learning_rate": 8.06930693069307e-05,
      "loss": 0.8458,
      "step": 2857
    },
    {
      "epoch": 18.888062783973563,
      "grad_norm": 2.569605827331543,
      "learning_rate": 8.01980198019802e-05,
      "loss": 0.7127,
      "step": 2858
    },
    {
      "epoch": 18.89467162329616,
      "grad_norm": 1.094773292541504,
      "learning_rate": 7.970297029702971e-05,
      "loss": 0.6307,
      "step": 2859
    },
    {
      "epoch": 18.901280462618754,
      "grad_norm": 8.544890403747559,
      "learning_rate": 7.920792079207921e-05,
      "loss": 1.3815,
      "step": 2860
    },
    {
      "epoch": 18.907889301941346,
      "grad_norm": 12.446104049682617,
      "learning_rate": 7.871287128712871e-05,
      "loss": 1.1234,
      "step": 2861
    },
    {
      "epoch": 18.91449814126394,
      "grad_norm": 2.016296148300171,
      "learning_rate": 7.821782178217822e-05,
      "loss": 1.109,
      "step": 2862
    },
    {
      "epoch": 18.921106980586533,
      "grad_norm": 2.782029390335083,
      "learning_rate": 7.772277227722772e-05,
      "loss": 0.3793,
      "step": 2863
    },
    {
      "epoch": 18.927715819909128,
      "grad_norm": 6.959339141845703,
      "learning_rate": 7.722772277227723e-05,
      "loss": 2.2477,
      "step": 2864
    },
    {
      "epoch": 18.934324659231724,
      "grad_norm": 1.8574069738388062,
      "learning_rate": 7.673267326732673e-05,
      "loss": 1.229,
      "step": 2865
    },
    {
      "epoch": 18.940933498554315,
      "grad_norm": 11.150751113891602,
      "learning_rate": 7.623762376237623e-05,
      "loss": 0.6668,
      "step": 2866
    },
    {
      "epoch": 18.94754233787691,
      "grad_norm": 12.804174423217773,
      "learning_rate": 7.574257425742574e-05,
      "loss": 1.2646,
      "step": 2867
    },
    {
      "epoch": 18.954151177199506,
      "grad_norm": 2.170085906982422,
      "learning_rate": 7.524752475247524e-05,
      "loss": 0.9063,
      "step": 2868
    },
    {
      "epoch": 18.960760016522098,
      "grad_norm": 1.8557192087173462,
      "learning_rate": 7.475247524752476e-05,
      "loss": 0.4734,
      "step": 2869
    },
    {
      "epoch": 18.967368855844693,
      "grad_norm": 7.777982234954834,
      "learning_rate": 7.425742574257426e-05,
      "loss": 1.1777,
      "step": 2870
    },
    {
      "epoch": 18.973977695167285,
      "grad_norm": 1.6006906032562256,
      "learning_rate": 7.376237623762375e-05,
      "loss": 0.5442,
      "step": 2871
    },
    {
      "epoch": 18.98058653448988,
      "grad_norm": 2.460594892501831,
      "learning_rate": 7.326732673267327e-05,
      "loss": 0.6874,
      "step": 2872
    },
    {
      "epoch": 18.987195373812476,
      "grad_norm": 3.684166669845581,
      "learning_rate": 7.277227722772277e-05,
      "loss": 0.7383,
      "step": 2873
    },
    {
      "epoch": 18.993804213135068,
      "grad_norm": 8.92137622833252,
      "learning_rate": 7.227722772277228e-05,
      "loss": 2.4828,
      "step": 2874
    },
    {
      "epoch": 18.993804213135068,
      "eval_validation_error_bar": 0.03555255354280848,
      "eval_validation_loss": 3.8031837940216064,
      "eval_validation_pearsonr": 0.7222559946211614,
      "eval_validation_rmse": 1.9501752853393555,
      "eval_validation_runtime": 28.5056,
      "eval_validation_samples_per_second": 7.121,
      "eval_validation_spearman": 0.7450996987550391,
      "eval_validation_steps_per_second": 7.121,
      "step": 2874
    },
    {
      "epoch": 18.993804213135068,
      "eval_test_error_bar": 0.03267759791998623,
      "eval_test_loss": 5.178013801574707,
      "eval_test_pearsonr": 0.6730509067410683,
      "eval_test_rmse": 2.275524854660034,
      "eval_test_runtime": 45.7229,
      "eval_test_samples_per_second": 7.13,
      "eval_test_spearman": 0.686816196608386,
      "eval_test_steps_per_second": 7.13,
      "step": 2874
    }
  ],
  "logging_steps": 1,
  "max_steps": 3020,
  "num_input_tokens_seen": 0,
  "num_train_epochs": 20,
  "save_steps": 500,
  "stateful_callbacks": {
    "TrainerControl": {
      "args": {
        "should_epoch_stop": false,
        "should_evaluate": false,
        "should_log": false,
        "should_save": true,
        "should_training_stop": false
      },
      "attributes": {}
    }
  },
  "total_flos": 0.0,
  "train_batch_size": 1,
  "trial_name": null,
  "trial_params": null
}
